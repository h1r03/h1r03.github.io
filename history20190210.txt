   59  python qutil.py -v cumsum
   60  python qutil.py --input='/Users/212339410/Box Sync/Analytics/Data/BSF/S1.csv'
   61  python qutil.py --input='/Users/212339410/Box Sync/Analytics/Data/BSF/S1.csv' cumsum
   62  head /Users/212339410/Box Sync/Analytics/Data/BSF/S1.csv
   63  head /Users/212339410/BoxÂ¥ Sync/Analytics/Data/BSF/S1.csv
   64  head()
   65  head -n 1 ../../Data/BSF/S1.csv
   66  python qutil.py --input='/Users/212339410/Box Sync/Analytics/Data/BSF/S1.csv' cumsum Tc_in
   67  python qutil.py --input='/Users/212339410/Box Sync/Analytics/Data/BSF/S1.csv' cumsum Tc_in --output 'output1012'
   68  less output1012
   69  cd Python/PB
   70  export http_proxy=https://proxy-src.research.ge.com:8080
   71  export https_proxy=$http_proxy
   72  less ~/.zshrc.old1012
   73  cp ~/.zshrc ~/.zshrcJP
   74  cp ~/.zshrc.old1012 ~/.zshrc
   75  rm ~/.zcompdump
   76  exec $SHELL -l
   77  less ~/.zshrcJP
   78  GEproxy
   79  cd PB
   80  pip install docopt
   81  python testxy.py
   82  python testxy.py -h
   83  python testxy.py -v
   84  python testxy.py test_code
   85  python testxy.py test_code x=1 y=3
   86  python testxy.py test_code 1 3
   87  git clone git@github.build.ge.com:IndustrialDataScience/tstk.git tstk.ghpages
   88  cd tstk.ghpages
   89  rm -rf tstk.ghpages
   90  cd TSkit
   91  git checkout --orphan
   92  git checkout --orphan gh-pages
   93  git rm -rf .
   94  git commit -m 'enable pages with gh-pages barnch and .nojekyll'
   95  echo "First commit" > index.html
   96  git commit -m 'test html'
   97  cd tstk.gh-pages/
   98  cd TSkit.io
   99  cd TSkit/
  100  sphinx-apidoc -o . ../tskit
  101  sphinx-build -a . ../../TSkit.io
  102  sphinx-build  . ../../TSkit.io
  103  sphinx-apidoc -o . ../tstk
  104  sphinx-apidoc -o . ../tstk/
  105  sphinx-build . ../../tstk.gh-pages
  106  git ci
  107  git ci -m "Generated gh-pages for `git log master -1 --pretty=short --abbrev-commit`" 
  108  git commit -m "Generated gh-pages for `git log master -1 --pretty=short --abbrev-commit`" 
  109  docker info
  110  docker start 3b93b4414591 
  111  docker start continuumio/anaconda1010
  112  docker run run
  113  docker run -i -t continuumio/anaconda1010:latest
  114  docker-machine ip default
  115  atom-rst-preview
  116  brew install pandoc
  117  apm install atom-rst-preview
  118  sudo docker run -d -p 8787:8787 rocker/rstudio
  119  docker ps -a
  120  docker stop continuumio/anaconda1010:latest
  121  docker stop agitated_lamarr
  122  docker rm <none>
  123  docker rm   6971cd34126d
  124  docker rm 
  125  docker rm --help
  126  docker rmi <none>
  127  docker rmi \<none\>
  128  docker rmi \<none\>/\<none\>
  129  docker rmi 6971cd34126d
  130  docker stop de07f67b0fb1
  131  docker stop 226a017b1c6b
  132  docker rmi 3b93b4414591
  133  docker rm continuumio/anaconda1010
  134  docker rmi -f 3b93b4414591
  135  docker rmi -f 0f2173f8e8d0
  136  docker stop rocker
  137  docker stop rocker/rstudio
  138  docker stop romantic_wing
  139  docker rmi 7e0be98eae06
  140  docker rmi 7e0be98eae06 -f
  141  tree ./*tstk
  142  tree ./tstk*
  143  tree ./tstk* n=2
  144  tree ./tstk* -L 2
  145  tree ./tstk* -L 0
  146  tree ./tstk* -L 1
  147  git checkout git@github.build.ge.com:IndustrialDataScience/tstk.git tstk.gh-pages2
  148  git clone git@github.build.ge.com:IndustrialDataScience/tstk.git tstk.gh-pages2
  149  cd tstk.gh-pages2
  150  cd tstk.gh-pages2/
  151  sphinx-build ./doc ../tstk.gh-pages2
  152  cd ../tstk.gh-pages2
  153  rm -rf tstk.gh-pages2
  154  sphinx-build -h
  155  git symbolic-ref -h
  156  git clean -h
  157  "Generated gh-pages for `git log master -1 --pretty=short \\n--abbrev-commit`" 
  158  echo "Generated gh-pages for `git log master -1 --pretty=short \\n--abbrev-commit`" 
  159  git log gh-pages
  160  ils
  161  export PYTHONPATH=$PYTHONPATH:/Users/212339410/Python/pymodule
  162  export PYTHONPATH=$PYTHONPATH:/Users/212339410
  163  git branch -h
  164  git branch models-pca
  165  git branch checkout models-pca
  166  git branch -D checkout
  167  git checkout models-pca
  168  py3
  169  brew install pkg-config
  170  brew install zeromq
  171  apm install hydrogen
  172  conda2 
  173  pip install h2o
  174  spyder --reset
  175  ipython --pylab
  176  spyder -h
  177  spyder --new
  178  brew test.py
  179  mkdir -p /Users/212339410/.local/lib/python3.5/site-packages
  180  brew info pyqt
  181  brew test pyqt
  182  spyder
  183  spyder 
  184  python -c 'import sip'
  185  python -c 'import PyQt5'
  186  python -c 'import sys;'
  187  . ./activate
  188  unset PYTHONPATH
  189  spyder --new-instance
  190  py35
  191  source activate rot
  192  ls /Users/212339410/Python/py
  193  ls /Users/212339410/Python/pymodule
  194  cd Python/pymodule/tstk
  195  touch pull-protext-test.txt
  196  git commit -m 'push protect test'
  197  cd tstk/tstk
  198  cd models/
  199  python hotellingT2.py
  200  git commit -m 'add working example command in docstring'
  201  git push origin models-pca
  202  python tstk/models/hotellingT2.py
  203  python tstk/tstk/models/hotellingT2.py
  204  rm iris_pcadata.pkl
  205  python -c 'import sys; print (sys.path)'
  206  cat ~/.local/lib/python3.5_back/site-packages/homebrew.pth 
  207  ln -s ~/Python/tstk/tstk ~/Python/pymodule
  208  unset $PYTHONPATH
  209  export PYTHONPATH=
  210  ln -s ~/Python/pymodule /Users/212339410/anaconda2/envs/py35/lib/python3.5/site-packages
  211  touch __init__.py
  212  cd iris/]
  213  cd iris/
  214  vim generte_iris.py
  215  cd '/Users/212339410/anaconda2/envs/py35/lib/python3.5/site-packages',
  216  ls -al tstk
  217  cd Users/212339410/anaconda2/envs/py35/lib/python3.5/site-packages'\nls \nq\n'
  218  cd Users/212339410/anaconda2/envs/py35/lib/python3.5/site-packages
  219  rm pymodule
  220  ls | grep pymodule
  221  export PYTHONPATH=~/Python/pymodule
  222  source ~/.bash_profile
  223  mkdir mymodule
  224  cd mymodule
  225  rm -r mymodule
  226  ls | mymodule
  227  ls | grep mymodule
  228  vim mymodules.pth
  229  import sys
  230  cd tstk/examples
  231  cd '/Users/212339410/anaconda2/envs/py35/lib/python3.5/site-packages'
  232  cat mymodules.pth
  233  conda1
  234  sphinx-apdoc -o ./doc ./tstk
  235  mkdir tstk.gh-pages.private
  236  python -c 'import sys; sys.path'
  237  python -c 'import sys; print sys.path'
  238  python -c 'import sys; print(sys.path)'
  239  python -c 'import sys; print(sys.path); import tstk'
  240  ssh -h 3.36.11.67 -u ctuser -p
  241  ssh 3.36.11.67 -u ctuser -p
  242  ssh 3.36.11.67 -l ctuser -p
  243  ssh 3.36.11.67 -l ctuser
  244  mysql 3.36.11.67 -u ctuser -p
  245  brew install mysql
  246  mysql -h 3.36.11.67 -u ctuser -p
  247  ssh 3.36.11.67 -l root
  248  vim GeSyslog.py
  249  sphinx-build ./doc ../tstk.gh-pages.private
  250  ls /usr/lib/j*
  251  which java_home
  252  cd /
  253  cd Users/212339410
  254  cd /Applications
  255  ;s
  256  cd /Library/Frameworks
  257  cd /usr/local
  258  mkdir bin
  259  vim .bash_profile
  260  mv /Applications/TOS_BD-macosx-cocoa -t ~/.local/bin/TOS_BD-20160704_1411-V6.2.1/
  261  mv /Applications/TOS_BD-macosx-cocoa ~/.local/bin/TOS_BD-20160704_1411-V6.2.1/
  262  mv /Applications/TOS_BD-macosx-cocoa.app ~/.local/bin/TOS_BD-20160704_1411-V6.2.1/
  263  git clone https://github.com/borismus/webvr-boilerplate.git
  264  cd Box\ Sync/Document_D/Development/WebApp/WebVR1
  265  pip install SimpleHTTPServer
  266  cd webvr-boilerplate
  267  python -m SimpleHTTPServer 8000
  268  python -m SimpleHTTPServer 8080
  269  git clone https://github.com/anjneymidha/webVR-Mars.git
  270  cd webVR-Mars
  271  python -m http.server 8080
  272  python -m http.server 8081
  273  python -m http.server 80800
  274  python -m http.server 808
  275  python -m http.server 8083
  276  cd Python/BoschKaggle/
  277  brew install wget
  278  wget http://www.cs.washington.edu/research/xmldatasets/data/SwissProt/SwissProt.xml
  279  wget http://www.cs.washington.edu/research/xmldatasets/data/SwissProt/SwissProt.xml -e use_proxy=yes 
  280  brew install caskroom/cask/brew-cask
  281  brew cask install mactex
  282  brew cask install texmaker
  283  which latex
  284  Ana
  285  cd Scripts
  286  git clone git@github.build.ge.com:212325745/AllisonTransmission.git
  287  cd Allison
  288  cd ServiceManual
  289  head 3245076_TS7149_217-10-19-32-en_US.xml
  290  head -v  3245076_TS7149_217-10-19-32-en_US.xml
  291  less  3245076_TS7149_217-10-19-32-en_US.xml
  292  vim  3245076_TS7149_217-10-19-32-en_US.xml
  293  man od
  294  head 3245076_TS7149_217-10-19-32-en_US.xml | od c
  295  head 3245076_TS7149_217-10-19-32-en_US.xml | less
  296  head 3245076_TS7149_217-10-19-32-en_US.xml | od -c
  297  cp StorageExplorer -t ~/.local/bin/
  298  mv StorageExplorer  ~/.local/bin/
  299  conda env export -f py35export.yml
  300  cat py35export.yml
  301  vim py35export.yml
  302  javac -version
  303  ls /Library/Java/JavaVirtualMachines
  304  man R
  305  sudo ln /Library/Java/JavaVirtualMachines/jdk1.8.0_101.jdk/Contents/Home/jre/lib/server/libjvm.dylib /usr/local/lib
  306  sudo  R CMD javareconf
  307  R CMD 
  308  R CMD javareconf
  309  brew install homebrew/dupes/gdb
  310  brew install gdb
  311  R -d
  312  java
  313  java -version
  314  R -d gdb
  315  java_homr
  316  java_home
  317  JAVA_HOME
  318  which java
  319  /usr/libexec/java_home
  320  cd Box\ Sync/Analytics/Projects/AllisonTransmission/data
  321  java -jar sqljdbc4.jar
  322  ls ~/Downloads/
  323  gzip -d sqljdbc_4.0.2206.100_enu.tar.gz
  324  tar -xf sqljdbc_4.0.2206.100_enu.tar
  325  cd ls
  326  ls -al /usr/local/lib/libjvm.dylib
  327  rm libjvm.dylib
  328  1. sudo ln -s $(/usr/libexec/java_home)/jre/lib/server/libjvm.dylib /usr/local/lib
  329  sudo ln -s $(/usr/libexec/java_home)/jre/lib/server/libjvm.dylib /usr/local/lib
  330  ls -al libjvm.dylib
  331  npminstall -g sql-cli
  332  npm install -g sql-cli
  333  mssql 
  334  cdd Box\ Sync
  335  cd data/Azure/St_Louis_Bus
  336  tree . -l 1
  337  tree . -l 0
  338  tree 
  339  tree . -L 1
  340  tree ./*csv -L 1 
  341  tree *csv -L 1 
  342  tree -L 1 -P *csv
  343  tree -P '*csv' -L 0
  344  tree -P '*csv' -L 1
  345  brew update && brew install unixODBC 
  346  wget 
  347  rm è·åçµæ­´æ¸.docx
  348  rm 11\ éè·_è»¢ç±ã«ã¤ãã¦.doc
  349  wget "http://cran.r-project.org/src/contrib/RODBC_1.3-10.tar.gz" 
  350  R CMD INSTALL RODBC_1.3-14.tar.gz
  351  brew install FreeTDS
  352  pip install pymsql
  353  pip install pymssqlÂ¨
  354  brew unlink freetds; brew install homebrew/versions/freetds091
  355  pip install pymssql
  356  pip install pyodbc
  357  less /usr/local/lib/libtdsodbc
  358  which freetds
  359  which homebrew
  360  ls /Library/Caches/C
  361  ssh alpidcdiha001v.cloud.ge.com -l 'sa'
  362  tsql -H '10.42.128.86' -p 1433 -U 'sa' -P 'datalake123'
  363  cd Box\ Sync/Analytics/Projects
  364  cd AllisonTransmission/data/
  365  cd Azure
  366  cd St_Louis_Bus
  367  tree
  368  tree -P *.csv 
  369  tree -P '*.csv '
  370  tree -P '*.csv'
  371  %doctest_mode
  372  which azure
  373  cd /Users/212339410/.local/bin/StorageExplorer
  374  echo $JAVA_HOME
  375  export PATH=$JAVA_HOME/bin:$PATH
  376  cd ~/Box\ Sync/Analytics/Projects/AllisonTransmission/data/Azure/St_Louis_Bus
  377  tree . -P '*csv'
  378  ls -al /usr/local/lib
  379  ls -al /usr/local/lib | grep libjvm
  380  mssql -s alpidcdiha001v.cloud.ge.com -u 'sa' -p 'datalake123' -d 'ATI_Data_Analysis' -e 
  381  conda create -n opencv numpy scipy scikit-learn matplotlib python=3
  382  source activate opencv
  383  conda install -c https://conda.binstar.org/menpo opencv3
  384  brew install imagemagick
  385  brew install ghostscript
  386  python3 -m pip install Pillow
  387  convert
  388  source .bash_profile
  389  whoami
  390  convert -h
  391  which convert
  392  cd /usr/local/bin/
  393  cd /Users/212339410/Box Sync/Analytics/Data/Hermle/Spindel
  394  cd /Users/212339410/BoxÂ¥ Sync/Analytics/Data/Hermle/Spindel
  395  cd /Users/212339410/Box\ Sync/Analytics/Data/Hermle/Spindel
  396  convert -density 300 Spindel_23005.pdf -quality 100 temp.jpg 
  397  convert -density 300 Spindel_23115.pdf -quality 100 temp2.jpg 
  398  cd Python/OpenCV
  399  git clone https://github.com/pannous/caffe-ocr.git 
  400  cd caffe-ocr
  401  bash train.sh
  402  brew install -vd snappy leveldb gflags glog szip lmdb
  403  brew tap homebrew/science
  404  brew install hdf5 opencv
  405  brew install --build-from-source --with-python -vd protobuf
  406  brew install --build-from-source -vd boost boost-python
  407  /usr/bin/cc --version
  408  caffe
  409  git clone https://github.com/pannous/tensorflow-ocr.git
  410  cd tensorflow-ocr
  411  import tensorflow
  412  source deactuvate 
  413  source activate OpenCV
  414  pip install -i https://pypi.anaconda.org/jjhelmus/simple tensorflow
  415  conda install -c jjhelmus tensorflow=0.10.0rc0
  416  pip3 install --upgrade https://storage.googleapis.com/tensorflow/mac/tensorflow-0.9.0-py3-none-any.whl
  417  pip install pip3
  418  which pip3
  419  python train_ocr_layer.py
  420  cd Box\ Sync/Analytics/Projects/AllisonTransmission/data/Azure/St_Louis_Bus/
  421  mkdir TalendTest
  422  cd TalendTest
  423  vim AddressFile.txt
  424  vim PersonFile.txt
  425  vim JoinedPersonAddressFile.txt
  426  cd Python/tstk/tstk/visualization
  427  vim plot.py
  428  cd ../../..
  429  cd Python/tstk/tstk/visualization/
  430  vim pca_plot.py
  431  ssh alpidcdiha001v.cloud.ge.com 
  432  ssh alpidcdiha001v.cloud.ge.com -l sa
  433  cd CRM_Data
  434  cd CRM_DATA
  435  head filterednew_dtc.rpt
  436  cd Box\ Sync/Analytics/Projects/AllisonTransmission/data/Azure/CRM_Data/CRM_DATA
  437  head filterednew_dtc.rpt -n 269
  438  head
  439  head -n 269 filterednew_dtc.rpt
  440  head -n 400  filterednew_dtc.rpt
  441  vim filterednew_dtc.rpt
  442  vim filteredactivityparty.rpt
  443  brew info mysql
  444  brew services start mysql
  445  ls /usr/local/Cellar/mysql/5.7.16/bin/mysql -u root -p
  446  ls /usr/local/Cellar/mysql/5.7.16/bin/mysql 
  447  which mysql
  448  ls al /usr/local/bin/mysql
  449  ls -al /usr/local/bin/mysql
  450  mysql -v
  451  mysqladmin -u root password 'sql'
  452  mysqladmin -u root -p create mytestdatabase
  453  mysql
  454  mysql -v -uroot -psql
  455  vim filteredactivitypointer_xml.txt
  456  cd lls
  457  mkdir MSSQL
  458  cd MSSQL
  459  vim createViewOpStLouis
  460  vim createViewOpStLouis.sql
  461  cd ../data/Azure
  462  vim test.txt 
  463  sed -i 's/[//g' test.txt
  464  sed -i 's/[/ /g' test.txt
  465  sed -i 's/\[/ /g' test.txt
  466  sed -i '.bak' 's/\[/ /g' test.txt
  467  sed -i '.bak' 's/\]//g' test.txt
  468  cat test.txt
  469  vim test.txt
  470  sed -i '.bak' 's/\[//g' BOM_SerialHeaders.csv
  471  sed -i '.bak' 's/\]//g' BOM_SerialHeaders.csv
  472  vim BOM_SerialHeaders.csv
  473  cd Box\ Sync/Analytics/Projects/AllisonTransmission/data/Azure/
  474  less BOM_SerialHeaders.csv.bak
  475  sed -i '.bak' 's/\//g' BOM_SerialHeaders.csv
  476  sed -i '.bak' 's/\"//g' BOM_SerialHeaders.csv
  477  unzip Updated_Claims_and_Service_Data.zip
  478  jar xvf Updated_Claims_and_Service_Data.zip
  479  man jar 
  480  docker pull microsoft/mssql-server-2014-express-windows
  481  cd Analytics/Projects/AllisonTransmission/data/Azure
  482  zip -FF Updated_Claims_and_Service_Data.zip --out fixedUpdate_Claims_Service.zip
  483  man zip 
  484  zip -fz Updated_Claims_and_Service_Data.zip --out fixedUpdate_Claims_Service.zip
  485  unzip fixedUpdate_Claims_Service.zip
  486  zip -FF fixedUpdate_Claims_Service.zip --out fixed_v2Update_Claims_Service.zip
  487  docker run -it -p 1433:1433 sqlexpress "powershell ./start"
  488  docker run -p 1433:1433 --env sa_password=datalake123 microsoft/mssql-server-2014-express-windows
  489  docker run -d -p 1433:1433 --env sa_password=datalake123 microsoft/mssql-server-2016-express-windows
  490  docker run -d -p 1433:1433 --env sa_password=Data_Lake_123 microsoft/mssql-server-2016-express-windows
  491  zip -fz Updated_Claims_and_Service_Data.zip --out fixedUpdate_Claims_Service.zipls
  492  cd ~/Box\ Sync/Analytics/Python
  493  cd pymodule
  494  pip install 2to2
  495  pip install 2to3
  496  python -c import 2to3
  497  python -c 'import 2to3'
  498  futurize 
  499  futurize --stage1 -w qutil.p
  500  man futurize
  501  futurize -h
  502  futurize --stage1 -w -v qutil.py
  503  scp
  504  scp joposch@sjc1ddpl07.crd.ge.com:/data0/datascience/ServiceDataFault.zip /Users/212339410/Box Sync/Analytics/Projects/AllisonTransmission/data/Azure/Updated_Claims_and_Service_Data
  505  scp joposch@sjc1ddpl07.crd.ge.com:/data0/datascience/ServiceDataFault.zip /Users/212339410/Box Sync/Analytics/Projects/AllisonTransmission/data/Azure/Updated_Claims_and_Service_Data/
  506  scp joposch@sjc1ddpl07.crd.ge.com:/data0/datascience/ServiceDataFault.zip /Users/212339410/Box\ Sync/Analytics/Projects/AllisonTransmission/data/Azure/Updated_Claims_and_Service_Data/
  507  scp joposch@sjc1ddpl07.crd.ge.com:/data0/datascience/ServiceData.zip /Users/212339410/Box\ Sync/Analytics/Projects/AllisonTransmission/data/Azure/Updated_Claims_and_Service_Data/
  508  scp joposch@sjc1ddpl07.crd.ge.com:/data0/datascience/ServiceDataStatic.zip /Users/212339410/Box\ Sync/Analytics/Projects/AllisonTransmission/data/Azure/Updated_Claims_and_Service_Data/
  509  scp joposch@sjc1ddpl07.crd.ge.com:/data0/datascience/ServiceDataHealthClutch.zip /Users/212339410/Box\ Sync/Analytics/Projects/AllisonTransmission/data/Azure/Updated_Claims_and_Service_Data/
  510  cd Box\ Sync/Analytics/ls
  511  cd Box\ Sync/Analytics/
  512  cd Projects/AllisonTransmission/data/Azure/Updated_Claims_and_Service_Data
  513  less CLAIMS_HEADER.csv
  514  vim CLAIMS_HEADER.csv
  515  less ServiceDataHealthClutch.csv
  516  less Claims_DTC.csv
  517  less ServiceDataStatic.csv
  518  ssh  joposch@sjc1ddpl07.crd.ge.com:/data0/datascience
  519  nslookup sjc1ddpl07.crd.ge.com
  520  ssh  joposch@sjc1ddpl07.crd.ge.com
  521  scp joposch@sjc1ddpl07.crd.ge.com:/data0/datascience/ServiceDataStaticDataDetail.zip /Users/212339410/Box\ Sync/Analytics/Projects/AllisonTransmission/data/Azure/Updated_Claims_and_Service_Data/
  522  scp joposch@sjc1ddpl07.crd.ge.com:/data0/datascience/ServiceDataHealthParameter.zip /Users/212339410/Box\ Sync/Analytics/Projects/AllisonTransmission/data/Azure/Updated_Claims_and_Service_Data/
  523  scp joposch@sjc1ddpl07.crd.ge.com:/data0/datascience/CLAIMS_DETAIL.zip /Users/212339410/Box\ Sync/Analytics/Projects/AllisonTransmission/data/Azure/Updated_Claims_and_Service_Data/
  524  ls Box\ Sync/Analytics/Projects/AllisonTransmission/data/Azure/Updated_Claims_and_Service_Data
  525  man unzip 
  526  unzip ServiceDataStaticDataDetail.zip -v 
  527  unzip ServiceDataStaticDataDetail.zip
  528  unzip CLAIMS_DETAIL.zip
  529  brew install p7zip
  530  7za x CLAIMS_DETAIL.zip
  531  7za x ServiceDataStaticDataDetail.zip
  532  7za x ServiceDataFaultDetail.zip
  533  head -n 1 ServiceDataStatic.csv
  534  head -n 2 ServiceDataStatic.csv
  535  head -n 3 ServiceDataStatic.csv
  536  vim ServiceDataHealthClutch.csv
  537  vim ServiceDataStatic.csv
  538  nslookup 10.42.128.86
  539  ping alpidcdiha001v.cloud.ge.com
  540  ping 10.42.128.86
  541  jupyter -v
  542  jupyter notebook -v
  543  jupyter notebook -version
  544  jupyter notebook -hjupyter-notebook cmd -h
  545  jupyter notebook -hjupyter-notebook cmd -h-help-all
  546  jupyter notebook -hjupyter-notebook cmd -help-all
  547  jupyter notebook -h
  548  jupyter-notebook -v
  549  jupyter-notebook -h
  550  jupyter-notebook --help-all
  551  jupyter-notebook 
  552  ; ~/.ssh
  553  ssh nxi_edge
  554  ssh nxi_jumpbox
  555  ssh -i ~/.ssh/id_rsa 212339410@52.35.107.124
  556  ssh -W
  557  ssh ssh -h
  558  man ssh-config
  559  ssh-config
  560  ssh  212339410@52.35.107.124
  561  vim ssh_config
  562  ssh_config
  563  man ssh_config
  564  ssh bastion
  565  ssh nxi_bastion1
  566  ssh -i ~/.ssh/github.build.ge.com_rsa 212339410@52.35.107.124
  567  cd Python/tstk/examples
  568  nxi_bastion2
  569  cat ~/.ssh/github.build.ge.com_rsa
  570  ssh nxi_bation
  571  ssh nxi_a
  572  nslookup 52.35.107.124
  573  cat .ssh/id_rsa
  574  ssh 52.35.107.124
  575  ssh 52.35.107.124 -i .ssh/id_rsa.pub
  576  ssh 52.35.107.124 
  577  chmod 600 ~/.ssh/id_rsa
  578  cat .ssh/id_rsa.pub
  579  man usermod
  580  ssh  -i .ssh/id_rsa 52.35.107.124
  581  ssh 52.35.107.124 -i .ssh/id_rsa
  582  ssh 52.35.107.124 -i .ssh/github.build.ge.com_rsa
  583  git checkout -b ysong/plotly origin/ysong/plotly
  584  vim doc/tstk.visualization.rst
  585  git add doc/tstk.visualization.rst
  586  git commit -m "resolve conflict"
  587  vim tstk/visualization/plot.py
  588  git add tstk/visualization/plot.py
  589  git add doc/tstk.visualization.rst 
  590  git commit -m "resolved conflicts"
  591  git merge --no-ff ysong/plotly
  592  cd Python/tstk.gh-pages
  593  spinx-apidoc
  594  commit -m 'update API documents based on realese'
  595  git commit -m 'update API documents based on realese'
  596  sphinx-apidoc -of ./doc ./tstk
  597  man sphinx-apidoc
  598  sphinx-apidoc
  599  sphinx-apidoc -f -E  -o ./doc ./tstk
  600  npm 
  601  npm help npm
  602  npm outdated 
  603  npm update oniguruma
  604  node-gyp rebuild
  605  apm uninstall markdown-preview
  606  ssh nxi_bastiion
  607  chown
  608  cd t
  609  rm -rf test 
  610  chown -R 212339410 test 
  611  chown -R 212339410. test 
  612  chown -R 700 test 
  613  branch 
  614  git clone git@github.build.ge.com:212339410/tstk.git
  615  cd ~/Box\ Sync/Analytics/Projects/AllisonTransmission/data
  616  echo /Library/Java/JavaVirtualMachines/jdk1.8.0_101.jdk/Contents/Home/jre/lib/server/libjvm.dylib
  617  l /usr/local/lib
  618  l /usr/local/lib | grep lib
  619  l /usr/local/lib | grep libjvm
  620  lls
  621  cd ModifiedData
  622  head SDFD.G5.csv
  623  ssh 10.42.50.159 -i  ~/.ssh/id_rsa
  624  ssh 10.42.50.159
  625  ssh -i ~/.ssh/github.build.ge.com_rsa.pub 10.42.50.159 
  626  ping 10.42.50.159
  627  ssh -t 52.35.107.124 ssh 10.42.50.159
  628  ssh -t 52.35.107.124 ssh -i ~/.ssh/id_rsa 10.42.50.159
  629  conda 2
  630  rm -rf tstk.gh-pages
  631  git clone git@github.build.ge.com:212339410/tstk.git tstk.gh-pages
  632  git commit -A
  633  git commit -m 'add notebook rst file'
  634  git fetch master
  635  git fetch mastergit push origin documentation
  636  git push documentation master
  637  git merge origin/master
  638  git git add -A
  639  git commit -m "Generated gh-pages for `git log master -1 --pretty=short \\ngit add -A\ngit commit -m "Generated gh-pages for `git log master -1 --pretty=short \\n    --abbrev-commit`" 
  640  git clone git@github.build.ge.com:IndustrialDataScience/tstk.git tstk.gh-pages.master
  641  cd tstk.gh-pages.master
  642  git push orign gh-pages
  643  git commit -m "Generated gh-pages for `git log master -1 --pretty=short \\n    --abbrev-commit`" 
  644  git clone https://github.build.ge.com/IndustrialDataScience/tstk.wiki.git tstk.wiki
  645  cd tstk.wiki
  646  pandoc --from-markdown --to-rst --output=Home.rst Home.md
  647  pandoc --from=markdown --to=rst --output=Home.rst Home.md
  648  pandoc --from=markdown --to=rst --output=API-Documentation.rst API-Documentation.md
  649  pandoc --from=markdown --to=rst --output=API-Development.rst Development.md
  650  rm development.html
  651  pip install tsfresh
  652  R 
  653  cat ~/.ssh/id_rsa
  654  cat ~/.ssh/id_rsa.ls
  655  rm Bepr0fessi0nal
  656  rm Bepr0fessi0nal*
  657  mkdir ~/.ssh/20161121
  658  cd ~/.ssh/20161121
  659  rm 20161121
  660  rm -rf 20161121
  661  ssh-keygen -t rsa -b 4096 
  662  cd ~/Library/Caches
  663  ls /Library/Java/JavaVirtualMachines/jdk1.8.0_101.jdk/Contents/Home/jre/lib/server/libjvm.dylib    
  664  cd Box\ Sync/Analytics/Projects/AllisonTransmission/data/Azure/St_Louis_Bus/DataElementLog
  665  head -n 000100_0
  666  head -n 10 000100_0
  667  kextstat | grep -v com.apple
  668  conda envs
  669  conda create 
  670  conda remove --name snowflakes
  671  conda create --name snowflake python3
  672  conda remove certifi
  673  conda update requests
  674  conda config --set ssl_verify False
  675  conda install certifi
  676  conda create --name snowflakes biopython
  677  conda create -n py3k python=3 anaconda
  678  source py3k 
  679  cd Box\ Sync/Analytics/Projects/AllisonTransmission/data/Azure/Updated_Claims_and_Service_Data
  680  vim ServiceDataHealthParameter.csv
  681  vim CLAIMS_DETAIL.csv
  682  head -n 100  CLAIMS_DETAIL.csv > CLAIMS_DETAIL100row.csv
  683  open CLAIMS_DETAIL100row.csv
  684  rm CLAIMS_DETAIL100row.csv
  685  pip install git+https://github.build.ge.com/IndustrialDataScience/tstk.git
  686  sudo pip install git+https://github.build.ge.com/IndustrialDataScience/tstk.git
  687  sp_msforeachtable 'Exec sp_spaceused [?]'
  688  sphinx-apidoc -f  -o ./doc ./tstkÂ¨diasjfoa
  689  cd Box\ Sync/Analytics/Data
  690  cd BHP\ \(shared\ zip\)
  691  unzip *.zip
  692  unzip \*.zip 
  693  unzip -h
  694  nzip -help
  695  unzip -help
  696  unzip -d
  697  find -name '*.zip'
  698  find ./ -name \*.zip 
  699  7z Live4DUserStory2-Inputs-BucketTraces.zip
  700  7z e  Live4DUserStory2-Inputs-BucketTraces.zip
  701  find 
  702  find -h
  703  find . 
  704  find .  "*.zip"
  705  find "./*.zip"
  706  find "./\*.zip"
  707  unzip "*.zip" -d "./Unzipped"
  708  7za for archive in *.zip; do 7z x -o"`basename \"$archive\" .zip`" "$archive"; done
  709  find . -name "*.7z" -type f| xargs -I {} 7z x {}
  710  find . -name
  711  find . -name "*.zip"
  712  find . -name "*.zip" -type f
  713  find . -name "*.zip" -type f|  xargs -I {} 7z x {}
  714  ls *.zip|awk -F'.zip' '{print "unzip "$0" -d "$1}'|sh
  715  cd Python/pymodule
  716  python -C 'import draft'
  717  python -c 'import draft'
  718  draft
  719  heatmap
  720  draft.py heatmap
  721  python draft.py heatmap 
  722  cd ~/Box\ Sync/Analytics/Data/BHP\ \(shared\)
  723  find . -r -name '*csv'
  724  ls ./*csv
  725  ls -r ./*csv
  726  ls -d ./*csv
  727  ls -d *csv
  728  ls -d '*csv'
  729  find . -name '*.csv' | python draft.py heatmap 
  730  python draft.py heatmap ./Live4DUserStory2-Inputs-BucketTraces/dippersERAug.csv
  731  cd Box\ Sync/Analytics/Data/BHP\ \(shared\)
  732  head -n 10000 ./Live4DUserStory2-Inputs-BucketTraces/dippersERAug.csv | ./Live4DUserStory2-Inputs-BucketTraces/dippersERAug10000row.csv
  733  head -n 10000 ./Live4DUserStory2-Inputs-BucketTraces/dippersERAug.csv > ./Live4DUserStory2-Inputs-BucketTraces/dippersERAug10000row.csv
  734  python draft.py heatmap ./Live4DUserStory2-Inputs-BucketTraces/dippersERAug10000row.csv
  735  head -n 10000 ./Live4DUserStory2-Inputs-BucketTraces/dippersERAug.csv > ./Live4DUserStory2-Inputs-BucketTraces/dippersERAug100000row.csv
  736  head -n 100000 ./Live4DUserStory2-Inputs-BucketTraces/dippersERAug.csv > ./Live4DUserStory2-Inputs-BucketTraces/dippersERAug100000row.csv
  737  python draft.py heatmap ./Live4DUserStory2-Inputs-BucketTraces/dippersERAug100000row.csv
  738  head -n 50000 ./Live4DUserStory2-Inputs-BucketTraces/dippersERAug.csv > ./Live4DUserStory2-Inputs-BucketTraces/dippersERAug50000row.csv
  739  head -n 50000 ./Live4DUserStory2-Inputs-BucketTraces/dippersERJun.csv > ./Live4DUserStory2-Inputs-BucketTraces/dippersERJun50000row.csv
  740  python draft.py heatmap ./Live4DUserStory2-Inputs-BucketTraces/dippersERJun50000row.csv
  741  python draft.py heatmap ./Live4DUserStory2-Inputs-BucketTraces/dippersERJun50000row.csv output='heatmap_dippersERJun50000'
  742  head -n 50000 ./Live4DUserStory2-Inputs-BucketTraces/dippersERJul.csv > ./Live4DUserStory2-Inputs-BucketTraces/dippersERJul50000row.csv
  743  python draft.py heatmap ./Live4DUserStory2-Inputs-BucketTraces/dippersERJul50000row.csv 'heatmap_dippersERJul50000'
  744  cd ~/Box\ Sync/Analytics/Python/tstk
  745  cat TSTK.egg-info
  746  cat TSTK.egg-info/PKG-INFO
  747  cat TSTK.egg-info/SOURCES.txt
  748  cat TSTK.egg-info/dependency_links.txt
  749  pip install git@github.build.ge.com:212339410/tstk.git
  750  source activate py3k
  751  pip install git+ssh://git@github.build.ge.com:212339410/tstk.git
  752  git config --global http.proxy https://proxy-src.research.ge.com:8080
  753  git config --global https.proxy https://proxy-src.research.ge.com:8080
  754  git config --get-all
  755  git config --get-all: *proxy*
  756  git config --get-all: "*proxy*"
  757  git config --get http.proxy
  758  git config --get https.proxy
  759  git clone -q
  760  git clone https://github.build.ge.com/212339410/tstk.git tstk.temp
  761  cat $HOME/.gitconfig
  762  git config --system --unset https.proxy
  763  git config --system --get https.proxy
  764  git config --system http.proxy https://proxy-src.research.ge.com:8080
  765  sudo git config --system http.proxy https://proxy-src.research.ge.com:8080
  766  sudo git config --system https.proxy https://proxy-src.research.ge.com:8080
  767  git config --system --get http.proxy
  768  sudo pip install git+https://github.build.ge.com/212339410/tstk.git
  769  git clone -q https://github.build.ge.com/212339410/tstk.git /private/tmp/pip-gh70niso-build
  770  sudo  git config --system --unset https.proxy
  771  sudo  git config --system --unset http.proxy
  772  sudo  git config --global  --unset http.proxy
  773  sudo  git config --global  --unset https.proxy
  774  git clone https://github.build.ge.com/IndustrialDataScience/tstk.git tmptstk
  775  pip freeze | tstk
  776  pip freeze | grep *tstk
  777  cd ~/.Trash/Â¥
  778  cd ~/.Trash/
  779  l |grep PCS
  780  unzip 
  781  unzip  -l PCS-DEMO.zip
  782  7z  i PCS-DEMO.zip
  783  7z 
  784  less /usr/local/etc/npmrc
  785  less /Users/212339410/.npm-init.js
  786  less /Users/212339410/.npm/http-proxy/1.15.1/package.tgz
  787  less /Users/212339410/.npm/http-proxy/1.15.1/package/package.json
  788  gyp
  789  brew install gyp
  790  npm install -g node-gyp
  791  npm config delete proxy
  792  atom-html-preview
  793  npm config --global edit 
  794  apm install atom-html-preview
  795  node-gyp
  796  node-gyp list
  797  apm install atom-html-preview --verbose
  798  chmod 400 ~/.ssh/20161121.pub
  799  chmod 400 ~/.ssh/20161121
  800  ls ~/.ssh/config
  801  BHP\ \(shared\)
  802  cd Live4DUserStory2-Inputs-BucketTraces
  803  cp draft.py -t ~/Box\ Sync/Analytics/Projects/AllisonTransmission/Python
  804  cp draft.py ~/Box\ Sync/Analytics/Projects/AllisonTransmission/Python
  805  cd Projects/AllisonTransmission/Python
  806  vim draft.py
  807  python draft.py heatmap ../cache/StLouisDELRRoverlap.spread.csv
  808  vim qutil.py
  809  cal 2016
  810  pip install tstk
  811  pip install git+https://github.build.ge.com/212339410/tstk.git
  812  ls ~/anaconda/envs/py3k/lib/python3.5/site-packages/tstk
  813  less  ~/anaconda/envs/py3k/lib/python3.5/site-packages/tstk/__init__.py
  814  deactivate py3k
  815  sphinxcontrib
  816  pip install sphinxcontriib-httpdomain2
  817  sphinx-build
  818  pip install sphinxcontrib
  819  cd Box\ Sync/Analytics/Python/tstk.gh-pages.master
  820  git push origin gh-pages'
  821  cd ../../../Python/tstk/examples
  822  git clone https://github.build.ge.com/212339410/tstk.git tstk.feature_importance
  823  git branch models/feature_importance
  824  git add tstk/models/feature_importance.py
  825  git commit -m 'add feature_imporance'
  826  git origin models/feature_importance
  827  cd ../Box\ Sync
  828  cd Analytics/Python
  829  pip install rpy2
  830  ls /Users/212339410/anaconda2/envs/py35/lib/python3.5/site-packages/tstk-0.1-py3.5.egg/tstk/cd ..
  831  git commit -m 'fix the installation bug'
  832  git commit -m 'update gitignore'
  833  pip install pip install git+https://github.build.ge.com/212339410/tstk.git
  834  git config edit
  835  git clone -q https://github.build.ge.com/212339410/tstk.git tstk.setup
  836  rm -rf tstk.setup
  837  git commit -m 'update fork'
  838  git remove
  839  git rm  examples/.ipynb_checkpoints/
  840  git rm  -r examples/.ipynb_checkpoints/
  841  git commit -m 'remove ipython checkpoints'
  842  checkout pip/setup.py
  843  git push origin pip/setup.py
  844  git checkout remotes/IndustrialDataScience/revert-24-revert-22-pip/setup.py
  845  git -A 
  846  git commit -m 'fix bug for pip install'
  847  git push origin remotes/IndustrialDataScience/revert-24-revert-22-pip/setup.py
  848  cat .git/HEAD
  849  git branch -d HEAD detached from IndustrialDataScience/revert-24-revert-22-pip/setup.py
  850  git branch -d IndustrialDataScience/revert-24-revert-22-pip/setup.py
  851  git checkout pip/setup.py
  852  git checkout -b 212339410-pip/setup.py master
  853  git pull https://github.build.ge.com/212339410/tstk.git pip/setup.py
  854  git merge --no-ff 212339410-pip/setup.py
  855  git merge
  856  git push -u origin 212339410-pip/setup.py
  857  cd tstk.feature_importance
  858  git pull master origin
  859  git commit -m 'update'
  860  git add remote upstream https://github.build.ge.com/212339410/tstk
  861  git remote add upstream https://github.build.ge.com/212339410/tstk
  862  less setup.py
  863  git pull upstream origin
  864  git rebase origin/master
  865  git push origin models/feature_importance
  866  cd ../tstk.feature_importance
  867  git checkout models/feature_importance
  868  cd ../tstk/models
  869  git clone https://github.build.ge.com/212339410/tstk.git
  870  git clone https://github.build.ge.com/212339410/tstk.git tstk.documentation
  871  git commit -m 'modify index and remove sub/modules titles'
  872  ls ./Box\ Sync/Analytics/Projects/AllisonTransmission/data
  873  cd  ./Box\ Sync/Analytics/Projects/AllisonTransmission/data
  874  rm test.py
  875  python show_summary.py 
  876  which tstk
  877  git clone https://github.build.ge.com/IndustrialDataScience/tstk.git tstk.ids.master
  878  plot_heatmap
  879  echo $path
  880  plot_heatmap.py
  881  sudo plot_heatmap.py
  882  python plot_heatmap.py
  883  plot_overlap.py 
  884  which plot_overlap.py 
  885  sudo  plot_overlap.py 
  886  cd ~/Box\ Sync/Analytics/Projects/AllisonTransmission/data/ModifiedData
  887  python plot_heatmap.py StLouisDELRRoverlap.spread_15GGB2711C1179661.csv StLouisDELRRoverlap.spread_15GGB2711C1179661_heatmap.html
  888  python "heatmap_plot.py"
  889  python /Users/212339410/Box Sync/Analytics/Python/tstk.ids.master/utils/plot_heatmap.py
  890  python '/Users/212339410/Box Sync/Analytics/Python/tstk.ids.master/utils/plot_heatmap.py'
  891  for x in 1, 2, 3; do echo x;\ndone
  892  for x in 1, 2, 3; do echo $x;\ndone
  893  for x in 1 2 3; do echo $x;\ndone
  894  for (i = 0, i <42; i++); do echo $i; done
  895  for (i = 0; i <42; i++); do echo $i; done
  896  for ((i = 0; i <42; i++)); do echo $i; done
  897  python '/Users/212339410/Box Sync/Analytics/Python/tstk.ids.master/utils/plot_heatmap.py' StLouisDELRRoverlap.spread_15GGB2711C1179661.csv StLouisDELRRoverlap.spread_15GGB2711C1179661_heatmap.html
  898  python '/Users/212339410/Box Sync/Analytics/Python/tstk.ids.master/utils/plot_stacked.py' StLouisDELRRoverlap.spread_15GGB2711C1179661.csv StLouisDELRRoverlap.spread_15GGB2711C1179661_heatmap.html
  899  python '/Users/212339410/Box Sync/Analytics/Python/tstk.ids.master/utils/plot_overlap.py' StLouisDELRRoverlap.spread_15GGB2711C1179661.csv StLouisDELRRoverlap.spread_15GGB2711C1179661_overlap.html
  900  python '/Users/212339410/Box Sync/Analytics/Python/tstk.ids.master/utils/show_summary.py' StLouisDELRRoverlap.spread_15GGB2711C1179661.csv StLouisDELRRoverlap.spread_15GGB2711C1179661_summary.html
  901  head -n 10 StLouisDELRRoverlap_15GGB2711C1179661.csv
  902  git fetch ipstream
  903  git merge 
  904  git pull master documentation
  905  git pull ls documentation
  906  git rebase master 
  907  git add doc/modules.rst
  908  vim doc/in
  909  git commit -m 'resolved conflict between master and documentation'
  910  git origin documentation
  911  git add
  912  git commit -m 'resolved conflict doc/moduels.rst'
  913  rm -rf examples/.ipynb_checkpoints
  914  git commit -m 'delete checkpoints for ipython'
  915  vim doc/modules.rst
  916  vim doc/example.rst
  917  vim doc/glossary.rst
  918  vim  doc/index.rst
  919  ls  examples/
  920  vim doc/AnomalyDetectionHotellingsT2.rst
  921  vim doc/PrincipalComponentAnalysis.rst
  922  vim doc/intro.rst
  923  vim examples/PrincipalComponentAnalysis.ipynb
  924  cd tstk.ids.master
  925  sphinx-build ./doc ../tstk.gh-pages.master
  926  cd ../tstk.gh-pages.master
  927  cd ../tstk.ids.master
  928  ln tstk.ids.master ./pymodule/tstk
  929  ln tstk.ids.master/ ./pymodule/tstk
  930  ln -s tstk.ids.master pymodule/tstk
  931  l pymodule/
  932  ln -s tstk.ids.master/tstk pymodule/tstk
  933  ln -s tstk.ids.master/tstk ./pymodule/
  934  ln -sf tstk.ids.master/tstk pymodule/tstk
  935  ln -sf "tstk.ids.master/tstk" pymodule/tstk
  936  ln -s ./tstk.ids.master/tstk ./pymodule
  937  sudo ln -s ./tstk.ids.master/tstk ./pymodule 
  938  sudo ln -s Python/tstk.ids.master/tstk Python/pymodule 
  939  cp Python/tstk.ids.master/tstk\ alias -t Python/pymodule
  940  cp Python/tstk.ids.master/tstk\ alias Python/pymodule
  941  pip install tstk 
  942  cd /Users/212339410/Box Sync/Analytics/Projects/AllisonTransmission/data/ModifiedData
  943  cd '/Users/212339410/Box Sync/Analytics/Projects/AllisonTransmission/data/ModifiedData'/Users/212339410/Box Sync/Analytics/Projects/AllisonTransmission/data/ModifiedDatauu
  944  cd '~/Box Sync/Analytics/Projects/AllisonTransmission/data/ModifiedData'
  945  cd '/Users/212339410/Box Sync/Analytics/Projects/AllisonTransmission/data/ModifiedData'
  946  git clone https://github.build.ge.com/IndustrialDataScience/tstk.git tstkIdsMaster
  947  python plot_heatmap.py aggregate_15GGB2711C1179661.csv aggregate_15GGB2711C1179661_heatmap.html
  948  python plot_stacked.py aggregate_15GGB2711C1179661.csv aggregate_15GGB2711C1179661_stacked.html
  949  pip uninstall tstk 
  950  cd AllisonTables
  951  head ServiceDataFault2016.csv
  952  head -n 2 ServiceData.csv
  953  head -n 2 ServiceDataFault.csv
  954  head -n 2 ServiceDataFaultDetail.csv
  955  head -n 2 ServiceDataHealthClutch.csv
  956  head -n 2 ServiceDataHealthParameter.csv
  957  cd Box\ Sync/Analytics/Projects/AllisonTransmission/cache
  958  scp 
  959  scp ~/Box\ Sync/Analytics/Projects/AllisonTransmission/cache/Allison_select\ \*\ from\ StLouisDELRRoverlap\ where\ vin\ =\ \'15GG*
  960  ls ~/Box\ Sync/Analytics/Projects/AllisonTransmission/cache/Allison_select\ \*\ from\ StLouisDELRRoverlap\ where\ vin\ =\ \'15GG*
  961  scp ~/Box\ Sync/Analytics/Projects/AllisonTransmission/cache/Allison_select\ \*\ from\ StLouisDELRRoverlap\ where\ vin\ =\ \'15GG* joposch@sjc1ddpl07.crd.ge.com:/data0/datascience/StLouisOperation
  962  less ~/.bashrc
  963  ln -s /Users/212339410/BoxÂ¥ Sync/Analytics/Python/tstk.ids.master/tstk tstk
  964  git clone https://github.build.ge.com/IndustrialDataScience/tstk.git
  965  export PYTHONPATH=~/test/tstk
  966  mv test testÂ¥ me
  967  mv test test\ me
  968  export PYTHONPATH="~/test me/test"
  969  echo $PYTONPATH
  970  export PYTHONPATH=~/test\ me/test
  971  export PYTHONPATH=~/test\ me/tstk
  972  ssh alpidcdiha001v.cloud.ge.com
  973  ssh alpidcdiha001v.cloud.ge.com -l sapm2
  974  nslookup alpidcdiha001v.cloud.ge.com
  975  mssql -s alpidcdiha001v.cloud.ge.com -u 'geds' -p 'Ang3l123' -d 'ATI_Data_Analysis' -e 
  976  ssh nxi_bastion2
  977  ssh nxi_hdp_edge2
  978  cp ~/Box\ Sync/Analytics/Projects/AllisonTransmission/data/sqljdbc4.jar /Library/Java/Extensions
  979  sudo  cp ~/Box\ Sync/Analytics/Projects/AllisonTransmission/data/sqljdbc4.jar /Library/Java/Extensions
  980  ls /Library/Java/Extensions
  981  cd ~/Box\ Sync/Analytics/R/AngelTrain/cache
  982  plot_heat.py Channel_VehicleID81.csv
  983  echo $TSTK_PATH
  984  vim ~/.zshrc]
  985  export PYTHONPATH=$PYTHONPATH:$TSTK_PATH/tstk
  986  git clone https://github.build.ge.com/IndustrialDataScience/tstk.git tstk.master
  987  sl -s tstk.master ~/test
  988  ln -s tstk.master ~/test
  989  ls -al test\ me/
  990  cd ~/test
  991  cd tstk.
  992  rm tstk.master
  993  ln -s /Users/212339410/Box\ Sync/Analytics/Python/tstk.master ~/test
  994  mkdir mypymodule
  995  ln -s /Users/212339410/Box\ Sync/Analytics/Python/tstk.master ~/mypymodule
  996  cd mypymodule
  997  ln -s ~/mypymodule/tstk.master/utils/plot_* ~/Box\ Sync/Analytics/R/AngelTrain/cache
  998  ls ~/mypymodule
  999  cd tstk.master
 1000  python -c import tstk
 1001  python plot_heatmap.py Channel_VehicleID81.csv Channel_VehicleID81_heatmap.html
 1002  vim plot_heatmap.py
 1003  pipi unintall tstk
 1004  ls /Users/212339410/mypymodule/tstk.master/
 1005  pyhton
 1006  cd ~/anaconda2/envs
 1007  cd bin
 1008  ./python
 1009  cd env
 1010  cd envs
 1011  cd py35
 1012  cd /Users/212339410/mypymodule/tstk.master/tstk
 1013  export PYTHONPATH=/Users/212339410/mypymodule/tstk.master/
 1014  (py35) 212339410@SFO1212339410M:
 1015  cd /usr/lib/python2.7
 1016  which pyhton
 1017  cd pack
 1018  cd python3.5/
 1019  cd site-packages
 1020  cd .local/o
 1021  cd .local
 1022  cd /usr/lib
 1023  cd pyh
 1024  cd python2.
 1025  cd /usr/local/lib
 1026  cd python2.7/site-packages/
 1027  cd ~/.local
 1028  cd site
 1029  cd python3.5
 1030  cd site-packages/
 1031  ls tstk*
 1032  ls -al tstk.egg-link
 1033  cat tstk.egg-link
 1034  cd /Users/212339410/Box Sync/Analytics/Python/tstk
 1035  cd "/Users/212339410/Box Sync/Analytics/Python/tstk"
 1036  mv tstk.egg-link tstk.egg-link.back1
 1037  mv tstk.egg-link.back1 ~
 1038  cd anaconda2
 1039  cd lib
 1040  cd python2.7
 1041  locate -u
 1042  locate -U
 1043  slocate- u
 1044  slocate -u
 1045  slocate
 1046  locate
 1047  locate help
 1048  sudo launchctl load -w /System/Library/LaunchDaemons/com.apple.locate.plist
 1049  cd '~/Box Sync/Analytics/R/AngelTrain/cache'
 1050  cd '/Box Sync/Analytics/R/AngelTrain/cache'
 1051  cd Box\ Sync/Analytics/R/AngelTrain/cache
 1052  python plot_heatmap.py Channel_VehicleID81_2000.csv Channel_VehicleID81_2000_heatmap.csv
 1053  python plot_heatmap.py Channel_VehicleID81_4000.csv Channel_VehicleID81_4000_heatmap.csv
 1054  scp ~/Box\ Sync/Analytics/Projects/AllisonTransmission/cache/Allison_A.X.RData joposch@sjc1ddpl07.crd.ge.com:/data0/datascience/StLouisOperation
 1055  scp ~/Box\ Sync/Analytics/Projects/AllisonTransmission/cache/Allison_A.X.SD.RData joposch@sjc1ddpl07.crd.ge.com:/data0/datascience/StLouisOperation
 1056  scp ~/Box\ Sync/Analytics/Projects/AllisonTransmission/cache/Allison_A.X.SDFM.RData  joposch@sjc1ddpl07.crd.ge.com:/data0/datascience/StLouisOperation
 1057  ssh sjc1ddpl07.crd.ge.com -l joposch
 1058  an locate
 1059  man locate
 1060  rm -rf test\ me
 1061  cd  /Users/212339410/mypymodule/tstk.master
 1062  python plot_heatmap.py Channel_VehicleID81_18col.csv Channel_VehicleID81_18col_heatmap.csv
 1063  python -c 'import tstk'
 1064  less ~/.local/lib/python3.5_back/site-packages/homebrew.pth
 1065  locate .local 
 1066  locate .local | grep py
 1067  locate pth | grep py
 1068  python plot_heatmap.py Channel_VehicleID81_18col.csv Channel_VehicleID81_18col_heatmap.html
 1069  less /Users/212339410/anaconda2/envs/py35/lib/python3.5/site-packages/mymodules.pth
 1070  less /Users/212339410/anaconda2/lib/python2.7/site-packages/Sphinx.pth
 1071  less /usr/local/Cellar/protobuf/3.1.0/libexec/lib/python2.7/site-packages/protobuf-3.1.0-py2.7-nspkg.pth
 1072  less /Users/212339410/anaconda2/envs/py35/lib/python3.5/site-packages/setuptools.pth
 1073  less /Users/212339410/anaconda2/envs/GEDS-conda/lib/python3.5/site-packages/aeosa.pth
 1074  vim /Users/212339410/anaconda2/envs/py35/lib/python3.5/site-packages/mymodules.pth
 1075  echo $PYTHONPATH
 1076  less /Applications/PyCharm CE.app/Contents/helpers/pydev/pydevconsole.py
 1077  less '/Applications/PyCharm CE.app/Contents/helpers/pydev/pydevconsole.py'
 1078  less '/Users/212339410/.local/lib/python3.5_back/site-packages/homebrew.pth'
 1079  vim '/Users/212339410/.local/lib/python3.5_back/site-packages/homebrew.pth;
 1080  vim '/Users/212339410/.local/lib/python3.5_back/site-packages/homebrew.pth'
 1081  locate .pth
 1082  cat ~/.ssh/20161121.pub
 1083  curl https://api.box.com/oauth2/token \\n-d 'grant_type=authorization_code&code={your_code}&client_id=2q495hks9hstlr9g3ewciiksgjcv3mmu\n&client_secret=m8uptGO74BMhgNwlptKXuhkDKzoZE3MW' \\n-X POST
 1084  GET https://account.box.com/api/oauth2/authorize\?response_type\=code\&client_id\= 2q495hks9hstlr9g3ewciiksgjcv3mmu&state=security_token%YXBwbGUzMkoNCg==
 1085  curl https://account.box.com/api/oauth2/authorize\?response_type\=code\&client_id\= 2q495hks9hstlr9g3ewciiksgjcv3mmu&state=security_token%YXBwbGUzMkoNCg==
 1086  cd Data/AngelTrain
 1087  touch sample.txt
 1088  head -n 10 ChannelValue_Vehicle1.csv
 1089  vim ChannelValue_Vehicle1.csv
 1090  python plot_heatmap.py Channel_VehicleID81_18col_4000row.csv Channel_VehicleID81_18col_4000row_heatmap.html
 1091  sl
 1092  ln -s '/Users/212339410/Box Sync/Analytics/Python/tstk.ids.master_modified' '/Users/212339410/anaconda2/envs/py35/lib/python3.5/site-packages/tstk'
 1093  ls -al /Users/212339410/anaconda2/envs/py35/lib/python3.5/site-packages/ | grep tstk
 1094  ln -s '/Users/212339410/Box Sync/Analytics/Python/tstk.ids.master_modified/tstk' '/Users/212339410/anaconda2/envs/py35/lib/python3.5/site-packages/tstk'
 1095  cd /Users/212339410/Box Sync/Analytics/Data/AngelTrain
 1096  cd '/Users/212339410/Box Sync/Analytics/Data/AngelTrain'
 1097  7z
 1098  7z -e ChannelValue_Vehicle*
 1099  7z e ChannelValue_Vehicle*
 1100  7z e *.zip
 1101  for f in *.zip \ndo \n7z e "$f" & \ndone 
 1102  l;s
 1103  git clone https://github.build.ge.com/IndustrialDataScience/tstk.git tstk.documentation
 1104  cd tstk.documentation
 1105  rm AnomalyDetectionHotellingsT2.py
 1106  rm PrincipalComponentAnalysis.py
 1107  rm PCADemo.ipynb
 1108  sphinx-apidoc -f  -o ./doc ./tstk
 1109  cd tstk/visualization
 1110  cd P
 1111  ln -s AngelTrain ../Python/AngelTrain
 1112  ln -s AngelTrain /Users/212339410/Box\ Sync/Analytics/Python/AngelTrain
 1113  ln -s /Users/212339410/Box\ Sync/Analytics/Projects/AngelTrain /Users/212339410/Box\ Sync/Analytics/Python/AngelTrain
 1114  cp AngelTrain/* AngelTrain2/
 1115  ls -al AngelTrain2
 1116  cd AngelTrain/
 1117  cd Box\ Sync/Analytics
 1118  ln -s /Users/212339410/Box\ Sync/Analytics/Projects /Users/212339410/
 1119  locate jabber-config.xml
 1120  locate jabber
 1121  vim /Applications/Cisco Jabber.app/Contents/Resources/jabber-config-defaults.xml
 1122  vim '/Applications/Cisco Jabber.app/Contents/Resources/jabber-config-defaults.xml'
 1123  vim '/Applications/Cisco Jabber.app/Contents/MacOS/jabber-config-defaults.xml'
 1124  locate jabber | xml
 1125  locate jabber | grep xml
 1126  locate myjabber
 1127  vim /Users/212339410/Library/Application\ Support/Cisco/Unified\ Communications/Jabber/CSF/Config/jabberLocalConfig.xml
 1128  vim /Users/212339410/Library/Application\ Support/Cisco/Unified\ Communications/Jabber/CSF/Config/service-location.xml
 1129  git rm cache/*
 1130  git rm result/*
 1131  git commit -m 'remove result and cache'
 1132  git initi
 1133  git commit -m 'modifygitignore'
 1134  git clone https://github.build.ge.com/212339410/AngelTrain.git AngelTrain2
 1135  cp -r AngelTrain/* AngelTrain2/
 1136  rm -rf AngelTrain
 1137  cd AngelTrain
 1138  cd ../Python/tstk.documentation
 1139  git commit -help
 1140  git comomit --short
 1141  git commit --short
 1142  git fetch remote
 1143  git fetch remote upstream
 1144  git branch -help
 1145  git pull -all
 1146  git fetch origin 
 1147  git remote add upstream git@github.build.ge.com:212339410/tstk.git
 1148  git pull updtream master
 1149  git push origin documentation 
 1150  git checkout documentatioin
 1151  git checkout remotes/origin/documentation
 1152  git branch documentation
 1153  git checkout  documentation
 1154  git push origin documentation
 1155  git commit -m 'rm unnecessary files'
 1156  git push -u origin documentation
 1157  git pull upstream master
 1158  cd ../../../Projects/ls
 1159  cd ../../../Projects/
 1160  cd AllisonTransmission/cache
 1161  python plot_heatmap.py Allison_StLouis_Aggregate_5VIN_X.Y.csv Allison_StLouis_Aggregate_5VIN_X.Y_heatmap.html
 1162  python ../Python/plot_stacked.py Allison_StLouis_Aggregate_5VIN_X.Y.csv Allison_StLouis_Aggregate_5VIN_X.Y_stacked.html
 1163  python ../Python/plot_stacked.py ../cache/Allison_StLouis_Aggregate_5VIN_X.Y.csv Allison_StLouis_Aggregate_5VIN_X.Y_stacked.html
 1164  python plot_stacked.py ../data/ModifiedData/Allison_StLouis_Aggregate_5VIN_X.Y.csv Allison_Stlouis_Aggregate_5VIN_X.Y_stacked.html
 1165  grep 'py' $(find tstk)
 1166  find
 1167  man ind
 1168  find Allison
 1169  find RR
 1170  find /User
 1171  find App
 1172  grep 'R' $(find App)
 1173  grep 'R' App/app.R 
 1174  cp plot_* ~/Box\ Sync/Analytics/Data/MBS2016
 1175  cd ~/Box\ Sync/Analytics/Data/MBS2016/
 1176  find *csv
 1177  find ./*csv
 1178  find ./ -name '*.csv'
 1179  find ./  '*.csv'
 1180  find -path ./ 
 1181  find -path ./  *.csv
 1182  find ./ -path '*.csv'
 1183  find ./ -iname "*.csv"
 1184  find . -iname "*.csv"
 1185  find  -iname "*.csv"
 1186  find  -iname pwd "*.csv"
 1187  find  -iname $(pwd) "*.csv"
 1188  find  $(pwd) -iname "*.csv"
 1189  find  '$(pwd)' -iname "*.csv"
 1190  find  \'$(pwd)\' -iname "*.csv"
 1191  $(pwd)
 1192  $('pwd')
 1193  for i_file in $(find *.csv);\ndo \necho i_file\ndone
 1194  for i_file in $(find *.csv);\ndo \necho $i_file\ndone
 1195  for i_file in $(find ./ -path *.csv);\ndo \necho $i_file\ndone
 1196  for i_file in $(find ./ -path '*.csv');\ndo \necho $i_file\ndone
 1197  python plot_heatmap.py ./Data\ form\ 20160721\ to\ 20160803/BSW/BSW_20160721_20160803.csv BSW_20160721_20160803_heatmap.html
 1198  python plot_heatmap.py ./Data\ form\ 20160721\ to\ 20160803/INOS/NSN1.20160721_20160723.csv   NSN1.20160721_20160723_heatmap.html
 1199  cd ../R/MBS2016/Python
 1200  python plot_stacked.py ../Src/dataset_BSW.csv dataset_BSQ_stacked.html
 1201  python plot_heatmap.py ../Src/dataset_BSW.csv dataset_BSQ_stacked.html
 1202  python plot_heatmap.py ../Src/dataset_OSS.csv dataset_OSS_heatmap.html
 1203  python plot_heatmap.py ../Src/dataset_WOT.csv dataset_WOT_heatmap.html
 1204  python plot_heatmap.py ../Src/dataset_TTE.csv dataset_TTE_heatmap.html
 1205  python plot_heatmap.py ../Src/dataset_WOTrow.csv  dataset_WOT_400row_heatmap.html
 1206  python plot_stacked.py ../Src/dataset_WOT_400row.csv   dataset_WOT_400row_stacked.html
 1207  python plot_stacked.py ../Src/dataset_TTE.csv   dataset_TTE_stacked.html
 1208  python plot_stacked.py ../Src/dataset_BSW.csv dataset_BSW_stacked.html
 1209  python plot_stacked.py ../Src/dataset_OSS_400row.csv dataset_OSS_400row_stacked.html
 1210  python plot_heatmap.py ../Src/dataset_OSS_400row.csv dataset_OSS_400row_heatmap.html
 1211  python plot_heatmap.py ../Src/dataset_INOS_400row.csv dataset_INOS_400row_heatmap.html
 1212  cd ~/mypymodule
 1213  rm ./mypymodule
 1214  rm -r ./mypymodule
 1215  cd ~/Box\ Sync/Analytics/Python/tstk.ids.master_modified/
 1216  gir branch
 1217  cd ../R
 1218  git clone https://github.build.ge.com/212325745/AllisonTransmission.git AllisonTransmission1212
 1219  cd AllisonTransmission1212
 1220  git add R/LoadHelper.R
 1221  git commit -m 'resolved the conflict in LoadHelper.R'
 1222  python plot_heatmap.py ../data/ModifiedData/Allison_StLouis_Aggregate_5VIN_X.Y_1212.csv ../result/Allison_StLouis_Aggregate_5VIN_X.Y_1212_heatmap.html
 1223  python plot_heatmap.py ../data/ModifiedData/Allison_StLouis_Aggregate_5VIN_X.Y_1212.csv ../result/Allison_StLouis_Aggregate_5VIN_X.Y_1212_heatmap2.html
 1224  python plot_heatmap.py ../data/ModifiedData/Allison_StLouis_Aggregate_5VIN_X.Y_1212.csv ../result/Allison_StLouis_Aggregate_5VIN_X.Y_1212_heatmap3.html
 1225  python plot_heatmap.py ../data/ModifiedData/Allison_StLouis_Aggregate_5VIN_X.Y_1212.csv ../result/Allison_StLouis_Aggregate_5VIN_X.Y_1212_heatmap4.html
 1226  head -n 2000 ../data/ModifiedData/StLouisDELRRoverlap.spread_15GGB271XC1179674.csv > ../data/ModifiedData/StLouisDELRRoverlap.spread_15GGB271XC1179674_2000row.csv
 1227  python plot_heatmap.py ../data/ModifiedData/StLouisDELRRoverlap.spread_15GGB271XC1179674_2000row.csv ../data/ModifiedData/StLouisDELRRoverlap.spread_15GGB271XC1179674_2000row_heatmap.html
 1228  mssql -s alpidcdiha001v.cloud.ge.com -u 'sa' -p 'datalake123' -d 'ATI_Data_Analysis' -e
 1229  python plot_heatmap.py ../data/ModifiedData/StLouisDELRRoverlap.spread_15GGB271XC1179674_2000row.csv ../data/ModifiedData/StLouisDELRRoverlap.spread_15GGB271XC1179674_2000row_heatmap2.html
 1230  python plot_heatmap.py ../data/ModifiedData/Allison_StLouis_StLouisDELRRoverlap.spread_15GGB2713C1179662_2000.csv  ../data/ModifiedData/StLouisDELRRoverlap.spread_15GGB2713C1179662_2000row_heatmap2.html
 1231  python plot_heatmap.py ../data/ModifiedData/Allison_StLouis_StLouisDELRRoverlap.spread_15GGB2713C1179662_4000.csv  ../data/ModifiedData/StLouisDELRRoverlap.spread_15GGB2713C1179662_4000row_heatmap.html
 1232  python plot_heatmap.py ../data/ModifiedData/Allison_StLouis_StLouisDELRRoverlap.spread_15GGB2713C1179662_40000.csv  ../data/ModifiedData/StLouisDELRRoverlap.spread_15GGB2713C1179662_40000row_heatmap.html
 1233  cd Box\ Sync/Analytics/Projects/AllisonTransmission/data/ModifiedData
 1234  python plot_heatmap.py StLouisDELRRoverlap.spread_15GGB271XC1179674_2000row.csv StLouisDELRRoverlap.spread_15GGB271XC1179674_2000row_heatmap2.html
 1235  vim StLouisDELRRoverlap.spread_15GGB271XC1179674_2000row.csv
 1236  python plot_heatmap.py Allison_StLouis_StLouisDELRRoverlap.spread_15GGB2713C1179662_2000.csv Allison_StLouis_StLouisDELRRoverlap.spread_15GGB2713C1179662_2000_heatmap.html
 1237  python plot_heatmap.py Allison_StLouis_StLouisDELRRoverlap.spread_15GGB2713C1179662_4000.csv Allison_StLouis_StLouisDELRRoverlap.spread_15GGB2713C1179662_4000_heatmap.html
 1238  git clone git@github.build.ge.com:212487744/Deep-Learning-LSTM-Survival.git
 1239  ls -al /Users/212339410/anaconda2/envs/py35/lib/python3.5/site-packages | tstk
 1240  ls -al /Users/212339410/anaconda2/envs/py35/lib/python3.5/site-packages | grep tstk
 1241  git@github.build.ge.com:IndustrialDataScience/tstk.git tstk.ids.master
 1242  git clone git@github.build.ge.com:IndustrialDataScience/tstk.git TSTK
 1243  git clone git@github.build.ge.com:IndustrialDataScience/tstk.git tstk.master
 1244  ln -s '/Users/212339410/Box Sync/Analytics/Python/tstk.master/tstk' '/Users/212339410/anaconda2/envs/py35/lib/python3.5/site-packages/tstk'
 1245  cd ../Projects/AllisonTransmission/
 1246  cd R
 1247  git clone https://github.build.ge.com/212325745/AllisonTransmission.git AllisonTmp
 1248  cd AllisonTmp
 1249  git checkout -b hiroaki/st-louis origin/hiroaki/st-louis
 1250  git merge master
 1251  git commit -m 'resoleved conflict'
 1252  git merge --no-ff hiroaki/st-louis
 1253  open R/LoadHelper2016.R
 1254  atom R/LoadHelper2016.R
 1255  git add R/LoadHelper2016.R
 1256  git commit -m 'remove the conflict part'
 1257  git revert c33634abd634d63107c9cc3aed79fd1d3e0b0ab3
 1258  rm -rf AllisonTmp
 1259  git clone git@github.build.ge.com:212325745/AllisonTransmission.git AllisonTmp
 1260  git revert -h
 1261  git revert c33634a
 1262  cd Projects/AllisonTmp
 1263  cd result
 1264  cd plots
 1265  js
 1266  mssql -s alpidcdiha001v.cloud.ge.com -u 'geds' -p 'Ang3l123' -e 
 1267  git log --oneline
 1268  git checkout hiroaki/st-louis
 1269  uname
 1270  bash -v
 1271  pip install pyxley
 1272  git clone https://github.com/realpython/ultimate-flask-front-end.git
 1273  cd ultimate-flask-front-end
 1274  sh run.sh
 1275  cd Projects/AllisonTransmission
 1276  cd Python/ultimate-flask-front-end
 1277  npm install -g bower
 1278  bower init 
 1279  npm init 
 1280  npm install --save-dev bower
 1281  vim /Library/Preferences/com.box.sync.plist
 1282  sudo vim /Library/Preferences/com.box.sync.plist
 1283  unlink ~/anaconda2/envs/py35/lib/python3.5/site-packages/tstk
 1284  ln -s ~/anaconda2/envs/py35/lib/python3.5/site-packages /Users/212339410/Box/Analytics/Python/tstk/tstk
 1285  ln
 1286  ls -al ~/anaconda2/envs/py35/lib/python3.5/site-packages/ | \ngrep
 1287  ls -al ~/anaconda2/envs/py35/lib/python3.5/
 1288  ln -s ~/anaconda2/envs/py35/lib/python3.5/site-packages/tstk /Users/212339410/Box/Analytics/Python/tstk/tstk
 1289  ls -al ~/anaconda2/envs/py35/lib/python3.5/site-packages | grep tstk
 1290  ls -al ~/anaconda2/envs/py35/lib/python3.5/site-packages | grep /Users
 1291  ls -al ~/anaconda2/envs/py35/lib/python3.5/site-packages 
 1292  ls /Users/212339410/Python
 1293  ls -al /Users/212339410/Box/Analytics/Python/tstk
 1294  ls -al /Users/212339410/anaconda/envs/py3k
 1295  ls -al /Users/212339410/anaconda2/envs/py35/lib/python3.5/site-packages/
 1296  ls -al ~/anaconda2/envs/py35/lib/python3.5/site-packages/tstk
 1297  ls -al ~/anaconda2/envs/py35/lib/python3.5/site-packages/ | grep tstk
 1298  rm Projects
 1299  rm Python
 1300  ln -s ./Box/Analytics/Projects/ Projects
 1301  cd App
 1302  git clone https://github.com/cldougl/plotly-shiny.git
 1303  cd ../../Box\ Sync\ Backup
 1304  cd Analytics/Projects/AllisonTransmission/
 1305  cp .gitignore ~/Box/Analytics/Projects/AllisonTransmission/.gitignore
 1306  cd Box/Analytics/Projects/AllisonTransmission
 1307  cp .git ~/Box/Analytics/Projects/AllisonTransmission
 1308  cp .git ~/Box/Analytics/Projects/AllisonTransmission/.git
 1309  cp -r .git ~/Box/Analytics/Projects/AllisonTransmission/.git
 1310  cd ~/Box/Analytics/Projects/AllisonTransmission
 1311  defaults write com.apple.finder AppleShowAllFiles YES
 1312  mssql -s alpidcdiha001v.cloud.ge.com -u 'dssb' -d 'SB' -e -t 20000
 1313  cd ~/Box\ Sync\ Backup/Analytics/Projects/AllisonTransmission/data/Azure/Updated_Claims_and_Service_Data
 1314  vim ServiceDataStatic_mod.csv
 1315  iconv
 1316  find ./ -type f -exec echo 
 1317  find ./ -type f -exec echo '{}'
 1318  find ./ -type f
 1319  find ./ -type f | grep .csv
 1320  find ./ -regex '.csv'  
 1321  find ./
 1322  find ./ -pattern '.csv'
 1323  find ./ -pattern '*.csv'
 1324  find ./ -pattern ''*.csv''
 1325  find ./ -regex ''*.csv''
 1326  find ./ -r -regex ''*.csv''
 1327  man find 
 1328  find ./  -E ''*.csv''
 1329  find ./ -type f -exec cp -i '{}' ./ ';'
 1330  for files in ./\ndo \necho "$files"\ndone
 1331  for files in ./*.csv\ndo \necho "$files" done\n\n
 1332  ln -s ./ ~/Box/Analytics/R/MBS2016/Src
 1333  ln -s ../ ~/Box/Analytics/R/MBS2016/Src
 1334  ln -s ../ ~/Box/Analytics/R/MBS2016/Src/Original_flat
 1335  ln -s ~/Box/Analytics/Data/MBS2016/Original_flat  ~/Box/Analytics/R/MBS2016/Src/Original_flat
 1336  ln -s ~/Box/Analytics/Data/MBS2016/UTF8_flat  ~/Box/Analytics/R/MBS2016/Src/UTF8_flat
 1337  for files in ./*.csv\ndo\niconv -f SHIFT_JIS -t UTF-8 '$files' '$files' \necho "$files"\ndone
 1338  for files in ./*.csv\ndo\nfile -I "$files" \necho "$files"\ndone
 1339  for files in ./*.csv\ndo\niconv -f SHIFT_JIS -t UTF-8 '$files' >  '$files' \necho "$files"\ndone
 1340  for files in ./*.csv\ndo\niconv -f SHIFT_JIS -t UTF-8 '$files' -o  '${files%.csv}.utf8.csv' \necho "$files"\ndone
 1341  iconv --list
 1342  iconv --list | grep shift
 1343  iconv --list | grep JIS
 1344  iconv --list | grep SHIFT_JIS
 1345  iconv --list | grep UTF8
 1346  find . -name '*.csv'
 1347  find . -name '*.csv' -exec iconv --verbose -f SHIFT_JIS -t UTF-8 -o {} {} \;
 1348  iconv --help
 1349  iconv -h
 1350  find . -exec ls '{}' 
 1351  find . -exec ls '{}' \;
 1352  find . -exec ls '{}' \+
 1353  find ./ -exec ls '{}' \+
 1354  find ./ -name '*csv' -exec ls '{}' \+
 1355  find -name '*csv' -exec ls '{}' \+
 1356  find . -name '*csv' -exec ls '{}' \+
 1357  find . -name '*csv' -exec echo '{}'  \+
 1358  find . -name '*csv' -exec echo '{file%}.txt'  \;
 1359  find . -name '*csv' -exec echo '{file%.csv}.txt'  \;
 1360  find . -name '*csv' -exec echo '{}.txt'  \;
 1361  find . -name '*csv' -exec echo '{1}.txt'  \;
 1362  find . -name '*csv' -exec echo {}.txt  \;
 1363  for files in ./*.csv\ndo \necho {$files}.csv \ndone
 1364  for files in ./*.csv\ndo \necho '$files'.csv \ndone
 1365  for files in ./*.csv\ndo \necho $files.csv \ndone
 1366  for files in ./*.csv\ndo \necho $files%.csv \ndone
 1367  for files in ./*.csv\ndo \necho ./$files%.csv \ndone
 1368  for files in ./*.csv\ndo\niconv -f SHIFT_JIS -t UTF-8 '$files' -o  '${files%.csv}.utf8.csv'\necho "$files"
 1369  for files in ./*.csv\ndo\niconv -f SHIFT_JIS -t UTF-8 '$files' > ../UTF8_flat/'${files}.utf8.csv'\necho "$files"\ndone
 1370  man iconv
 1371  iconv -f SHIFT_JIS -t UTF-8 ./BSW_20160721_20160803.csv
 1372  iconv -f SHIFT_JIS -t UTF-8 ./BSW_20160721_20160803.csv > ./BSW_20160721_20160803.utf8.csv
 1373  iconv -f SHIFT_JIS -t UTF-8 ./BSW_20160721_20160803.csv >> BSW_20160721_20160803.utf8.csv
 1374  iconv -c -f SHIFT_JIS -t UTF-8 ./BSW_20160721_20160803.csv >> BSW_20160721_20160803.utf8.csv
 1375  for files in ./*.csv\ndo\nECHO "{$files%csv}"\ndone
 1376  for files in ./*.csv\ndo\nECHO "$files%csv"\ndone
 1377  for files in ./*.csv\ndo\nECHO "$files%%csv"\ndone
 1378  for files in ./*.csv\ndo\nECHO "${files%%*}"\ndone
 1379  for files in ./*.csv\ndo\nECHO "${files%*}"\ndone
 1380  for files in ./*.csv\ndo\nECHO "${files%csv}"\ndone
 1381  for files in ./*.csv\ndo\nECHO "${files##*/%csv}"\ndone
 1382  for files in ./*.csv\ndo\nECHO "${files##*./%csv}"\ndone
 1383  for files in ./*.csv\ndo\nECHO "${files##}"\ndone
 1384  for files in ./*.csv\ndo\nECHO "${%%files%%.csv}"\ndone
 1385  for files in ./*.csv\ndo\nECHO "${%%/files%%.csv}"\ndone
 1386  for files in ./*.csv\ndo\nECHO "${files%Â¥\.csv}"\ndone
 1387  for files in ./*.csv\ndo\nECHO "${files#\.\/%%.csv}"\ndone
 1388  for files in ./*.csv\ndo\nECHO "${files#\.\/ %%.csv}"\ndone
 1389  for files in ./*.csv\ndo\nECHO "${files%%.csv}"\ndone
 1390  for files in ./*.csv\ndo\nECHO "${files##./}"\ndone
 1391  for files in ./*.csv\ndo\nECHO "${files##./%%.csv}"\ndone
 1392  vim shiftjis2utf8.sh
 1393  sh shiftjis2utf8.sh
 1394  cd Projects/AllisonTransmission/
 1395  cd /Users/212339410/Box/Analytics/Data/MBS2016/SBtoGE Data 12282016_0103_0950/Data from 20161201 to 20161227
 1396  cd '/Users/212339410/Box/Analytics/Data/MBS2016/SBtoGE Data 12282016_0103_0950/Data from 20161201 to 20161227'
 1397  head OSS_20160720_20160803.utf8.csv > OSS_header.csv
 1398  mssql -s alpidcdiha001v.cloud.ge.com -u 'dssb' -d 'SB' -e -t 20000 -p
 1399  ln -s ~/Box/Analytics/R/MBS2016 ~/Box/Analytics/Projects
 1400  cd Src/
 1401  cd UTF8_flat
 1402  head OSS_20160720_20160803.utf8.csv
 1403  head OSS_20160720_20160803.utf8.csv -n 1
 1404  head -n 1 OSS_20160720_20160803.utf8.csv 
 1405  head -n 2 OSS_20160720_20160803.utf8.csv 
 1406  head -n 2 OSS_20160804_20160817.utf8.csv
 1407  head -n 2 OSS_20160720_20160803.csv
 1408  head -n 2 OSS_20160804_20160817.csv
 1409  head -n `0 OSS_20160804_20160817.csv > OSS_20160804_20160817_n10.csv
 1410  head -n 10 OSS_20160804_20160817.csv > OSS_20160804_20160817_n10.csv
 1411  head -n 10 OSS_20160720_20160803.csv > OSS_20160720_20160803_n10.csv
 1412  cd AllisonTransmission
 1413  git push origin hiroaki/st-louis
 1414  mssql -s alpidcdiha001v.cloud.ge.com -u 'dssb' -p 'b@nklak3_123' -d 'SB' -e -t 20000
 1415  cd /Users/212339410/Box/Analytics/R/MBS2016/MBS2016.Rproj
 1416  cd '/Users/212339410/Box/Analytics/R/MBS2016/MBS2016.Rproj'
 1417  cd '/Users/212339410/Box/Analytics/R/MBS2016/'
 1418  git-log
 1419  vim SB.Rproj
 1420  git clone git@github.build.ge.com:212339410/SB.git
 1421  git git remote -v
 1422  git remote add origin git@github.build.ge.com:212339410/SB.git
 1423  add Python
 1424  git add Report/
 1425  git rm --cached Report/ReportForMBSDescriptiveAnalysis.Rmd Report/ReportForMBSDescriptiveAnalysis.html
 1426  mssql -s alpidcdiha001v.cloud.ge.com -u 'dssb' -p 'b@nklak3_123' -d 'SB' -e
 1427  man mssql
 1428  mssql -help
 1429  .quit
 1430  cd Src/UTF8_flat
 1431  head -n 5 OSS_20160720_20160803.utf8.csv
 1432  cd ../cache
 1433  touch void.txt
 1434  git branch --set-upstream-to=origin/master master
 1435  git pull origin master --allow-unrelated-histories
 1436  git commit -m 'resolve the conflict'
 1437  ls'\nls\n'
 1438  for files in ./*.csv\ndo\necho "$files"\ndone
 1439  for files in ./Original_flat/*.csv\ndo\necho "$files"\ndone
 1440  for files in ./Original_flat/*.csv\ndo\necho "$files"\necho â${files%.csv}.utf8.csv'\ndone\n'\n'
 1441  for files in ./Original_flat/*.csv\ndo\necho "$files"\necho â${files%.csv}.utf8.csvâ\ndone
 1442  man gsub
 1443  gsub
 1444  for files in ./Original_flat/*.csv\ndo\necho "$files"\necho â${files%.csv}.utf8.csvâ | sed âs/Original_flat/UTF8_flat2â\ndone
 1445  for files in ./Original_flat/*.csv\ndo\necho "$files"\necho â${files%.csv}.utf8.csvâ | sed âs/Original_flat/UTF8_flat2/â\ndone
 1446  echo 'orig' | sed 's/o/aaa/'
 1447  for files in ./Original_flat/*.csv\ndo\necho "$files"\necho â${files%.csv}.utf8.csvâ | sed âs+Original_flat+UTF8_flat2+â\ndone
 1448  for files in ./Original_flat/*.csv\ndo\necho '$files'\necho '${files%.csv}.utf8.csv' | sed 's+Original_flat+UTF8_flat2+'\ndone
 1449  for files in ./Original_flat/*.csv\ndo\necho "$files"\necho â${files%.csv}.utf8.csvâ | sed 's+Original_flat+UTF8_flat2+'\ndone
 1450  sl -s Box/Analytics/Data/MBS2016/UTF8_bom Projects/SB/Src/
 1451  ln -s Box/Analytics/Data/MBS2016/UTF8_bom Projects/SB/Src/
 1452  ln -s Box/Analytics/Data/MBS2016/UTF8_bom Box/Analytics/Projects/SB/Src
 1453  python ../Python/convert_utf8.py Original_flat/BSW_20160721_20160803.csv UTF8_bom/test.csv
 1454  vim Original_flat/BSW_20160721_20160803.csv
 1455  sed -i 's/Original_flat/UTF8_flat/'
 1456  sed -i 's/Original_flat/UTF8_flat/' 'Original_flat'
 1457  sed -i 's+Original_flat+UTF8_flat+' 'Original_flat'
 1458  sed -i 's+Original_flat+UTF8_flat+' Original_flat
 1459  echo day | sed s/day/night/
 1460  var=`echo day | sed s/day/night/`
 1461  for files in ./Original_flat/*.csv\ndo\necho "$files"\n\nnew_file_name=`echo ${files%.csv}.utf8.csvâ | sed 's+Original_flat+UTF8_flat2+'`\n#python  convert_utf8.py â$files' '$new_file_name'\n\ndone
 1462  for files in ./Original_flat/*.csv\ndo\necho "$files"\n\nnew_file_name=`echo ${files%.csv}.utf8.csvâ | sed 's+Original_flat+UTF8_flat2+'`\n#python  convert_utf8.py â$files' '$new_file_name'\necho "$new_file_name"\ndone
 1463  ls ../
 1464  ls ../Python/
 1465  for files in ./Original_flat/*.csv\ndo\necho "$files"\n\nnew_file_name=`echo ${files%.csv}.utf8.csvâ | sed 's+Original_flat+UTF8_bom+'`\npython  ../Python/convert_utf8.py â$files' '$new_file_name'\necho "$new_file_name"\ndone\n
 1466  for files in ./Original_flat/*.csv\ndo\necho "$files"\n\nnew_file_name=`echo ${files%.csv}.utf8.csvâ | sed 's+Original_flat+UTF8_bom+'`\npython  ../Python/convert_utf8.py '$files' '$new_file_name'\necho "$new_file_name"\ndone
 1467  for files in ./Original_flat/*.csv\ndo\necho "$files"\nnew_file_name=`echo ${files%.csv}.utf8.csvâ | sed 's+Original_flat+UTF8_bom+'`\npython  ../Python/convert_utf8.py "$files" "$new_file_name"\necho "$new_file_name"\ndone
 1468  locate command
 1469  ln -s /Users/212339410/Box/Analytics/Data/
 1470  l /
 1471  ln -s /Users/212339410/Box/Analytics/Data/MBS2016/UTF8_bom /Users/212339410/Box/Analytics/Projects/SB/Src
 1472  ln -s /Users/212339410/Box/Analytics/Data/MBS2016/UTF8_bom /Users/212339410/Box/Analytics/Projects/SB/Src 
 1473  ln -s /Users/212339410/Box/Analytics/Data/MBS2016/UTF8_bom /Users/212339410/Box/Analytics/Projects/SB/Src/
 1474  ln -s /Users/212339410/Documents/Analytics/Data/MBS2016/UTF8_bom /Users/212339410/Documents/Analytics/Projects/SB/Src/
 1475  history  | grep cp
 1476  history  | grep flat
 1477  cd Data/MBS2016/
 1478  for files in ./Original_flat/*.csv\ndo\necho "$files"\nnew_file_name=`echo ${files%.csv}.utf8.csv | sed 's+Original_flat+UTF8_bom+'`\n# python  ../Python/convert_utf8.py "$files" "$new_file_name"\necho "$new_file_name"\ndone
 1479  ln -s /Users/212339410/Documents/Analytics/Data/MBS2016/Original_flat /Users/212339410/Documents/Analytics/Projects/SB/Src 
 1480  for files in ./Original_flat/*.csv\ndo\necho "$files"\nnew_file_name=`echo ${files%.csv}.utf8.csv | sed 's+Original_flat+UTF8_bom+'`\npython  ../Python/convert_utf8.py "$files" "$new_file_name"\necho "$new_file_name"\ndone
 1481  python ../../../Projects/SB/Python/convert_utf8.py BSW_20160804_20160816.csv BSW_20160804_20160816.utf8.csv
 1482  ssh nxi_bastion
 1483  ssh nxi_hdp_edge
 1484  files=BSW_ERI_20151201_20170201.csv
 1485  echo new_file_name
 1486  echo n$ew_file_name
 1487  files=./Original_flat/BSW_ERI_20151201_20170201.csv
 1488  files=./Original_flat/BSW_NOK_20151201_20170201.csv
 1489  new_file_name=`echo ${files%.csv}.utf8.csv | sed 's+Original_flat+UTF8_bom+'`
 1490  python  ../Python/convert_utf8.py "$files" "$new_file_name"
 1491  cat BSW2016_bom.csv| wc -l
 1492  cd OSS_
 1493  tar -zxvf OSSRC_20170116.tar.gz
 1494  tar -zxvf OSSRC_20170116.tar.gzfor files in ./*tar.gz
 1495  tar -zxvf $files
 1496  for files in ./*tar.gz\ndo\necho $files\n# tar -zxvf $files\ndone
 1497  for files in ./*.tar.gz\ndo\necho $files\n# tar -zxvf $files\ndone
 1498  for files in ./*.tar.gz\ndo\necho $files\ntar -zxvf $files\ndone
 1499  rm *tar.gz
 1500  vim Tokyo_RAN_OSS-RC_01_20170116.txt
 1501  cd OSS_Nok
 1502  tar -xzf OSS_NOK_20170127-aa.tgz
 1503  for files in ./*.tgz\ndo\necho $files\ntar -zxvf $files\ndone
 1504  echo $file
 1505  new=${file/.csv}
 1506  echo $new
 1507  cut -d '-' -f 2
 1508  cut -d '-' -f 2 <<< $files
 1509  head $files
 1510  cd Pr
 1511  cd Documents/Analytics
 1512  Projects
 1513  scd ..
 1514  cd UTF8_bom/
 1515  head Tokyo_RAN_OSS-RC_01_20170116.txt -n 100 > Tokyo_RAN_OSS-RC_01_20170116_100.txt
 1516  head -n 100 Tokyo_RAN_OSS-RC_01_20170116.txt > Tokyo_RAN_OSS-RC_01_20170116_100.txt
 1517  man vim
 1518  cp OSS_NOK_20170117-aa.csv OSS_NOK_20170117-aa.bk.csv
 1519  cd Documents/Analytics/Projects/SB/Src
 1520  head Tokyo_RAN_OSS-RC_01_20170116.txt
 1521  cp Tokyo_RAN_OSS-RC_01_20170116_100.txt Tokyo_RAN_OSS-RC_01_20170116_100.backup.txt
 1522  sed -i '2~2 s/^M//' Tokyo_RAN_OSS-RC_01_20170116_100.txt
 1523  sed -i "2~2 s/^M//" Tokyo_RAN_OSS-RC_01_20170116_100.txt >> Tokyo_RAN_OSS-RC_01_20170116_100_mod.txt
 1524  sed 
 1525  man sed
 1526  sed -n '1~2!p' file
 1527  sed -n '1~2!p' Tokyo_RAN_OSS-RC_01_20170116_100.txt
 1528  sed -n '1,2!p' Tokyo_RAN_OSS-RC_01_20170116_100.txt
 1529  sed -i "2,2 s/^M//" Tokyo_RAN_OSS-RC_01_20170116_100.txt >> Tokyo_RAN_OSS-RC_01_20170116_100_mod.txt
 1530  sed -i "2,2 s/^M//" 'Tokyo_RAN_OSS-RC_01_20170116_100.txt' >> 'Tokyo_RAN_OSS-RC_01_20170116_100_mod.txt'
 1531  sed -i "2,2 s/^M//" 'Tokyo_RAN_OSS-RC_01_20170116_100.txt' 
 1532  sed 's/^M//2'  'Tokyo_RAN_OSS-RC_01_20170116_100.txt' >> 'Tokyo_RAN_OSS-RC_01_20170116_100_mod.txt'
 1533  sed 's/\r$//2' Tokyo_RAN_OSS-RC_01_20170116_100.txt >> Tokyo_RAN_OSS-RC_01_20170116_100_mod.txt
 1534  vim Tokyo_RAN_OSS-RC_01_20170116_100_mod.txt
 1535  sed -i 's/\r$//g2' Tokyo_RAN_OSS-RC_01_20170116_100.txt >> Tokyo_RAN_OSS-RC_01_20170116_100_mod.txt
 1536  sed -i 's/\r$//2g' Tokyo_RAN_OSS-RC_01_20170116_100.txt >> Tokyo_RAN_OSS-RC_01_20170116_100_mod.txt
 1537  sed -i 's/\r$//2' Tokyo_RAN_OSS-RC_01_20170116_100.txt >> Tokyo_RAN_OSS-RC_01_20170116_100_mod.txt
 1538  sed -i 's/\r$//g' Tokyo_RAN_OSS-RC_01_20170116_100.txt 
 1539  sed 's/\r$//2' Tokyo_RAN_OSS-RC_01_20170116_100.txt 
 1540  sed 's/\r$//2g' Tokyo_RAN_OSS-RC_01_20170116_100.txt 
 1541  sed 's/\r//2' Tokyo_RAN_OSS-RC_01_20170116_100.txt 
 1542  sed 's/\r//g' Tokyo_RAN_OSS-RC_01_20170116_100.txt 
 1543  head  Tokyo_RAN_OSS-RC_01_20170116_100.txt 
 1544  head  Tokyo_RAN_OSS-RC_01_20170116_100_mod.txt 
 1545  head Tokyo_RAN_OSS-RC_01_20170116_100_mod.txt cd ..
 1546  head Tokyo_RAN_OSS-RC_01_20170116_100.mod.csv
 1547  vim Tokyo_RAN_OSS-RC_01_20170116_100.mod.csv
 1548  s
 1549  cd ../../UTF8_bom
 1550  cd OSS_Eri
 1551  head Tokyo_RAN_OSS-RC_01_20170117.mod.mod.txt
 1552  vim Tokyo_RAN_OSS-RC_01_20170117.mod.mod.txt
 1553  rm Tokyo_RAN_OSS-RC_01_20170117.mod.mod.txt Tokyo_RAN_OSS-RC_01_20170117.mod.txt
 1554  rm Tokyo_RAN_OSS-RC_01_20170116_100.mod.txt
 1555  vim Tokyo_RAN_OSS-RC_01_20170116_100.mod.txt
 1556  vim Tokyo_RAN_OSS-RC_01_20170116_100.txt
 1557  python read_ericsson.py
 1558  python read_ericsson.py >> /Users/212339410/Documents/Analytics/Data/MBS2016/UTF8_bom/OSS_Eri/Tokyo_RAN_OSS-RC_01_20170116_100.mod.txt
 1559  l ../Src/UTF8_bom/OSS_Eri/*.txt
 1560  l ../Src/UTF8_bom/OSS_Eri/*.txtfor files in ../Src/UTF8_bom/OSS_Eri/*.txt
 1561  new_file_name=`echo ${files%.csv}.mod.csv`
 1562  echo $new_file_name
 1563  #python read_ericsson.py $files >> $new_file_name
 1564  for files in ../Src/UTF8_bom/OSS_Eri/*.txt\ndo\necho $files\nnew_file_name=`echo ${files%.csv}.mod.csv`\necho $new_file_name\n#python read_ericsson.py $files >> $new_file_name\ndone
 1565  for files in ../Src/UTF8_bom/OSS_Eri/*.txt\ndo\necho $files\nnew_file_name=`echo ${files%.txt}.mod.txt`\necho $new_file_name\n#python read_ericsson.py $files >> $new_file_name\ndone
 1566  for files in ../Src/UTF8_bom/OSS_Eri/*.txt\ndo\necho $files\nnew_file_name=`echo ${files%.txt}.mod.txt`\necho $new_file_name\npython read_ericsson.py $files >> $new_file_name\ndone
 1567  ls Documents/Analytics/Projects/SB/
 1568  cd Documents/Analytics/Projects/SB/
 1569  cd UTF8_bom/OSS_Eri
 1570  wc -l Tokyo_RAN_OSS-RC_01_20170117.mod.txt
 1571  wc -l Tokyo_RAN_OSS-RC_01_20170117.txt
 1572  vim Tokyo_RAN_OSS-RC_01_20170117.mod.txt
 1573  wc -l Tokyo_RAN_OSS-RC_01_20170118.mod.txt
 1574  wc -l Tokyo_RAN_OSS-RC_01_20170118.txt
 1575  dc Python
 1576  cp ../../Src/UTF8_bom/OSS_Eri/Tokyo_RAN_OSS-RC_0* .
 1577  rm *.mod.txt
 1578  rm Tokyo_RAN_OSS-RC_01_20170116_100.txt
 1579  python read_ericsson.py test/Tokyo_RAN_OSS-RC_01_20170116.txt
 1580  vim read_ericsson.py
 1581  python read_ericsson.py test/Tokyo_RAN_OSS-RC_01_20170116.txthistory
 1582  vim Tokyo_RAN_OSS-RC_01_20170117.txt
 1583  vim ../Src/UTF8_bom/OSS_Eri/Tokyo_RAN_OSS-RC_01_20170118.txt
 1584  vim ../Src/UTF8_bom/OSS_Eri/Tokyo_RAN_OSS-RC_01_20170119.txt
 1585  vim ../Src/UTF8_bom/OSS_Eri/Tokyo_RAN_OSS-RC_01_20170120.txt
 1586  vim ../Src/UTF8_bom/OSS_Eri/Tokyo_RAN_OSS-RC_01_20170121.txt
 1587  /bin/bash
 1588  vim read_ericsson_step2.py
 1589  tail -f output_processed.csv
 1590  df -h
 1591  brew mc
 1592  cd /Users/212339410/Documents/Analytics/Projects/SB/Src/Original_flat
 1593  vim OSS_20160707_20160721.csv
 1594  for files in ./; do echo $files done\n\n;\n;\ndone
 1595  for files in ./; do; echo $files; done
 1596  for files in ./OSS_NOK*\ndo\necho $files\ndone
 1597  for files in ./OSS_NOK*\ndo\necho $files\nnew_file_name=`echo $files | sed 's+Original_flat+UTF8_bom+'`\n#ln -s $files $new_file_name\ndone
 1598  cd ../../../Data/MBS2016/
 1599  for files in ./OSS_NOK*\ndo\n#echo $files\nnew_file_name=`echo $files | sed 's+Original_flat+UTF8_bom+'`\necho $new_file_name\n#ln -s $files $new_file_name\ndone
 1600  for files in ./Original_flat/OSS_NOK*\ndo\n#echo $files\nnew_file_name=`echo $files | sed 's+Original_flat+UTF8_bom+'`\necho $new_file_name\n#ln -s $files $new_file_name\ndone
 1601  for files in ./Original_flat/OSS_NOK*\ndo\n#echo $files\nnew_file_name=`echo $files | sed 's+Original_flat+UTF8_bom+'`\necho $new_file_name\nln -s $files $new_file_name\ndone
 1602  for files in ./Original_flat/OSS_NOK*\ndo\n#echo $files\nnew_file_name=`echo $files | sed 's+Original_flat+UTF8_bom+'`\necho $new_file_name\nln $files $new_file_name\ndone
 1603  vim UTF8_bom/OSS_NOK_20170117-aa.csv
 1604  cp OSS_NOK_20170117-aa.csv OSS_NOK_20170117-aa.bak.csv
 1605  head -n 1 OSS_NOK_20170117-aa.bak.csv
 1606  sed -i '' '/SQL>/d' ./OSS_NOK_20170117-aa.csv
 1607  wc -l OSS_NOK_20170117-aa.bak.csv
 1608  wc -l OSS_NOK_20170117-aa.csv
 1609  vim OSS_NOK_20170117-ab.csv
 1610  cd ../../Projects/SB/Python/
 1611  python read_OSS_nok.py ../Src/Original_flat/OSS_NOK_20170117-aa.csv
 1612  python read_OSS_nok.py ../Src/Original_flat/OSS_NOK_20170117-aa.csv 
 1613  for files in ../Src/Original_flat/OSS_NOK_*\ndo\n#echo $files\nnew_file_name=`echo $files | sed 's+Original_flat+UTF8_bom+'`\necho $new_file_name\n#python read_OSS_nok.py $files >> $new_file_name\ndone
 1614  for files in ../Src/Original_flat/OSS_NOK_*\ndo\n#echo $files\nnew_file_name=`echo $files | sed 's+Original_flat+UTF8_bom+'`\necho $new_file_name\npython read_OSS_nok.py $files >> $new_file_name\nfjiasdj\nj\nendone\ndone
 1615  cd /Users/212339410/Documents/Analytics/Projects/SB/Src/UTF8_bom
 1616  vim OSS_NOK_20170117-aa.csv
 1617  for files in ../Src/Original_flat/OSS_NOK_*\ndo\n#echo $files\nnew_file_name=`echo $files | sed 's+Original_flat+UTF8_bom+'`\necho $new_file_name\npython read_OSS_nok.py $files >> $new_file_name\ndone
 1618  vim OSS_NOK_20170127-aa.csv
 1619  for files in ../Src/Original_flat/OSS_NOK_*\ndo\n#echo $files\nnew_file_name=`echo $files | sed 's+Original_flat+UTF8_bom+'`\necho $new_file_name\npython read_OSS_nok.py $files > $new_file_name\ndone
 1620  cd Documents/Analytics/Projects/SB/Src/
 1621  vim OSS_20170117_20170127.csv
 1622  cd Box/Analytics\ \(user\ 227311313\)
 1623  cd ../Documents/Analytics
 1624  pip install https://github.com/chokkan/simstring.git
 1625  sudo geproxy;sudo pip install simstring
 1626  sudo pip install simstring
 1627  conda help
 1628  pyenv
 1629  conda create -n python2 python=2.7 anaconda
 1630  source activate python2
 1631  alias py2 source deactivate python2
 1632  py2
 1633  man alias
 1634  cd /Users/212339410/Documents/Analytics/Projects/SB/Python
 1635  gunzip -c simstring-1.0.tar.gz| tar xopft - 
 1636  gunzip -c simstring-1.0.tar.gz | tar xopf - 
 1637  tar -zxvf simstring-1.0.tar.gz
 1638  ls simstring-1.0
 1639  cd simstring-1.0/frontend
 1640  make 
 1641  l ./frontend
 1642  cd ./frontend
 1643  vim ../include/simstring/memory_mapped_file.h
 1644  cd swig/python
 1645  ./prepare.sh
 1646  python setup.py build_ext
 1647  pip install simstring
 1648  pip install jupyter-themes
 1649  pip install git+https://github.com/dunovank/jupyter-themes.git
 1650  jupyter-theme -l
 1651  jt -t onedork
 1652  conda install scikit-learn
 1653  vim /Users/212339410/Downloads/Tokyo_RAN_OSS-RC_05_20170125.txt
 1654  pip uninstall tstk
 1655  ls -al /Users/212339410/anaconda/envs/py3k/lib/python3.5/site-packages/
 1656  ls -al /Users/212339410/anaconda/envs/py3k/lib/python3.5/site-packages/ | grep tstk
 1657  rm -r /Users/212339410/anaconda/envs/py3k/lib/python3.5/site-packages/tstk/*
 1658  rm -r /Users/212339410/anaconda/envs/py3k/lib/python3.5/site-packages/tstk/
 1659  rm -r /Users/212339410/anaconda/envs/py3k/lib/python3.5/site-packages/tstk
 1660  rm /Users/212339410/anaconda/envs/py3k/lib/python3.5/site-packages/tstk-0.1-py3.5.egg-info
 1661  rm -r /Users/212339410/anaconda/envs/py3k/lib/python3.5/site-packages/tstk-0.1-py3.5.egg-info
 1662  cd ~/Documents/Analytics/Projects/SB/
 1663  git clone git@github.build.ge.com:IndustrialDataScience/tstk.git tstk
 1664  cd Documents/
 1665  cd Analytics/Python/
 1666  ls -al 
 1667  locate python35
 1668  ln -s /Users/212339410/Documents/Analytics/Python/tstk /Users/212339410/anaconda/envs/py3k/lib/python3.5/site-packages/tstk 
 1669  vim test.py
 1670  python test.py
 1671  l /Users/212339410/anaconda/envs/py3k/lib/python3.5/site-packages/
 1672  l /Users/212339410/anaconda/envs/py3k/lib/python3.5/site-packages/ | grep
 1673  rm /Users/212339410/anaconda/envs/py3k/lib/python3.5/site-packages/tstk
 1674  l /Users/212339410/anaconda/envs/py3k/lib/python3.5/site-packages/ | grep tstk
 1675  ln -s /Users/212339410/Documents/Analytics/Python/tstk/tstk /Users/212339410/anaconda/envs/py3k/lib/python3.5/site-packages/tstk 
 1676  deactivate l /Users/212339410/anaconda/envs/py3k/lib/python3.5/site-packages/tstk
 1677  pip install pdfkit
 1678  jt -t onedork -T 
 1679  zsh 
 1680  zsh -h
 1681  -h
 1682  update
 1683  pip install hide_code
 1684  jt
 1685  jt -t chesterish
 1686  jt -h
 1687  jt -r 
 1688  jt -l
 1689  jt -t onedork -T
 1690  cd Documents/Analytics/Projects/SB/Python
 1691  pip install Orange
 1692  sudo pip install Orange
 1693  head OSS_20170117_20170127.csv
 1694  cd Documents/Analytics/Projects/SB
 1695  wc -l BSW2016_bom.csv
 1696  ls -al BSW2016_bom.csv
 1697  ls -al BSW*
 1698  ln -s -F /Users/212339410/Documents/Analytics/Projects/ /Users/212339410/Projects
 1699  ls -al ` locate tstk`
 1700  ln -s /Users/212339410/anaconda2/envs/py35/lib/python3.5/site-packages/tstk 
 1701  ln -s /Users/212339410/Documents/Analytics/Python/tstk/tstk /Users/212339410/anaconda2/envs/py35/lib/python3.5/site-packages/tstk 
 1702  cd ./Python
 1703  git clone https://github.com/litaotao/IPython-Dashboard.git 
 1704  cd IPython-Dashboard
 1705  pip install CofigParser
 1706  pip install configparser==3.3.0.post2
 1707  pip install configparser==3.5.0b2
 1708  pip install mysqlclient
 1709  dash-server
 1710  sudo nosetest --with-coverage --cover-package=dashboard
 1711  pip install ipython-dashboard
 1712  rm -rf OSS_Eri
 1713  cd ModData/
 1714  wc -l OSS_Nokia_bsid_20160707_20170127.csv
 1715  mssql -s alpidcdiha001v.cloud.ge.com -u 'dsfaâ -p 'villa1hermosa9' -d 'FemsaAlpla'
 1716  mssql -s alpidcdiha001v.cloud.ge.com -u 'dsfa' -p 'villa1hermosa9' -d 'FemsaAlplaa'
 1717  mssql -s alpidcdiha001v.cloud.ge.com -u 'dsfa' -p 'villa1hermosa9' -d 'FemsaAlpla'
 1718  mssql -s alpidcdiha001v.cloud.ge.com -u 'dsfa' -p '123' -d 'FemsaAlplaa'
 1719  mssql -s alpidcdiha001v.cloud.ge.com -u 'dsfa' -p '123' -d 'FemsaAlpha'
 1720  mssql -s alpidcdiha001v.cloud.ge.com -u 'dsfa' -p 'villa1hermosa9' -d 'FemsaAlpha'
 1721  python update_schema.py '../Src/ModData/BSW2016.csv' '../Src/Schema/bsw2016.xml' '../Src/Schema/bsw2016_mod.xml'
 1722  python update_schema.py '../Src/ModData/BSW2016.csv' '../Src/Schema/bsw2016_v1.xml' '../Src/Schema/bsw2016_mod.xml'
 1723  conda install -c r ipython-notebook r-irkernel
 1724  python update_schema.py '../Src/ModData/NEDB_NOKERI_TQ.utf8.csv' '../Src/Schema/nedb.xml'
 1725  python update_schema.py '../Src/ModData/NEDB_NOKERI_TQ.utf8.csv' '../Src/Schema/nedb.xml' '' ''
 1726  python update_schema.py '../Src/ModData/NEDB_NOKERI_TQ.utf8.csv' '../Src/Schema/nedb.xml' 
 1727  brew install pkg-config libffi openssl python
 1728  env LDFLAGS="-L$(brew --prefix openssl)/lib" CFLAGS="-I$(brew --prefix openssl)/include" pip install cryptography
 1729  fabmanager
 1730  pip install flask-appbuilder
 1731  fabmanager create-app
 1732  cd superset_demo
 1733  fabmanager create-admin
 1734  fabmanager run
 1735  rm -rf superset_demo
 1736  abmanager create-admin --app superset
 1737  pip install cryptography-1.5
 1738  pip install cryptography
 1739  pip install cryptography 1.5
 1740  pip install cryptography==1.5.0
 1741  pip install cryptography==1.7.2
 1742  ls -al ~/ | superset
 1743  ls -al ~/ | grep superset
 1744  locate sqlite
 1745  locate sqlite | superset.db
 1746  locate sqlite | grep superset.db
 1747  locate sqlite | grep superset
 1748  locate sqlite | grep db
 1749  locate sqlite | grep .db
 1750  locate sqlite | grep *\.db
 1751  locate sqlite | grep \.db
 1752  locate sqlite | grep ".db"
 1753  locate sqlite | grep ls
 1754  ls /opt/
 1755  fabmanager create-admin --app superset
 1756  pip uninstall superset
 1757  pip install --upgrade setuptools pip
 1758  pip install superset
 1759  pip install superset --upgrade
 1760  super
 1761  superset db upgrade
 1762  superset load_examples
 1763  superset init
 1764  jupyter
 1765  jupyter no
 1766  mv Untitled.ipynb preprocessing_oss_nok.ipynb
 1767  cd Ingest
 1768  ls / | grep OSS
 1769  ls /
 1770  rm OSS_NOK_20170207-aa._cl.csv
 1771  mv Untitled.ipynb preprocessing_tte.ipynb
 1772  cd ../Src/Original_flat/
 1773  head -n 1 OSS_20160707_20160721.csv
 1774  ld
 1775  cd ~/Box
 1776  ln -s Box "Box Sync"
 1777  cd Box\ Sync
 1778  cd "~/Box Sync"
 1779  cd ~/Box\ Sync
 1780  split
 1781  cp ../BSW_20160707_20160721.csv .
 1782  split BSW_20160707_20160721.csv 
 1783  man split
 1784  man -l 100 split
 1785  split -l 100 BSW_20160707_20160721.csv
 1786  cat xa* > bsw.csv
 1787  cat bsw.csv
 1788  tail bsw.csv
 1789  cl
 1790  rm -rf test
 1791  mv *.ipynb ../Notebook
 1792  add Bash/
 1793  git add Bash/
 1794  git add Notebook/
 1795  git push master origin 
 1796  cf cd builspxka
 1797  cf cd builspacks
 1798  cd ../R/App
 1799  https://github.com/pivotalsoftware/superzip.git
 1800  git clone https://github.com/pivotalsoftware/superzip.git 
 1801  cf buildpacks
 1802  mv preprocessing_oss_nok.ipynb preprocessing_oss_nok_20170128_20170220.ipynb
 1803  cp preprocessing_oss_nok_20170128_20170220.ipynb preprocessing_oss_nok_20170104 -20170127.ipynb
 1804  cp preprocessing_oss_nok_20170128_20170220.ipynb preprocessing_oss_nok_20170104-20170127.ipynb
 1805  cd UTF8_bom/Ingest
 1806  cd OSS_Nok_20170128_20170220
 1807  rm OSS_NOK_20170207-aa.csv OSS_NOK_20170207-aa_cl.csv OSS_NOK_20170207-ab.csv OSS_NOK_20170207-ab_cl.csv OSS_NOK_20170207-ac.csv OSS_NOK_20170207-ac_cl.csv OSS_NOK_20170207-ad.csv OSS_NOK_20170207-ad_cl.csv OSS_NOK_20170207-ae.csv OSS_NOK_20170207-ae_cl.csv OSS_NOK_20170207-af.csv OSS_NOK_20170207-af_cl.csv OSS_NOK_20170207-ag.csv OSS_NOK_20170207-ag_cl.csv OSS_NOK_20170207-ah.csv OSS_NOK_20170207-ah_cl.csv OSS_NOK_20170207-ai.csv OSS_NOK_20170207-ai_cl.csv OSS_NOK_20170217-aa.csv OSS_NOK_20170217-aa_cl.csv OSS_NOK_20170217-ab.csv OSS_NOK_20170217-ab_cl.csv OSS_NOK_20170217-ac.csv OSS_NOK_20170217-ac_cl.csv OSS_NOK_20170217-ad.csv OSS_NOK_20170217-ad_cl.csv OSS_NOK_20170217-ae.csv OSS_NOK_20170217-ae_cl.csv OSS_NOK_20170217-af.csv OSS_NOK_20170217-af_cl.csv OSS_NOK_20170217-ag.csv OSS_NOK_20170217-ag_cl.csv OSS_NOK_20170217-ah.csv OSS_NOK_20170217-ah_cl.csv OSS_NOK_20170217-ai.csv OSS_NOK_20170217-ai_cl.csv 
 1808  cp preprocessing_oss_nok_20170104-20170127.ipynb preprocessing_wot_20151201_20170201.ipynb
 1809  head -n 1 /Users/212339410/Documents/Analytics/Data/MBS2016/UTF8_bom/WOT_20160707_20160721.utf8.csv
 1810  rm preprocessing_wot.ipynb
 1811  mv TTE_ERI_20151201_20170201.utf8.csv ./Ingest/
 1812  mv TTE_NOK_20151201_20170201.utf8.csv ./Ingest
 1813  cp preprocessing_wot_20151201_20170201.ipynb preprocessing_tte_20151201_20170201.ipynb
 1814  cp preprocessing_wot_20151201_20170201.ipynb preprocessing_bsw_20151201_20170201.ipynb
 1815  cp Projects/SB/Notebook/preprocessing_oss_nok_20170104_20170127.ipynb 
 1816  cd Projects/SB/Notebook/
 1817  cp preprocessing_oss_nok_20170104_20170127.ipynb preprocessing_oss_nok_20160707_20160828.ipynb
 1818  python update_schema.py '../Src/UTF8_bom/Ingest/tte_20151201_20170201.csv' '../Src/Schema/tte2016_original.xml'  
 1819  python update_schema.py '../Src/UTF8_bom/Ingest/tte_20151201_20170201.csv' '../Src/Schema/tte2016_original.xml'  '../Src/Schema/tte2016_mod.xml'
 1820  ls '~/Box Sync/'
 1821  l ~/Box\ Sync/DSS\ Internal-Softbank/12\ Working\ Directory\ for\ Ingestion/20170306_Ingestion/
 1822  python update_schema.py '../Src/UTF8_bom/Ingest/wot_20151201_20170201.csv' '../Src/Schema/wot2016_original.xml'  '../Src/Schema/wot2016_mod.xml'
 1823  python update_schema.py '/Users/212339410/Box/DSS Internal-Softbank/12 Working Directory for Ingestion/20170306_Ingestion/Output/wot_20151201_20170201.csv' '../Src/Schema/wot2016_original.xml'  '../Src/Schema/wot2016_mod.xml'
 1824  ln /Users/212339410/Box/DSS\ Internal-Softbank/12\ Working\ Directory\ for\ Ingestion 
 1825  ln -s /Users/212339410/Box/DSS\ Internal-Softbank/12\ Working\ Directory\ for\ Ingestion /Users/212339410/SB_ingest_working_directory
 1826  cd OSS_Nok_20160707_20160828
 1827  head -n 2 OSS_20160707_20160721.utf8.csv
 1828  rm _mod.xml 
 1829  python update_schema.py '/Users/212339410/Box/DSS Internal-Softbank/12 Working Directory for Ingestion/20170306_Ingestion/Output/oss_nok_all_20160707_20160828.csv' '../Src/Schema/oss_nok_original.xml'  '../Src/Schema/oss_nok_mod.xml'
 1830  cd Projects/SB/P
 1831  python update_schema.py '/Users/212339410/Box/DSS Internal-Softbank/12 Working Directory for Ingestion/20170306_Ingestion/Output/oss_nok_all_20170128_20170220.csv' '../Src/Schema/oss_nok_mod_v2.xml'  '../Src/Schema/oss_nok_mod_v3.xml'
 1832  cd R/App/
 1833  grproxy
 1834  cf env dashboard_v1.R
 1835  cd cf-r-shiny-app-master
 1836  cf push dashboard_v1.R -m 500m -k 1000m -b http://github.com/alexkago/cf-buildpack-r.git
 1837  cf push start_shiny_app.R -m 500m -k 1000m -b http://github.com/alexkago/cf-buildpack-r.git
 1838  head -n 2 bsw_20151201_20170201.csv
 1839  wc -l oss_nok_all_20160707_20160828.csv
 1840  wc -l oss_nok_all_20170104_20170127.csv
 1841  wc -l oss_nok_all_20170128_20170220.csv
 1842  python update_schema.py '/Users/212339410/Box/DSS Internal-Softbank/12 Working Directory for Ingestion/20170306_Ingestion/Output/oss_nok_all_20170128_20170220.csv' '../Src/Schema/oss_nok_mod_v3.xml'  '../Src/Schema/oss_nok_mod_v4.xml'
 1843  python update_schema.py '/Users/212339410/Box/DSS Internal-Softbank/12 Working Directory for Ingestion/20170306_Ingestion/Output/oss_nok_all_20170104_20170127.csv' '../Src/Schema/oss_nok_mod_v4.xml'  '../Src/Schema/oss_nok_mod_v4.xml'
 1844  ln -s /Users/212339410/anaconda/envs/py3k/lib/python3.5/site-packages/tstk
 1845  ln -s /Users/212339410/Documents/Analytics/Python/tstk/tstk /Users/212339410/anaconda/envs/py3k/lib/python3.5/site-packages/tstk
 1846  ls -al /Users/212339410/anaconda/envs/py3k/lib/python3.5/site-packages/tstk
 1847  ls -al /Users/212339410/anaconda2/envs/py35/lib/python3.5/site-packages/tstk/
 1848  ls -al /Users/212339410/anaconda2/envs/py35/lib/python3.5/site-packages/tstk
 1849  ls -al /Users/212339410/anaconda2/envs/py35/lib/python3.5/site-packages/tstk/visualization
 1850  less /Users/212339410/anaconda2/envs/py35/lib/python3.5/site-packages/tstk/visualization/plot.py
 1851  git getch
 1852  python update_schema.py '../Src/ModData/NEDB_NOKERI_TQ.utf8.csv' '../Src/Schema/nedb.xml'  '../Src/Schema/nedb_mod.xml'
 1853  python update_schema.py '../Src/UTF8_bom/NEDB_BASE_INFO.20160721085837_2.utf8.csv' '../Src/Schema/nedb.xml'  '../Src/Schema/nedb_mod.xml'
 1854  python update_schema.py '../Src/UTF8_bom/NEDB_BASE_INFO.20160721085837_2.utf8.csv' '../Src/Schema/nedb_summer.xml'  '../Src/Schema/nedb_mod.xml'
 1855  vim update_schema.py
 1856  vim ../Src/Schema/nedb_summer.xml
 1857  python update_schema.py '../Src/UTF8_bom/NEDB_BASE_INFO.20160721085837_2.utf8.csv' '../Src/Schema/nedb_summer.xml'  '../Src/Schema/nedb_summer_mod.xml'
 1858  python update_schema.py '../Src/ModData/NEDB_NOKERI_TQ.utf8_st_drop.csv' '../Src/Schema/nedb_summer_mod.xml'  '../Src/Schema/nedb_summer_mod_v2.xml'
 1859  head -n 2 ../Src/ModData/NEDB_NOKERI_TQ.utf8_st_drop.csv
 1860  vim ../Src/ModData/NEDB_NOKERI_TQ.utf8_st_drop.csv
 1861  python convert_utf8.py ../Src/ModData/NEDB_NOKERI_TQ.utf8_st_drop.csv ../Src/ModData/NEDB_NOKERI_TQ.utf8_st_drop.utf8.csv
 1862  vim ../Src/ModData/NEDB_NOKERI_TQ.utf8.csv
 1863  vim "../Src/ModData/NEDB_NOKERI_TQ.utf8.csv"
 1864  python update_schema.py '../Src/ModData/NEDB_NOKERI_TQ.utf8.csv' '../Src/Schema/nedb_jan.xml'  '../Src/Schema/nedb_jan_mod.xml'
 1865  cd /Users/212339410
 1866  cd ./.atom/storage/
 1867  vim application.json
 1868  ls al
 1869  ls -al .
 1870  cp preprocessing_oss_nok_20170128_20170220.ipynb preprocessing_oss_nok_20170214_20170227.ipynb
 1871  cp preprocessing_bsw_20151201_20170201.ipynb preprocessing_bsw_20170201_20170301.ipynb
 1872  cd Box/DSS\ Internal-Softbank/11\ Processed\ Dataset/201702_OSS\ \(Jan\)
 1873  vim OSS_Ericsson_processed_20171116_20170127.csv
 1874  cd Box/DSS\ Internal-Softbank
 1875  cd 11\ Processed\ Dataset/201702_OSS\ \(Jan\)
 1876  head OSS_Ericsson_processed_20171116_20170127.csv
 1877  head OSS_Ericsson_processed_20171116_20170127.csv -> tmp.csv
 1878  head OSS_Ericsson_processed_20171116_20170127.csv > tmp.csv
 1879  cd ../Python/
 1880  python update_schema.py ~/Box/DSS\ Internal-Softbank/12\ Working\ Directory\ for\ Ingestion/20170310_Ingestion/Output/bsw_20170201_20170301.csv '../Src/Schema/bsw0301.xml'  '../Src/Schema/bsw0301_mod.xml'
 1881  cd ../../12\ Working\ Directory\ for\ Ingestion
 1882  cd 20170310_Ingestion
 1883  cd ../../20170306_Ingestion/Output
 1884  vim bsw_20151201_20170201.csv
 1885  cd ../../20170310_Ingestion/
 1886  vim bsq
 1887  vim bsw_20170201_20170301.csv
 1888  python update_schema.py ~/Box/DSS\ Internal-Softbank/12\ Working\ Directory\ for\ Ingestion/20170310_Ingestion/Output/bsw_20170201_20170301.csv '../Src/Schema/bsw2016_v4_mod_tmap.xml'  '../Src/Schema/bsw2016_v5.xml'
 1889  wc -l oss_nok_all_20170214_20170227.csv
 1890  vim ../../20170310_Ingestion/Output
 1891  git push pull
 1892  ls ~/Projects
 1893  ls -al ~/Projects
 1894  vim ../../20170310_Ingestion/Output/OSS_Eri_20170128_20170220.csv
 1895  vim ../Output/oss_nok_all_20170214_20170227.csv
 1896  head -n 10000 OSS_Eri_20170128_20170220.csv > ~/Projects/SB/Src/ModData/oss_eri_20170128_10000.csv
 1897  vim OSS_Eri_20170128_20170220.csv
 1898  cd Original_flat
 1899  cd 20170209_20170215_YM_logs
 1900  md5 OSS_Eri_20170221_20170228.csv
 1901  md5 OSS_Eri_20170128_20170220.csv
 1902  pip install XlsxWriter
 1903  git commit -m 'merge changes for preprocessing_oss_eri'
 1904  cp feature_nok_0314.csv feature_nok_0314_bom.csv
 1905  vim feature_nok_0314_bom.csv
 1906  cd ~/212339410@SFO1212339410M:~/Projects/SB/Src/Original_flat/
 1907  head -n 10000 20170209_0610_SB_nodelist2000.log > internal_log_sample.txt
 1908  head -n 100000 20170209_0610_SB_nodelist2000.log > internal_log_sample.txt
 1909  grep readclock internal_log_sample.txt
 1910  grep temp internal_log_sample.txt
 1911  cp NEDB_BASE_INFO.20160721085837_2.csv wc -l NEDB_BASE_INFO.20160721085837_2.csv
 1912  wc -l NEDB_BASE_INFO.20160721085837_2.csv
 1913  wc -l NEDB_ERI_T.csv
 1914  wc -l NEDB_NOK_Q.csv
 1915  cd ../ModData
 1916  open bsw_prob_area_eri.csv
 1917  vim bsw_prob_area_eri.csv
 1918  vim bsw_prob_area_eri_2017.csv
 1919  head SB_internal_log_201702.csv
 1920  head -n 1 SB_internal_log_201702.csv
 1921  vim 20170209_20170215_SB_logs_0309_1929/20170209_20170215_SB_logs/20170209_0610_SB_nodelist2000.log
 1922  cd ../Src/Original_flat/20170309_Internal\ Logs
 1923  cd 20170209_20170215_SB_logs_0309_1929/20170209_20170215_SB_logs
 1924  head -n 2 SB_internal_log_201702.csv
 1925  head -n 13 SB_internal_log_201702.csv
 1926  cls
 1927  open SB_internal_log_201702_fuigettemp.csv
 1928  open SB_internal_log_201702_mbs_PC145U.csv 
 1929  cf 
 1930  head  feature_eri_0315.csv
 1931  head  feature_eri_raw2_0315.csv
 1932  head  feature_eri_raw_0315.csv
 1933  head  -n 1 feature_eri_raw_0315.csv
 1934  head  -n 2 feature_eri_raw_0315.csv
 1935  date; fc read
 1936  readlink
 1937  date; ls
 1938  vim 20170209_0610_YM_nodelist1000.log
 1939  grep sfp 20170209_0610_YM_nodelist1000.log > 20170209_0610_YM_nodelist1000_sfp.log
 1940  less 20170209_0610_YM_nodelist1000_sfp.log
 1941  grep temp 20170209_0610_YM_nodelist1000.log > 20170209_0610_YM_nodelist1000_sfp.log
 1942  less 20170209_0610_YM_nodelist1000_temp.log
 1943  grep temp 20170209_0610_YM_nodelist1000.log > 20170209_0610_YM_nodelist1000_temp.log
 1944  grep clock 20170209_0610_YM_nodelist1000.log > 20170209_0610_YM_nodelist1000_clock.log
 1945  less 20170209_0610_YM_nodelist1000_clock.log
 1946  rm 20170209_0610_YM_nodelist1000_clock.log
 1947  zip df_event_oss.csv
 1948  grep 33720 BSW2016.csv 
 1949  grep 33720 BSW2016.csv  > tmp.csv
 1950  vim tmp.csv
 1951  open(tmp.csv)
 1952  open tmp.csv
 1953  python3
 1954  vim 20170208_1355_SB_nodelist100.log
 1955  vim RESURLT_20170208_1355_SB_nodelist100.log
 1956  vim prob_area_nok_after0317.csv
 1957  vim prob_area_nok_before0317.csv
 1958  less select_\*_from_event_view_oss_nok.csv
 1959  cd Projects/SB/Src/Original_flat
 1960  cd 20170309_Internal\ Logs
 1961  cd 20170209_20170215_SB_logs_0309_1929
 1962  cd 20170209_20170215_SB_logs
 1963  vim 20170209_0610_SB_nodelist2000.log
 1964  grep fault 20170209_0610_SB_nodelist2000.log 
 1965  grep fault 20170209_0610_SB_nodelist2000.log  | grep 34428
 1966  cd ../20170209_20170215_YM_logs
 1967  vim 20170209_0610_YM_nodelist1000_sfp.log
 1968  vim run.sh
 1969  rm run.sh
 1970  cd ../../20170309_Internal\ Logs
 1971  python confmat_mbs_nok.py 
 1972  python confmat_mbs_nok.py -h
 1973  python confmat_mbs_nok.py
 1974  python preprocessing_nok.py 
 1975  cd ../Src/Original_flat/20170309_Internal\ Logs/20170209_20170215_SB_logs_0309_1929/20170209_20170215_SB_logs
 1976  mv training_nok.py eval_model_nok.py
 1977  python eval_model_nok.py 
 1978  mv training_nok.py eval_model_nok.py --save_flag True --use_cache True --input_file ../Src/ModData/feature__nok_0320.csv
 1979  python eval_model_nok.py --save_flag True --use_cache True --input_file ../Src/ModData/feature__nok_0320.csv
 1980  cd Projects/SB/cache
 1981  head select_\*_from_bswn.csv
 1982  head -n 1 select_\*_from_bswn.csv
 1983  head -n 2 select_\*_from_bswn.csv
 1984  head -n 2 select_\*_from_event_view_bswn.csv
 1985  head -n 2 select_\*_from_event_view_oss_nok.csv
 1986  python preprocessing_nok.py
 1987  python preprocessing_nok.py -sample_days 30 -o test
 1988  python preprocessing_nok.py --sample_days 30 --o test
 1989  cd Projects/SB/Src/Original_flat/
 1990  less RESURLT_20170208_1355_SB_nodelist100.log
 1991  python preprocessing_nok.py -h
 1992  cd Projects/SB/Src
 1993  cd ModData
 1994  less df_event_uh_proc_test_nok_lbh14_lah1_0321.csv
 1995  python eval_model_nok.py -i "../Src/ModData/feature_select_nok_0321.csv" -o "test" 
 1996  python eval_model_nok.py --i "../Src/ModData/feature_select_nok_0321.csv" --o "test" 
 1997  python eval_model_nok.py --i "../Src/ModData/feature_nok_0321.csv" --o "test" 
 1998  python eval_model_nok.py --i "../Src/ModData/feature_select_nok_0321.csv" --o "test" --target Urgent
 1999  python eval_model_nok.py --i "../Src/ModData/feature_select_nok_0321.csv" --o "test" --target 'Urgent'
 2000  python eval_model_nok.py --i "../Src/ModData/feature_select_nok_0321.csv" --o "test" --target 'Prob_Urgent'
 2001  ls | grep df
 2002  python eval_model_nok.py --i "../Src/ModData/feature_select_nok_0321.csv" --o "../Src/ModData/" --target 'Prob_Urgent'
 2003  python eval_model_nok.py --i "../Src/ModData/feature_select_nok_0321.csv" --o "../Src/ModData/select_" --target 'Prob_Urgent'
 2004  python eval_model_nok.py --i "../Src/ModData/feature_nok_0321.csv" --o "../Src/ModData/" --target 'Prob_Urgent' --n_healthy '500, 1000'
 2005  open ../Src/ModData/df_result_rf_nok_0321.csv
 2006  python eval_model_nok.py --i "../Src/ModData/feature_nok_0321.csv" --o "../Src/ModData/" --target 'Prob_Urgent' --n_healthy '500, 1000, 3000, 5000' 
 2007  python eval_model_nok.py -h
 2008  python eval_model_nok.py --i "../Src/ModData/feature_nok_0321.csv" --o "../Src/ModData/" --target 'Prob_Urgent' --n_healthy '500, 1000, 3000' --h  
 2009  python eval_model_nok.py --i "../Src/ModData/feature_nok_0321.csv" --o "../Src/ModData/" --target 'Prob_Urgent' --n_healthy '500, 1000, 3000'   
 2010  python eval_model_nok.py --i "../Src/ModData/feature_nok_0321.csv" -o "../Src/ModData/" -target 'Prob_Urgent' --n_healthy '500, 1000, 3000'   
 2011  python eval_model_nok.py -i "../Src/ModData/feature_nok_0321.csv" -o "../Src/ModData/" --target 'Prob_Urgent' --n_healthy '500, 1000, 3000'  --n_unhealthy '500'
 2012  python eval_model_nok.py -i "../Src/ModData/feature_nok_0321.csv" -o "../Src/ModData/" --target 'Prob_Urgent' --n_healthy '500, 1000, 3000'   
 2013  python eval_model_nok.py -i "../Src/ModData/feature_nok_0321.csv" -o "../Src/ModData/" --target 'Prob_Urgent' --n_healthy '500, 1000, 3000'   --top_problem "'', 'FSME', 'Rectifier', 'FSMF', 'Breaker', 'FRGP', 'FXDA', 'BATT', 'FeederCable', 'OpticalCable', 'non-urgent', 'Others'"
 2014  less ../Src/ModData/feature_eri_v2_0316.csv
 2015  python eval_model_nok.py -i "../Src/ModData/feature_eri_v2_0316.csv" -o "../Src/ModData/" --target 'Prob_Urgent' --n_healthy '500, 1000, 3000'   --top_problem "'', 'DUW', 'DUL', 'AC', 'RRUS', 'Rectifier', 'RUS', 'BATT', 'DUS', 'OpticalCable', 'è¨ç»åé»ã«ä¼´ãé»æºä¾çµ¦',"
 2016  python eval_model_nok.py -i "../Src/ModData/feature_eri_v2_0316.csv" -o "../Src/ModData/" --target 'Prob_Urgent' --n_healthy '500, 1000, 3000'   --top_problem "'', 'DUW', 'DUL', 'AC', 'RRUS', 'Rectifier', 'RUS', 'BATT', 'DUS', 'OpticalCable', 'è¨ç»åé»ã«ä¼´ãé»æºä¾çµ¦'"
 2017  python eval_model_nok.py -i "../Src/ModData/feature_eri_v2_0316.csv" -o "../Src/ModData/" --target 'Prob_Urgent' --n_healthy '500, 1000, 3000'   --top_problem " 'DUW', 'DUL', 'RRUS', 'Rectifier', 'RUS', 'BATT', 'DUS', 'OpticalCable''"
 2018  python eval_model_nok.py -i "../Src/ModData/feature_eri_v2_0316.csv" -o "../Src/ModData/" --target 'Prob_Urgent' --n_healthy '500, 1000, 3000'   --top_problem " 'DUW', 'DUL', 'RRUS', 'Rectifier', 'RUS', 'BATT', 'DUS', 'OpticalCable'"
 2019  python eval_model_nok.py -i "../Src/ModData/feature_eri_v2_0316.csv" -o "../Src/ModData/" --target 'Prob_Urgent' --n_healthy '500, 1000, 3000'   --top_problem " 'DUW', 'DUL', 'RRUS', 'Rectifier', 'RUS', 'BATT', 'DUS', 'OpticalCable', 'Urgent'"
 2020  cp ../Src/ModData/feature_eri_v2_0316.csv ../Src/ModData/feature_eri_0321.csv
 2021  vim ../Src/ModData/feature_eri_0321.csv
 2022  python eval_model_nok.py -i "../Src/ModData/feature_eri_0321.csv" -o "../Src/ModData/" --target 'Prob_Urgent' --n_healthy '500, 1000, 3000'   --top_problem " 'DUW', 'DUL', 'RRUS', 'Rectifier', 'RUS', 'BATT', 'DUS', 'OpticalCable', 'Urgent'" --vendor 'eri'
 2023  python eval_model_nok.py -i "../Src/ModData/feature_eri_0321.csv" -o "../Src/ModData/" --target 'Prob_Urgent' --n_healthy '500, 1000, 3000'   --top_problem " 'DUW', 'DUL', 'RRUS', 'Rectifier', 'RUS', 'BATT', 'DUS', 'OpticalCable', 'Urgent'"
 2024  python eval_model_nok.py -i "../Src/ModData/feature_nok_0321.csv" -o "../Src/ModData/" --target 'Prob_Urgent' --n_healthy '500, 1000, 3000'   --top_problem "'FSME', 'Rectifier', 'FSMF', 'Breaker', 'FRGP', 'FXDA', 'BATT', 'FeederCable', 'OpticalCable', 'Urgent', 'Others'"
 2025  python eval_model_nok.py -i "../Src/ModData/feature_select_nok_0321.csv" -o "../Src/ModData/select" --target 'Prob_Urgent' --n_healthy '500, 1000, 3000'   --top_problem "'FSME', 'Rectifier', 'FSMF', 'Breaker', 'FRGP', 'FXDA', 'BATT', 'FeederCable', 'OpticalCable', 'Urgent', 'Others'"
 2026  mv eval_model_nok.py eval_model_rf.py
 2027  python eval_model.py -i "../Src/ModData/feature_nok_0321.csv" -o "../Src/ModData/" --target 'Prob_Urgent' --n_healthy '500, 1000, 3000'   --top_problem "'FSME', 'Rectifier', 'FSMF', 'Breaker', 'FRGP', 'FXDA', 'BATT', 'FeederCable', 'OpticalCable', 'Urgent', 'Others'"
 2028  python eval_model_rf.py -i "../Src/ModData/feature_nok_0321.csv" -o "../Src/ModData/" --target 'Prob_Urgent' --n_healthy '500, 1000, 3000'   --top_problem "'FSME', 'Rectifier', 'FSMF', 'Breaker', 'FRGP', 'FXDA', 'BATT', 'FeederCable', 'OpticalCable', 'Urgent', 'Others'"
 2029  python eval_model_dt.py -i "../Src/ModData/feature_nok_0321.csv" -o "../Src/ModData/" --target 'Prob_Urgent' --n_healthy '500, 1000, 3000'   --top_problem "'FSME', 'Rectifier', 'FSMF', 'Breaker', 'FRGP', 'FXDA', 'BATT', 'FeederCable', 'OpticalCable', 'Urgent', 'Others'"
 2030  python eval_model_dt.py -i "../Src/ModData/feature_select_nok_0321.csv" -o "../Src/ModData/select_" --target 'Prob_Urgent' --n_healthy '500, 1000, 3000'   --top_problem "'FSME', 'Rectifier', 'FSMF', 'Breaker', 'FRGP', 'FXDA', 'BATT', 'FeederCable', 'OpticalCable', 'Urgent', 'Others'"
 2031  onalData:_id_,_unitId_,_portNo_,_cause_-()_(Mixed_Mode)_The_alarm_is_currently_toggling.","Link_Stability@Temporary_not_in_operation,_additionalData:_id_,_unitId_,_portNo_,_cause_BER_LOS()","Link_Stability@Temporary_not_in_operation,_additionalData:_id_,_unitId_,_portNo_,_cause_BER_LOS()_(Mixed_Mode)","Link_Stability@Temporary_not_in_operation,_additionalData:_id_,_unitId_,_portNo_,_cause_BER_LOS()_(Mixed_Mode)_The_alarm_is_currently_toggling.","Link_Stability@Temporary_not_in_operation,_additionalData:_id_,_unitId_,_portNo_,_cause_BER_LOS()_The_alarm_is_currently_toggling.","Link_Stability@Temporary_not_in_operation,_additionalData:_id_,_unitId_,_portNo_,_cause_RAI_received()_(Mixed_Mode)","Link_Stability@Temporary_not_in_operation,_additionalData:_id_,_unitId_,_portNo_,_cause_RAI_received()_(Mixed_Mode)_The_alarm_is_currently_toggling.","Link_Stability@Temporary_not_in_operation,_additionalData:_id_,_unitId_,_portNo_,_cause_SFP_LOS()_(Mixed_Mode)","Link_Stability@Temporary_not_in_operation,_additionalData:_id_,_unitId_,_portNo_,_cause_SFP_not_connected()_(Mixed_Mode)","Link_Stability@Temporary_not_in_operation,_additionalData:_id_,_unitId_,_portNo_,_cause_version/revision_mismatch()_(Mixed_Mode)","Link_Stability@Temporary_not_in_operation,_additionalData:_id_,_unitId_,_portNo_,_cause_version/revision_mismatch()_(Mixed_Mode)_The_alarm_is_currently_toggling.",Link_Stability@This_alarm_is_generated_by_FM_Kernel_to_Clear_the_uncorrelated_alarm,"Link_Stability@Unexpected_values_on_Control_Words_in_CPRI_protocol,_additionalData:_id_,_unitId_,_portNo","Link_Stability@Unexpected_values_on_Control_Words_in_CPRI_protocol,_additionalData:_id_,_unitId_,_portNo_(Mixed_Mode)","Link_Stability@Unexpected_values_on_Control_Words_in_CPRI_protocol,_additionalData:_id_,_unitId_,_portNo_(Mixed_Mode)_The_alarm_is_currently_toggling.","Link_Stability@Unexpected_values_on_Control_Words_in_CPRI_protocol,_additionalData:_id_,_unitId_,_portNo_The_alarm_is_currently_toggling.",Loss_of_Sync_over_CPRI_Connections@,Loss_of_Sync_over_CPRI_Connections@This_alarm_is_generated_by_FM_Kernel_to_Clear_the_uncorrelated_alarm,MBSNo_XXXX_,MixedModeLicenseNotValid@,MixedModeLicenseNotValid@This_alarm_is_generated_by_FM_Kernel_to_Clear_the_uncorrelated_alarm,MsmmRadioPhaseNotAligned@,MsmmRadioPhaseNotAligned@This_alarm_is_generated_by_FM_Kernel_to_Clear_the_uncorrelated_alarm,NGSM_Configuration_Fault@More_than_one_node_has_the_same_Node_Priority,NGSM_Configuration_Fault@This_alarm_is_generated_by_FM_Kernel_to_Clear_the_uncorrelated_alarm,NTP_System_Time_Sync_Fault@NTP_sync_alarm,NTP_System_Time_Sync_Fault@NTP_sync_fault,NTP_System_Time_Sync_Fault@This_alarm_is_generated_by_FM_Kernel_to_Clear_the_uncorrelated_alarm,NbirLicenseNotValid@,NbirLicenseNotValid@This_alarm_is_generated_by_FM_Kernel_to_Clear_the_uncorrelated_alarm,No_Connection@(Mixed_Mode),No_Connection@AlarmResourceId:NoProcessing,No_Connection@AlarmResourceId:NoProcessing_eriAlarmNObjResourceId,No_Connection@Contact_lost_with_resource_object,No_Connection@This_alarm_is_generated_by_FM_Kernel_to_Clear_the_uncorrelated_alarm,No_Connection@Timeout:_Failed_to_get_anuConnectIndication,No_Connection@eriAlarmNObjResourceId:,None@AlarmResourceId:NoProcessing,None@AlarmResourceId:NoProcessing_eriAlarmNObjResourceId,None@This_alarm_is_generated_by_FM_Kernel_to_Clear_the_uncorrelated_alarm,"None@eriAlarmNObjMoreAdditionalText:No_SFP_plugged_in,_additionalData:_id_,_unitId_,",Number_of_HW_Entities_Mismatch@AlarmResourceId:NoProcessing,Number_of_HW_Entities_Mismatch@This_alarm_is_generated_by_FM_Kernel_to_Clear_the_uncorrelated_alarm,Number_of_HW_Entities_Mismatch@eriAlarmNObjResourceId:,PLMN_Service_Degraded@PLMN_mcc:_mnc:,PLMN_Service_Degraded@This_alarm_is_generated_by_FM_Kernel_to_Clear_the_uncorrelated_alarm,PLMN_Service_Degraded@eriAlarmNObjResourceId:,PLMN_Service_Unavailable@AlarmResourceId:NoProcessing,PLMN_Service_Unavailable@PLMN_mcc:_mnc:,"PLMN_Service_Unavailable@PLMN_mcc:_mnc:,_PLMN_mcc:_mnc:",PLMN_Service_Unavailable@PLMN_mcc:_mnc:_The_alarm_is_currently_toggling.,PLMN_Service_Unavailable@This_alarm_is_generated_by_FM_Kernel_to_Clear_the_uncorrelated_alarm,PLMN_Service_Unavailable@eriAlarmNObjResourceId:,Packet_Server_Availability_Fault@,"PciConflictDetected@PCI_Alarm,_the_same_PCI_value_has_been_reused_for_cells_that_could_interfere_with_each_other.[name:_--,_cgi:","PciConflictDetected@PCI_Alarm,_the_same_PCI_value_has_been_reused_for_cells_that_could_interfere_with_each_other.[name:_--,_cgi:_////,","PciConflictDetected@PCI_Alarm,_the_same_PCI_value_has_been_reused_for_cells_that_could_interfere_with_each_other.[name:_--,_cgi:_////,_eNodeB:","PciConflictDetected@PCI_Alarm,_the_same_PCI_value_has_been_reused_for_cells_that_could_interfere_with_each_other.[name:_--,_cgi:_////,_eNodeB:_]",Power_Failure_Left_Slot@,RF_Reflected_Power_High@Reflected_power_too_high_[_A_],Remote_IP_Address_Unreachable@,Remote_IP_Address_Unreachable@This_alarm_is_generated_by_FM_Kernel_to_Clear_the_uncorrelated_alarm,Resource_Activation_Timeout@,Resource_Activation_Timeout@Cyclic_Resetup_detected,Resource_Activation_Timeout@Cyclic_Resetup_detected_The_alarm_is_currently_toggling.,Resource_Activation_Timeout@Resource_Activation_Timeout,Resource_Activation_Timeout@This_alarm_is_generated_by_FM_Kernel_to_Clear_the_uncorrelated_alarm,Resource_Allocation_Failure@Configuration_not_valid_due_to_Channel_Bandwidth_license_shortage,Resource_Allocation_Failure@Failed_to_allocate_path,Resource_Allocation_Failure_Service_Degraded@Insufficient_RF_power_Hardware_Activation_Codes,Resource_Allocation_Failure_Service_Degraded@Insufficient_RF_power_in_radio_equipment,Resource_Allocation_Failure_Service_Degraded@Insufficient_RF_power_in_radio_equipment_The_alarm_is_currently_toggling.,Resource_Allocation_Failure_Service_Degraded@No_RF_power_Hardware_Activation_Code_available,Resource_Allocation_Failure_Service_Degraded@No_RF_power_available_in_radio_equipment,Resource_Allocation_Failure_Service_Degraded@No_RF_power_available_in_radio_equipment_The_alarm_is_currently_toggling.,Resource_Allocation_Failure_Service_Degraded@This_alarm_is_generated_by_FM_Kernel_to_Clear_the_uncorrelated_alarm,Resource_Configuration_Failure@Max_PUCCH_PRB_Pairs_per_BBM_exceeded.,SFP_Not_Present@AlarmResourceId:NoProcessing,SFP_Not_Present@This_alarm_is_generated_by_FM_Kernel_to_Clear_the_uncorrelated_alarm,"SFP_Not_Present@eriAlarmNObjMoreAdditionalText:No_SFP_plugged_in,_additionalData:_id_,_unitId","SFP_Not_Present@eriAlarmNObjMoreAdditionalText:No_SFP_plugged_in,_additionalData:_id_,_unitId_,","SFP_Not_Present@eriAlarmNObjMoreAdditionalText:No_SFP_plugged_in,_additionalData:_id_,_unitId_,_portNo",SFP_Not_Present@eriAlarmNObjResourceId:,"SFP_Stability_Problem@SFP_VCC-LOW_alarm,_additionalData:_id_,_unitId_,_portNo","SFP_Stability_Problem@SFP_VCC-LOW_alarm,_additionalData:_id_,_unitId_,_portNo_(Mixed_Mode)",SFP_Stability_Problem@This_alarm_is_generated_by_FM_Kernel_to_Clear_the_uncorrelated_alarm,SW_Error@This_alarm_is_generated_by_FM_Kernel_to_Clear_the_uncorrelated_alarm,SW_Error@eriAlarmNObjResourceId:,Service_Degraded@,Service_Degraded@Cyclic_Resetup_detected,Service_Degraded@Cyclic_Resetup_detected_The_alarm_is_currently_toggling.,Service_Degraded@Failed_to_allocate_path!,Service_Degraded@Failed_to_allocate_path!_The_alarm_is_currently_toggling.,Service_Degraded@InconsistentConfiguration,"Service_Degraded@Requested_TX-power_is_not_available_totalPowermW_=_.,_availPowermW_=_.",Service_Degraded@The_alarm_is_currently_toggling.,Service_Degraded@This_alarm_is_generated_by_FM_Kernel_to_Clear_the_uncorrelated_alarm,Service_Unavailable@,Service_Unavailable@Failed_to_allocate_path!,Service_Unavailable@Failed_to_allocate_path!_The_alarm_is_currently_toggling.,Service_Unavailable@InconsistentConfiguration,Service_Unavailable@InconsistentConfiguration_The_alarm_is_currently_toggling.,Service_Unavailable@The_alarm_is_currently_toggling.,Service_Unavailable@This_alarm_is_generated_by_FM_Kernel_to_Clear_the_uncorrelated_alarm,Service_Unavailable@eriAlarmNObjResourceId:,SfpModuleHwError@This_alarm_is_generated_by_FM_Kernel_to_Clear_the_uncorrelated_alarm,Subrack_Power_Configuration_Fault@Conflicting_power_configuration,Sync_Reference_PDV_Problem@,Sync_Reference_PDV_Problem@This_alarm_is_generated_by_FM_Kernel_to_Clear_the_uncorrelated_alarm,SystemUndervoltage@This_alarm_is_generated_by_FM_Kernel_to_Clear_the_uncorrelated_alarm,TU_Oscillator_Temperature_Fault@,TemperatureExceptionalTakenOutOfService@Temp_fault,TemperatureExceptionalTakenOutOfService@This_alarm_is_generated_by_FM_Kernel_to_Clear_the_uncorrelated_alarm,TemperatureSensorFailure@,TemperatureSensorFailure@The_alarm_is_currently_toggling.,UlCeGracePeriodExpired@LicencedCapacity=,UlCeGracePeriodExpired@This_alarm_is_generated_by_FM_Kernel_to_Clear_the_uncorrelated_alarm,"UlCeGracePeriodExpiring@LicencedCapacity=,CapacityLimitEnforceDate=--","UlCeGracePeriodStarted@problem "'FSME', 'Rectifier', 'FSMF', 'Breaker', 'FRGP', 'FXDA', 'BATT', 'FeederCable', 'OpticalCable', 'Urgent', 'Others'"
 2032  Namespace(i='../Src/ModData/feature_nok_0321.csv', n_healthy='500, 1000, 3000', n_unhealthy='400', o='../Src/ModData/', save_flag=True, str_today=None, str_vendor='nok', target='Prob_Urgent', threshold='0.5', top_problem="'FSME', 'Rectifier', 'FSMF', 'Breaker', 'FRGP', 'FXDA', 'BATT', 'FeederCable', 'OpticalCable', 'Urgent', 'Others'", use_cache=True)
 2033  #####################################
 2034  Prob_Urgent 500 400 0.5
 2035  Prob_Urgent 1000 400 0.5
 2036  Dropped cols: ['problem_area', 'Prob_FSME', 'Prob_Rectifier', 'Prob_FSMF', 'Prob_Breaker', 'Prob_FRGP', 'Prob_FXDA', 'Prob_BATT', 'Prob_FeederCable', 'Prob_OpticalCable', 'Prob_Urgent', 'Prob_Others']
 2037  target columns: Prob_Urgent
 2038  Prob_Urgent 3000 400 0.5
 2039  Dropped cols: ['problem_area', 'Prob_FSME', 'Prob_Rectifier', 'Prob_FSMF', 'Prob_Breaker', 'Prob_FRGP', 'PrDropped cols: ['problem_area', 'Probable', 'Prob_OpticalCable', 'Prob_Urgent', 'Prob_Others']Dropped cols: ['probleUrgent
 2040  RF training and test for target variable: [Prob_Urgent]
 2041  CV iteration: (0)
 2042  CV iteration: (1)
 2043  CV iteration: (2)
 2044  CV iteration: (3)
 2045  CV iteration: (4)
 2046  result is saved: ../Src/ModData/df_result_rf_nok_0result is saved: ../Src/Mo##result is saved: ../Src/ModData/df_result_rf_nok_0resu##################
 2047  (py35) 212339410@SFO1212339410M:~/Projects/SB/Python% python eval_model_rf.py -i "../Src/ModData/feature_nok_0321.csv" -o "../Src/ModData/" --target 'Prob_Urgent' --n_healthy '500, 1000, 3000'   --top_problem "'FSME', 'Rectifier', 'FSMF', 'Breaker', 'FRGP', 'FXDA', 'BATT', 'FeederCable', 'OpticalCable', 'Urgent', 'Others'"
 2048  Namespace(i='../Src/ModData/feature_nok_0321.csv', n_healthy='500, 1000, 3000', n_unhealthy='400', o='../Src/ModData/', save_flag=True, str_today=None, target='Prob_Urgent', threshold='0.5', top_problem="'FSME', 'Rectifier', 'FSMF', 'Breaker', 'FRGP', 'FXDA', 'BATT', 'FeederCable', 'OpticalCable', 'Urgent', 'Others'", use_cache=True, vendor='')
 2049  loading feature from: ../Src/ModData/feature_nok_0321.csv
 2050  (row, col) = (11521, 303)
 2051  feature cols: Index(['alarm_database_upload_in_progress',\n       'atm_interface_disabled_on_slot_port', 'atm_vcrdi_on_slot_port_vpi_vci',\n       'atm_vp       'atm_vp       'atm_vp       'atm_vp       'atm_vp       'atm_vp       'atm_vp       'atm_vp       'atm_vp  ction_local_management_connection',\n       'axc_rebooted_due_to_power_off_power_off',\n       'axc_rebooted_due_to_software_trigger_software_trigger',\n       'base_station_connectivity_degraded_transport_layer_connection_failure_in_s_interface',\n       'base_station_connectivity_degraded_transport_layer_connection_failure_in_x       'base_station_connectivity_degraded_transport_layer_connection_failure_in_x       'base_station_connectivity_degraded_transport_layer_connection_failure_in_x       'base_station_connectivity_degraded_transport_layer_connection_failure_in_x       'base_station_connectivity_degraded_transport_layer_connection_failure_in_x       'base_sta400       'base_station_connectivity_degraded_transport_layec       'baProb_FSMF', 'Prob_Breaker', 'Prob_FRGP', 'Prob_FXDA', 'Prob_BATT', 'Prob_FeederCable', 'Prob_OpticalCable', 'Prob_Urgent', 'Prob_Others']\ntarget columns: Prob_Urgent\nRF training and test for target variable: [Prob_Urgent]\nCV iteration: (0)\nCV iteration: (1)\nCV iteration: (2)\nCV iteration: (3)\nCV iteration: (4)\nProb_Urgent 1000 400 0.5\nDropped cols: ['problem_area', 'Prob_FSME', 'Prob_Rectifier', 'Prob_FSMF', 'Prob_Breaker', 'Prob_FRGP', 'Prob_FXDA', 'Prob_BATT', 'Prob_FeederCable', 'Prob_OpticalCable', 'Prob_Urgent', 'Prob_Others']\ntarget columns: Prob_Urgent\nRF training and test for target variable: [Prob_Urgent]\nCV iteration: (0)\nCV iteration: (1)\nCV iteration: (2)\nCV iteration: (3)\nCV iteration: (4)\nProb_Urgent 3000 400 0.5\nDropped cols: ['problem_area', 'Prob_FSME', 'Prob_Rectifier', 'Prob_FSMF', 'Prob_Breaker', 'Prob_FRGP', 'Prob_FXDA', 'Prob_BATT', 'Prob_FeederCable', 'Prob_OpticalCable', 'Prob_Urgent', 'Prob_Others']\ntarget columns: Prob_Urgent\nRF training and test for target variable: [Prob_Urgent]\nCV iteration: (0)\nCV iteration: (1)\nCV iteration: (2)\nCV iteration: (3)\nCV iteration: (4)\nTraceback (most recent call last):\n  File "/Users/212339410/anaconda2/envs/py35/lib/python3.5/site-packages/pandas/indexes/base.py", line 1945, in get_loc\n    return self._engine.get_loc(key)\n  File "pandas/index.pyx", line 137, in pandas.index.IndexEngine.get_loc (pandas/index.c:4154)\n  File "pandas/index.pyx", line 159, in pandas.index.IndexEngine.get_loc (pandas/index.c:4018)\n  File "pandas/hashtable.pyx", line 675, in pandas.hashtable.PyObjectHashTable.get_item (pandas/hashtable.c:12368)\n  File "pandas/hashtable.pyx", line 683, in pandas.hashtable.PyObjectHashTable.get_item (pandas/hashtable.c:12322)\nKeyError: 'F N_te'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File "eval_model_rf.py", line 147, in <module>\n    input_file=args.i, output_file=args.o)\n  File "eval_model_rf.py", line 109, in eval_model\n    df_result['Total_event_te'] = df_result['TP_te'] + df_result['F N_te']\n  File "/Users/212339410/anaconda2/envs/py35/lib/python3.5/site-packages/pandas/core/frame.py", line 1997, in __getitem__\n    return self._getitem_column(key)\n  File "/Users/212339410/anaconda2/envs/py35/lib/python3.5/site-packages/pandas/core/frame.py", line 2004, in _getitem_column\n    return self._get_item_cache(key)\n  File "/Users/212339410/anaconda2/envs/py35/lib/python3.5/site-packages/pandas/core/generic.py", line 1350, in _get_item_cache\n    values = self._data.get(item)\n  File "/Users/212339410/anaconda2/envs/py35/lib/python3.5/site-packages/pandas/core/internals.py", line 3290, in get\n    loc = self.items.get_loc(item)\n  File "/Users/212339410/anaconda2/envs/py35/lib/python3.5/site-packages/pandas/indexes/base.py", line 1947, in get_loc\n    return self._engine.get_loc(self._maybe_cast_indexer(key))\n  File "pandas/index.pyx", line 137, in pandas.index.IndexEngine.get_loc (pandas/index.c:4154)\n  File "pandas/index.pyx", line 159, in pandas.index.IndexEngine.get_loc (pandas/index.c:4018)\n  File "pandas/hashtable.pyx", line 675, in pandas.hashtable.PyObjectHashTable.get_item (pandas/hashtable.c:12368)\n  File "pandas/hashtable.pyx", line 683, in pandas.hashtable.PyObjectHashTable.get_item (pandas/hashtable.c:12322)\nKeyError: 'F N_te'\n(py35) 212339410@SFO1212339410M:~/Projects/SB/Python% python eval_model_dt.py -i "../Src/ModData/feature_nok_0321.csv" -o "../Src/ModData/" --target 'Prob_Urgent' --n_healthy '500, 1000, 3000'   --top_problem "'FSME', 'Rectifier', 'FSMF', 'Breaker', 'FRGP', 'FXDA', 'BATT', 'FeederCable', 'OpticalCable', 'Urgent', 'Others'"\nNamespace(i='../Src/ModData/feature_nok_0321.csv', n_healthy='500, 1000, 3000', n_unhealthy='400', o='../Src/ModData/', save_flag=True, str_today=None, target='Prob_Urgent', threshold='0.5', top_problem="'FSME', 'Rectifier', 'FSMF', 'Breaker', 'FRGP', 'FXDA', 'BATT', 'FeederCable', 'OpticalCable', 'Urgent', 'Others'", use_cache=True, vendor='')\nloading feature from: ../Src/ModData/feature_nok_0321.csv\n(row, col) = (11521, 303)\nfeature cols: Index(['alarm_database_upload_in_progress',\n       'atm_interface_disabled_on_slot_port', 'atm_vcrdi_on_slot_port_vpi_vci',\n       'atm_vpais_on_slot_port_vpi', 'atm_vprdi_on_slot_port_vpi',\n       'axc_in_maintenance_mode_due_to_local_management_connection_local_management_connection',\n       'axc_rebooted_due_to_power_off_power_off',\n       'axc_rebooted_due_to_software_trigger_software_trigger',\n       'base_station_connectivity_degraded_transport_layer_connection_failure_in_s_interface',\n       'base_station_connectivity_degraded_transport_layer_connection_failure_in_x_interface',\n       ...\n       'Prob_Others', 'Prob_FSME', 'Prob_OpticalCable', 'Prob_Rectifier',\n       'Prob_BATT', 'Prob_FRGP', 'Prob_FeederCable', 'Prob_FXDA', 'Prob_FSMF',\n       'Prob_Breaker'],\n      dtype='object', length=303)\n#####################################\n  Training Model\n#####################################\nProb_Urgent 500 400 0.5\nDropped cols: ['problem_area', 'Prob_FSME', 'Prob_Rectifier', 'Prob_FSMF', 'Prob_Breaker', 'Prob_FRGP', 'Prob_FXDA', 'Prob_BATT', 'Prob_FeederCable', 'Prob_OpticalCable', 'Prob_Urgent', 'Prob_Others']\ntarget columns: Prob_Urgent\nRF training and test for target variable: [Prob_Urgent]\nCV iteration: (0)\nCV iteration: (1)\nCV iteration: (2)\nCV iteration: (3)\nCV iteration: (4)\nProb_Urgent 1000 400 0.5\nDropped cols: ['problem_area', 'Prob_FSME', 'Prob_Rectifier', 'Prob_FSMF', 'Prob_Breaker', 'Prob_FRGP', 'Prob_FXDA', 'Prob_BATT', 'Prob_FeederCable', 'Prob_OpticalCable', 'Prob_Urgent', 'Prob_Others']\ntarget columns: Prob_Urgent\nRF training and test for target variable: [Prob_Urgent]\nCV iteration: (0)\nCV iteration: (1)\nCV iteration: (2)\nCV iteration: (3)\nCV iteration: (4)\nProb_Urgent 3000 400 0.5\nDropped cols: ['problem_area', 'Prob_FSME', 'Prob_Rectifier', 'Prob_FSMF', 'Prob_Breaker', 'Prob_FRGP', 'Prob_FXDA', 'Prob_BATT', 'Prob_FeederCable', 'Prob_OpticalCable', 'Prob_Urgent', 'Prob_Others']\ntarget columns: Prob_Urgent\nRF training and test for target variable: [Prob_Urgent]\nCV iteration: (0)\nCV iteration: (1)\nCV iteration: (2)\nCV iteration: (3)\nCV iteration: (4)\nTraceback (most recent call last):\n  File "/Users/212339410/anaconda2/envs/py35/lib/python3.5/site-packages/pandas/indexes/base.py", line 1945, in get_loc\n    return self._engine.get_loc(key)\n  File "pandas/index.pyx", line 137, in pandas.index.IndexEngine.get_loc (pandas/index.c:4154)\n  File "pandas/index.pyx", line 159, in pandas.index.IndexEngine.get_loc (pandas/index.c:4018)\n  File "pandas/hashtable.pyx", line 675, in pandas.hashtable.PyObjectHashTable.get_item (pandas/hashtable.c:12368)\n  File "pandas/hashtable.pyx", line 683, in pandas.hashtable.PyObjectHashTable.get_item (pandas/hashtable.c:12322)\nKeyError: 'F N_te'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File "eval_model_dt.py", line 147, in <module>\n    input_file=args.i, output_file=args.o)\n  File "eval_model_dt.py", line 109, in eval_model\n    df_result['Total_event_te'] = df_result['TP_te'] + df_result['F N_te']\n  File "/Users/212339410/anaconda2/envs/py35/lib/python3.5/site-packages/pandas/core/frame.py", line 1997, in __getitem__\n    return self._getitem_column(key)\n  File "/Users/212339410/anaconda2/envs/py35/lib/python3.5/site-packages/pandas/core/frame.py", line 2004, in _getitem_column\n    return self._get_item_cache(key)\n  File "/Users/212339410/anaconda2/envs/py35/lib/python3.5/site-packages/pandas/core/generic.py", line 1350, in _get_item_cache\n    values = self._data.get(item)\n  File "/Users/212339410/anaconda2/envs/py35/lib/python3.5/site-packages/pandas/core/internals.py", line 3290, in get\n    loc = self.items.get_loc(item)\n  File "/Users/212339410/anaconda2/envs/py35/lib/python3.5/site-packages/pandas/indexes/base.py", line 1947, in get_loc\n    return self._engine.get_loc(self._maybe_cast_indexer(key))\n  File "pandas/index.pyx", line 137, in pandas.index.IndexEngine.get_loc (pandas/index.c:4154)\n  File "pandas/index.pyx", line 159, in pandas.index.IndexEngine.get_loc (pandas/index.c:4018)\n  File "pandas/hashtable.pyx", line 675, in pandas.hashtable.PyObjectHashTable.get_item (pandas/hashtable.c:12368)\n  File "pandas/hashtable.pyx", line 683, in pandas.hashtable.PyObjectHashTable.get_item (pandas/hashtable.c:12322)\nKeyError: 'F N_te'\n(py35) 212339410@SFO1212339410M:~/Projects/SB/Python% python eval_model_dt.py -i "../Src/ModData/feature_nok_0321.csv" -o "../Src/ModData/" --target 'Prob_Urgent' --n_healthy '500, 1000, 3000'   --top_problem "'FSME', 'Rectifier', 'FSMF', 'Breaker', 'FRGP', 'FXDA', 'BATT', 'FeederCable', 'OpticalCable', 'Urgent', 'Others'"\nNamespace(i='../Src/ModData/feature_nok_0321.csv', n_healthy='500, 1000, 3000', n_unhealthy='400', o='../Src/ModData/', save_flag=True, str_today=None, target='Prob_Urgent', threshold='0.5', top_problem="'FSME', 'Rectifier', 'FSMF', 'Breaker', 'FRGP', 'FXDA', 'BATT', 'FeederCable', 'OpticalCable', 'Urgent', 'Others'", use_cache=True, vendor='')\nloading feature from: ../Src/ModData/feature_nok_0321.csv\n(row, col) = (11521, 303)\nfeature cols: Index(['alarm_database_upload_in_progress',\n       'atm_interface_disabled_on_slot_port', 'atm_vcrdi_on_slot_port_vpi_vci',\n       'atm_vpais_on_slot_port_vpi', 'atm_vprdi_on_slot_port_vpi',\n       'axc_in_maintenance_mode_due_to_local_management_connection_local_management_connection',\n       'axc_rebooted_due_to_power_off_power_off',\n       'axc_rebooted_due_to_software_trigger_software_trigger',\n       'base_station_connectivity_degraded_transport_layer_connection_failure_in_s_interface',\n       'base_station_connectivity_degraded_transport_layer_connection_failure_in_x_interface',\n       ...\n       'Prob_Others', 'Prob_FSME', 'Prob_OpticalCable', 'Prob_Rectifier',\n       'Prob_BATT', 'Prob_FRGP', 'Prob_FeederCable', 'Prob_FXDA', 'Prob_FSMF',\n       'Prob_Breaker'],\n      dtype='object', length=303)\n#####################################\n  Training Model\n#####################################\nProb_Urgent 500 400 0.5\nDropped cols: ['problem_area', 'Prob_FSME', 'Prob_Rectifier', 'Prob_FSMF', 'Prob_Breaker', 'Prob_FRGP', 'Prob_FXDA', 'Prob_BATT', 'Prob_FeederCable', 'Prob_OpticalCable', 'Prob_Urgent', 'Prob_Others']\ntarget columns: Prob_Urgent\nRF training and test for target variable: [Prob_Urgent]\nCV iteration: (0)\nCV iteration: (1)\nCV iteration: (2)\nCV iteration: (3)\nCV iteration: (4)\nProb_Urgent 1000 400 0.5\nDropped cols: ['problem_area', 'Prob_FSME', 'Prob_Rectifier', 'Prob_FSMF', 'Prob_Breaker', 'Prob_FRGP', 'Prob_FXDA', 'Prob_BATT', 'Prob_FeederCable', 'Prob_OpticalCable', 'Prob_Urgent', 'Prob_Others']\ntarget columns: Prob_Urgent\nRF training and test for target variable: [Prob_Urgent]\nCV iteration: (0)\nCV iteration: (1)\nCV iteration: (2)\nCV iteration: (3)\nCV iteration: (4)\nProb_Urgent 3000 400 0.5\nDropped cols: ['problem_area', 'Prob_FSME', 'Prob_Rectifier', 'Prob_FSMF', 'Prob_Breaker', 'Prob_FRGP', 'Prob_FXDA', 'Prob_BATT', 'Prob_FeederCable', 'Prob_OpticalCable', 'Prob_Urgent', 'Prob_Others']\ntarget columns: Prob_Urgent\nRF training and test for target variable: [Prob_Urgent]\nCV iteration: (0)\nCV iteration: (1)\nCV iteration: (2)\nCV iteration: (3)\nCV iteration: (4)\nresult is saved: ../Src/ModData/df_result_rf__0322.csv\n#####################################\n  End of process\n#####################################\n(py35) 212339410@SFO1212339410M:~/Projects/SB/Python% python eval_model_dt.py -i "../Src/ModData/feature_select_nok_0321.csv" -o "../Src/ModData/select_" --target 'Prob_Urgent' --n_healthy '500, 1000, 3000'   --top_problem "'FSME', 'Rectifier', 'FSMF', 'Breaker', 'FRGP', 'FXDA', 'BATT', 'FeederCable', 'OpticalCable', 'Urgent', 'Others'"\nNamespace(i='../Src/ModData/feature_select_nok_0321.csv', n_healthy='500, 1000, 3000', n_unhealthy='400', o='../Src/ModData/select_', save_flag=True, str_today=None, target='Prob_Urgent', threshold='0.5', top_problem="'FSME', 'Rectifier', 'FSMF', 'Breaker', 'FRGP', 'FXDA', 'BATT', 'FeederCable', 'OpticalCable', 'Urgent', 'Others'", use_cache=True, vendor='')\nloading feature from: ../Src/ModData/feature_select_nok_0321.csv\n(row, col) = (11521, 22)\nfeature cols: Index(['base_station_connectivity_degraded_transport_layer_connection_failure_in_s_interface',\n       'base_station_notification_difference_between_bts_master_clock_and_reference_frequency',\n       'base_station_notification_peer_system_module_connection_lost',\n       'external_al_ac_v_input_failure', 'external_al_air_conditioner_failure',\n       'external_al_m_canceller_failure',\n       'failure_in_wcdma_wbts_om_connection', 'synchronization_lost', 'Urgent',\n       'Semi urgent', 'problem_area', 'Prob_FSME', 'Prob_Rectifier',\n       'Prob_FSMF', 'Prob_Breaker', 'Prob_FRGP', 'Prob_FXDA', 'Prob_BATT',\n       'Prob_FeederCable', 'Prob_OpticalCable', 'Prob_Others', 'Prob_Urgent'],\n      dtype='object')\n#####################################\n  Training Model\n#####################################\nProb_Urgent 500 400 0.5\nDropped cols: ['problem_area', 'Prob_FSME', 'Prob_Rectifier', 'Prob_FSMF', 'Prob_Breaker', 'Prob_FRGP', 'Prob_FXDA', 'Prob_BATT', 'Prob_FeederCable', 'Prob_OpticalCable', 'Prob_Urgent', 'Prob_Others']\ntarget columns: Prob_Urgent\nRF training and test for target variable: [Prob_Urgent]\nCV iteration: (0)\nCV iteration: (1)\nCV iteration: (2)\nCV iteration: (3)\nCV iteration: (4)\nProb_Urgent 1000 400 0.5\nDropped cols: ['problem_area', 'Prob_FSME', 'Prob_Rectifier', 'Prob_FSMF', 'Prob_Breaker', 'Prob_FRGP', 'Prob_FXDA', 'Prob_BATT', 'Prob_FeederCable', 'Prob_OpticalCable', 'Prob_Urgent', 'Prob_Others']\ntarget columns: Prob_Urgent\nRF training and test for target variable: [Prob_Urgent]\nCV iteration: (0)\nCV iteration: (1)\nCV iteration: (2)\nCV iteration: (3)\nCV iteration: (4)\nProb_Urgent 3000 400 0.5\nDropped cols: ['problem_area', 'Prob_FSME', 'Prob_Rectifier', 'Prob_FSMF', 'Prob_Breaker', 'Prob_FRGP', 'Prob_FXDA', 'Prob_BATT', 'Prob_FeederCable', 'Prob_OpticalCable', 'Prob_Urgent', 'Prob_Others']\ntarget columns: Prob_Urgent\nRF training and test for target variable: [Prob_Urgent]\nCV iteration: (0)\nCV iteration: (1)\nCV iteration: (2)\nCV iteration: (3)\nCV iteration: (4)\nresult is saved: ../Src/ModData/select_df_result_rf__0322.csv\n#####################################\n  End of process\n#####################################\n(py35) 212339410@SFO1212339410M:~/Projects/SB/Python% python eval_model_dt.py -i "../Src/ModData/feature_select_nok_0321.csv" -o "../Src/ModData/select_" --target 'Prob_Urgent' --n_healthy '500, 1000, 3000'   --top_problem "'FSME', 'Rectifier', 'FSMF', 'Breaker', 'FRGP', 'FXDA', 'BATT', 'FeederCable', 'OpticalCable', 'Urgent', 'Others'" -exp True\nNamespace(exp='True', i='../Src/ModData/feature_select_nok_0321.csv', n_healthy='500, 1000, 3000', n_unhealthy='400', o='../Src/ModData/select_', save_flag=True, str_today=None, target='Prob_Urgent', threshold='0.5', top_problem="'FSME', 'Rectifier', 'FSMF', 'Breaker', 'FRGP', 'FXDA', 'BATT', 'FeederCable', 'OpticalCable', 'Urgent', 'Others'", use_cache=True, vendor='')\nloading feature from: ../Src/ModData/feature_select_nok_0321.csv\n(row, col) = (11521, 22)\nfeature cols: Index(['base_station_connectivity_degraded_transport_layer_connection_failure_in_s_interface',\n       'base_station_notification_difference_between_bts_master_clock_and_reference_frequency',\n       'base_station_notification_peer_system_module_connection_lost',\n       'external_al_ac_v_input_failure', 'external_al_air_conditioner_failure',\n       'external_al_m_canceller_failure',\n       'failure_in_wcdma_wbts_om_connection', 'synchronization_lost', 'Urgent',\n       'Semi urgent', 'problem_area', 'Prob_FSME', 'Prob_Rectifier',\n       'Prob_FSMF', 'Prob_Breaker', 'Prob_FRGP', 'Prob_FXDA', 'Prob_BATT',\n       'Prob_FeederCable', 'Prob_OpticalCable', 'Prob_Others', 'Prob_Urgent'],\n      dtype='object')\n#####################################\n  Training Model\n#####################################\nProb_Urgent 500 400 0.5\nDropped cols: ['problem_area', 'Prob_FSME', 'Prob_Rectifier', 'Prob_FSMF', 'Prob_Breaker', 'Prob_FRGP', 'Prob_FXDA', 'Prob_BATT', 'Prob_FeederCable', 'Prob_OpticalCable', 'Prob_Urgent', 'Prob_Others']\ntarget columns: Prob_Urgent\nRF training and test for target variable: [Prob_Urgent]\nCV iteration: (0)\nCV iteration: (1)\nCV iteration: (2)\nCV iteration: (3)\nCV iteration: (4)\nProb_Urgent 1000 400 0.5\nDropped cols: ['problem_area', 'Prob_FSME', 'Prob_Rectifier', 'Prob_FSMF', 'Prob_Breaker', 'Prob_FRGP', 'Prob_FXDA', 'Prob_BATT', 'Prob_FeederCable', 'Prob_OpticalCable', 'Prob_Urgent', 'Prob_Others']\ntarget columns: Prob_Urgent\nRF training and test for target variable: [Prob_Urgent]\nCV iteration: (0)\nCV iteration: (1)\nCV iteration: (2)\nCV iteration: (3)\nCV iteration: (4)\nProb_Urgent 3000 400 0.5
 2052  conda install -c conda-forge pydotplus
 2053  python eval_model_rf.py -i "../Src/ModData/feature_nok_0321.csv" -o "../Src/ModData/" --target 'Prob_Urgent' --n_healthy '500, 1000, 3000'   --top_problem "'FSME', 'Rectifier', 'FSMF', 'Breaker', 'FRGP', 'FXDA', 'BATT', 'FeederCable', 'OpticalCable', 'Urgent', 'Others'" --vendor 'nok'
 2054  python eval_model.py -i "../Src/ModData/feature_eri_0321.csv" -o "../Src/ModData/" --target 'Prob_Urgent' --n_healthy '500, 1000, 3000'   --top_problem " 'DUW', 'DUL', 'RRUS', 'Rectifier', 'RUS', 'BATT', 'DUS', 'OpticalCable', 'Urgent'" --vendor 'eri\n'
 2055  python eval_model.py -i "../Src/ModData/feature_eri_0321.csv" -o "../Src/ModData/" --target 'Prob_Urgent' --n_healthy '500, 1000, 3000'   --top_problem " 'DUW', 'DUL', 'RRUS', 'Rectifier', 'RUS', 'BATT', 'DUS', 'OpticalCable', 'Urgent'" --vendor 'eri'\n'\n'
 2056  python eval_model.py -i "../Src/ModData/feature_eri_0321.csv" -o "../Src/ModData/" --target 'Prob_Urgent' --n_healthy '500, 1000, 3000'   --top_problem " 'DUW', 'DUL', 'RRUS', 'Rectifier', 'RUS', 'BATT', 'DUS', 'OpticalCable', 'Urgent'" --vendor "eri"
 2057  python eval_model_rf.py -i "../Src/ModData/feature_eri_0321.csv" -o "../Src/ModData/" --target 'Prob_Urgent' --n_healthy '500, 1000, 3000'   --top_problem " 'DUW', 'DUL', 'RRUS', 'Rectifier', 'RUS', 'BATT', 'DUS', 'OpticalCable', 'Urgent'" --vendor "eri"
 2058  brew install graphviz
 2059  python eval_model_dt.py -i "../Src/ModData/feature_select_nok_0321.csv" -o "../Src/ModData/select_" --target 'Prob_Urgent' --n_healthy '500, 1000, 3000'   --top_problem "'FSME', 'Rectifier', 'FSMF', 'Breaker', 'FRGP', 'FXDA', 'BATT', 'FeederCable', 'OpticalCable', 'Urgent', 'Others'" -exp True
 2060  python eval_model_dt.py -i "../Src/ModData/feature_nok_0321.csv" -o "../Src/ModData/" --target 'Prob_Urgent' --n_healthy '500, 1000, 3000'   --top_problem "'FSME', 'Rectifier', 'FSMF', 'Breaker', 'FRGP', 'FXDA', 'BATT', 'FeederCable', 'OpticalCable', 'Urgent', 'Others'" -exp True
 2061  python eval_model_dt.py -i "../Src/ModData/feature_nok_0321.csv" -o "../Src/ModData/" --target 'Prob_Urgent' --n_healthy '500, 1000, 3000'   --top_problem "'FSME', 'Rectifier', 'FSMF', 'Breaker', 'FRGP', 'FXDA', 'BATT', 'FeederCable', 'OpticalCable', 'Urgent', 'Others'" -exp True --vendor 'nok'
 2062  python eval_model_dt.py -i "../Src/ModData/feature_select_nok_0321.csv" -o "../Src/ModData/select_" --target 'Prob_Urgent' --n_healthy '500, 1000, 3000'   --top_problem "'FSME', 'Rectifier', 'FSMF', 'Breaker', 'FRGP', 'FXDA', 'BATT', 'FeederCable', 'OpticalCable', 'Urgent', 'Others'" -exp True --vendor 'nok'
 2063  python eval_model_dt.py -i "../Src/ModData/feature_eri_0321.csv" -o "../Src/ModData/" --target 'Prob_Urgent' --n_healthy '500, 1000, 3000'   --top_problem " 'DUW', 'DUL', 'RRUS', 'Rectifier', 'RUS', 'BATT', 'DUS', 'OpticalCable', 'Urgent'" --vendor "eri"
 2064  python eval_model_dt.py -i "../Src/ModData/feature_eri_0321.csv" -o "../Src/ModData/" --target 'Prob_Urgent' --n_healthy '500, 1000, 3000'   --top_problem " 'DUW', 'DUL', 'RRUS', 'Rectifier', 'RUS', 'BATT', 'DUS', 'OpticalCable', 'Urgent'" --vendor "eri" --exp True
 2065  python eval_model_dt.py -i "../Src/ModData/feature_eri_0321.csv" -o "../Src/ModData/" --target 'Prob_Urgent' --n_healthy '500, 1000, 3000'   --top_problem " 'DUW', 'DUL', 'RRUS', 'Rectifier', 'RUS', 'BATT', 'DUS', 'OpticalCable', 'Urgent'" --vendor "eri" -exp True
 2066  python eval_model_dt.py -i "../Src/ModData/feature_eri_0321.csv" -o "../Src/ModData/" --target 'Prob_Urgent' --n_healthy '500, 1000, 3000'   --top_problem " 'DUW', 'DUL', 'RRUS', 'Rectifier', 'RUS', 'BATT', 'DUS', 'OpticalCable', 'Others','Prob_L2SW', 'Urgent'" --vendor "eri" -exp True
 2067  python eval_model_dt.py -i "../Src/ModData/feature_eri_0321.csv" -o "../Src/ModData/" --target 'Prob_Urgent' --n_healthy '500, 1000, 3000'   --top_problem " 'DUW', 'DUL', 'RRUS', 'Rectifier', 'RUS', 'BATT', 'DUS', 'OpticalCable', 'Others','L2SW', 'Urgent'" --vendor "eri" -exp True
 2068  python eval_model_dt.py -i "../Src/ModData/feature_eri_0321.csv" -o "../Src/ModData/" --target 'Prob_Urgent' --n_healthy '500, 1000, 3000'   --top_problem " 'DUW', 'DUL', 'RRUS', 'Rectifier', 'RUS', 'BATT', 'DUS', 'OpticalCable', 'Others','L2SW', 'æ¬é', 'FeederCable','Urgent'" --vendor "eri" -exp True
 2069  python eval_model_dt.py -i "../Src/ModData/feature_eri_0321.csv" -o "../Src/ModData/setcolor_" --target 'Prob_Urgent' --n_healthy '1000'   --top_problem " 'DUW', 'DUL', 'RRUS', 'Rectifier', 'RUS', 'BATT', 'DUS', 'OpticalCable', 'Others','L2SW', 'æ¬é', 'FeederCable','Urgent'" --vendor "eri" -exp True
 2070  python eval_model_dt.py -i "../Src/ModData/feature_eri_0321.csv" -o "../Src/ModData/setcolor_" --target 'Prob_Urgent' --n_healthy '500 1000 3000'   --top_problem " 'DUW', 'DUL', 'RRUS', 'Rectifier', 'RUS', 'BATT', 'DUS', 'OpticalCable', 'Others','L2SW', 'æ¬é', 'FeederCable','Urgent'" --vendor "eri" -exp True
 2071  python eval_model_dt.py -i "../Src/ModData/feature_eri_0321.csv" -o "../Src/ModData/setcolor_" --target 'Prob_Urgent' --n_healthy '500, 1000, 3000'   --top_problem " 'DUW', 'DUL', 'RRUS', 'Rectifier', 'RUS', 'BATT', 'DUS', 'OpticalCable', 'Others','L2SW', 'æ¬é', 'FeederCable','Urgent'" --vendor "eri" -exp True
 2072  python eval_model_rf.py -i "../Src/ModData/feature_nok_0321.csv" -o "../Src/ModData/select_" --target 'Prob_Urgent, Prob_FSME, Prob_FSMF, Prob_FXDA' --n_healthy '1000'   --top_problem " 'DUW', 'DUL', 'RRUS', 'Rectifier', 'RUS', 'BATT', 'DUS', 'OpticalCable', 'Others','L2SW', 'æ¬é', 'FeederCable','Urgent'" --vendor "eri" -exp True
 2073  python eval_model_dt.py -i "../Src/ModData/feature_eri_0321.csv" -o "../Src/ModData/setcolor_" --target 'Prob_Urgent' --n_healthy '500, 1000, 3000'   --top_problem " 'DUW', 'DUL', 'RRUS', 'Rectifier', 'RUS', 'BATT', 'DUS', 'OpticalCable', 'Others','L2SW', 'æ¬é', 'FeederCable','Urgent'" --vendor "eri" -exp True --plot_imp True
 2074  ls /Library/LaunchAgents
 2075  ls /Library/LaunchDaemons
 2076  l ~/Library/LaunchAgents
 2077  vim ~/Library/LaunchAgents/org.virtualbox.vboxwebsrv.plist
 2078  pip install percol
 2079  percol /var/log/syslog
 2080  man percol
 2081  percol_cd_history
 2082  zle -l
 2083  which -a echo
 2084  ../Notebook
 2085  pip install plotly --upgrade
 2086  cp preprocessing_bsw_20151201_20170201.ipynb preprocessing_nedb_201607.ipynb
 2087  cd Input
 2088  wc -l NEDB_BASE_INFO.20160721085837_2.utf8.csv
 2089  ls | wc -l 
 2090  ls | wc -l
 2091  wc -l $(ls)
 2092  man wc
 2093  awk -F ',' 'print NF; exit' NEDB_BASE_INFO.20160721085837_2.utf8.csv
 2094  head -1 NEDB_
 2095  head -1 NEDB_BASE_INFO.20160721085837_2.utf8.csv| tr ',' '\n' | wc -l
 2096  head -1 $(ls) | tr ',' '\n' | wc -l
 2097  wc man awk
 2098  man awk
 2099  awk -F ',' '' NEDB_BASE_INFO.20160721085837_2.utf8.csv
 2100  awk -F ',' '{print NF; exit}' NEDB_BASE_INFO.20160721085837_2.utf8.csv
 2101  awk -F ',' '{print NF; exit}' NEDB_ERI_T.utf8.csv
 2102  awk -F ',' '{print NF; exit}' NEDB_NOK_Q.utf8.csv
 2103  awk -F ',' '{print NF; exit}' NEDB_NOKERI_TQ.utf8_st_drop.csv
 2104  python feature_create.py --vendor 'nok' --lb 14 --la 1 --nh 500 --nu 400 --sample_days 40 --save True --cache True --i '../Src/ModData/' --o '../Src/ModData/test'
 2105  python feature_create.py --vendor 'nok' --lb 14 --la 1 --nh 500 --nu 400 --sample_days 40 --save True --cache True --i '../Src/ModData/' --o '../Src/ModData/'
 2106  pip install --upgrade gensim
 2107  jumanpp
 2108  brew install jumanpp
 2109  pip install six
 2110  tar xvf pyknp-0.3.tar.gz
 2111  cd pyknp-0.3
 2112  rm pyknp-0.3.tar.gz
 2113  rm -rf pyknp-0.3
 2114  sudo rm -rf pyknp-0.3
 2115  jumanpp -v
 2116  mkdir knp
 2117  cd knp
 2118  wget http://nlp.ist.i.kyoto-u.ac.jp/nl-resource/knp/knp-4.16.tar.bz2
 2119  knp
 2120  cd Src/ModData/
 2121  vim df_dt_result_rf_eri_0322.csv
 2122  vim df_event_uh_proc_test_nok_lbh14_lah1_0321.csv
 2123  vim df_event_uh_proc_test_nok_lbh14_lah1_0323.csv
 2124  vim df_event_uh_proc_nok_lbh14_lah1_0323.csv
 2125  python feature_create.py --vendor 'nok' --lb 14 --la 1 --nh 500 --nu 400 --sample_days 40 --save True --cache False  --o '../Src/ModData/'
 2126  vim NEDB_NOKERI_TQ.utf8.csv
 2127  python eval_model_dt.py -i "../Src/ModData/feature_eri_0321.csv" -o "../Src/ModData/setcolor_" --target 'Prob_Urgent, DUW, DUL, RRUS, BATT, Rectifier, L2SW' --n_healthy '1000'   --top_problem " 'DUW', 'DUL', 'RRUS', 'Rectifier', 'RUS', 'BATT', 'DUS', 'OpticalCable', 'Others','L2SW', 'æ¬é', 'FeederCable','Urgent'" --vendor "eri" -exp True --plot_imp True
 2128  python eval_model_dt.py -i "../Src/ModData/feature_eri_0321.csv" -o "../Src/ModData/setcolor_" --target 'Prob_Urgent, DUW, DUL, RRUS, BATT, Rectifier, L2SW' --n_healthy '1000'   --top_problem " 'DUW', 'DUL', 'RRUS', 'Rectifier', 'RUS', 'BATT', 'DUS', 'OpticalCable', 'Others','L2SW', 'æ¬é', 'FeederCable','Urgent'" --vendor "eri" -exp True --export_tree True
 2129  python eval_model_dt.py -i "../Src/ModData/feature_eri_0321.csv" -o "../Src/ModData/setcolor_" --target 'Prob_Urgent, DUW, DUL, RRUS, BATT, Rectifier, L2SW' --n_healthy '1000'   --top_problem " 'DUW', 'DUL', 'RRUS', 'Rectifier', 'RUS', 'BATT', 'DUS', 'OpticalCable', 'Others','L2SW', 'æ¬é', 'FeederCable','Urgent'" --vendor "eri" -exp True --exp True
 2130  python eval_model_dt.py -i "../Src/ModData/feature_eri_0321.csv" -o "../Src/ModData/setcolor_" --target 'Prob_Urgent, DUW, DUL, RRUS, BATT, Rectifier, L2SW' --n_healthy '1000'   --top_problem " 'DUW', 'DUL', 'RRUS', 'Rectifier', 'RUS', 'BATT', 'DUS', 'OpticalCable', 'Others','L2SW', 'æ¬é', 'FeederCable','Urgent'" --vendor "eri" -exp True 
 2131  python eval_model_dt.py -i "../Src/ModData/feature_eri_0321.csv" -o "../Src/ModData/setcolor_" --target 'Prob_Urgent, Prob_DUW, Prob_DUL, Pro_RRUS, Prob_BATT, Prob_Rectifier, Prob_L2SW' --n_healthy '1000'   --top_problem " 'DUW', 'DUL', 'RRUS', 'Rectifier', 'RUS', 'BATT', 'DUS', 'OpticalCable', 'Others','L2SW', 'æ¬é', 'FeederCable','Urgent'" --vendor "eri" -exp True 
 2132  python eval_model_dt.py -i "../Src/ModData/feature_eri_0321.csv" -o "../Src/ModData/setcolor_" --target 'Prob_Urgent, Prob_DUW, Prob_DUL, Prob_RRUS, Prob_BATT, Prob_Rectifier, Prob_L2SW' --n_healthy '1000'   --top_problem " 'DUW', 'DUL', 'RRUS', 'Rectifier', 'RUS', 'BATT', 'DUS', 'OpticalCable', 'Others','L2SW', 'æ¬é', 'FeederCable','Urgent'" --vendor "eri" -exp True 
 2133  python eval_model_dt.py -i "../Src/ModData/feature_eri_0321.csv" -o "../Src/ModData/setcolor_" --target 'Prob_Urgent, Prob_DUW, Prob_DUL, Pro_RRUS, Prob_RUS, Prob_L2SW' --n_healthy '1000'   --top_problem " 'DUW', 'DUL', 'RRUS', 'Rectifier', 'RUS', 'BATT', 'DUS', 'OpticalCable', 'Others','L2SW', 'æ¬é', 'FeederCable','Urgent'" --vendor "eri" -exp True 
 2134  python eval_model_dt.py -i "../Src/ModData/feature_eri_0321.csv" -o "../Src/ModData/setcolor_" --target 'Prob_Urgent, Prob_DUW, Prob_DUL, Prob_RRUS, Prob_RUS, Prob_L2SW' --n_healthy '1000'   --top_problem " 'DUW', 'DUL', 'RRUS', 'Rectifier', 'RUS', 'BATT', 'DUS', 'OpticalCable', 'Others','L2SW', 'æ¬é', 'FeederCable','Urgent'" --vendor "eri" -exp True 
 2135  python eval_model_dt.py -i "../Src/ModData/feature_eri_0321.csv" -o "../Src/ModData/setcolor_" --target 'Prob_Urgent, Prob_DUW, Prob_DUL, Prob_RRUS, Prob_RUS' --n_healthy '1000'   --top_problem " 'DUW', 'DUL', 'RRUS', 'Rectifier', 'RUS', 'BATT', 'DUS', 'OpticalCable', 'Others','L2SW', 'æ¬é', 'FeederCable','Urgent'" --vendor "eri" -exp True 
 2136  python eval_model_dt.py -i "../Src/ModData/feature_eri_0321.csv" -o "../Src/ModData/setcolor_" --target "Prob_Urgent, Prob_DUW, Prob_DUL, Prob_RRUS, Prob_RUS" --n_healthy '1000'   --top_problem "'DUW', 'DUL', 'RRUS', 'Rectifier', 'RUS', 'BATT', 'DUS', 'OpticalCable', 'Others','L2SW', 'æ¬é', 'FeederCable','Urgent'" --vendor "nok" -exp True 
 2137  python eval_model_dt.py -i "../Src/ModData/feature_nok_0321.csv" -o "../Src/ModData/setcolor_" --target 'Prob_Urgent, Prob_FSME, Prob_Rectifier, Prob_FSMF', Prob_Breaker, Prob_FRGP, Prob_FXDA, Prob_BATT, Prob_FeederCable, prob_OpticalCable, Prob_Others' --n_healthy '1000'   --top_problem "'FSME', 'Rectifier', 'FSMF', 'Breaker', 'FRGP', 'FXDA', 'BATT', 'FeederCable', 'OpticalCable', 'Others', 'non-urgent'"  --vendor "nok" -exp True --plot_imp True
 2138  python eval_model_dt.py -i "../Src/ModData/feature_nok_0321.csv" -o "../Src/ModData/setcolor_" --target "Prob_Urgent, Prob_FSME, Prob_Rectifier, Prob_FSMF, Prob_Breaker, Prob_FRGP, Prob_FXDA, Prob_BATT, Prob_FeederCable, prob_OpticalCable, Prob_Others" --n_healthy '1000'   --top_problem "'FSME', 'Rectifier', 'FSMF', 'Breaker', 'FRGP', 'FXDA', 'BATT', 'FeederCable', 'OpticalCable', 'Others', 'non-urgent'"  --vendor "nok" -exp True --plot_imp True
 2139  python eval_model_dt.py -i "../Src/ModData/feature_nok_0321.csv" -o "../Src/ModData/setcolor_" --target "Prob_Urgent, Prob_FSME, Prob_Rectifier, Prob_FSMF, Prob_Breaker, Prob_FRGP, Prob_FXDA, Prob_BATT, Prob_FeederCable, prob_OpticalCable, Prob_Others" --n_healthy '1000'   --top_problem "'FSME', 'Rectifier', 'FSMF', 'Breaker', 'FRGP', 'FXDA', 'BATT', 'FeederCable', 'OpticalCable', 'Others', 'non-urgent'"  --vendor "nok" -exp True 
 2140  python eval_model_dt.py -i "../Src/ModData/feature_nok_0321.csv" -o "../Src/ModData/setcolor_" --target "Prob_Urgent, Prob_FSME, Prob_Rectifier, Prob_FSMF, Prob_Breaker, Prob_FRGP, Prob_FXDA, Prob_BATT, Prob_FeederCable, prob_OpticalCable, Prob_Others" --n_healthy '1000'   --top_problem "'FSME', 'Rectifier', 'FSMF', 'Breaker', 'FRGP', 'FXDA', 'BATT', 'FeederCable', 'OpticalCable', 'Others', 'Urgent'"  --vendor "nok" -exp True 
 2141  python eval_model_dt.py -i "../Src/ModData/feature_nok_0321.csv" -o "../Src/ModData/setcolor_" --target "Prob_Urgent, Prob_FSME, Prob_Rectifier, Prob_FSMF, Prob_Breaker, Prob_FRGP, Prob_FXDA, Prob_BATT, Prob_FeederCable, prob_OpticalCable, Prob_Others" --n_healthy '1000  --vendor "nok" -exp True 
 2142  python eval_model_dt.py -i "../Src/ModData/feature_nok_0321.csv" -o "../Src/ModData/setcolor_" --target "Prob_Urgent, Prob_FSME, Prob_Rectifier, Prob_FSMF, Prob_Breaker, Prob_FRGP, Prob_FXDA, Prob_BATT, Prob_FeederCable, prob_OpticalCable, Prob_Others" --n_healthy '1000'  --vendor "nok" -exp True 
 2143  python eval_model_dt.py -i "../Src/ModData/feature_eri_0321.csv" -o "../Src/ModData/setcolor_" --target "Prob_Urgent, Prob_DUW, Prob_DUL, Prob_RRUS, Prob_RUS" --n_healthy '1000'   --top_problem "'DUW', 'DUL', 'RRUS', 'Rectifier', 'RUS', 'BATT', 'DUS', 'OpticalCable', 'Others','L2SW', 'æ¬é', 'FeederCable','Urgent'" --vendor "eri" -exp True 
 2144  locate docker
 2145  locate docker | grep agent
 2146  locate docker | grep deamon
 2147  locate docker | grep start
 2148  less launchctl list launchctl list 
 2149  launchctl list 
 2150  launchctl list | pulse
 2151  lanchctl remove com.docker.docker.238112
 2152  launchctl remove com.docker.docker.238112
 2153  launchctl list | grep docker
 2154  launchctl remove net.pulsesecure.pulsetray
 2155  launchctl list | grep pulse
 2156  launchctl list | grep net.pulsesecure.Pulse-Secure.259872
 2157  launchctl remove net.pulsesecure.Pulse-Secure.259872
 2158  launchctl list | grep jetbeans
 2159  launchctl list | grep pycharm
 2160  launchctl list | grep toolbox
 2161  python eval_model_dt.py -i "../Src/ModData/feature_nok_0321.csv" -o "../Src/ModData/setcolor_" --target "Prob_Urgent, Prob_FSME, Prob_Rectifier, Prob_FSMF, Prob_Breaker, Prob_FRGP, Prob_FXDA, Prob_BATT, Prob_FeederCable, Prob_OpticalCable, Prob_Others" --n_healthy '1000'  --vendor "nok" -exp True 
 2162  python eval_model_rf.py -i "../Src/ModData/feature_nok_0321.csv" -o "../Src/ModData/RF_10tree_0324/" --target "Prob_Urgent, Prob_FSME, Prob_Rectifier, Prob_FSMF, Prob_Breaker, Prob_FRGP, Prob_FXDA, Prob_BATT, Prob_FeederCable, Prob_OpticalCable, Prob_Others" --n_healthy '1000'  --vendor "nok" -exp True  --exp True
 2163  mkdir ../Src/ModData/RF_10tree_0324
 2164  python eval_model_rf.py -i "../Src/ModData/feature_nok_0321.csv" -o "../Src/ModData/RF_10tree_0324/" --target "Prob_Urgent, Prob_FSME, Prob_Rectifier, Prob_FSMF, Prob_Breaker, Prob_FRGP, Prob_FXDA, Prob_BATT, Prob_FeederCable, Prob_OpticalCable, Prob_Others" --n_healthy '1000'  --vendor "nok" -exp True  -exp True
 2165  python eval_model_rf.py -i "../Src/ModData/feature_nok_0321.csv" -o "../Src/ModData/RF_10tree_0324/" --target "Prob_Urgent, Prob_FSME, Prob_Rectifier, Prob_FSMF, Prob_Breaker, Prob_FRGP, Prob_FXDA, Prob_BATT, Prob_FeederCable, Prob_OpticalCable, Prob_Others" --n_healthy '1000'  --vendor "nok" -exp True  -exp True 
 2166  python eval_model_rf.py -i "../Src/ModData/feature_nok_0321.csv" -o "../Src/ModData/RF_10tree_0324/" --target "Prob_Urgent, Prob_FSME, Prob_Rectifier, Prob_FSMF, Prob_Breaker, Prob_FRGP, Prob_FXDA, Prob_BATT, Prob_FeederCable, Prob_OpticalCable, Prob_Others" --n_healthy '1000'  --vendor "nok" -exp True  
 2167  python eval_model_rf.py -i "../Src/ModData/feature_nok_0321.csv" -o "../Src/ModData/RF_10tree_0324/" --target "Prob_Urgent" --n_healthy '1000'  --vendor "nok" -exp True  
 2168  python eval_model_rf.py -i "../Src/ModData/feature_eri_0321.csv" -o "../Src/ModData/RF_10tree_0324/" --target "Prob_Urgent" --n_healthy '1000'  --vendor "eri" -exp True  
 2169  pip install jython
 2170  python reduce_dimension.py
 2171  python reduce_dimension.py -h
 2172  ls ../Src/ModData
 2173  python reduce_dimension.py -i ../Src/ModData/testfeature_nok_lbh14_lah1_0323.csv
 2174  python reduce_dimension.py -i ../Src/ModData/testfeature_nok_lbh14_lah1_0323.csv -plot 1 -o 'test_dim_reduction0325' 
 2175  python -c 'import qutils; qutils.col('../Src/ModData/testfeature_nok_lbh14_lah1_0323.csv')'
 2176  python -c "import qutils; qutils.col('../Src/ModData/testfeature_nok_lbh14_lah1_0323.csv')"
 2177  python qutils -col -i ../Src/ModData/feature_eri_0321.csv
 2178  python qutils.py -col -i ../Src/ModData/feature_eri_0321.csv
 2179  python qutils.py -col '' -i ../Src/ModData/feature_eri_0321.csv
 2180  python qutils.py -col 1 -i ../Src/ModData/feature_eri_0321.csv
 2181  python reduce_dimension.py -i ../Src/ModData/testfeature_nok_lbh14_lah1_0323.csv -plot 1 -o 'test_dim_reduction0325'  -drop ''problem_area'\n 'Prob_BATT' 'Prob_DUL' 'Prob_DUS' 'Prob_DUW' 'Prob_FeederCable'\n 'Prob_L2SW' 'Prob_OpticalCable' 'Prob_Others' 'Prob_RRU' 'Prob_RRUS'
 2182  python reduce_dimension.py -i ../Src/ModData/testfeature_nok_lbh14_lah1_0323.csv -plot 1 -o 'test_dim_reduction0325'  -drop "'problem_area'\n 'Prob_BATT' 'Prob_DUL' 'Prob_DUS' 'Prob_DUW' 'Prob_FeederCable'\n 'Prob_L2SW' 'Prob_OpticalCable' 'Prob_Others' 'Prob_RRU' 'Prob_RRUS'"
 2183  python reduce_dimension.py -i ../Src/ModData/feature_eri_0321.csv  -plot 1 -o 'test_dim_reduction0325'  -drop "'problem_area' 'Prob_BATT' 'Prob_DUL' 'Prob_DUS' 'Prob_DUW' 'Prob_FeederCable' 'Prob_L2SW' 'Prob_OpticalCable' 'Prob_Others' 'Prob_RRU' 'Prob_RRUS'"
 2184  python reduce_dimension.py -n_comp 20 -i ../Src/ModData/feature_eri_0321.csv  -plot 1 -o 'test_dim_reduction0325'  -drop "'problem_area' 'Prob_BATT' 'Prob_DUL' 'Prob_DUS' 'Prob_DUW' 'Prob_FeederCable' 'Prob_L2SW' 'Prob_OpticalCable' 'Prob_Others' 'Prob_RRU' 'Prob_RRUS'"
 2185  less com.jamfsoftware.jamf.agent.plist
 2186  less net.pulsesecure.pulsetray.plist 
 2187  vim net.pulsesecure.pulsetray.plist
 2188  sudo vim net.pulsesecure.pulsetray.plist
 2189  less org.macosforge.xquartz.startx.plist
 2190  vim com.docker.vmnetd.plist
 2191  vim net.pulsesecure.
 2192  vim net.pulsesecure.AccessService.plist
 2193  vim net.pulsesecure.UninstallPulse.plist
 2194  vim com.jetbrains.toolbox.plist
 2195  cd /System/Library/LaunchAgents
 2196  ../LaunchDaemons
 2197  cd ~/Library/LaunchAgents
 2198  vim com.google.keystone.agent.plist
 2199  cd test/Â¥
 2200  cd test/
 2201  l seq-test.txt
 2202  less seq-test.txt
 2203  vim seq-test.txt
 2204  cd /Library/Launch
 2205  cd /Library/LaunchAgents
 2206  cd ../LaunchDaemons
 2207  cd ~/Projects/SB/
 2208  cd ~/Documents/Analytics/Python/tstk
 2209  locate git/contrib/
 2210  locate git | grep contrib
 2211  locate git | grep completion
 2212  source ~/.zshrch
 2213  locate git-completion.zsh
 2214  brew install bash-completion
 2215  curl -o ~/.git-prompt.sh \\n    https://raw.githubusercontent.com/git/git/master/contrib/completion/git-prompt.sh
 2216  source ~/.git-prompt.sh
 2217  echo $ZSH
 2218  cd ../Python/tstk
 2219  PROMPT
 2220  PROMPT='%n'
 2221  PROMPT='%n@'
 2222  PROMPT='%n@%m'
 2223  PROMPT='%n@%#'
 2224  PROMPT='%n@%m%#'
 2225  PROMPT='%n@%m%# '
 2226  python -c 'import plotly'
 2227  PROMPT hiro
 2228  PROMPT=hiro
 2229  git noproxy
 2230  vim ../Src/ModData/testfeature_nok_lbh14_lah1_0323.csv
 2231  python feature_create.py --vendor 'nok' --lb 14 --la 1 --nh 500 --nu 400 --sample_days 40 --save False --cache False  --o '../Src/ModData/'
 2232  vim ../Src/ModData/df_event_uh_proc_nok_lbh14_lah1_0323.csv
 2233  head -n 100  ../Src/ModData/df_event_uh_proc_nok_lbh14_lah1_0323.csv > tmp100.csv
 2234  open tmp100.csv
 2235  head -n 100  ../Src/ModData/df_event_uh_proc_nok_lbh14_lah1_0323.csv
 2236  python feature_create.py --vendor 'nok' --lb 14 --la 1 --nh 500 --nu 400 --sample_days 40 --save False --cache True  --o '../Src/ModData/'
 2237  head ../Src/ModData/feature_nok_lbh14_lah1_0326.csv
 2238  head '../Src/ModData/feature_nok_lbh14_lah1_0326.csv'
 2239  open temp.csv
 2240  head Src/ModData/df_event_uh_proc_nok_lbh14_lah1_0326.csv > temp.csv
 2241  less Src/ModData/df_event_uh_proc_nok_lbh14_lah1_0326.csv
 2242  vim ../Src/ModData/feature_nok_lbh14_lah1_0326.csv
 2243  pip install stop_words
 2244  pip install brew install jython
 2245  brew install jython
 2246  mv -r ~/Downloads/UCanAccess-4.0.1-bin ~/.local/bin/
 2247  mv  ~/Downloads/UCanAccess-4.0.1-bin ~/.local/bin/
 2248  mv ~/Downloads/ERI_YM_20170212.accdb ~/Projects/SB/Src/Original_flat/
 2249  cp preprocessing_oss_nok_20170214_20170227.ipynb preprocessing_oss_nok_20170327.ipynb
 2250  mv preprocessing_oss_nok_20170327.ipynb preprocessing_oss_nok_20170221_20170317.ipynb
 2251  ls prep*
 2252  cd 20170327_Ingestion/Input/
 2253  vim BSW_
 2254  vim BSW_ERI_20170301_20170327.csv
 2255  python feature_create.py --vendor 'nok' --lb 14 --la 1 --nh 500 --nu 400 --sample_days 40 --save True  --cache True  --o '../Src/ModData/'
 2256  vim ../Src/ModData/feature_nok_lbh14_lah1_0327.csv
 2257  python feature_create.py --vendor 'eri' --lb 14 --la 1 --nh 500 --nu 400 --sample_days 40 --save True  --cache True  --o '../Src/ModData/'
 2258  vim ../Src/ModData/feature_eri_lbh14_lah1_0327.csv
 2259  mkdir ../Src/ModData/feature0327
 2260  cd ../Src/ModData/feature0327
 2261  python feature_create.py --vendor 'eri' --lb 7 --la 1 --nh 500 --nu 400 --sample_days 40 --save True  --cache True  --o '../Src/ModData/feature0327/'
 2262  python feature_create.py --vendor 'eri' --lb 7 --la 8 --nh 500 --nu 400 --sample_days 40 --save True  --cache True  --o '../Src/ModData/feature0327/'
 2263  python feature_create.py --vendor 'nok' --lb 7 --la 1 --nh 500 --nu 400 --sample_days 40 --save True  --cache True  --o '../Src/ModData/feature0327'
 2264  pip install ConfigParser
 2265  pythohn model_selection.py -h
 2266  python model_selection.py --ifeat ../Src/ModData/feature0327/feature_eri_lbh7_lah1_0327.csv --conf config.ini --t Prob_Urgent --o ../Src/ModData/model_select
 2267  python model_selection.py --ifeat ../Src/ModData/feature0327/feature_eri_lbh7_lah1_0327.csv --iconf config.ini --t Prob_Urgent --o ../Src/ModData/model_select
 2268  python model_selection.py --ifeat ../Src/ModData/feature0327/feature_eri_lbh7_lah1_0327.csv --iconf config.ini --t Prob_Urgent --o ../Src/ModData/model_select --loglevel info --olog 'model_selection.log'
 2269  python model_selection.py --ifeat ../Src/ModData/feature0327feature_nok_lbh7_lah1_target_Urgent_0327.csv --iconf config.ini --t Prob_Urgent --o ../Src/ModData/model_select --loglevel info --olog 'model_selection.log'
 2270  python model_selection.py --ifeat ../Src/ModData/feature0327feature_nok_lbh7_lah1_target_Urgent_0327.csv --iconf config.ini --t Prob_Urgent --o ../Src/ModData/model_select --loglevel info --olog 'model_selection.log' --t Urgent
 2271  python model_selection.py --ifeat ../Src/ModData/feature0327feature_nok_lbh7_lah1_target_Urgent_0327.csv --iconf config.ini --t Prob_Urgent --o ../Src/ModData/model_select --loglevel info --olog 'model_selection.log' --t Prob_Urgent
 2272  python model_selection.py --ifeat ../Src/ModData/feature0327feature_nok_lbh7_lah1_target_Urgent_0327.csv --iconf config.ini --t Prob_Urgent --o ../Src/ModData/model_select --loglevel info --olog 'model_selection.log' 
 2273  python model_selection.py --ifeat ../Src/ModData/feature0327feature_nok_lbh7_lah1_target_Urgent_0327.csv --iconf config.ini --t Prob_Urgent --o ../Src/ModData/model_select --loglevel info --olog 'model_selection.log' --report 'std'
 2274  python model_selection.py --ifeat ../Src/ModData/feature0327feature_nok_lbh7_lah1_target_Urgent_0327.csv --iconf config.ini --t Prob_Urgent --o ../Src/ModData/model_select0328 --loglevel info --olog 'model_selection.log' --report 'std'
 2275  python model_selection.py --ifeat ../Src/ModData/feature0327feature_nok_lbh7_lah1_target_Urgent_0327.csv --iconf config.ini --t Prob_Urgent --o ../Src/ModData/model_select0328 --loglevel info --olog 'model_selection.log' --report 'csv'
 2276  python model_selection.py --ifeat ../Src/ModData/feature0327feature_nok_lbh7_lah1_target_Urgent_0327.csv --iconf config.ini --t Prob_Urgent --o ../Src/ModData/model_select0328.csv --loglevel info --olog 'model_selection.log' --report 'csv'
 2277  python model_selection.py --ifeat ../Src/ModData/feature0327feature_nok_lbh7_lah1_target_Urgent_0327.csv --iconf config.ini --t Prob_Urgent --o ../Src/ModData/model_select_batch_dt_0328.csv --loglevel info --olog 'model_selection.log' --report 'csv'
 2278  python model_selection.py --ifeat ../Src/ModData/feature_nok_lbh14_lah1_Prob_Urgent0327.csv --iconf config.ini --t Prob_Urgent --o ../Src/ModData/model_select_batch_dt_nok_0328.csv --loglevel info --olog 'model_selection.log' --report 'csv'
 2279  python model_selection.py --ifeat ../Src/ModData/feature_nok_lbh14_lah1_Prob_Urgent0327.csv --iconf config.ini --t Prob_Urgent --o ../Src/ModData/model_select_batch_dt_use_feature_nok_lbh14_lah1_Prob_Urgent0327.csv --loglevel info --olog 'model_selection.log' --report 'csv'
 2280  python model_selection.py --ifeat ../Src/ModData/feature_nok_lbh14_lah1_Prob_Urgent0327.csv --iconf config.ini --t Prob_Urgent --o ../Src/ModData/model_select_batch_use_feature_nok_lbh14_lah1_Prob_Urgent0327.csv --loglevel info --olog 'model_selection.log' --report 'csv'
 2281  python model_selection.py --ifeat ../Src/ModData/feature_nok_lbh14_lah1_Prob_Urgent0327.csv --iconf config.ini --t Prob_Urgent --o ../Src/ModData/model_select_batch_use_feature_nok_lbh14_lah1_Prob_Urgent0327.csv --loglevel info --olog 'model_selection.log' --report 'csv' --scoring 'precision'
 2282  python model_selection.py --ifeat ../Src/ModData/feature0327feature_nok_lbh7_lah1_target_Urgent_0327.csv --iconf config.ini --t Prob_Urgent --o ../Src/ModData/model_select_batch_0328.csv --loglevel info --olog 'model_selection.log' --report 'csv'
 2283  python eval_model_dt.py -i ../Src/ModData/feature_nok_lbh14_lah1_Prob_Urgent0327.csv  -o "../Src/ModData/setcolor_" --target "Prob_Urgent, Prob_FSME, Prob_Rectifier, Prob_FSMF, Prob_Breaker, Prob_FRGP, Prob_FXDA, Prob_BATT, Prob_FeederCable, Prob_OpticalCable, Prob_Others" --n_healthy '1000'  --vendor "nok" -exp True 
 2284  python eval_model_dt.py -i ../Src/ModData/feature_nok_lbh14_lah1_Prob_Urgent0327.csv  -o "../Src/ModData/best_dt_" --target Prob_Urgent --n_healthy '1000'  --vendor "nok" -exp True 
 2285  python feature_create.py --vendor 'eri' --lb 14 --la 1 --nh 2000 --nu 600 --sample_days 40 --save True  --cache True  --o '../Src/ModData/'
 2286  python feature_create.py --vendor 'nok' --lb 14 --la 1 --nh 2000 --nu 600 --sample_days 40 --save True  --cache True  --o '../Src/ModData/'
 2287  python eval_model_dt.py -i ../Src/ModData/feature_nok_lbh14_lah1_Prob_Urgent0328.csv  -o "../Src/ModData/best_dt_" --target Prob_Urgent --n_healthy 10000 --n_unhealthy 10000  --vendor "nok" -exp True 
 2288  pip install pypyodbc
 2289  jython
 2290  locate UcanAccess
 2291  locate ucanaccess
 2292  ls ~/.local/bin
 2293  ls ~/.local/bin/UCanAccess-4.0.1-bin
 2294  vz
 2295  python load_internal_log.py
 2296  echo $CLASSPATH
 2297  vim ../Src/Original_flat/ERI_YM_20170212.accdb
 2298  ln -s ~/.local/bin/UCanAccess-4.0.1-bin/loader/ucanload.jar ~/Documents/Driver/ucanload.jar
 2299  ln 
 2300  unlink
 2301  unlink ~/Documents/Driver/ucanload.jar
 2302  cp ~/.local/bin/UCanAccess-4.0.1-bin/loader/ucanload.jar ~/Documents/Driver/ucanload.jar
 2303  cd Projects/SB/Notebook
 2304  cd 20170306_Ingestion/Output
 2305  cd 20170310_Ingestion/Output
 2306  gzip -c OSS_Eri_20170116_20170127.csv > OSS_Eri_20170116_20170127.gz
 2307  jython load_internal_log.py
 2308  echo geproxy
 2309  cd SB_box_ingest_working_dir
 2310  cd 20170306_Ingestion
 2311  gzip -c oss_*
 2312  .tables 
 2313  for file in ./\necho $file 
 2314  echo $file 
 2315  for i in ./ \ndo \necho $i\ndone
 2316  for files in ./\ndo\necho $files\ndone
 2317  l .
 2318  for files in ./*\ndo\necho $files\ndone
 2319  echo $files
 2320  echo $files.gz
 2321  source convert_gz.sh
 2322  cd ../../Talend\ Working\ Directory
 2323  l ./*
 2324  vim convert_gz.sh
 2325  cd ../20170323_Ingestion/Input
 2326  for f in ./*\ndo\ngzip -c $f > ../../Talend\ Working\ Directory/$f.gz\ndone
 2327  cd ../../20170310_Ingestion/Output
 2328  gzip -c bsw_20170201_20170301.csv > bsw_20170201_20170301.csv.gz
 2329  cd 20170323_Ingestion
 2330  gzip -c nedb_2016summer_2017TQ.csv > ../../Talen_Working_Directory/nedb_2016summer_2017TQ.csv.gz
 2331  locate odbc
 2332  locate .freetds
 2333  ls /usr/local/etc/freetds.conf
 2334  tsql -S MSSQL_SB -U dssb -P b@nklak3_123
 2335  brew uninstall greetds
 2336  brew uninstall freetds
 2337  brew install freetds --with-unixodbc
 2338  less /usr/local/etc/freetds.conf
 2339  tsql -S [mssqlServerAlpha]
 2340  tsql
 2341  tsql -H
 2342  less /Library/ODBC/odbcinst.ini
 2343  isql alpidcdiha001v dssb b@nklak3_123
 2344  isql alpidcdiha001v dssb b@nklak3_123 -v
 2345  vim /Library/ODBC/odbcinst.ini 
 2346  vim /Library/ODBC/odbc.ini
 2347  less /Library/ODBC/odbc.ini
 2348  isql mssqlServerAlpha dssb b@nklak3_123
 2349  l .odbc.ini
 2350  l /usr/local/Cellar/freetds/1.00.27/etc
 2351  vim /usr/local/Cellar/freetds/1.00.27/etc/freetds.conf
 2352  l /usr/local/Cellar/unixodbc/2.3.4/etc/
 2353  l /usr/local/Cellar/unixodbc/2.3.4/etc/odbc.ini
 2354  cp /usr/local/Cellar/unixodbc/2.3.4/etc/odbc.ini /usr/local/Cellar/unixodbc/2.3.4/etc/odbc.ini.bak
 2355  rm /usr/local/Cellar/unixodbc/2.3.4/etc/odbc.ini
 2356  ln -s  /Library/ODBC/odbc.ini /usr/local/Cellar/unixodbc/2.3.4/etc/odbc.ini /usr/local/Cellar/unixodbc/2.3.4/etc/odbc.ini
 2357  ln -s /Library/ODBC/odbc.ini /usr/local/Cellar/unixodbc/2.3.4/etc/odbc.ini
 2358  cp /usr/local/Cellar/unixodbc/2.3.4/etc/odbcinst.ini /usr/local/Cellar/unixodbc/2.3.4/etc/odbcinst.ini.bak
 2359  rm /usr/local/Cellar/unixodbc/2.3.4/etc/odbcinst.ini
 2360  ln -s /Library/ODBC/odbcinst.ini /usr/local/Cellar/unixodbc/2.3.4/etc/odbcinst.ini
 2361  source  /usr/local/Cellar/unixodbc/2.3.4/etc/odbc.ini
 2362  less /usr/local/Cellar/unixodbc/2.3.4/etc/odbc.ini
 2363  less /Users/212339410/.odbc.ini
 2364  vim ~/.odbc.ini
 2365  tsql -S mssqlServerAlpha  -U dssb -P b@nklak3_123 
 2366  isql mssqlServerAlpha  dssb b@nklak3_123 -vodbcinst
 2367  odbcinst
 2368  odbcinst -l
 2369  cat /usr/local/Cellar/unixodbc/2.3.4/etc/ODBCDataSources
 2370  less /usr/local/Cellar/unixodbc/2.3.4/etc/ODBCDataSources
 2371  cat /usr/local/Cellar/unixodbc/2.3.4/etc/odbc.ini
 2372  isql
 2373  isql help
 2374  isql 
 2375  l /usr/local/Cellar/unixodbc/2.3.4/etc/odbcinst.ini
 2376  cd 20170306_Ingestion/Input
 2377  python preprocessing_bsw.py -h
 2378  tsql -C
 2379  odbcinst -j
 2380  python preprocessing_bsw.py --i ~/SB_box_ingest_working_dir/20170310_Ingestion/Input/BSW_ERI_20170201_0301.csv  --o ./ --dfrom 20170201 --dto 20170301
 2381  python preprocessing_bsw.py --i ~/SB_box_ingest_working_dir/20170310_Ingestion/Input/  --o ./ --dfrom 20170201 --dto 20170301
 2382  brew unlink midnight-commander
 2383  brew install mc
 2384  l *gz
 2385  vim /Users/212339410/.odbc.ini
 2386  cp /usr/local/Cellar/unixodbc/2.3.4/etc/odbc.ini /usr/local/Cellar/unixodbc/2.3.4/etc/odbc.bak0330.ini
 2387  vim /usr/local/etc/freetds.conf
 2388  isql mssqlServerAlpha  dssb b@nklak3_123 -v
 2389  vim /usr/local/Cellar/unixodbc/2.3.4/etc/odbc.ini
 2390  vim /usr/local/Cellar/unixodbc/2.3.4/etc/odbcinst.ini
 2391  unlink Projects
 2392  cd Data
 2393  ln -s /Users/212339410/Box/Analytics/Projects /Users/212339410/Projects
 2394  SB
 2395  cd Src
 2396  pip install upgrade
 2397  brew install gcc --without-multilib    
 2398  mkdir ~/tmp20170331
 2399  locate tstk
 2400  l /Users/212339410/anaconda/envs/py3k/lib/python3.5/site-packages/tstk
 2401  unlink /Users/212339410/anaconda/envs/py3k/lib/python3.5/site-packages/tstk
 2402  unlink /Users/212339410/anaconda2/envs/py35/lib/python3.5/site-packages/tstk
 2403  ln -s /Users/212339410/anaconda2/envs/py35/lib/python3.5/site-packages/tstk /Users/212339410/Box/Analytics/Python/tstk/tstk
 2404  l /Users/212339410/Box/Analytics/Python/tstk/tstk
 2405  unlink /Users/212339410/Box/Analytics/Python/tstk/tstk
 2406  l /Users/212339410/anaconda2/envs/py35/lib/python3.5/site-packages/tstk
 2407  l /Users/212339410/anaconda2/envs/py35/lib/python3.5/site-packages/
 2408  l /Users/212339410/anaconda2/envs/py35/lib/python3.5/site-packages/ | grep tstk
 2409  ln -s /Users/212339410/Box/Analytics/Python/tstk/tstk /Users/212339410/anaconda2/envs/py35/lib/python3.5/site-packages/tstk
 2410  /Users/212339410/Box/Analytics/Python/tstk/
 2411  unlink tstk
 2412  python feature_create.py --vendor 'nok' --lb 14 --la $n --nh 2000 --nu 600 --sample_days 40 --save True  --cache True  --o ~/tmp20170331/feature
 2413  python feature_create.py --vendor 'nok' --lb 14 --la 1 --nh 2000 --nu 600 --sample_days 40 --save True  --cache True  --o ~/tmp20170331/feature
 2414  {1..5}
 2415  for i in {1..5}\ndo \necho i\ndone
 2416  for i in {1..5}\ndo \necho $i\ndone
 2417  seq 1 10
 2418  echo `seq 1 10`
 2419  echo `seq 1 10 -s`
 2420  echo `seq -s 1 10 `
 2421  echo `seq -s ,  1 10 `
 2422  python feature_create.py --vendor 'nok' --lb 14 --la "2,3,4,5,6,7,8,9,10,11,12,13,14,15"  --nh 2000 --nu 600 --sample_days 40 --save True  --cache True  --o ~/tmp20170331/
 2423  python feature_create.py --vendor 'nok' --lb 14 --la `seq -s , 2 15 `  --nh 2000 --nu 600 --sample_days 40 --save True  --cache True  --o ~/tmp20170331/
 2424  superset
 2425  superset runserver
 2426  history | grep 
 2427  hisotry | grep fabmanager
 2428  history | grep password
 2429  history | grep superset
 2430  history 
 2431  history | 8283
 2432  history | grep 8283
 2433  for n in `seq 1 15`;\ndo \npython feature_create.py --vendor 'nok' --lb 14 --la $n --nh 2000 --nu 600 --sample_days 40 --save True  --cache True  --o ~/tmp20170331/feature\ndone
 2434  sudo pip install xgboost
 2435  sudo -
 2436  sudo
 2437  mkdir xgboost
 2438  git clone --recursive https://github.com/dmlc/xgboost 
 2439  export CC = gcc
 2440  cp make/config.mk .
 2441  vim make/config.mk
 2442  cp make/config.mk ./config.mk
 2443  python model_selection.py --ifeat ../Src/ModData/feature_nok_lbh14_lah1_Prob_Urgent0327.csv --iconf config.ini --t Prob_Urgent --o ../Src/ModData/model_select_batch_use_feature_nok_lbh14_lah1_Prob_Urgent0327.csv --loglevel info --olog 'model_selection.log' --report 'csv' --score 'precision'
 2444  l ../Src/ModData/feature_
 2445  l ../Src/ModData/feature_n*
 2446  python model_selection.py --ifeat ../Src/ModData/0328/feature_nok_lbh14_lah1_Prob_Urgent0327.csv --iconf config.ini --t Prob_Urgent --o ../Src/ModData/model_select_batch_use_feature_nok_lbh14_lah1_Prob_Urgent0327.csv --loglevel info --olog 'model_selection.log' --report 'csv' --score 'precision'
 2447  cd ~/Box/Analytics/Py
 2448  cd ~/Box/Analytics/Python
 2449  git commit -m 'fix detect_ts.py'
 2450  git push origin pr/36
 2451  git push -f origin 78f42c322546f748e9f8556044ed7db9de8daf63:pr/36
 2452  git checkout -- examples/Plotting using tstk.visualization.plot.ipynb
 2453  git checkout -- 
 2454  git branch -d pr/36
 2455  git branch -D pr/36
 2456  git fetch -all
 2457  git branch -vv
 2458  git remote show remote
 2459  git remote prune refs/remotes/origin/pr/36 stale
 2460  git fetch tstk
 2461  git tag
 2462  git fetch origin /remotes/origin/plot_update
 2463  git fetch origin origin/plot_update
 2464  git fetch origin plot_update
 2465  rm tstk/preprocessing/util.py
 2466  rm tstk/visualization/plotlyjs.py
 2467  git checkout -b plot_update origin/plot_update
 2468  branch
 2469  cp preprocessing_bsw_20170201_20170301.ipynb preprocessing_bsw_20170301_20170327.ipynb
 2470  python feature_create.py --vendor 'nok' --lb 14 --la 1  --nh 2000 --nu 600 --sample_days 40 --save True  --cache True  --o ../Src/ModData/0402/
 2471  python feature_create_v2.py --vendor 'nok' --lb 14 --la 1  --nh '' --nu 1000 --sample_days 40 --save True  --cache True  --o ../Src/ModData/0402/
 2472  seq 1, 15
 2473  for n in `seq 1 15`\ndo \necho $n\ndone
 2474  for n in `seq 1 15`\ndo \necho filename_lbh$n\ndone
 2475  for n in `seq 1 15`\ndo\necho model_selection.py --ifeat ../Src/ModData/feature_0331/featurefeature_nok_lbh14_lah$n_0331.csv  --iconf config.ini --t Prob_Urgent --o ../Src/ModData/model_select_batch_use_feature_nok_lbh14_lah$n_Prob_Urgent0327.csv --loglevel info --olog 'model_selection.log' --report 'csv' --score 'precision'\ndone
 2476  for n in `seq 1 15`\ndo\necho âmodel_selection.py --ifeat ../Src/ModData/feature_0331/featurefeature_nok_lbh14_lah$n_0331.csv  --iconf config.ini --t Prob_Urgent --o ../Src/ModData/model_select_batch_use_feature_nok_lbh14_lah$n_Prob_Urgent0327.csv --loglevel info --olog 'model_selection.log' --report 'csv' --score 'precisionââ\ndone\n
 2477  for n in `seq 1 15`\ndo\npython model_selection.py --ifeat ../Src/ModData/feature_0331/featurefeature_nok_lbh14_lah$n_0331.csv  --iconf config.ini --t Prob_Urgent --o ../Src/ModData/model_select_batch_use_feature_nok_lbh14_lah$n_Prob_Urgent0331.csv --loglevel info --olog 'model_selection.log' --report 'csv' --score 'precisionâ\ndone\n
 2478  for n in 'seq 1 15'\ndo\npython model_selection.py --ifeat ../Src/ModData/feature_0331/featurefeature_nok_lbh14_lah$n_0331.csv  --iconf config.ini --t Prob_Urgent --o ../Src/ModData/model_select_batch_use_feature_nok_lbh14_lah$n_Prob_Urgent0331.csv --loglevel info --olog 'model_selection.log' --report 'csv' --score 'precisionâ\ndone\n
 2479  for n in `seq 1 15`\ndo\npython model_selection.py --ifeat ../Src/ModData/feature_0331/featurefeature_nok_lbh14_lah$n_0331.csv  --iconf config.ini --t Prob_Urgent --o ../Src/ModData/model_select_batch_use_feature_nok_lbh14_lah$n_Prob_Urgent0331.csv --loglevel info --olog 'model_selection.log' --report 'csv' --score 'precision'\ndone
 2480  for n in `seq 1 2` \ndo \necho $n\ndone
 2481  for n in `ls ./*` \ndo \necho $n\ndone
 2482  for n in `ls ./*.py` \ndo \necho $n\ndone
 2483  for n in `ls ./*.py` \ndo \ngrep $n\ndone
 2484  for n in {1..10}\ndo \necho $n\ndone
 2485  for n in {1..10}\ndo \necho $n $\ndone
 2486  for n in {1..10}\ndo \necho '$n' $\ndone
 2487  for n in {1..10}\ndo \necho "$n" $\ndone
 2488  for n in `seq 1 15`\ndo\npython model_selection.py --ifeat ../Src/ModData/feature_0331/featurefeature_nok_lbh14_lah{$n}_0331.csv  --iconf config.ini --t Prob_Urgent --o ../Src/ModData/model_select_batch_use_feature_nok_lbh14_lah{$n}_Prob_Urgent0331.csv --loglevel info --olog 'model_selection.log' --report 'csv' --score 'precision'\ndone
 2489  for n in `seq 1 15`\ndo\necho $n; \npython model_selection.py --ifeat ../Src/ModData/feature_0331/featurefeature_nok_lbh14_lah$n_0331.csv  --iconf config.ini --t Prob_Urgent --o ../Src/ModData/model_select_batch_use_feature_nok_lbh14_lah$n_Prob_Urgent0331.csv --loglevel info --olog 'model_selection.log' --report 'csv' --score 'precision'\ndone
 2490  for $n in {1..10}
 2491  for n in {1..10}\ndo\necho $n+$n\ndone
 2492  for n in {1..3}\ndo\necho $n; \npython model_selection.py --ifeat ../Src/ModData/feature_0331/featurefeature_nok_lbh14_lah$n_0331.csv  --iconf config.ini --t Prob_Urgent --o ../Src/ModData/model_select_batch_use_feature_nok_lbh14_lah$n_Prob_Urgent0331_ --loglevel info --olog 'model_selection.log' --report 'csv' --score 'precision'\ndone
 2493  for n in {1..3}\ndo\necho lbh$n_0331; \n#python model_selection.py --ifeat ../Src/ModData/feature_0331/featurefeature_nok_lbh14_lah$n_0331.csv  --iconf config.ini --t Prob_Urgent --o ../Src/ModData/model_select_batch_use_feature_nok_lbh14_lah$n_Prob_Urgent0331_ --loglevel info --olog 'model_selection.log' --report 'csv' --score 'precision'\n\ndone
 2494  for n in {1..3}\ndo\necho lbh${n}_0331; \n#python model_selection.py --ifeat ../Src/ModData/feature_0331/featurefeature_nok_lbh14_lah$n_0331.csv  --iconf config.ini --t Prob_Urgent --o ../Src/ModData/model_select_batch_use_feature_nok_lbh14_lah$n_Prob_Urgent0331_ --loglevel info --olog 'model_selection.log' --report 'csv' --score 'precision'\n\ndone
 2495  for n in {1..3}\ndo\necho lbh${n}_0331; \n#python model_selection.py --ifeat ../Src/ModData/feature_0331/featurefeature_nok_lbh14_lah${n}_0331.csv  --iconf config.ini --t Prob_Urgent --o ../Src/ModData/model_select_batch_use_feature_nok_lbh14_lah${n}_Prob_Urgent0331_ --loglevel info --olog 'model_selection.log' --report 'csv' --score 'precision'\n\ndone
 2496  for n in {1..3}\ndo\necho lbh${n}_0331; \npython model_selection.py --ifeat ../Src/ModData/feature_0331/featurefeature_nok_lbh14_lah${n}_0331.csv  --iconf config.ini --t Prob_Urgent --o ../Src/ModData/model_select_batch_use_feature_nok_lbh14_lah${n}_Prob_Urgent0331_ --loglevel info --olog 'model_selection.log' --report 'csv' --score 'precision'\n\ndone
 2497  for n in {1..15}\ndo\necho lbh${n}_0331; \npython model_selection.py --ifeat ../Src/ModData/feature_0331/featurefeature_nok_lbh14_lah${n}_0331.csv  --iconf config.ini --t Prob_Urgent --o ../Src/ModData/model_select_batch_use_feature_nok_lbh14_lah${n}_Prob_Urgent0331_ --loglevel info --olog 'model_selection.log' --report 'csv' --score 'precision'\n\ndone
 2498  cd ../Src/ModData/feature_0331
 2499  cd ../Src/ModData/feature_0331\necho lbh${n}_0331; \npython model_selection.py --ifeat ../Src/ModData/feature_0331/featurefeature_nok_lbh14_lah${n}_0331_Prob_Urgent.csv  --iconf config.ini --t Prob_Urgent --o ../Src/ModData/model_select_batch_use_feature_nok_lbh14_lah${n}_Prob_Urgent0331_ --loglevel info --olog 'model_selection.log' --report 'csv' --score 'precision'\n\ndone
 2500  for n in {1..15}\ndo\necho lbh${n}_0331; \npython model_selection.py --ifeat ../Src/ModData/feature_0331/featurefeature_nok_lbh14_lah${n}_0331_Prob_Urgent.csv  --iconf config.ini --t Prob_Urgent --o ../Src/ModData/model_select_batch_use_feature_nok_lbh14_lah${n}_Prob_Urgent0331_ --loglevel info --olog 'model_selection.log' --report 'csv' --score 'precision'\n\ndone
 2501  for n in {1..1}\ndo\necho lbh${n}_0331; \npython model_selection.py --ifeat ../Src/ModData/feature_0331/featurefeature_nok_lbh14_lah${n}_0331_Prob_Urgent.csv  --iconf config.ini --t Prob_Urgent --o ../Src/ModData/model_select_batch_use_feature_nok_lbh14_lah${n}_Prob_Urgent0331_ --loglevel info --olog 'model_selection.log' --report 'csv' --score 'precision'\n\ndone
 2502  cp model_selection.py ./xgboost/model_selection.py
 2503  rm ./xgboost/model_selection.py
 2504  mv model_selection.py ./xgboost/model_selection.py
 2505  ln -s ./xgboost/model_selection.py model_selection.py
 2506  ln -s mbs.py ./xgboost/mbs.py
 2507  ln -s mbs.py /xgboost/mbs.py
 2508  ln -s `pwd`/mbs.py `pwd`/xgboost/mbs.py
 2509  ln -s `pwd`/qutils.py `pwd`/xgboost/qutils.py
 2510  ln -s `pwd`/process_alarm_names.py `pwd`/xgboost//process_alarm_names.py
 2511  ln -s `pwd`/process_alarm_names_eri.py `pwd`/xgboost/process_alarm_names_eri.py
 2512  ln -s /Users/212339410/Projects/SB/Python/process_pro.py `pwd`/xgboost/process_problem_areas.py
 2513  ln -s /Users/212339410/Projects/SB/Python/process_problem_areas.py `pwd`/xgboost/process_problem_areas.py
 2514  ln -s /Users/212339410/Projects/SB/Python/plot_v1.py `pwd`/xgboost/plot_v1.py
 2515  for n in {1..1}\ndo\necho lbh${n}_0331; \npython model_selection.py --ifeat ../Src/ModData/feature_0331/featurefeature_nok_lbh14_lah${n}_0331_Prob_Urgent.csv  --iconf config.ini --t Prob_Urgent --o ../Src/ModData/model_select_batch_use_feature_nok_lbh14_lah${n}_Prob_Urgent0331_ --loglevel info --olog 'model_selection.log' --report 'csv' --score 'accuracy'\n\ndone
 2516  python feature_create_v2.py --vendor 'nok' --lb 14 --la 1  --nh '' --nu 1000 --sample_days 40 --save True  --cache True  --o ../Src/ModData/0403/
 2517  for n in {1..10}\ndo\necho lbh${n}_0331; \npython model_selection.py --ifeat ../Src/ModData/feature_0331/featurefeature_nok_lbh14_lah${n}_0331_Prob_Urgent.csv  --iconf config.ini --t Prob_Urgent --o ../Src/ModData/model_select_batch_use_feature_nok_lbh14_lah${n}_Prob_Urgent0331_ --loglevel info --olog 'model_selection.log' --report 'csv' --score 'accuracy'\n\ndone
 2518  for n in {1..10}\ndo\necho lbh${n}_0331; \npython model_selection.py --ifeat ../Src/ModData/feature_0331/featurefeature_nok_lbh14_lah${n}_0331_Prob_Urgent.csv  --iconf config.ini --t Prob_Urgent --o ../Src/ModData/model_select_batch_use_feature_nok_lbh14_lah${n}_Prob_Urgent0331.csv --loglevel info --olog 'model_selection.log' --report 'csv' --score 'accuracy'\n\ndone
 2519  mkdir ../Src/ModData/model_selection_0403
 2520  for n in {1..10}\ndo\necho lbh${n}_0331; \npython model_selection.py --ifeat ../Src/ModData/feature_0331/featurefeature_nok_lbh14_lah${n}_0331_Prob_Urgent.csv  --iconf config.ini --t Prob_Urgent --o ../Src/ModData/model_selection_0403/model_select_feature_nok_lbh14_lah${n}_Prob_Urgent0331.csv --loglevel info --olog 'model_selection.log' --report 'csv' --score 'accuracy'\n\ndone
 2521  locate xgboost
 2522  locate -h
 2523  man locate 
 2524  /usr/libexec/locate.updatedb
 2525  python predict_top_prob.py 
 2526  python predict_top_prob.py  --t Prob_Urgent --i ''
 2527  python predict_top_prob.py  --t Prob_Urgent --i '' --o ../Src/ModData/results_predict_top10.csv
 2528  for n in {1..1}\ndo\necho lbh${n}_0331; \npython model_selection.py --ifeat ../Src/ModData/feature_0331/featurefeature_nok_lbh14_lah${n}_0331_Prob_Urgent.csv  --iconf config.ini --t Prob_Urgent --o ../Src/ModData/model_selection_0403/model_select_feature_nok_lbh14_lah${n}_Prob_Urgent0331.csv --loglevel info --olog 'model_selection.log' --report 'csv' --score 'accuracy'\n\ndone
 2529  for n in {1..15}\ndo\necho lbh${n}_0331; \npython model_selection.py --ifeat ../Src/ModData/feature_0331/featurefeature_nok_lbh14_lah${n}_0331_Prob_Urgent.csv  --iconf config.ini --t Prob_Urgent --o ../Src/ModData/model_selection_0403/model_select_feature_nok_lbh14_lah${n}_Prob_Urgent0331.csv --loglevel info --olog 'model_selection.log' --report 'csv' --score 'accuracy'\n\ndone
 2530  for n in {2..15}\ndo\necho lbh${n}_0331; \npython model_selection.py --ifeat ../Src/ModData/feature_0331/featurefeature_nok_lbh14_lah${n}_0331_Prob_Urgent.csv  --iconf config.ini --t Prob_Urgent --o ../Src/ModData/model_selection_0403/model_select_feature_nok_lbh14_lah${n}_Prob_Urgent0331.csv --loglevel info --olog 'model_selection.log' --report 'csv' --score 'accuracy'\n\ndone
 2531  python model_selection.py --ifeat ../Src/ModData/0403/feature_nok_lbh14_lah1_Prob_Urgent_0403.csv --iconf config.ini --t Prob_Urgent --o ../Src/ModData/model_select_use_week_feature_nok_lbh14_lah1_Prob_Urgent0403.csv --loglevel info --olog 'model_selection.log' --report 'csv' --score 'precision'
 2532  python model_selection.py --ifeat ../Src/ModData/0403/feature_nok_lbh14_lah1_Prob_Urgent_0403.csv --iconf config.ini --t Prob_Urgent --o ../Src/ModData/model_select_use_week_feature_nok_lbh14_lah1_Prob_Urgent0403.csv --loglevel info --olog 'model_selection.log' --report 'csv' --score 'accracy'
 2533  python model_selection.py --ifeat ../Src/ModData/0403/feature_nok_lbh14_lah1_Prob_Urgent_0403.csv --iconf config.ini --t Prob_Urgent --o ../Src/ModData/model_select_use_week_feature_nok_lbh14_lah1_Prob_Urgent0403.csv --loglevel info --olog 'model_selection.log' --report 'csv' --score 'accuracy'
 2534  python eval_model_rf.py -i "../Src/ModData/0403/feature_nok_lbh14_lah1_Prob_Urgent_0403.csv" -o "../Src/ModData/week_model_" --target "Prob_Urgent" --n_healthy '1000'  --vendor "nok" -exp True  
 2535  python eval_model_rf.py -i "../Src/ModData/0403/feature_nok_lbh14_lah1_Prob_Urgent_0403.csv" -o "../Src/ModData/week_model_" --target "Prob_Urgent" --n_healthy '1000'  --n_unhealthy '1000'  --vendor "nok" -exp True  
 2536  python eval_model_dt.py -i "../Src/ModData/0403/feature_nok_lbh14_lah1_Prob_Urgent_0403.csv" -o "../Src/ModData/week_model_" --target "Urgent" --n_healthy '1000'  --n_unhealthy '1000'  --vendor "nok" -exp True  
 2537  python eval_model_dt.py -i "../Src/ModData/0403/feature_nok_lbh14_lah1_Prob_Urgent_0403.csv" -o "../Src/ModData/week_model_" --target "Prob_Urgent" --n_healthy '1000'  --n_unhealthy '1000'  --vendor "nok" -exp True  
 2538  python eval_model_dt.py -i "../Src/ModData/0403/feature_nok_lbh14_lah1_Prob_Urgent_0403.csv" -o "../Src/ModData/week_model_" --target "Prob_Urgent" --n_healthy '1000'  --n_unhealthy '10000'  --vendor "nok" -exp True  
 2539  python eval_model_dt.py -i "../Src/ModData/0403/feature_nok_lbh14_lah1_Prob_Urgent_0403.csv" -o "../Src/ModData/week_model_" --target "Prob_Urgent" --n_healthy '01000'  --n_unhealthy '10000'  --vendor "nok" -exp True  
 2540  python eval_model_rf.py -i "../Src/ModData/0403/feature_nok_lbh14_lah1_Prob_Urgent_0403.csv" -o "../Src/ModData/week_model_" --target "Prob_Urgent" --n_healthy '01000'  --n_unhealthy '10000'  --vendor "nok" -exp True  
 2541  python eval_model_rf.py -i "../Src/ModData/0403/feature_nok_lbh14_lah1_Prob_Urgent_0403.csv" -o "../Src/ModData/week_model_" --target "Prob_Urgent" --n_healthy '01000'  --n_unhealthy '10000'  --vendor "nok" -exp True  --top_problem Urgent
 2542  python feature_create.py --vendor 'nok' --lb 14 --la 1  --nh '' --nu 1000 --sample_days 40 --save True  --cache True  --o ../Src/ModData/0402/
 2543  python feature_create.py --vendor 'nok' --lb 14 --la 1  --nh '' --nu 1000 --sample_days 40 --save True  --cache True  --o ../Src/ModData/0402/gil
 2544  python pip install flake8
 2545  flake8 ./
 2546  flake8 ../../../Python/tstk/tstk
 2547  rm preprocessing_oss_nok_20170327.ipynb
 2548  git rm void.txt
 2549  cd Box/Analytics/Python/tstk
 2550  python feature_create_v2.py --vendor 'nok' --lb 14 --la 1  --nh '' --nu 1000 --sample_days 40 --save True  --cache True  --o ../Src/ModData/0404_dataset/ --dfrom '2016-07-07 07:27:46' --dto '2016-08-29 16:07:41'
 2551  python feature_create_v2.py --vendor 'nok' --lb 14 --la 1  --nh '' --nu 1000 --sample_days 40 --save True  --cache False  --o ../Src/ModData/0404_dataset/ --dfrom '2016-07-07 07:27:46' --dto '2016-08-29 16:07:41'
 2552  python update_schema.py ~/Box/DSS\ Internal-Softbank/12\ Working\ Directory\ for\ Ingestion/20170328_Ingestion/Output/Tokyo_RAN_OSS_Eri_20170322_20170327.csv  ../Src/Schema/oss_nok_mod_v4.xml  ../Src/Schema/oss_nok_v5.xml
 2553  vim ~/Box/DSS\ Internal-Softbank/12\ Working\ Directory\ for\ Ingestion/20170328_Ingestion/Output/Tokyo_RAN_OSS_Eri_20170322_20170327.csv
 2554  head -n 100  ~/Box/DSS\ Internal-Softbank/12\ Working\ Directory\ for\ Ingestion/20170328_Ingestion/Output/Tokyo_RAN_OSS_Eri_20170322_20170327.csv > tmp_oss_nok_0322.csv
 2555  open tmp_oss_nok_0322.csv
 2556  vim tmp_oss_nok_0322.csv
 2557  head -n 100  ~/Box/DSS\ Internal-Softbank/12\ Working\ Directory\ for\ Ingestion/20170310_Ingestion/Output/OSS_Eri_20170128_20170220.csv > tmp_oss_eri_0128.csv
 2558  mv tmp_oss_nok_0322.csv tmp_oss_eri_0322.csv
 2559  open tmp_oss_eri_0128.csv
 2560  python update_schema.py "/Users/212339410/Box/DSS Internal-Softbank/12 Working Directory for Ingestion/20170327_Ingestion/Output/oss_nok_all_20170221_20170317.csv"  ../Src/Schema/oss_nok_mod_v4.xml  ../Src/Schema/oss_nok_v5.xml
 2561  python feature_create_v2.py --vendor 'nok' --lb 14 --la 1  --nh 100 --nu 1000 --sample_days 40 --save True  --cache False  --o ../Src/ModData/0404_dataset/ --dfrom '2016-07-07 07:27:46' --dto '2016-08-29 16:07:41'
 2562  pip install --upgrade notebook
 2563  .schema BSWn
 2564  python feature_create_v2.py --vendor 'nok' --lb 14 --la 1  --nh 10 --nu 10 --sample_days 40 --save True  --cache False  --o ../Src/ModData/0404_dataset/ --dfrom '2016-07-07 07:27:46' --dto '2016-08-29 16:07:41'
 2565  python feature_create_v2.py --vendor 'nok' --lb 14 --la 1  --nh 10 --nu 10 --sample_days 40 --save True  --cache True  --o ../Src/ModData/0404_dataset/ --dfrom '2016-07-07 07:27:46' --dto '2016-08-29 16:07:41'
 2566  python feature_create_v2.py --vendor 'nok' --lb 14 --la 1  --nh ""  --nu "" --sample_days 40 --save True  --cache True  --o ../Src/ModData/0404_dataset_mass/ --dfrom '2016-07-07 00:00:00' --dto '2016-08-28 23:59:59'
 2567  seq 1 14
 2568  seq 1, 14
 2569  seq 1 2
 2570  for n in 1..14\necho $n
 2571  for n in {1..14}\necho $n
 2572  for n in {1..14}\ndo ;echo $n; done
 2573  sq -s, 1 14
 2574  seq -s, 1 14
 2575  python feature_create_v2.py --vendor 'nok' --lb "1,2,3,4,5,6,7,8,9,10,11,12,13,14" --la 1  --nh ""  --nu "" --sample_days 40 --save True  --cache True  --o ../Src/ModData/0404_dataset_mass/ --dfrom '2016-07-07 00:00:00' --dto '2016-08-28 23:59:59'
 2576  python feature_create_v2.py --vendor 'nok' --lb "1,2,3,4,5,6,7,8,9,10,11,12,13,14" --la 1  --nh ""  --nu "" --sample_days 40 --save True  --cache True  --o ../Src/ModData/0404_dataset_mass/ --dfrom '2017-01-04 00:00:00' --dto '2017-02-28 23:59:59'
 2577  python feature_create_v2.py --vendor 'nok' --lb 14 --la 1  --nh 10 --nu 10 --sample_days 40 --save True  --cache True  --o ../Src/ModData/0404_dataset/ --dfrom '2017-01-04 00:00:00' --dto '2017-02-28 23:59:59'
 2578  seq -s, 14 1
 2579  python feature_create_v2.py --vendor 'nok' --lb "1,2,3,4,5,6,7,8,9,10,11,12,13,14" --la 1  --nh ""  --nu "" --sample_days 40 --save True  --cache True  --o ../../../../DSS\ Internal-Softbank/40\ Intermediate\ Results/20170403\ Feature\ Set\ \(training\ vs\ validation\)/  --dfrom '2017-01-04 00:00:00' --dto '2017-02-28 23:59:59'
 2580  python feature_create_v2.py --vendor 'nok' --lb "14,13,12,11,10,9,8,7,6,5,4,3,2,1" --la 1  --nh ""  --nu "" --sample_days 20 --save True  --cache True  --o ../../../../DSS\ Internal-Softbank/40\ Intermediate\ Results/20170403\ Feature\ Set\ \(training\ vs\ validation\)/  --dfrom '2017-01-04 00:00:00' --dto '2017-02-28 23:59:59'
 2581  python feature_create_v2.py --vendor 'nok' --lb "14,12,10,8,6,4,2" --la 1  --nh ""  --nu "" --sample_days 40 --save True  --cache True  --o ../Src/ModData/0404_dataset_mass/ --dfrom '2016-07-07 00:00:00' --dto '2016-08-28 23:59:59'
 2582  python feature_create_v2.py --vendor 'eri' --lb 14 --la 1  --nh 10 --nu 10 --sample_days 40 --save True  --cache True  --o ../Src/ModData/0404_dataset/ --dfrom '2017-01-04 00:00:00' --dto '2017-02-28 23:59:59'
 2583  python feature_create_v2.py --vendor 'nok' --lb "14,12,10,8,6,4,2" --la 1  --nh ""  --nu "" --sample_days 20 --save True  --cache True  --o ../../../../DSS\ Internal-Softbank/40\ Intermediate\ Results/20170403\ Feature\ Set\ \(training\ vs\ validation\)/  --dfrom '2017-01-04 00:00:00' --dto '2017-02-28 23:59:59'
 2584  python feature_create_v2.py --vendor 'nok' --lb "14,12,10,8,6,4,2" --la 1  --nh ""  --nu "" --sample_days 40 --save True  --cache True  --o ../../../../DSS\ Internal-Softbank/40\ Intermediate\ Results/20170403\ Feature\ Set\ \(training\ vs\ validation\)/ --dfrom '2016-07-07 00:00:00' --dto '2016-08-28 23:59:59'
 2585  python feature_create_v2.py --vendor 'eri' --lb 14 --la 1  --nh 10 --nu 10 --sample_days 40 --save True  --cache True  --o ../Src/ModData/0404_dataset/ --dfrom '2017-01-16 00:00:00' --dto '2017-02-28 23:59:59'
 2586  python feature_create_v2.py --vendor 'eri' --lb 14 --la 1  --nh "" --nu "" --sample_days 10 --save True  --cache True  --o ../../../../DSS\ Internal-Softbank/40\ Intermediate\ Results/20170403\ Feature\ Set\ \(training\ vs\ validation\)/ --dfrom '2017-01-16 00:00:00' --dto '2017-03-12 23:59:59'
 2587  python feature_create_v2.py --vendor 'eri' --lb 14 --la 1  --nh "" --nu "" --sample_days 10 --save True  --cache True  --o ../../../../DSS\ Internal-Softbank/40\ Intermediate\ Results/20170403\ Feature\ Set\ \(training\ vs\ validation\)/ --dfrom '2017-02-13 00:00:00' --dto '2017-03-27 23:59:59'
 2588  python feature_create_v2.py --vendor 'eri' --lb 14 --la 1  --nh "" --nu "" --sample_days 10 --save True  --cache True  --o ../../../../DSS\ Internal-Softbank/40\ Intermediate\ Results/20170403\ Feature\ Set\ \(training\ vs\ validation\)/ --dfrom '2017-03-13 00:00:00' --dto '2017-03-27 23:59:59'
 2589  python feature_create_v2.py --vendor 'eri' --lb 14 --la 1  --nh "" --nu "" --sample_days 13 --save True  --cache True  --o ../../../../DSS\ Internal-Softbank/40\ Intermediate\ Results/20170403\ Feature\ Set\ \(training\ vs\ validation\)/ --dfrom '2017-03-13 00:00:00' --dto '2017-03-27 23:59:59'
 2590  python feature_create_v2.py --vendor 'eri' --lb "14, 8, 4" --la 1  --nh "" --nu "" --sample_days 13 --save True  --cache True  --o ../../../../DSS\ Internal-Softbank/40\ Intermediate\ Results/20170403\ Feature\ Set\ \(training\ vs\ validation\)/ --dfrom '2017-03-13 00:00:00' --dto '2017-03-27 23:59:59'
 2591  python feature_create_v2.py --vendor 'eri' --lb "14, 8, 4" --la 1  --nh "" --nu "" --sample_days 13 --save True  --cache True  --o ../../../../DSS\ Internal-Softbank/40\ Intermediate\ Results/20170403\ Feature\ Set\ \(training\ vs\ validation\)/ --dfrom '2017-03-11 00:00:00' --dto '2017-03-27 23:59:59'
 2592  python feature_create_v2.py --vendor 'nok' --lb "14,8,4" --la 1  --nh ""  --nu "" --sample_days 40 --save True  --cache True  --o ../../../../DSS\ Internal-Softbank/40\ Intermediate\ Results/20170403\ Feature\ Set\ \(training\ vs\ validation\)/ --dfrom '2017-03-01 00:00:00' --dto '2017-03-17 23:59:59'
 2593  python feature_create_v2.py --vendor 'nok' --lb "10" --la 1  --nh ""  --nu "" --sample_days 40 --save True  --cache True  --o ../../../../DSS\ Internal-Softbank/40\ Intermediate\ Results/20170403\ Feature\ Set\ \(training\ vs\ validation\)/ --dfrom '2017-03-01 00:00:00' --dto '2017-03-17 23:59:59'
 2594  python feature_create_v2.py --vendor 'nok' --lb "2, 6" --la 1  --nh ""  --nu "" --sample_days 40 --save True  --cache True  --o ../../../../DSS\ Internal-Softbank/40\ Intermediate\ Results/20170403\ Feature\ Set\ \(training\ vs\ validation\)/ --dfrom '2017-03-01 00:00:00' --dto '2017-03-17 23:59:59'
 2595  python feature_create_v2.py --vendor 'nok' --lb "12" --la 1  --nh ""  --nu "" --sample_days 40 --save True  --cache True  --o ../../../../DSS\ Internal-Softbank/40\ Intermediate\ Results/20170403\ Feature\ Set\ \(training\ vs\ validation\)/ --dfrom '2017-03-01 00:00:00' --dto '2017-03-17 23:59:59'
 2596  python feature_create_v2.py --vendor 'eri' --lb "14, 8, 4" --la 1  --nh "" --nu "" --sample_days 10 --save True  --cache True  --o ../../../../DSS\ Internal-Softbank/40\ Intermediate\ Results/20170403\ Feature\ Set\ \(training\ vs\ validation\)/ --dfrom '2017-01-16 00:00:00' --dto '2017-03-12 23:59:59'
 2597  python feature_create_v2.py --vendor 'eri' --lb "14, 8, 4" --la 1  --nh "" --nu "" --sample_days 40 --save True  --cache True  --o ../../../../DSS\ Internal-Softbank/40\ Intermediate\ Results/20170403\ Feature\ Set\ \(training\ vs\ validation\)/s40_ --dfrom '2017-01-16 00:00:00' --dto '2017-03-12 23:59:59'
 2598  python feature_create_v2.py --vendor 'nok' --lb 14  --la 0  --nh ""  --nu "" --sample_days 40 --save True  --cache True  --o ../../../../DSS\ Internal-Softbank/40\ Intermediate\ Results/20170403\ Feature\ Set\ \(training\ vs\ validation\)/ --dfrom '2017-03-01 00:00:00' --dto '2017-03-17 23:59:59'
 2599  less /Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170403 Feature Set (training vs validation)/feature_date170116_170312_eri_t0_lbh14_lah1_0405.csv
 2600  less '/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170403 Feature Set (training vs validation)/feature_date170116_170312_eri_t0_lbh14_lah1_0405.csv'
 2601  less '/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170403 Feature Set (training vs validation)/feature_date170311_170327_eri_t0_lbh14_lah1_0405.csv\n'
 2602  less '/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170403 Feature Set (training vs validation)/feature_date170311_170327_eri_t0_lbh14_lah1_0405.csv'
 2603  python feature_create_v2.py --vendor 'eri' --lb 14 --la 1Â  --nh 2000 --nu "" --sample_days 40 --save TrueÂ  --cache TrueÂ  --o ../../../../DSS\ Internal-Softbank/40\ Intermediate\ Results/20170403\ Feature\ Set\ \(training\ vs\ validation\)/s40_nh2000_ --dfrom '2017-03-11 00:00:00' --dto '2017-03-27 23:59:59'
 2604  python feature_create_v2.py --vendor 'eri' --lb '14' --la 1Â  --nh '2000' --nu "" --sample_days 40 --save TrueÂ  --cache TrueÂ  --o ../../../../DSS\ Internal-Softbank/40\ Intermediate\ Results/20170403\ Feature\ Set\ \(training\ vs\ validation\)/s40_nh2000_ --dfrom '2017-03-11 00:00:00' --dto '2017-03-27 23:59:59'
 2605  python feature_create_v2.py --vendor 'eri' --lb '14' --la 1Â  --nh '2000' --nu "" --sample_days 40  --save True --cache TrueÂ  --o ../../../../DSS\ Internal-Softbank/40\ Intermediate\ Results/20170403\ Feature\ Set\ \(training\ vs\ validation\)/s40_nh2000_ --dfrom '2017-03-11 00:00:00' --dto '2017-03-27 23:59:59'
 2606  python feature_create_v2.py --vendor 'eri' --lb '14' --la 1Â  --nh '2000' --nu "" --sample_days 40  --save True --cache True --o ../../../../DSS\ Internal-Softbank/40\ Intermediate\ Results/20170403\ Feature\ Set\ \(training\ vs\ validation\)/s40_nh2000_ --dfrom '2017-02-28 00:00:00' --dto '2017-03-27 23:59:59'
 2607  python feature_create_v2.py --vendor 'eri' --lb '14' --la 1Â  --nh '2000' --nu "" --sample_days 40  --save True --cache True --o ../../../../DSS\ Internal-Softbank/40\ Intermediate\ Results/20170403\ Feature\ Set\ \(training\ vs\ validation\)/s40_nh2000_ --dfrom '2017-03-11 00:00:00' --dto '2017-03-27 23:59:59'
 2608  python feature_create_v2.py --vendor 'eri' --lb '14' --la 1Â  --nh '2000' --nu "" --sample_days 40  --save True --cache True --o ../../../../DSS\ Internal-Softbank/40\ Intermediate\ Results/20170403\ Feature\ Set\ \(training\ vs\ validation\)/s40_nh2000_deep --dfrom '2017-03-11 00:00:00' --dto '2017-03-27 23:59:59'
 2609  python feature_create_v2.py --vendor 'eri' --lb '14' --la 1Â  --nh '2000' --nu "" --sample_days 40  --save True --cache True --o ../../../../DSS\ Internal-Softbank/40\ Intermediate\ Results/20170403\ Feature\ Set\ \(training\ vs\ validation\)/s40_nh2000_deep --dfrom '2017-02-28 00:00:00' --dto '2017-03-27 23:59:59'
 2610  cp preprocessing_oss_nok_20170221_20170317.ipynb preprocessing_oss_nok_20170317_20170327.ipynb
 2611  python feature_create_v2.py --vendor 'nok' --lb 14  --la 1  --nh ""  --nu "" --sample_days 40 --save True  --cache True  --o ../../../../DSS\ Internal-Softbank/40\ Intermediate\ Results/20170403\ Feature\ Set\ \(training\ vs\ validation\)/ --dfrom '2017-02-15 00:00:00' --dto '2017-03-17 23:59:59'
 2612  ln -s process_alarm_names_nok.py ./xgboost/process_alarm_names_nok.py
 2613  ln -s process_alarm_names_nok.py `pwd`/xgboost/process_alarm_names_nok.py
 2614  ln -s process_alarm_names_nok.py /Users/212339410/Box/Analytics/Projects/SB/Python/xgboost/process_alarm_names_nok.py
 2615  ln -s /Users/212339410/Box/Analytics/Projects/SB/Python/process_alarm_names_nok.py /Users/212339410/Box/Analytics/Projects/SB/Python/xgboost/process_alarm_names_nok.py
 2616  python model_selection.py --ifeat ../Src/ModData/feature_nok_0406.csv  --iconf config.ini --t Prob_Urgent --o ../Src/ModData/model_select_use_week_feature_nok_0406.csv --loglevel info --olog 'model_selection.log' --report 'csv' --score 'accracy'
 2617  python model_selection.py --ifeat ../Src/ModData/feature_nok_0406.csv  --iconf config.ini --t Prob_Urgent --o ../Src/ModData/model_select_use_week_feature_nok_0406.csv --loglevel info --olog 'model_selection.log' --report 'csv' --score 'accuracy'
 2618  python model_selection.py --ifeat ../Src/ModData/feature_nok_0406.csv  --iconf prediction_validation.ini --t Prob_Urgent --o ../Src/ModData/model_select_use_week_feature_nok_0406.csv --loglevel info --olog 'model_selection.log' --report 'csv' --score 'accuracy'
 2619  python model_selection.py --ifeat ../Src/ModData/feature_nok_0406.csv  --iconf prediction_validation.ini --t Prob_Urgent --o ../Src/ModData/0406_validation/model_select_use_feature_date170104_170228_nok_t0_lbh14_lah1_0405.csv --loglevel info --olog 'model_selection.log' --report 'csv' --score 'accuracy'
 2620  python feature_create_v2.py --vendor 'nok' --lb 14  --la 0  --nh ""  --nu "" --sample_days 40 --save True  --cache True  --o ../../../../DSS\ Internal-Softbank/40\ Intermediate\ Results/20170403\ Feature\ Set\ \(training\ vs\ validation\)/prediction_ --dfrom '2017-03-13 00:00:00' --dto '2017-03-27 23:59:59'
 2621  python feature_create_v2.py --vendor 'nok' --lb 14  --la 0  --nh ""  --nu "" --sample_days 40 --save True  --cache True  --o ../../../../DSS\ Internal-Softbank/40\ Intermediate\ Results/20170403\ Feature\ Set\ \(training\ vs\ validation\)/prediction_ --dfrom '2017-02-27 00:00:00' --dto '2017-03-27 23:59:59'
 2622  python model_selection.py --ifeat ../Src/ModData/feature_nok_0406.csv  --iconf prediction_validation.ini --t Prob_Urgent --o ../Src/ModData/0406_validation/model_select_prec_use_feature_date170104_170228_nok_t0_lbh14_lah1_0405.csv --loglevel info --olog 'model_selection.log' --report 'csv' --score 'precision'
 2623  python feature_create_v2.py --vendor 'nok' --lb 14  --la 0  --nh ""  --nu "" --sample_days 40 --save True  --cache True  --o ../../../../DSS\ Internal-Softbank/40\ Intermediate\ Results/20170403\ Feature\ Set\ \(training\ vs\ validation\)/prediction_ --dfrom '2017-01-31 00:00:00' --dto '2017-02-28 23:59:59'
 2624  python feature_create_v2.py --vendor 'nok' --lb 14  --la 0  --nh ""  --nu "" --sample_days 40 --save True  --cache True  --o ../Src/ModData/prediction_ --dfrom '2017-02-13 00:00:00' --dto '2017-02-28 23:59:59'
 2625  python model_selection.py --ifeat ../Src/ModData/feature_date170116_170312_eri_t0_lbh14_lah1_0405.csv  --iconf prediction_validation.ini --t Prob_Urgent --o ../Src/ModData/0406_validation/model_select_prec_use_feature_date170116_170312_eri_t0_lbh14_lah1_0405.csv --loglevel info --olog 'model_selection.log' --report 'csv' --score 'precision'
 2626  python eval_model_dt.py -i "../Src/ModData/prediction_feature_date170213_170228_nok_t0_lbh14_lah0_0406.csv" -o "../Src/ModData/model_" --target "Prob_Urgent" --n_healthy '1000'  --n_unhealthy '10000'  --vendor "nok" -exp True  
 2627  cd python-package
 2628  sudo python setup.py install
 2629  head -n 2000 OSS_Eri_20170116_20170127.csv > OSS_Eri_20170116_20170127_2000.csv
 2630  head -n 2000 BSW2016.csv > BSW2016_2000.csv
 2631  head -n 2000 NEDB_NOKERI_TQ.utf8.csv > NEDB_NOKERI_TQ_2000.csv
 2632  cd ~/Box/Analytics/Data/MBS2016
 2633  cd UTF8_
 2634  tail -n 2000 WOT_NOK_20151201_20170201.utf8.csv > WOT_NOK_20151201_20170201.utf8_2000.csv
 2635  cd ../Original_flat
 2636  tail -n 2000 NSN1.20160707_20160721.csv > NSN1.20160707_20160721_2000.csv
 2637  head -n 2000 WOT_NOK_20151201_20170201.utf8.csv > WOT_NOK_20151201_20170201.utf8_2000.csv
 2638  tail -n 2000 TTE_NOK_20151201_20170201.csv > TTE_NOK_20151201_20170201_2000.csv
 2639  cd ../UTF8_bom
 2640  head -n 2000 NEDB_NOK_Q.utf8.csv NEDB_NOK_Q.utf8_2000.csv
 2641  head -n 2000 NEDB_NOK_Q.utf8.csv > NEDB_NOK_Q.utf8_2000.csv
 2642  brew install ffmpeg
 2643  brew cask install x-quartz #dependency for gifsicle, only required for mountain-lion and above
 2644  brew cask install xquartz #dependency for gifsicle, only required for mountain-lion and above
 2645  which quartzz
 2646  which quartz
 2647  ln -s /Users/212339410/Box/Analytics/Python /Users/212339410/Python
 2648  rm abc.rst
 2649  git clone git@github.build.ge.com:IndustrialDataScience/tstk.git tstk.gh-pages
 2650  cd tstk.gh-pages
 2651  cd ~/Documents/WebEx
 2652  uninstall ffmpeg
 2653  brew uninstall ffmpeg
 2654  .tree
 2655  tree .
 2656  tree -l 2
 2657  tree -L 2
 2658  pip install git+ssh://github.build.ge.com/IndustrialDataScience/tstk.git
 2659  pip install pip
 2660  sudo pip install git+ssh://github.build.ge.com/IndustrialDataScience/tstk.git
 2661  git ls-remote
 2662  git remote add upstream git@github.build.ge.com:IndustrialDataScience/tstk.git
 2663  git checkout --track upstream/gh-pages
 2664  gut checkout master
 2665  brew install gifsicle
 2666  ffmpeg -i TSTK_documentation_clone_ghpages0410.mov -s 600x400 -pix_fmt rgb24 -r 10  -f gif - | gifsicle --optimize=3 --delay=3 > tstk_doc_clone_ghpages.gif 
 2667  ffmpeg -i TSTK_documentation_clone_ghpages0410.mov -s 800x600 -pix_fmt rgb8 -r 10  -f gif - | gifsicle --optimize=3 --delay=3 > tstk_doc_clone_ghpages.gif 
 2668  ffmpeg -i TSTK_documentation_clone_ghpages0410.mov -s 600x600 -r 10  -f gif - | gifsicle --optimize=3 --delay=3 > tstk_doc_clone_ghpages.gif 
 2669  ffmpeg -i TSTK_documentation_clone_ghpages0410.mov  -r 10  -f gif - | gifsicle --optimize=3 --delay=3 > tstk_doc_clone_ghpages.gif 
 2670  brew cask install xquartz
 2671  xquartz
 2672  brew install ffmpeg imagemagick gifsicle pkg-config
 2673  gem install screengif     
 2674  sudo gem install screengif     
 2675  screenfig
 2676  screengif
 2677  screengif -i TSTK_documentation_clone_ghpages0410.mov -o tstk_clone_ghpages.gif 
 2678  cd tstk/
 2679  mkdir images
 2680  cp tstk_doc_clone_ghpages.gif ~/Python/tstk/doc/images
 2681  cp tstk_doc_clone_ghpages.gif ~/Python/tstk/doc/images/tstk_doc_clone_ghpages.gif
 2682  cp tstk_clone_ghpages.gif ~/Python/tstk/doc/images/tstk_doc_clone_ghpages.gif
 2683  mkdir predix_studio
 2684  cd ~/Box/Analytics/Data
 2685  cd MBS2016
 2686  cd UTF8_bom
 2687  cd 20170328_Ingestion
 2688  head Tokyo_RAN_OSS_Eri_20170322_20170327.csv 
 2689  head -n 3000 Tokyo_RAN_OSS_Eri_20170322_20170327.csv > ~/Projects/SB/Python/predix_studio/Tokyo_RAN_OSS_Eri_20170322_20170327_3000.csv
 2690  cd ../../20170306_Ingestion
 2691  head -n 3000 bsw_20151201_20170201.csv > ~/Projects/SB/Python/predix_studio/bsw_20151201_20170201_3000.csv
 2692  head -n 3000 tte_20151201_20170201.csv > ~/Projects/SB/Python/predix_studio/tte_20151201_20170201_3000.csv
 2693  head -n 3000 wot_20151201_20170201.csv > ~/Projects/SB/Python/predix_studio/wot_20151201_20170201_3000.csv
 2694  cd ../20170310_Ingestion
 2695  cd INOS
 2696  gzip INOS_ERI_20170223.csv.gz | head -n 3000 > ~/Projects/SB/Python/predix_studio/INOS_ERI_20170223_3000.csv
 2697  gzip -d INOS_ERI_20170223.csv.gz | head -n 3000 > ~/Projects/SB/Python/predix_studio/INOS_ERI_20170223_3000.csv
 2698  cd predix_studio
 2699  gzip -d INOS_ERI_20170223.csv.gz  ~/Projects/SB/Python/predix_studio/INOS_ERI_20170223_3000.csv
 2700  gzip -d INOS_ERI_20170223.csv.gz  > ~/Projects/SB/Python/predix_studio/INOS_ERI_20170223_3000.csv
 2701  gzip -d INOS_ERI_20170223.csv.gz  > ~/Projects/SB/Python/predix_studio/
 2702  gzip -d INOS_ERI_20170223.csv.gz  > ~/Projects/SB/Python/predix_studio
 2703  gzip -d INOS_ERI_20170223.csv.gz  -C  ~/Projects/SB/Python/predix_studio
 2704  unzip INOS_ERI_20170223.csv.gz  -C  ~/Projects/SB/Python/predix_studio
 2705  man unzip
 2706  unzip INOS_ERI_20170223.csv -d ~/Projects/SB/Python/predix_studio
 2707  gunzip -d INOS_ERI_20170307.csv.gz -d ~/Projects/SB/Python/predix_studio
 2708  cp INOS_ERI_20170223.csv ~/Projects/SB/Python/predix_studio/INOS_ERI_20170223.csv
 2709  gunzip INOS_ERI_20170223.csv
 2710  gunzip -d INOS_ERI_20170223.csv
 2711  head -n 3000 INOS_ERI_20170223.csv > INOS_ERI_20170223_3000.csv
 2712  ../../20170327_Ingestion
 2713  cd ../../20170321_Ingestion
 2714  cd ../20170328_Ingestion
 2715  cd ../../20170310_Ingestion
 2716  mc
 2717  10
 2718  :10
 2719  Q
 2720  cd ../../20170323_Ingestion
 2721  cd Output
 2722  head -n 3000 nedb_2016summer_2017TQ.csv ~/Projects/SB/Python/predix_studio/nedB_3000.csv
 2723  head -n 3000 nedb_2016summer_2017TQ.csv > ~/Projects/SB/Python/predix_studio/nedB_3000.csv
 2724  pip install lifelines
 2725  screengif -i tstk_sphinx_build_gif.mov  -o tstk_sphinx_build.gif 
 2726  cp tstk_sphinx_build.gif ~/Python/tstk/doc/images/tstk_sphinx_build.gif
 2727  docker -h
 2728  docker start
 2729  docker start --help
 2730  docker start -i
 2731  docker pull pathtrk/opencv-python3
 2732  docker run pathtrk/opencv-python3
 2733  docker run pathtrk/opencv-python3; ipython
 2734  docker run pathtrk/opencv-python3 --version
 2735  docker run python  python --version
 2736  docker run python ls
 2737  docker run pathtrk/opencv-python3 ls
 2738  docker run pathtrk/opencv-python3 ipython
 2739  docker run pathtrk/opencv-python3 python --version
 2740  docker run images
 2741  docker run -i -t pathtrk/opencv-python3
 2742  docker run -i -t pathtrk/opencv-python3 /bin/bash 
 2743  run ln -s /usr/local/bin/python3.5 /usr/local/bin/python3
 2744  docker run ln -s /usr/local/bin/python3.5 /usr/local/bin/python3
 2745  python update_schema.py "/Users/212339410/Box/DSS Internal-Softbank/12 Working Directory for Ingestion/20170321_Ingestion/INOS/INOS_ERI_20170223.csv"  ../Src/Schema/inos_v1.xml  ../Src/Schema/inos_v2.xml
 2746  less "/Users/212339410/Box/DSS Internal-Softbank/12 Working Directory for Ingestion/20170321_Ingestion/INOS/INOS_ERI_20170223.csv"
 2747  cd Projects/SB/Py
 2748  cd ~/SB_box_ingest_working_dir/Talen_Working_Directory
 2749  vim INOS_ERI_20170223.csv
 2750  python model_selection.py --demo 
 2751  python feature_create_v2.py --vendor 'nok' --lb 14  --la 0  --nh 3000  --nu "" --sample_days 40 --save True  --cache True  --o ../Src/ModData/prediction_ --dfrom '2017-02-13 00:00:00' --dto '2017-02-28 23:59:59'
 2752  vim ~/SB_box_ingest_working_dir/Talen_Working_Directory/INOS_ERI_20170223.csv
 2753  vim ~/SB_box_ingest_working_dir/Talen_Working_Directory/INOS_ERI_20170223.csv.gz
 2754  pip install tpot
 2755  pip install xgboost
 2756  cd `which python`
 2757  cd /Users/212339410/anaconda2/envs/py35/lib/python3.5/site-packages
 2758  cd xgboost-0.6-py3.5.egg/
 2759  cd xgboost
 2760  ln -s ~/anaconda2/envs/py35/lib/python3.5/site-packages/xgboost-0.6-py3.5.egg/xgboost ~/anaconda2/envs/py35/lib/python3.5/site-packages/xgboost
 2761  pip install prettytable
 2762  python update_schema.py "/Users/212339410/Box/DSS Internal-Softbank/12 Working Directory for Ingestion/Archive_for_ingested_data/INOS_NOK_20170307.csv.gz"  ../Src/Schema/inos_v4.xml  ../Src/Schema/inos_v5.xml
 2763  python update_schema.py "/Users/212339410/Box/DSS Internal-Softbank/12 Working Directory for Ingestion/Archive_for_ingested_data/INOS_NOK_20170307.csv.csv"  ../Src/Schema/inos_v4.xml  ../Src/Schema/inos_v5.xml
 2764  iconv -c -f SHIFT_JIS -t UTF-8 ./BSW_ERI_20170327_20170411.csv >> ./BSW_ERI_20170327_20170411.utf8.csv
 2765  vim BSW_ERI_20170327_20170411.csv
 2766  open BSW_ERI_20170327_20170411.
 2767  vim BSW_ERI_20170327_20170411.utf8.csv
 2768  open BSW_ERI_20170327_20170411.utf8.csv
 2769  python update_schema.py "/Users/212339410/Box/DSS Internal-Softbank/12 Working Directory for Ingestion/Archive_for_ingested_data/INOS_NOK_20170307.csv"  ../Src/Schema/inos_v4.xml  ../Src/Schema/inos_v5.xml
 2770  git clone git@github.build.ge.com:IndustrialDataScience/AutoModel.git
 2771  cd AutoModel
 2772  mkdir automodel
 2773  cd automodel
 2774  rm -rf AutoModel
 2775  git clone git@github.build.ge.com:IndustrialDataScience/OptModel.git
 2776  mkdir optmodel
 2777  vim bsw_20170327_20170411.csv
 2778  vim bsw_20170327_20170411\ copy.csv
 2779  python model_selection.py 
 2780  python model_selection.py -h
 2781  python model_selection.py --demo
 2782  mv `ls *.log` ./log/
 2783  ls *.log
 2784  cd log
 2785  tpot ~/Downloads/mnist_train.csv -is , -target class -o tpot_exported_pipeline.py -g 5 -p 20 -cv 5 -s 42 -v 2
 2786  vagrant
 2787  rm Vagrantfile
 2788  mkdir DataScienceToolbox
 2789  cd DataScienceToolbox
 2790  vagrant init data-science-toolbox/data-science-at-the-command-line
 2791  Vagrant.configure(2) do |config|\n  config.vm.box = "data-science-toolbox/data-science-at-the-command-line"
 2792  vagrant login
 2793  cat Vagrantfile
 2794  vagrant up
 2795  man header
 2796  header
 2797  man head
 2798  csvlook ~/Downloads/mnist_train.csv
 2799  csvlook ~/Downloads/mnist_train.csv | less -S
 2800  csvcut -n NEDB_NOKERI_TQ.utf8.csv
 2801  csvcut -n bsw0412.csv
 2802  csvcut -n BSW2016_2000.csv
 2803  csvcut -c BSW_Company_name | head
 2804  head BSW2016_2000.csv | csvcut -c BSW_Work_hours | csvstat
 2805  head BSW2016_2000.csv | csvcut -c BSW_Rank_of_Work | csvstat
 2806  head BSW2016_2000.csv | csvstat
 2807  csvstat BSW2016_2000.csv
 2808  csvcut -c BSW_Company_name | csvlook
 2809  csvcut -c BSW_Company_name BSW2016_2000.csv| csvlook
 2810  head feature0327df_event_uh_proc_nok_lbh7_lah1_0327.csv| csvlook
 2811  head feature0327df_event_uh_proc_nok_lbh7_lah1_0327.csv| csvstat
 2812  csvcut -n feature0327df_event_uh_proc_nok_lbh7_lah1_0327.csv
 2813  head ~/Downloads/mnist_train.csv | csvlook
 2814  cd 0403_feature_nok
 2815  tpot feature_nok_lbh14_lah1_Prob_Urgent_0403.csv -target 'Prob_Urgent' -o tpot_exported_pipeline.py -g -p 20 -cv 5 -s 42 -v 2
 2816  tpot feature_nok_lbh14_lah1_Prob_Urgent_0403.csv -target 'Prob_Urgent' -o tpot_exported_pipeline.py -g 5 -p 20 -cv 5 -s 42 -v 2
 2817  head feature_nok_lbh14_lah1_Prob_Urgent_0403.csv | csvlook
 2818  head feature_nok_lbh14_lah1_Prob_Urgent_0403.csv | csvcut -n
 2819  head feature_nok_lbh14_lah1_Prob_Urgent_0403.csv | csvcut -n -C incident_id
 2820  csvcut -C incident_id -n 
 2821  csvcut -C incident_id feature_nok_lbh14_lah1_Prob_Urgent_0403.csv| tpot -is , -target Prob_Urgent -o tpot_exported_pipeline.py -g 5 -p 20 -cv 5 -s 42 -v 2
 2822  csvcut -C incident_id feature_nok_lbh14_lah1_Prob_Urgent_0403.csv
 2823  csvcut -C incident_id feature_nok_lbh14_lah1_Prob_Urgent_0403.csv > feature_nok_lbh14_lah1_Prob_Urgent_0403_tpot.csv
 2824  tpot feature_nok_lbh14_lah1_Prob_Urgent_0403_tpot.csv -is , -target Prob_Urgent -o tpot_exported_pipeline.py -g 5 -p 20 -cv 5 -s 42 -v 2
 2825  head feature_nok_lbh14_lah1_Prob_Urgent_0403.csv | csvcut -n 
 2826  cd ../0327_feature
 2827  cd ../0402_feature_nok
 2828  cd ../0331_feature
 2829  csvstat -n featurefeature_nok_lbh14_lah10_0331_Prob_Urgent.csv
 2830  csvstat featurefeature_nok_lbh14_lah10_0331_Prob_Urgent.csv -C incident_id > featurefeature_nok_lbh14_lah10_0331_Prob_Urgent_tpot.csv
 2831  csvcut featurefeature_nok_lbh14_lah10_0331_Prob_Urgent.csv -C incident_id > featurefeature_nok_lbh14_lah10_0331_Prob_Urgent_tpot.csv
 2832  tpot featurefeature_nok_lbh14_lah10_0331_Prob_Urgent_tpot.csv
 2833  vagrant ssh
 2834  awk
 2835  tpot mnist.csv.gz -is , -target class -o tpot_exported_pipeline.py -g 5 -p 20 -cv 5 -s 42 -v 2
 2836  csvstat -n mnist.csv
 2837  head mnist.csv
 2838  head mnist.csv -n 1
 2839  head -n 1 mnist.csv 
 2840  tpot mnist.csv -is \t  -target class -o tpot_exported_pipeline.py -g 5 -p 20 -cv 5 -s 42 -v 2
 2841  vim mnist.csv
 2842  tpot mnist.csv -is ,  -target class -o tpot_exported_pipeline.py -g 5 -p 20 -cv 5 -s 42 -v 2
 2843  mkdir test_script
 2844  mv test_* ./test_script
 2845  mv test_* > ./test_script
 2846  mv test_*.py > ./test_script
 2847  python feature_create_v3.py --vendor nok --dto 20170327 
 2848  python feature_create_v3.py --vendor nok --dto 20170327 --lb 14
 2849  python feature_create_v3.py --vendor nok --dto 20170327 -o ../Src/ModData/0413_feature
 2850  python feature_create_v3.py --vendor nok --dto 20170327 --o ../Src/ModData/0413_feature
 2851  python feature_create_v3.py --vendor nok --dto 20170327 --o ../Src/ModData/0413_feature/
 2852  python feature_create_v3.py --vendor nok --dto 20170327 --o ../Src/ModData/0413_feature/ 
 2853  python feature_create_v3.py --vendor nok --dto 20170327 --o ../Src/ModData/0413_feature/ --cache
 2854  seq {1..15}
 2855  seq 1..15
 2856  seq 115
 2857  seq 1 15
 2858  seq 20170301 20170327
 2859  for i in seq 20170301 20170327\ndo \necho $i\ndone
 2860  for i in {20170301.. 20170327} do \necho $i\ndone
 2861  `seq 20170301 20170327`
 2862  for i in `seq 20170301 20170327`\ndo \necho $i\ndone
 2863  for i in `seq 20170301 20170327`\ndo\necho $i\npython feature_create_v3.py --vendor nok --dto $i --cache\ndone
 2864  seq -g 20170301 20170327
 2865  seq --g 20170301 20170327
 2866  seq 1 5 | xargs -I {} date -d "2017030{}" +%Y%m%d
 2867  python feature_create_v3.py --vendor eri --dto 20170327 --o ../Src/ModData/0413_feature/ --cache
 2868  enddate=20170327
 2869  for (( date="$startdate"; date != enddate; )); do\n    dates+=( "$date" )\n    date="$(date --date="$date + 1 days" +'%Y%m%d')"\ndone
 2870  echo "${dates[@]}"
 2871  dates=()
 2872  for (( date="$startdate"; date != enddate; ))\ndo\ndates+=("$date")\ndate="$(date --date="$date + 1 days" +'%Y%m%d')"\ndone
 2873  find . -mtime $start -mtime $end
 2874  cd Projects/SB/Bash
 2875  date 20170120
 2876  date 20170120 -f %Y%m%d
 2877  date 20170120 -f yyyymmdd
 2878  date 20170120 -f yymmdd
 2879  gdate
 2880  todaty
 2881  yoday
 2882  today
 2883  date
 2884  sh run_feature_create_v3.sh
 2885  date -j -f '%d-%b-%Y' "22-Aug-2013" "+%s"
 2886  date -j -f '%Y%b%d' startdate "+%s"
 2887  date -j -f '%s' startdate "+%s"
 2888  startdate=170120
 2889  date -j -f '%s' startdate
 2890  date 032717
 2891  date 0327175013
 2892  date -j -vJulm -v20d -v1999y '+%A'
 2893  date -f ddMMyy 
 2894  date -f 101010
 2895  date "+DATE: %Y-%m-%d%nTIME: %H:%M:%S"
 2896  date "+DATE: %Y-%m-%d TIME: %H:%M:%S"
 2897  date "DATE: %Y-%m-%d TIME: %H:%M:%S"
 2898  +DATE
 2899  date "+%Y-%m-%d %H:%M:%S"
 2900  date "%Y-%m-%d %H:%M:%S"
 2901  date "+%Y%m%d"
 2902  echo $TIMESTAMP
 2903  for i in `seq 10`; do date '+%Y%m%d' -d ${i}; echo -e "\n\n\n"; done
 2904  for i in `seq 10`; do date '+%Y%m%d' -d ${i}day; echo -e "\n\n\n"; done
 2905  for i in `seq 100`; do date '+%Y/%m/%d (%a)' -d ${i}day; echo -e "\n\n\n"; done
 2906  date -d `1 day ago`
 2907  echo `date -v-1m` +"%Y%m%d"
 2908  echo `date -v-1m` +"%Y%m%d"`\ne\n[\n`
 2909  echo `date -v-1m +"%Y%m%d"`
 2910  for D in `seq 1 60`\ndo\necho  $(date -v-${D}d +%Y-%m-%d)\ndone
 2911  for D in `seq 1 80`\ndo\necho $(date -v-${D}d +%Y%m%d)\ndone
 2912  for D in `seq 1 90`\ndo\necho $(date -v-${D}d +%Y%m%d)\ndone
 2913  for D in `seq 1 85`\ndo\necho $(date -v-${D}d +%Y%m%d)\ndone
 2914  for D in `seq 1 10`\ndo\necho $(date -v-${D}d +%Y%m%d)\npython feature_create_v3.py --vendor eri --dto $(date -v-${D}d +%Y%m%d) --o ../Src/ModData/0413_feature/ --cache\ndone
 2915  for D in `seq 10 83`\ndo\necho $(date -v-${D}d +%Y%m%d)\npython feature_create_v3.py --vendor eri --dto $(date -v-${D}d +%Y%m%d) --o ../Src/ModData/0413_feature/ --cache\ndone
 2916  for D in `seq 1 83`\ndo\necho $(date -v-${D}d +%Y%m%d)\npython feature_create_v3.py --vendor nok --dto $(date -v-${D}d +%Y%m%d) --o ../Src/ModData/0413_feature/ --cache\ndone
 2917  cd 0413_feature
 2918  for f in ./*.csv\ndo\necho $f.gzip\n#gzip $f.gzip\ndone
 2919  for f in ./*.csv$\ndo\necho $f.gzip\n#gzip $f.gzip\ndone
 2920  ls | grep *.csv$
 2921  ls | grep *
 2922  ls | grep .
 2923  ls | grep .*.csv
 2924  ls | grep *.csv
 2925  ls | grep . *csv
 2926  ls | grep csv
 2927  ls | grep csv$
 2928  ls | grep csv$for f in `ls | grep csv$`
 2929  for f in `ls | grep csv$`\ndo\necho $f.gzip\n#gzip $f.gzip\ndone
 2930  gzip feature_nok_0106_0120_lbh14_lah1_ival14.csv -S .gzip
 2931  gzip feature_nok_0106_0120_lbh14_lah1_ival14.csv
 2932  gzip feature_nok_0106_0120_lbh14_lah1_ival14.csv --suffix ".gzip"
 2933  gzip feature_nok_0106_0120_lbh14_lah1_ival14.csv --suffix .gzip
 2934  for f in `ls | grep csv$`\ndo\necho $f.gzip\ngzip $f -S .gzip\ndone
 2935  for D in `seq 1 69`\ndo\necho $(date -v-${D}d +%Y%m%d)\npython feature_create_v3.py --vendor nok --dto $(date -v-${D}d +%Y%m%d) --lb 28 --o ../Src/ModData/0413_feature/ --cache\ndone
 2936  for D in `seq 1 69`\ndo\necho $(date -v-${D}d +%Y%m%d)\npython feature_create_v3.py --vendor eri --dto $(date -v-${D}d +%Y%m%d) --lb 28 --o ../Src/ModData/0413_feature/ --cache\ndone
 2937  drop view OSS_Count_table
 2938  ln -s /Users/212339410/Box/DSS\ Internal-Softbank/40\ Intermediate\ Results/20170414\ Feature/0414_feature /Users/212339410/Box/Analytics/Projects/SB/Src/ModData/0414_feature 
 2939  for D in `seq 2 72`\ndo\necho $(date -v-${D}d +%Y%m%d)\npython feature_create_v3.py --vendor eri --dto $(date -v-${D}d +%Y%m%d) --o ../Src/ModData/0414_feature/ --cache\ndone
 2940  for D in `seq 2 84`\ndo\necho $(date -v-${D}d +%Y%m%d)\npython feature_create_v3.py --vendor nok --dto $(date -v-${D}d +%Y%m%d) --o ../Src/ModData/0414_feature/ --cache\ndone
 2941  cp preprocessing_oss_nok_20170317_20170327.ipynb preprocessing_oss_nok_20170327_20170410.ipynb
 2942  for D in `seq 72 86`\ndo\necho $(date -v-${D}d +%Y%m%d)\npython feature_create_v3.py --vendor eri --dto $(date -v-${D}d +%Y%m%d) --o ../Src/ModData/0414_feature/ --cache\ndone
 2943  for D in `seq 85 86`\ndo\necho $(date -v-${D}d +%Y%m%d)\npython feature_create_v3.py --vendor nok --dto $(date -v-${D}d +%Y%m%d) --o ../Src/ModData/0414_feature/ --cache\ndone
 2944  for D in `seq 73 774`\ndo\necho $(date -v-${D}d +%Y%m%d)\npython feature_create_v3.py --vendor eri --dto $(date -v-${D}d +%Y%m%d) --o ../Src/ModData/0414_feature/ --cache\ndone
 2945  D=15
 2946  D=18
 2947  python feature_create_v3.py --vendor nok --dto $(date -v-${D}d +%Y%m%d) --o ../Src/ModData/0414_feature_duration/ --cache
 2948  for D in `seq 2 84`\ndo\necho $(date -v-${D}d +%Y%m%d)\npython feature_create_v3.py --vendor nok --dto $(date -v-${D}d +%Y%m%d) --o ../Src/ModData/0414_feature_duration/ --cache
 2949  for D in `seq 17 84`\ndo\necho $(date -v-${D}d +%Y%m%d)\npython feature_create_v3.py --vendor nok --dto $(date -v-${D}d +%Y%m%d) --o ../Src/ModData/0414_feature_duration/ --cache\ndone
 2950  for D in `seq 17 72`\ndo\necho $(date -v-${D}d +%Y%m%d)\npython feature_create_v3.py --vendor eri --dto $(date -v-${D}d +%Y%m%d) --o ../Src/ModData/0414_feature_duration/ --cache\ndone
 2951  python model_selection.py --ifeat ../../../../DSS\ Internal-Softbank/40\ Intermediate\ Results/20170414\ Feature/0414_Xy_set/Train_Validation_Test/Ericsson/xy_train_eri_0118_0312_lbh14_lah1_ival14_t1_f10.csv --iconf prediction_validation.ini --t Prob_Urgent --o ../Src/ModData/0414_model_selection/xy_train_eri_0118_0312_lbh14_lah1_ival14_t1_f10.csv  --loglevel info --olog 'model_selection.log' --report 'csv' --score 'precision'
 2952  python model_selection.py --ifeat ../../../../DSS\ Internal-Softbank/40\ Intermediate\ Results/20170414\ Feature/0414_Xy_set/Train_Validation_Test/Ericsson/xy_train_eri_0118_0312_lbh14_lah1_ival14_t1_f10.csv --iconf prediction_validation.ini --t Prob_Urgent --o ../Src/ModData/0414_model_selection/xy_train_eri_0118_0312_lbh14_lah1_ival14_t1_f10.csv  --loglevel info --olog 'model_selection.log' --report 'csv' --score 'f1'
 2953  python model_selection.py --ifeat ../../../../DSS\ Internal-Softbank/40\ Intermediate\ Results/20170414\ Feature/0414_Xy_set/Train_Validation_Test/Ericsson/xy_train_eri_0118_0312_lbh14_lah1_ival14_t1_f10.csv --iconf prediction_validation.ini --t Prob_Urgent --o ../Src/ModData/0414_model_selection/xy_train_eri_0118_0312_lbh14_lah1_ival14_t1_f10.csv  --loglevel info --olog 'model_selection.log' --report 'csv' --score 'accuracy'
 2954  python model_selection.py --ifeat ../../../../DSS\ Internal-Softbank/40\ Intermediate\ Results/20170414\ Feature/0414_Xy_set/Train_Validation_Test/Ericsson/xy_train_eri_0118_0312_lbh14_lah1_ival14_t1_f10.csv --iconf model_selection.ini --t Prob_Urgent --o ../Src/ModData/0414_model_selection/xy_train_eri_0118_0312_lbh14_lah1_ival14_t1_f10.csv  --loglevel info --olog 'model_selection.log' --report 'csv' --score 'precision'
 2955  python prediction_validation.py -h
 2956  python prediction_validation.py --ifeat /Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170414 Feature/0414_Xy_set/Train_Test/Ericsson/xy_test_eri_0328_0410.csv --model_dir ../Src/ModData/0414_model_selection/ --vendor nok --o 0414_test/0328_0410_ --target Prob_Urgent
 2957  python prediction_validation.py --ifeat '/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170414 Feature/0414_Xy_set/Train_Test/Ericsson/xy_test_eri_0328_0410.csv' --model_dir ../Src/ModData/0414_model_selection/ --vendor nok --o 0414_test/0328_0410_ --target Prob_Urgent
 2958  python prediction_validation.py --ifeat '/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170414 Feature/0414_Xy_set/Train_Test/Ericsson/xy_test_eri_0328_0410.csv' --model_dir ../Src/ModData/0414_model_selection/ --vendor nok --o 0414_test/0328_0410_ --t Prob_Urgent
 2959  OA
 2960  python prediction_validation.py --ifeat '/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170414 Feature/0414_Xy_set/Train_Test/Ericsson/xy_test_eri_0328_0410.csv' --model_dir ../Src/ModData/0414_model_selection/ --vendor nok --o ../Src/ModData/0414_test/0328_0410_ --t Prob_Urgent
 2961  python prediction_validation.py --ifeat '/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170414 Feature/0414_Xy_set/Train_Test/Ericsson/xy_test_eri_0328_0410.csv' --model_dir ../Src/ModData/0414_model_selection/ --vendor eri --o ../Src/ModData/0414_test/0328_0410_ --t Prob_Urgent
 2962  python prediction_validation.py --ifeat '/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170414 Feature/0414_Xy_set/Train_Validation_Test/Ericsson/xy_test_eri_0328_0410.csv' --model_dir ../Src/ModData/0414_model_selection/ --vendor eri --o ../Src/ModData/0414_test/0328_0410_ --t Prob_Urgent
 2963  python prediction_validation.py --ifeat '/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170414 Feature/0414_Xy_set/Train_Validation_Test/Ericsson/xy_validation_eri_0328_0410.csv' --model_dir ../Src/ModData/0414_model_selection/ --vendor eri --o ../Src/ModData/0414_test/0328_0410_validation_ --t Prob_Urgent
 2964  csvcut '/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170414 Feature/0414_Xy_set/Train_Validation_Test/Ericsson/xy_validation_eri_0328_0410.csv' Prob_Urgent
 2965  csvcut '/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170414 Feature/0414_Xy_set/Train_Validation_Test/Ericsson/xy_validation_eri_0328_0410.csv' -c Prob_Urgent
 2966  csvcut ''/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170414 Feature/0414_Xy_set/Train_Validation_Test/Ericsson/xy_validation_eri_0328_0410.csv'
 2967  csvcut '/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170414 Feature/0414_Xy_set/Train_Validation_Test/Ericsson/xy_validation_eri_0328_0410.csv' -n
 2968  csvcut -n "/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170414 Feature/0414_Xy_set/Train_Validation_Test/Ericsson/xy_validation_eri_0328_0410.csv"
 2969  csvcut "/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170414 Feature/0414_Xy_set/Train_Validation_Test/Ericsson/xy_validation_eri_0313_0327_lbh14_lah1_ival14.csv" -n
 2970  csvcut "/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170414 Feature/0414_Xy_set/Train_Validation_Test/Ericsson/xy_validation_eri_0313_0327_lbh14_lah1_ival14.csv" -C sum > ../Src/ModData/xy_validation_eri_0313_0327_lbh14_lah1_ival14.csv
 2971  python model_selection.py --ifeat ../../../../DSS\ Internal-Softbank/40\ Intermediate\ Results/20170414\ Feature/0414_Xy_set/Train_Validation_Test/Ericsson/xy_train_eri_0118_0312_lbh14_lah1_ival14_t1_f10.csv --iconf model_selection.ini --t Prob_Urgent --o ../Src/ModData/0414_model_selection/xy_train_eri_0118_0312_lbh14_lah1_ival14_t1_f10.csv  --loglevel info --olog 'model_selection.log'  --score 'accuracy'
 2972  python prediction_validation.py --ifeat '/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170414 Feature/0414_Xy_set/Train_Validation_Test/Ericsson/xy_validation_eri_0313_0327_lbh14_lah1_ival14.csv' --model_dir ../Src/ModData/0414_model_selection/ --vendor eri --o ../Src/ModData/0414_test/0328_0410_validation_ --t Prob_Urgent
 2973  python prediction_validation.py --ifeat '../Src/ModData/xy_validation_eri_0313_0327_lbh14_lah1_ival14.csv' --model_dir ../Src/ModData/0414_model_selection/ --vendor eri --o ../Src/ModData/0414_test/0328_0410_validation_ --t Prob_Urgent
 2974  python prediction_validation.py --ifeat '/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170414 Feature/0414_Xy_set/Train_Test/Ericsson/xy_test_eri_0328_0410.csv' --model_dir ../Src/ModData/0414_model_selection/ --vendor eri --o ../Src/ModData/0414_test/0328_0410_test_ --t Prob_Urgent
 2975  csvcut /Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170414 Feature/0414_Xy_set/Train_Test/Ericsson/xy_train_eri_0118_0327_lbh14_lah1_ival14_t1_f10.csv -n
 2976  csvcut -n "/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170414 Feature/0414_Xy_set/Train_Test/Ericsson/xy_train_eri_0118_0327_lbh14_lah1_ival14_t1_f10.csv"
 2977  csvcut -n "/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170414 Feature/0414_Classification_Results/pred_nok_Apr_14.csv"
 2978  csvcut -c [BSID, PROB] "/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170414 Feature/0414_Classification_Results/pred_nok_Apr_14.csv" > ~/Projects/SB/Src/ModData/pred_nok_0414_young.csv
 2979  csvcut -c BSID, PROB  "/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170414 Feature/0414_Classification_Results/pred_nok_Apr_14.csv" > ~/Projects/SB/Src/ModData/pred_nok_0414_young.csv
 2980  csvcut   "/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170414 Feature/0414_Classification_Results/pred_nok_Apr_14.csv" -c BSID, PROB  > ~/Projects/SB/Src/ModData/pred_nok_0414_young.csv
 2981  csvcut   "/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170414 Feature/0414_Classification_Results/pred_nok_Apr_14.csv" -c BSID,PROB  > ~/Projects/SB/Src/ModData/pred_nok_0414_young.csv
 2982  csvcut   "/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170414 Feature/0414_Classification_Results/pred_eri_Apr_14.csv" -c BSID,PROB  > ~/Projects/SB/Src/ModData/pred_eri_0414_young.csv
 2983  python prediction_validation.py --ifeat '/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170414 Feature/0414_Xy_set/Train_Test/Nokia/xy_test_nok_0328_0410.csv' --model_dir ../Src/ModData/0414_model_selection/ --vendor nok --o ../Src/ModData/0414_test/0328_0410_test_ --t Prob_Urgent
 2984  python model_selection.py --ifeat ../../../../DSS\ Internal-Softbank/40\ Intermediate\ Results/20170414\ Feature/0414_Xy_set/Train_Validation_Test/Nokia/xy_train_nok_0106_0312_lbh14_lah1_ival14_t1_f10.csv --iconf model_selection.ini --t Prob_Urgent --o ../Src/ModData/0414_model_selection/xy_train_nok_0106_0312_lbh14_lah1_ival14_t1_f10.csv
 2985  python model_selection.py --ifeat ../../../../DSS\ Internal-Softbank/40\ Intermediate\ Results/20170414\ Feature/0414_Xy_set/Train_Validation_Test/Ericsson/xy_train_eri_0118_0312_lbh14_lah1_ival14_t1_f10.csv --iconf model_selection.ini --t Prob_Urgent --o ../Src/ModData/0414_model_selection/xy_train_eri_0118_0312_lbh14_lah1_ival14_t1_f10.csv  --olog /log/model_selection_eri0414.log
 2986  python model_selection.py --ifeat ../../../../DSS\ Internal-Softbank/40\ Intermediate\ Results/20170414\ Feature/0414_Xy_set/Train_Validation_Test/Ericsson/xy_train_eri_0118_0312_lbh14_lah1_ival14_t1_f10.csv --iconf model_selection.ini --t Prob_Urgent --o ../Src/ModData/0414_model_selection/xy_train_eri_0118_0312_lbh14_lah1_ival14_t1_f10.csv  --olog ./log/model_selection_eri0414.log
 2987  python model_selection.py --ifeat ../../../../DSS\ Internal-Softbank/40\ Intermediate\ Results/20170414\ Feature/0414_Xy_set/Train_Validation_Test/Nokia/xy_train_nok_0106_0312_lbh14_lah1_ival14_t1_f10.csv --iconf model_selection.ini --t Prob_Urgent --o ../Src/ModData/0414_model_selection/xy_train_nok_0106_0312_lbh14_lah1_ival14_t1_f10.csv --score precision
 2988  python model_selection.py --ifeat ../../../../DSS\ Internal-Softbank/40\ Intermediate\ Results/20170414\ Feature/0414_Xy_set/Train_Validation_Test/Nokia/xy_train_nok_0106_0312_lbh14_lah1_ival14_t1_f10.csv --iconf model_selection.ini --t Prob_Urgent --o ../Src/ModData/0415_model_selection/xy_train_nok_0106_0312_lbh14_lah1_ival14_t1_f10.csv --score precision
 2989  python model_selection.py --ifeat ../../../../DSS\ Internal-Softbank/40\ Intermediate\ Results/20170414\ Feature/0414_Xy_set/Train_Validation_Test/Nokia/xy_train_nok_0106_0312_lbh14_lah1_ival14_t1_f10.csv --iconf model_selection.ini --t Prob_Urgent --o ../Src/ModData/0415_model_selection/xy_train_nok_0106_0312_lbh14_lah1_ival14_t1_f10.csv --score precision --olog ./log/model_selection_nok0415.log
 2990  python prediction_validation.py --ifeat '/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170414 Feature/0414_Xy_set/Train_Test/Nokia/xy_test_nok_0328_0410.csv' --model_dir ../Src/ModData/0415_model_selection/ --vendor nok --o ../Src/ModData/0414_test/precision_0328_0410_test_ --t Prob_Urgent
 2991  python prediction_validation.py --ifeat '/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170414 Feature/0414_Xy_set/Train_Validation_Test/Nokia/xy_validation_nok_0313_0327_lbh14_lah1_ival14.csv' --model_dir ../Src/ModData/0414_model_selection/ --vendor nok --o ../Src/ModData/0414_test/0328_0410_valid_ --t Prob_Urgent
 2992  python prediction_validation.py --ifeat '/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170414 Feature/0414_Xy_set/Train_Validation_Test/Nokia/xy_validation_nok_0313_0327_lbh14_lah1_ival14.csv' --model_dir ../Src/ModData/0415_model_selection/ --vendor nok --o ../Src/ModData/0414_test/0328_0410_valid_ --t Prob_Urgent
 2993  csvcut -n /Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170414 Feature/0414_Xy_set/Train_Validation_Test/Nokia/xy_validation_nok_0313_0327_lbh14_lah1_ival14.csv
 2994  csvcut -n "/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170414 Feature/0414_Xy_set/Train_Validation_Test/Nokia/xy_validation_nok_0313_0327_lbh14_lah1_ival14.csv"
 2995  python prediction_validation.py --ifeat ../Src/ModData/xy_validation_eri_0313_0327_lbh14_lah1_ival14.csv  --model_dir ../Src/ModData/0415_model_selection/ --vendor nok --o ../Src/ModData/0414_test/0328_0410_valid_ --t Prob_Urgent
 2996  python prediction_validation.py --ifeat ../Src/ModData/xy_validation_eri_0313_0327_lbh14_lah1_ival14.csv  --model_dir ../Src/ModData/0413_model_selection/ --vendor eri --o ../Src/ModData/0414_test/0328_0410_valid_ --t Prob_Urgent
 2997  python prediction_validation.py --ifeat ../Src/ModData/xy_validation_eri_0313_0327_lbh14_lah1_ival14.csv  --model_dir ../Src/ModData/0414_model_selection/ --vendor eri --o ../Src/ModData/0414_test/0328_0410_valid_ --t Prob_Urgent
 2998  csvcut -C sum "/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170414 Feature/0414_Xy_set/Train_Validation_Test/Nokia/xy_validation_nok_0313_0327_lbh14_lah1_ival14.csv" > ../Src/ModData/xy_validation_nok_0313_0327_lbh14_lah1_ival14.csv
 2999  python prediction_validation.py --ifeat ../Src/ModData/xy_validation_nok_0313_0327_lbh14_lah1_ival14.csv  --model_dir ../Src/ModData/0415_model_selection/ --vendor nok --o ../Src/ModData/0414_test/0328_0410_valid_ --t Prob_Urgent
 3000  python ./Python/my
 3001  python ./Python/Generic/myfunct.py
 3002  python ./Python/Generic/myprogram.py
 3003  python ./Python/Generic/convert_temp.py
 3004  python ./Python/Generic/udem/scrip1.py
 3005  python ./Python/Generic/udem/script6.py
 3006  python ./Python/Generic/udem/password_test.py
 3007  python ./Python/Generic/udem/script7.py
 3008  python calc_survival_time.py -h
 3009  python calc_survival_time.py --dfrom 20170101 --dto 20170410 -o ../Src/ModData/survival_time.csv
 3010  rm ../SB/Src/Schema/internalL_v1og.xml
 3011  cd SB/Python
 3012  cp model_selection.py ../../../Python/OptModel/optmodel/optmodel.py
 3013  cp ../Projects/SB/Python/model_selection.ini OptModel/optmodel/optmodel.ini
 3014  open demo_result_accuracy.csv
 3015  python OptModel/optmodel/optmodel.py --demo
 3016  cat optmodel/optmodel.ini
 3017  rm demo_result_accuracy.csv
 3018  cd OptModel/optmodel
 3019  python optmodel.py
 3020  cd Projects/SB/Src/ModData/
 3021  ks
 3022  git scheckout plot_update
 3023  git checkout plot_update
 3024  git checkout -b plot_update
 3025  cp doc/API-Documentation.rst ../Archive/tstk\ archive/
 3026  cp doc/index.rst ../Archive/tstk\ archive/
 3027  cp doc/tstk.rst ../Archive/tstk\ archive/
 3028  rm -rf tstk
 3029  git clone doc/API-Documentation.rst
 3030  git clone git@github.build.ge.com:IndustrialDataScience/tstk.git
 3031  vim '/Users/212339410/Box/DSS External-Softbank/21 Data from SB/20170329_Internal_Log_formatted (Ericsson)/20170209_0610_SB_nodelist2000_out.zip\n'
 3032  vim /Users/212339410/Box/DSS External-Softbank/21 Data from SB/20170329_Internal_Log_formatted (Ericsson)/20170209_0610_SB_nodelist2000_out.zip
 3033  vim "/Users/212339410/Box/DSS External-Softbank/21 Data from SB/20170329_Internal_Log_formatted (Ericsson)/20170209_0610_SB_nodelist2000_out.zip\n"
 3034  vim â/Users/212339410/Box/DSS External-Softbank/21 Data from SB/20170329_Internal_Log_formatted (Ericsson)/20170209_0610_SB_nodelist2000_out.zipâ
 3035  vim â/Users/212339410/Box/DSS External-Softbank/21 Data from SB/20170329_Internal_Log_formatted (Ericsson)/20170209_0610_SB_nodelist2000_out.zip"
 3036  vim "/Users/212339410/Box/DSS External-Softbank/21 Data from SB/20170329_Internal_Log_formatted (Ericsson)/20170209_0610_SB_nodelist2000_out.zip"
 3037  git checkout --track origin/plot_update
 3038  python calc_survival_time.py --dfrom 20170101 --dto 20170410 --o ../Src/ModData/survival_time.csv
 3039  cd ../Src/ModData/0418_internal_log
 3040  rm internal_log_SB_02010410_bsid32996.csv internal_log_SB_02010410_bsid33360.csv internal_log_SB_02010410_bsid33460.csv internal_log_SB_02010410_bsid34357.csv internal_log_SB_02010410_bsid34409.csv internal_log_SB_02010410_bsid34416.csv internal_log_SB_02010410_bsid34580.csv internal_log_SB_02010410_bsid35516.csv internal_log_SB_02010410_bsid35898.csv internal_log_SB_02010410_bsid36230.csv internal_log_SB_02010410_bsidPB210V.csv internal_log_SB_02010410_bsidPF394V.csv internal_log_SB_02010410_bsidPF402V.csv internal_log_SB_02010410_bsidSA110R.csv internal_log_SB_02010410_bsidSA263R.csv internal_log_SB_02010410_bsidSB107V.csv internal_log_SB_02010410_bsidSB171V.csv internal_log_SB_02010410_bsidSB687V.csv internal_log_SB_02010410_bsidSE111T.csv internal_log_SB_02010410_bsidSE408V.csv internal_log_SB_02010410_bsidSE612V.csv internal_log_SB_02010410_bsidSJ053U.csv internal_log_SB_02010410_bsidSJ765G.csv internal_log_SB_02010410_bsidSK294U.csv internal_log_SB_02010410_bsidSK870T.csv internal_log_SB_02010410_bsidSK939U.csv internal_log_SB_02010410_bsidSL526U.csv internal_log_SB_02010410_bsidSL622G.csv internal_log_SB_02010410_bsidSM682T.csv internal_log_SB_02010410_bsidWD260V.csv internal_log_SB_02010410_bsidWD795V.csv internal_log_SB_02010410_bsidWE429V.csv internal_log_SB_02010410_bsidWG043V.csv internal_log_SB_02010410_bsidWG561V.csv internal_log_SB_02010410_bsidWG571V.csv internal_log_SB_02010410_bsidWG954V.csv internal_log_SB_02010410_bsidWH020V.csv 
 3041  vim bsw_overlap_internallog.csv
 3042  gzip -9 internal_log_SB_02010410_bsw.csv
 3043  pip install ggplot
 3044  python subset_internal_log.py
 3045  wc -l /Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170418 Internal Log (subset by BSID, all command) & BSW 0201_0406/0418_internal_log/bsid_EM_unique.txt
 3046  vim '/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170418 Internal Log (subset by BSID, all command) & BSW 0201_0406/0418_internal_log/bsid_EM_unique.txt'
 3047  vim /Users/212339410/Box/DSS\ Internal-Softbank/40\ Intermediate\ Results/20170418\ Internal\ Log\ \(subset\ by\ BSID,\ all\ command\)\ \&\ BSW\ 0201_0406/0418_internal_log/bsw_overlap_internallog_EM.csv
 3048  vim '/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170418 Internal Log (subset by BSID, all command) & BSW 0201_0406/0418_internal_log/bsw_overlap_internallog_EM_Lsuffix.csv\n'
 3049  vim '/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170418 Internal Log (subset by BSID, all command) & BSW 0201_0406/0418_internal_log/bsw_overlap_internallog_EM_Lsuffix.csv'
 3050  vim new.py
 3051  rm new.py
 3052  git branch -b new_branch
 3053  cd Predix
 3054  rm -rf predix-seed
 3055  cd predix-seed  geproxy
 3056  git clone https://github.com/PredixDev/predix-seed.git  
 3057  git clone https://github.com/PredixDev/predix-seed.git
 3058  cd predix-seed
 3059  cd Box/Development
 3060  cd Predix/predix-seed
 3061  bom install gulp-cli -g
 3062  vim /Users/212339410/.npmrc
 3063  npm install gulp-cli g
 3064  vim test_um_unittest.py
 3065  vim unnecessary_math.py
 3066  python -m doctest -v unnecessary_math.py
 3067  cd visualization/
 3068  python -m doctest -v plot.py
 3069  python -m doctest -v pca_plot.py
 3070  cd ../preprocessing
 3071  python -m doctest -v align_ts.py
 3072  python -m doctest -v detect_ts.py
 3073  python -m doctest -v preprop.py
 3074  cd ../../../Generic
 3075  python test_um_unittest.py
 3076  python test_um_unittest.py -v
 3077  mkdir simple_example 
 3078  mv test_um_unittest.py simple_example/
 3079  mv unnecessary_math.py ./simple_example
 3080  python -m unittest discover simple_example
 3081  python -m unittest discover simple_example -v
 3082  python -m
 3083  man python 
 3084  pip install nose
 3085  nosetest -h
 3086  mkdir tests
 3087  python -m doctest optmodel.py
 3088  python -m doctest optmodel.py -v
 3089  git commit -m 'DOC update the doc to pass doctest'
 3090  pip install mdptoolbox
 3091  mkdir 0420_report_internallog
 3092  cd 0420_report_internallog
 3093  cd Projects/SB/Src/ModData/0420_report_internallog
 3094  ipython nbconvert Asset_view.ipynb --to rst
 3095  mkdir html_output
 3096  ls ./*.rst
 3097  ipython nbconvert *.ipynb --to rst
 3098  sphinx-build ./*.rst ./html_output
 3099  sphinx-build ./Asset_view.rst ./Overview.rst ./Variable_view.rst ./index.rst  ./html_output
 3100  pip install guzzle_sphinx_theme
 3101  cd html_output
 3102  cd embeded_reports
 3103  rm 0419_internal_log
 3104  ln -s /Users/212339410/Box/Analytics/Projects/SB/Src/ModData/0419_internal_log /Users/212339410/Projects/SB/Src/ModData/0420_report_internallog/html_output/embeded_reports/0419_internal_log
 3105  jupyter nbconvert --to html 
 3106  mkdir test_html_from_nb
 3107  jupyter nbconvert --to html Asset_view.ipynb Overview.ipynb Variable_view.ipynb --output ./test_html_from_nb
 3108  jupyter nbconvert --to html Asset_view.ipynb  --output ./test_html_from_nb
 3109  jupyter nbconvert --to rst *.ipynb
 3110  jupyter nbconvert --to rst *.ipynb --template output_toggle_html
 3111  jupyter nbconvert --to rst *.ipynb 
 3112  sphinx-build ./  ./html_output/
 3113  mkdir 0420_wd_internal_log
 3114  ln -s /Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170418 Internal Log (subset by BSID, all command) & BSW 0201_0406/0418_internal_log/bsw_overlap_internallog.csv /Users/212339410/Projects/SB/Src/ModData/0420_wd_internal_log/bsw_overlap_internallog.csv
 3115  ln -s '/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170418 Internal Log (subset by BSID, all command) & BSW 0201_0406/0418_internal_log/bsw_overlap_internallog.csv' /Users/212339410/Projects/SB/Src/ModData/0420_wd_internal_log/bsw_overlap_internallog.csv
 3116  ln -s '/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170418 Internal Log (subset by BSID, all command) & BSW 0201_0406/0418_internal_log/bsw_overlap_internallog_EM.csv  /Users/212339410/Projects/SB/Src/ModData/0420_wd_internal_log/bsw_overlap_internallog_EM.csv
 3117  ln -s '/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170418 Internal Log (subset by BSID, all command) & BSW 0201_0406/0418_internal_log/bsw_overlap_internallog_EM.csv'  /Users/212339410/Projects/SB/Src/ModData/0420_wd_internal_log/bsw_overlap_internallog_EM.csv
 3118  ln -s '/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170418 Internal Log (subset by BSID, all command) & BSW 0201_0406/0418_internal_log/bsw_overlap_internallog_EM_Lsuffix.csv'  /Users/212339410/Projects/SB/Src/ModData/0420_wd_internal_log/bsw_overlap_internallog_EM_Lsuffix.csv
 3119  ln -s "/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170418 Internal Log (subset by BSID, all command) & BSW 0201_0406/0418_internal_log/internal_log_YM_selected.csv.gz" /Users/212339410/Projects/SB/Src/ModData/0420_wd_internal_log/internal_log_YM_selected.csv.gz
 3120  ln -s "/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170418 Internal Log (subset by BSID, all command) & BSW 0201_0406/0418_internal_log/internal_log_SB_selected.csv.gz" /Users/212339410/Projects/SB/Src/ModData/0420_wd_internal_log/internal_log_SB_selected.csv.gz
 3121  ln -s '/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170418 Internal Log (subset by BSID, all command) & BSW 0201_0406/0418_internal_log/Internal_log_YM' /Users/212339410/Projects/SB/Src/ModData/0420_wd_internal_log/Internal_log_YM
 3122  ln -s '/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170418 Internal Log (subset by BSID, all command) & BSW 0201_0406/0418_internal_log/Internal_log_SB' /Users/212339410/Projects/SB/Src/ModData/0420_wd_internal_log/Internal_log_SB
 3123  cd 0420_wd_internal_log
 3124  gzip -9 internal_log_YM_02010410_bsw.csv
 3125  ln -s internal_log_SB_02010410_bsw.csv.gz /Users/212339410/Projects/SB/Src/ModData/0420_wd_internal_log/internal_log_SB_02010410_bsw.csv.gz
 3126  ln -s `pwd`internal_log_SB_02010410_bsw.csv.gz /Users/212339410/Projects/SB/Src/ModData/0420_wd_internal_log/internal_log_SB_02010410_bsw.csv.gz
 3127  ln -s `pwd`/internal_log_SB_02010410_bsw.csv.gz /Users/212339410/Projects/SB/Src/ModData/0420_wd_internal_log/internal_log_SB_02010410_bsw.csv.gz
 3128  ln -s `pwd`/internal_log_YM_02010410_bsw.csv.gz /Users/212339410/Projects/SB/Src/ModData/0420_wd_internal_log/internal_log_YM_02010410_bsw.csv.gz
 3129  vim /Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170418 Internal Log (subset by BSID, all command) & BSW 0201_0406/0418_internal_log/bsw_overlap_internallog.csv
 3130  vim '/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170418 Internal Log (subset by BSID, all command) & BSW 0201_0406/0418_internal_log/bsw_overlap_internallog.csv'
 3131  python descriptive_internal_log.py
 3132  git clone git@github.build.ge.com:IndustrialDataScience/OptModel.git OptModel.gh-pages
 3133  checkout gh-pages
 3134  rm -rf optmodel
 3135  rm README.md
 3136  ls Makefile
 3137  ls build
 3138  ls build/
 3139  sphinx-apidoc -o ./doc ./optmodel
 3140  cd ../../OptModel.gh-pages
 3141  ls source
 3142  cp ./* ../OptModel/doc
 3143  cp ./source/* ../OptModel/doc
 3144  cp -rf ./source/* ../OptModel/doc
 3145  sphinx-build ./doc ../OptModel.gh-pages/
 3146  rm make.bat
 3147  sphinx-apidoc -o . ../optmodel
 3148  sphinx-build . ../../OptModel.gh-pages
 3149  sphinx-apidoc -o . ../optmodel/optmodel.py
 3150  sphinx-apidoc -o . ../optmodel/
 3151  vim optmodel.rst
 3152  vim modules.rst
 3153  mkdir ../Src/ModData/0421_feature
 3154  brew install gawk
 3155  gawk
 3156  man date
 3157  date -j -f "%Y%m%d" "20151010"
 3158  cd ../bas
 3159  cd ../Bash
 3160  echo x
 3161  startd=$(date -j -f '%Y%m%d' "$x" +'%Y%m%d'); 
 3162  echo "$startd";
 3163  x="20151010";
 3164  echo $x
 3165  startd=$(date -j -f '%Y%m%d' "$x" +'%Y%m%d');
 3166  echo "$startd";startdate=20170115
 3167  # startd=$(date -j -f '%Y%m%d' "$startdate" +'%Y%m%d');
 3168  # endd=$(date -j -f '%Y%m%d' "$enddate" +'%Y%m%d');
 3169  endDateTs=$(date -j -f "%Y%m%d" $endd "+%s")
 3170  echo $currentDateTs
 3171  echo $endDateTs
 3172  while [ "$currentDateTs" -le "$endDateTs" ]\ndo\n  date=$(date -j -f "%s" $currentDateTs "+%Y-%m-%d")\n  echo $date\n  currentDateTs=$(($currentDateTs+$offset))\ndone
 3173  while [ "$startdateTs" -le "$enddateTs" ]\ndo\n  date=$(date -j -f "%s" $startdateTs "+%Y%m%d")\n  echo $date\n  startdateTs=$(($startdateTs+$offset))\ndone
 3174  ln -s /Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170421 Feature/0421_feature ../Src/ModData/0421_feature
 3175  cd Src/ModData
 3176  ln -s "/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170421 Feature/0421_feature" /Users/212339410/Box/Analytics/Projects/SB/Src/ModData/0421_feature
 3177  startdate=20170106
 3178  startdateTs=$(date -j -f "%Y%m%d" $startdate "+%s")
 3179  enddateTs=$(date -j -f "%Y%m%d" $enddate "+%s")
 3180  while [ "$startdateTs" -le "$enddateTs" ]\ndo\n  date=$(date -j -f "%s" $startdateTs "+%Y%m%d")\n  echo $date\n  python feature_create_v3.py --vendor nok --dto $date --o ../Src/ModData/0421_feature/ --cache\n\n  startdateTs=$(($startdateTs+$offset))\ndone
 3181  cd Notebook
 3182  cp preprocessing_wot_20151201_20170201.ipynb preprocessing_wot_20170201_20170301.ipynb
 3183  cp preprocessing_wot_20151201_20170301.ipynb preprocessing_wot_20170301_20170327.ipynb
 3184  startdate=20170129
 3185  gzip survival_time.csv
 3186  startdate=20170229
 3187  enddate=20170310
 3188  startdate=20170311
 3189  vim '/Users/212339410/Box/Analytics/Python/Generic/data/servo1.csv'
 3190  vim '/Users/212339410/Box/Analytics/Python/Generic/data/Servo_Guide_CSV.csv'
 3191  cd Projects/SB/
 3192  startdate=20170410
 3193  python feature_create_v3.py --vendor nok --dto 20170410 --o ../Src/ModData/0414_feature_duration/ --cache
 3194  python feature_create_v3.py --vendor nok --dto 20170410 --o ../Src/ModData/0421_feature/ --cache
 3195  python feature_create_v3.py --vendor eri --dto 20170410 --o ../Src/ModData/0421_feature/ --cache
 3196  python feature_create_v3.py --vendor eri --dto 20170411 --o ../Src/ModData/0421_feature/ --cache
 3197  python feature_create_v3.py --vendor nok --dto 20170411 --o ../Src/ModData/0421_feature/ --cache
 3198  python feature_create_v3.py --vendor nok --dto 20170411 --o ../Src/ModData/0421_feature/ 
 3199  cd Projects/SB/ms
 3200  while [ "$currentDateTs" -le "$endDateTs" ]\ndo\n  date=$(date -j -f "%s" $currentDateTs "+%Y%m%d")\n  echo $date\n  currentDateTs=$(($currentDateTs+$offset))\ndone
 3201  while [ "$currentDateTs" -le "$endDateTs" ]\ndo\n  date=$(date -j -f "%s" $currentDateTs "+%Y%m%d")\n  echo $date\n  python feature_create_v3.py --vendor nok --dto $date --o ../Src/ModData/0421_feature/ --cache\n\n  currentDateTs=$(($currentDateTs+$offset))\ndone
 3202  python feature_create_v3.py --vendor eri --dto 20170411 --o ../Src/ModData/0421_feature/ 
 3203  startdate=20170327
 3204  enddate=20170410
 3205  while [ "$currentDateTs" -le "$endDateTs" ]\ndo\n  date=$(date -j -f "%s" $currentDateTs "+%Y%m%d")\n  echo $date\n  python feature_create_v3.py --vendor eri --dto $date --o ../Src/ModData/0421_feature/ --cache\n\n  currentDateTs=$(($currentDateTs+$offset))\ndone
 3206  feature_path='../../../../DSS\ Internal-Softbank/40\ Intermediate\ Results/20170414\ Feature/0414_Xy_set/Train_Validation_Test/Nokia/xy_train_nok_0106_0312_lbh14_lah1_ival14_t1_f10.csv'
 3207  echo $feature_path
 3208  cd SB
 3209  cd Report/
 3210  git clone git@github.build.ge.com:212310464/Softbank_report.git
 3211  ln -s ~/Box/DSS\ Internal-Softbank/40\ Intermediate\ Results/20170421\ Feature/0421_Xy_set/ /Users/212339410/Projects/SB/Src/ModData/0421_Xy_set
 3212  feature_path='../Src/ModData/Train_Test/Nokia/xy_train_nok_0106_0327_lbh14_lah1_ival14_t1_f10.csv'
 3213  python model_selection.py --ifeat $feature_path --iconf model_selection.ini --t Prob_Urgent --o $result_path --score precision --olog ./log/model_selection_nok0421.logfeature_path='/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170421 Feature/0421_Xy_set/Train_Test/Nokia/xy_train_nok_0106_0327_lbh14_lah1_ival14_t1_f10.csv'
 3214  python model_selection.py --ifeat $feature_path --iconf model_selection.ini --t Prob_Urgent --o $result_path --score precision --olog ./log/model_selection_nok0421.log
 3215  feature_path='/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170421 Feature/0421_Xy_set/Train_Test/Nokia/xy_train_nok_0106_0327_lbh14_lah1_ival14_t1_f10.csv'
 3216  model_path0328
 3217  echo $model_path0328
 3218  echo $model_path 0328
 3219  echo {$model_path}0328
 3220  echo ${model_path}0328
 3221  python model_selection.py --ifeat $feature_path --iconf model_selection.ini --t Prob_Urgent --o $result_path --score accuracy --olog ./log/model_selection_nok0421.log
 3222  echo ${model_path}
 3223  python prediction_validation.py --ifeat $feature_test_path  --model_dir $model_path --vendor nok --o ${model_path}0328_0411_validation_ --t Prob_Urgent
 3224  feature_valid_path='/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170421 Feature/0421_Xy_set/Train_Validation_Test/Nokia/xy_test_nok_0328_0411_lbh14_lah1_ival14_t1_f0.csv'
 3225  python prediction_validation.py --ifeat feature_valid_path  --model_dir $model_path --vendor nok --o ${model_path}0328_0411_validation_ --t Prob_Urgent
 3226  result_path='../Src/ModData/0421_model/xy_train_nok_0106_0312_lbh14_lah1_ival14_t1_f10.csv'
 3227  result_path='../Src/ModData/0421_model/xy_train_eri_0115_0312_lbh14_lah1_ival14_t1_f10.csv'
 3228  result_path='../Src/ModData/0421_model/xy_train_eri_0115_0327_lbh14_lah1_ival14_t1_f10.csv'
 3229  result_path='../Src/ModData/0421_model/xy_train_nok_0106_0327_lbh14_lah1_ival14_t1_f10.csv'
 3230  model_path='../Src/ModData/0421_model/'
 3231  python eval_model_dt.py -i "../Src/ModData/0421_Xy_set/Train_Test/Nokia/xy_train_nok_0106_0327_lbh14_lah1_ival14_t1_f10.csv" -o "../Src/ModData/model_eval_rf" --target "Prob_Urgent"  --vendor "nok" -exp True  
 3232  echo 'Nokia'
 3233  feature_train_path='/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170421 Feature/0421_Xy_set/Train_Validation_Test/Nokia/xy_train_nok_0106_0312_lbh14_lah1_ival14_t1_f10.csv'
 3234  feature_valid_path='/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170421 Feature/0421_Xy_set/Train_Validation_Test/Nokia/xy_validation_nok_0313_0327_lbh14_lah1_ival14_t1_f0.csv'
 3235  result_path='../Src/ModData/0422_model/xy_train_nok_0106_0312_lbh14_lah1_ival14_t1_f10.csv'
 3236  python model_selection.py --ifeat $feature_train_path --iconf model_selection.ini --t Prob_Urgent --o $result_path --score accuracy --olog ./log/model_selection_nok0422.log
 3237  echo 'Ericsson'
 3238  feature_train_path='/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170421 Feature/0421_Xy_set/Train_Validation_Test/Ericsson/xy_train_eri_0115_0312_lbh14_lah1_ival14_t1_f10.csv'
 3239  feature_valid_path='/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170421 Feature/0421_Xy_set/Train_Validation_Test/Ericsson/xy_validation_eri_0313_0327_lbh14_lah1_ival14_t1_f0.csv'
 3240  result_path='../Src/ModData/0422_model/xy_train_eri_0115_0312_lbh14_lah1_ival14_t1_f10.csv'
 3241  model_path='../Src/ModData/0422_model/'
 3242  python model_selection.py --ifeat $feature_train_path --iconf model_selection.ini --t Prob_Urgent --o $result_path --score accuracy --olog ./log/model_selection_eri0421.log
 3243  python prediction_validation.py --ifeat $feature_valid_path  --model_dir $model_path --vendor eri --o ${model_path}0328_0411_validation_ --t Prob_Urgent
 3244  result_path='../Src/ModData/0421_test/xy_train_nok_0106_0327_lbh14_lah1_ival14_t1_f10.csv'
 3245  result_path='../Src/ModData/0421_test/xy_train_eri_0115_0327_lbh14_lah1_ival14_t1_f10.csv'
 3246  model_path='../Src/ModData/0421_test/'
 3247  # Ericsson
 3248  python model_selection.py --ifeat $feature_train_path --iconf model_selection.ini --t Prob_Urgent --o $result_path --score precision --olog ./log/model_selection_nok0421.log
 3249  python model_selection.py --ifeat $feature_train_path --iconf model_selection.ini --t Prob_Urgent --o $result_path --score accuracy --olog ./log/model_selection_nok0421.log
 3250  feature_train_path='/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170421 Feature/0421_Xy_set/Train_Test/Ericsson/xy_train_eri_0115_0327_lbh14_lah1_ival14_t1_f10.csv'
 3251  feature_test_path='/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170421 Feature/0421_Xy_set/Train_Test/Ericsson/xy_test_eri_0328_0411_lbh14_lah1_ival14_t1_f0.csv'
 3252  result_path='../Src/ModData/0422_test/xy_train_eri_0115_0327_lbh14_lah1_ival14_t1_f10.csv'
 3253  python prediction_validation.py --ifeat $feature_test_path  --model_dir $model_path --vendor eri --o ${model_path}0328_0411_test_ --t Prob_Urgent
 3254  feature_train_path='/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170421 Feature/0421_Xy_set/Train_Test/Nokia/xy_train_nok_0106_0327_lbh14_lah1_ival14_t1_f10.csv'
 3255  feature_test_path='/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170421 Feature/0421_Xy_set/Train_Test/Nokia/xy_test_nok_0328_0411_lbh14_lah1_ival14_t1_f0.csv'
 3256  result_path='../Src/ModData/0422_test/xy_train_nok_0106_0327_lbh14_lah1_ival14_t1_f10.csv'
 3257  model_path='../Src/ModData/0422_test/'
 3258  #python model_selection.py --ifeat $feature_train_path --iconf model_selection.ini --t Prob_Urgent --o $result_path --score accuracy --olog ./log/model_selection_nok0421.log
 3259  python prediction_validation.py --ifeat $feature_test_path  --model_dir $model_path --vendor nok --o ${model_path}0328_0411_test_ --t Prob_Urgent
 3260  cd Projects/SB/py
 3261  mkdir ../Src/ModData/0424_feature
 3262  python feature_create_v3.py --vendor eri --dto 20170410 --o ../Src/ModData/0424_feature/ 
 3263  python feature_create_v3.py --vendor eri --dto 20170410 --o ../Src/ModData/0424_feature/ --cache
 3264  startdate=20170120
 3265  startdate=20170130
 3266  while [ "$currentDateTs" -le "$endDateTs" ]\ndo\n  date=$(date -j -f "%s" $currentDateTs "+%Y%m%d")\n  echo $date\n  python feature_create_v3.py --vendor $vendor --dto $date --o ../Src/ModData/0424_feature/ --cache\n\n  currentDateTs=$(($currentDateTs+$offset))\ndone
 3267  python feature_create_v3.py --vendor eri --dto 20170411 --o ../Src/ModData/0424_feature/ --cache
 3268  python feature_create_v3.py --vendor nok --dto 20170411 --o ../Src/ModData/0424_feature/ --cache
 3269  python model_selection.py --ifeat $feature_train_path --iconf model_selection.ini --t Prob_Urgent --o $result_path --score roc_auc  --olog ./log/model_selection0424.log
 3270  feature_train_path=''
 3271  declare -a vendor=("nok" "eri")
 3272  man declare
 3273  declare
 3274  declare -a
 3275  vendor=("nok" "eri")
 3276  for i in "${vendor[@]}"\ndo\necho $i\n\n\ndone
 3277  if [ vendor=='nok' ]\nthen\necho $i\nelif [ vendor=='eri'  ]\nthen\necho $i\nfi
 3278  echo $vendor
 3279  for i in "${vendor[@]}"\ndo\necho $i\n\nif [ $i  -eq 'nok' ]\nthen\necho $i\nelif [ $i -eq 'eri'  ]\nthen\necho $i\nfi\n\ndone
 3280  for i in "${vendor[@]}"\ndo\necho $i\n\nif [ $i  == 'nok' ]\nthen\necho $i\n\nelif [ $i == 'eri'  ]\nthen\necho $i\nfi\n\n\ndone
 3281  for i in "${vendor[@]}"\ndo\necho $i\n\nif [ "$i"  == "nok" ]\nthen\necho $i\n\nelif [ "$i" == "eri"  ]\nthen\necho $i\nfi\n\n\ndone
 3282  echo $model_path'xy_train_nok_0106_0312_lbh14_lah1_ival14_t1_f10.csv'
 3283  model_path='../Src/ModData/0424_model/'
 3284  feature_train_path='/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170424 Feature/0424_Xy_set/Train_Validation_Test/Nokia/xy_train_nok_0106_0312_lbh14_lah1_ival14_t1_f10.csv'
 3285  feature_valid_path='/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170424 Feature/0424_Xy_set/Train_Validation_Test/Nokia/xy_validation_nok_0313_0327_lbh14_lah1_ival14_t1_f0.csv'
 3286  result_path=$model_path'xy_train_nok_0106_0312_lbh14_lah1_ival14_t1_f10.csv'
 3287  cd ../Src/ModData
 3288  cd 0424_model_selection
 3289  mv ./*0326* ../0424_test
 3290  mv xy_train_nok_0106_0327* ../0424_test/
 3291  python prediction_validation.py --ifeat $feature_valid_path  --model_dir $model_path --vendor nok --o ${model_path}0328_0411_validation_ --t Prob_Urgent
 3292  python prediction_validation.py --ifeat $feature_valid_path  --model_dir $model_path --vendor nok --o ${model_path}0328_0411_validation_ --t Prob_Urgentfeature_train_path='/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170424 Feature/0424_Xy_set/Train_Test/Nokia/xy_train_nok_0106_0327_lbh14_lah1_ival14_t1_f10.csv'
 3293  feature_train_path='/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170424 Feature/0424_Xy_set/Train_Test/Nokia/xy_train_nok_0106_0327_lbh14_lah1_ival14_t1_f10.csv'
 3294  feature_test_path='/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170424 Feature/0424_Xy_set/Train_Test/Nokia/xy_test_nok_0328_0411_lbh14_lah1_ival14_t1_f0.csv'
 3295  result_path=$model_path'xy_train_nok_0106_0327_lbh14_lah1_ival14_t1_f10.csv'
 3296  python prediction_validation.py --ifeat $feature_valid_path  --model_dir $model_path --vendor nok --o ${model_path}0328_0411_test_ --t Prob_Urgent
 3297  # Train_Validation_Test
 3298  feature_train_path='/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170424 Feature/0424_Xy_set/Train_Validation_Test/Ericsson/xy_train_eri_0116_0312_lbh14_lah1_ival14_t1_f10.csv'
 3299  python prediction_validation.py --ifeat $feature_valid_path  --model_dir $model_path --vendor nok --o ${model_path}0313_0327_validation_ --t Prob_Urgent
 3300  csvcut -n $feature_train_path
 3301  csvcut -n $feature_valid_path
 3302  csvcut -n $feature_valid_pathfeature_train_path='/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170424 Feature/0424_Xy_set/Train_Validation_Test/Ericsson/xy_train_eri_0116_0312_lbh14_lah1_ival14_t1_f10.csv'
 3303  feature_valid_path='/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170424 Feature/0424_Xy_set/Train_Validation_Test/Ericsson/xy_validation_eri_0313_0327_lbh14_lah1_ival14_t1_f0.csv'
 3304  model_path='../Src/ModData/0424_model_selection/'
 3305  result_path=$model_path'xy_train_eri_0116_0312_lbh14_lah1_ival14_t1_f10.csv'
 3306  model_path='../Src/ModData/0424_test/'
 3307  result_path=$model_path'xy_train_eri_0116_0327_lbh14_lah1_ival14_t1_f10.csv'
 3308  python prediction_validation.py --ifeat $feature_valid_path  --model_dir $model_path --vendor $vendor --o ${model_path}0328_0411_test_ --t Prob_Urgent
 3309  python prediction_validation.py --ifeat $feature_valid_path  --model_dir $model_path --vendor $vendor --o ${model_path}0313_0327_validation_ --t Prob_Urgent
 3310  # Common config
 3311  log_path='./log/model_selection_0424.log'
 3312  scoring='roc_auc'
 3313  model_path_validation='../Src/ModData/0424_model_selection/'
 3314  model_path_test='../Src/ModData/0424_test/'
 3315  # Train-Test
 3316  feature_train_path='/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170424 Feature/0424_Xy_set/Train_Test/Ericsson/xy_train_eri_0116_0327_lbh14_lah1_ival14_t1_f10.csv'
 3317  feature_test_path='/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170424 Feature/0424_Xy_set/Train_Test/Ericsson/xy_train_eri_0116_0327_lbh14_lah1_ival14_t1_f1.csv'
 3318  result_path=$model_path_test'xy_train_eri_0116_0327_lbh14_lah1_ival14_t1_f10.csv'
 3319  python model_selection.py --ifeat $feature_train_path --iconf model_selection.ini --t Prob_Urgent --o $result_path --score $scoring --olog $log_path
 3320  python prediction_validation.py --ifeat feature_test_path  --model_dir $model_path_test --vendor $vendor --o ${model_path_test}0328_0411_test_ --t Prob_Urgent
 3321  mv ~/Downloads/colfax-access-key-3447 ~/.ssh
 3322  chmod 600 ~/.ssh/colfax-access-key-3447
 3323  man chmod
 3324  chmod 600 ~/.ssh/config
 3325  ssh colfax
 3326  man qsub
 3327  python model_selection.py --ifeat ../Src/ModData/0425_model_selection/xy_train_nok_0106_0312_lbh14_lah1_ival14_t1_f10.csv  --iconf model_selection.ini --t Prob_Urgent --o ../Src/ModData/0425_model_selection --score roc_auc --olog $log_path
 3328  pip install cufflinks 
 3329  python feature_create_v3.py 
 3330  python feature_create_v3.py -h
 3331  while [ "$currentDateTs" -le "$endDateTs" ]\ndo\n  date=$(date -j -f "%s" $currentDateTs "+%Y%m%d")\n  echo $date\n  python feature_create_v3.py --vendor $vendor --dto $date --o ../Src/ModData/0424_feature/ --cache -lb 28 -la 14 \n\n  currentDateTs=$(($currentDateTs+$offset))\ndone
 3332  while [ "$currentDateTs" -le "$endDateTs" ]\ndo\n  date=$(date -j -f "%s" $currentDateTs "+%Y%m%d")\n  echo $date\n  python feature_create_v3.py --vendor $vendor --dto $date --o ../Src/ModData/0424_feature/ --cache --lb 28 --la 14\n\n  currentDateTs=$(($currentDateTs+$offset))\ndone
 3333  while [ "$currentDateTs" -le "$endDateTs" ]\ndo\n  date=$(date -j -f "%s" $currentDateTs "+%Y%m%d")\n  echo $date\n  python feature_create_v3.py --vendor $vendor --dto $date --o ../Src/ModData/0424_feature/ --cache --lb 14 --la 14\n\n  currentDateTs=$(($currentDateTs+$offset))\ndone
 3334  # eri
 3335  startdate=20170116
 3336  vendor='eri'
 3337  # Look back = 14
 3338  # Look ahead = 14
 3339  # Nokia
 3340  startdate=2017028
 3341  enddate=20170411
 3342  vendor='nok'currentDateTs=$(date -j -f "%Y%m%d" $startdate "+%s")
 3343  vendor='nok'
 3344  currentDateTs=$(date -j -f "%Y%m%d" $startdate "+%s")
 3345  endDateTs=$(date -j -f "%Y%m%d" $enddate "+%s")
 3346  offset=86400
 3347  while [ "$currentDateTs" -le "$endDateTs" ]\ndo\n  date=$(date -j -f "%s" $currentDateTs "+%Y%m%d")\n  echo $date\n  python feature_create_v3.py --vendor $vendor --dto $date --o ../Src/ModData/0425_feature/ --cache --lb 14 --la 14\n\n  currentDateTs=$(($currentDateTs+$offset))\ndone
 3348  cd ../Report
 3349  open SB_template.pdf
 3350  cd submit
 3351  python run_feature_file1.0.py
 3352  cd ~/Box/DSS-Internal-Newmont-Challenge
 3353  cd 10\ Data\ Received
 3354  cd PI\ Data
 3355  cd Weather
 3356  csvcut -n AI699001S1E.PV.csv
 3357  head -n 5 AI699001S1E.PV.csv | csvcut -l
 3358  head -n 5 AI699001S1E.PV.csv | csvcut -l -d | 
 3359  head -n 5 AI699001S1E.PV.csv | csvcut -l | csvlook
 3360  head -n 5 AI699001S1E.PV.csv | csvlook
 3361  csvlook --max-rows 5 AI699001S1E.PV.csv
 3362  .tables
 3363  cd ../Notebook
 3364  cd ~/Projects/SB/Notebook
 3365  python preprocessing_bsw.py --i ~/SB_box_ingest_working_dir/20170428_Ingestion/Input/  --o ./ --dfrom 20170411 --dto 20170425
 3366  l | grep bsw
 3367  mv bsw_20170411_20170425.csv.gz ~/SB_box_ingest_working_dir/20170428_Ingestion/Output
 3368  wc -l ~/SB_box_ingest_working_dir/Talen_Working_Directory
 3369  wc -l ~/SB_box_ingest_working_dir/Talen_Working_Directory/bsw_20170411_20170425.csv
 3370  vim  ~/SB_box_ingest_working_dir/Talen_Working_Directory/bsw_20170411_20170425.csv
 3371  gzipcat 
 3372  zcat /Users/212339410/Box/DSS Internal-Softbank/12 Working Directory for Ingestion/20170428_Ingestion/Output/bsw_20170411_20170425.csv.gz
 3373  zcat '/Users/212339410/Box/DSS Internal-Softbank/12 Working Directory for Ingestion/20170428_Ingestion/Output/bsw_20170411_20170425.csv.gz'
 3374  zcat "/Users/212339410/Box/DSS Internal-Softbank/12 Working Directory for Ingestion/20170428_Ingestion/Output/bsw_20170411_20170425.csv.gz"
 3375  zmore /Users/212339410/Box/DSS Internal-Softbank/12 Working Directory for Ingestion/20170428_Ingestion/Output/bsw_20170411_20170425.csv.gz
 3376  qqzless /Users/212339410/Box/DSS Internal-Softbank/12 Working Directory for Ingestion/20170428_Ingestion/Output/bsw_20170411_20170425.csv.gz
 3377  zless /Users/212339410/Box/DSS Internal-Softbank/12 Working Directory for Ingestion/20170428_Ingestion/Output/bsw_20170411_20170425.csv.gz
 3378  zless "/Users/212339410/Box/DSS Internal-Softbank/12 Working Directory for Ingestion/20170428_Ingestion/Output/bsw_20170411_20170425.csv.gz"
 3379  vim /Users/212339410/Box/Analytics/Projects/SB/cache/select_*_from_bsw_count_table_where_vendor_=_'nokia'__and_date_>_'2017-04-12'_and_date_<_'2017-04-13'.csv.gzip
 3380  vim "/Users/212339410/Box/Analytics/Projects/SB/cache/select_*_from_bsw_count_table_where_vendor_=_'nokia'__and_date_>_'2017-04-12'_and_date_<_'2017-04-13'.csv.gzip"
 3381  zless "/Users/212339410/Box/Analytics/Projects/SB/cache/select_*_from_bsw_count_table_where_vendor_=_'nokia'__and_date_>_'2017-04-12'_and_date_<_'2017-04-13'.csv.gzip"
 3382  zmore "/Users/212339410/Box/Analytics/Projects/SB/cache/select_*_from_bsw_count_table_where_vendor_=_'nokia'__and_date_>_'2017-04-12'_and_date_<_'2017-04-13'.csv.gzip"
 3383  vim "/Users/212339410/Box/Analytics/Projects/SB/cache/select_*_from_bsw_count_table_where_vendor_=_'nokia'__and_date_>_'2017-04-12'_and_date_<_'2017-04-13'.csv"
 3384  vim /Users/212339410/Box/DSS Internal-Softbank/12 Working Directory for Ingestion/20170428_Ingestion/Output/bsw_20170411_20170425.csv
 3385  vim "/Users/212339410/Box/DSS Internal-Softbank/12 Working Directory for Ingestion/20170428_Ingestion/Output/bsw_20170411_20170425.csv"
 3386  pip install rpy2 --upgrade
 3387  conda install -c r rpy2 
 3388  echo $LC_ALL
 3389  locale -a
 3390  cd ~/Applications/Utilities
 3391  python script_feature_create_v3.py
 3392  cd '/Users/212339410/Box/GE Digital Data Science 2016/MBS Personal/';clear;
 3393  cd '/Applications/';clear;
 3394  cd '/Users/212339410/Box/';clear;
 3395  PATH=/Users/212339410/Python/OptModel/optmodel/optmodel.py:$PATH
 3396  export PATH=/Users/212339410/Python/OptModel/optmodel/optmodel.py:$PATH
 3397  optmodel
 3398  python optmodel --demo
 3399  python script_feature_create_v3.py -h
 3400  csvcut -n ../../SB/Src/ModData/demo_result.csv
 3401  csvstat ../../SB/Src/ModData/demo_result.csv
 3402  python optmodel.py h
 3403  python optmodel.py -f ../data/boston.csv -t target -c optmodel.ini
 3404  python optmodel.py -f ../data/boston.csv -t target -c optmodel.ini -o ./
 3405  python optmodel.py -f ../data/boston.csv -t target -c optmodel.ini -o './result'
 3406  python optmodel.py -f ../data/boston.csv -t target -c optmodel.ini -o './'
 3407  gless '/Users/212339410/Box/Analytics/Projects/SB/Src/ModData/0428_feature/feature_ingredients/feature_eri_0312_0326_lbh14_lah1_ival14.csv.gz'
 3408  zless '/Users/212339410/Box/Analytics/Projects/SB/Src/ModData/0428_feature/feature_ingredients/feature_eri_0312_0326_lbh14_lah1_ival14.csv.gz'
 3409  zvim '/Users/212339410/Box/Analytics/Projects/SB/Src/ModData/0428_feature/feature_ingredients/feature_eri_0312_0326_lbh14_lah1_ival14.csv.gz'
 3410  vim '/Users/212339410/Box/Analytics/Projects/SB/Src/ModData/0428_feature/feature_ingredients/feature_eri_0312_0326_lbh14_lah1_ival14.csv.gz'
 3411  file optmodel.py
 3412  vim '/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170424 Feature/0424_Xy_set/Train_Test/Nokia/xy_train_nok_0106_0327_lbh14_lah1_ival14_t1_f1.csv'
 3413  cd ~/SB_box_ingest_working_dir
 3414  pyrhon create_xy_feature.py
 3415  man csvcut
 3416  csvlook '/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170502_Feature/0502_Xy_set/xy_train_nok_0106_0327_lbh14_lah30_ival14.csv.gz'
 3417  vim '/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170502_Feature/0502_Xy_set/xy_train_nok_0106_0327_lbh14_lah30_ival14.csv.gz'
 3418  head -n 5 '/Users/212339410/Box/Analytics/Projects/SB/Src/ModData/survival_time_v2.csv' | csvlook
 3419  head -n 100 '/Users/212339410/Box/Analytics/Projects/SB/Src/ModData/survival_time_v2.csv' | csvlook
 3420  head -n 1000 '/Users/212339410/Box/Analytics/Projects/SB/Src/ModData/survival_time_v2.csv' | csvlook
 3421  csvsql
 3422  jupyter notebook --NotebookApp.iopub_data_rate_limit=10000000  Showcase.ipynb 
 3423  jupyter notebook --NotebookApp.iopub_data_rate_limit=10000000  tstk\ Visualization\ Module\ Part\ 2\ Advanced\ Topics.ipynb
 3424  python create_xy_feature_final.py
 3425  sphinx-apidoc -of ./doc ./tstk 
 3426  git aad -A 
 3427  rm -rf f
 3428  git commit -m 'remove unwanted file and dir'
 3429  git reset --hard HEAD~1
 3430  git checkout -b document_update
 3431  man  ipython nbconvert
 3432  ipython nbconvert tstk\ Tutorial\ -\ Text\ and\ Annotations.ipynb  --to rst --output ../doc
 3433  ipython nbconvert tstk\ Tutorial\ -\ Text\ and\ Annotations.ipynb  --to rst --output ../doc/tstk_tutorial_text_annotations.ipynb
 3434  ipython nbconvert examples/tstk\ Tutorial\ -\ Scatter\ Plots.ipynb  --to rst --output ./doc/tstk_tutorial_scatter_plots.rst
 3435  ipython nbconvert tstk\ Tutorial\ -\ Scatter\ Plots.ipynb  --to rst --output ./doc/tstk_tutorial_scatter_plots.rst
 3436  ipython nbconvert tstk\ Tutorial\ -\ Scatter\ Plots.ipynb  --to rst --output doc/tstk_tutorial_scatter_plots.rst
 3437  ipython nbconvert tstk\ Tutorial\ -\ Scatter\ Plots.ipynb  --to rst 
 3438  cd '/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170502_Feature/0502_Xy_set_final'
 3439  rm xy_train_eri_0106_0327_lbh14_lah30_ival14.csv.gz xy_train_nok_0106_0327_lbh14_lah14_ival14.csv.gz xy_train_nok_0106_0327_lbh14_lah1_ival14.csv.gz xy_train_nok_0106_0327_lbh14_lah30_ival14.csv.gz 
 3440  cd ~/Python/Op
 3441  head -n 10 ../data/boston.csv| csvlook 
 3442  head -n 20 ../data/boston.csv| csvlook 
 3443  head -n 30 ../data/boston.csv| csvlook 
 3444  open â/Users/212339410/Box/Analytics/Python/OptModel/optmodel/results/optmodel_report_roc_auc.csvâ
 3445  open â/Users/212339410/Box/Analytics/Python/OptModel/optmodel/results/optmodel_report_roc_auc.csvâ
 3446  open '/Users/212339410/Box/Analytics/Python/OptModel/optmodel/results/optmodel_report_roc_auc.csv'
 3447  cd Python/OptModel/optmodel
 3448  head -n 30 ../data/boston.csv| csvlook
 3449  python optmodel.py -f ../data/boston.csv -t target -c optmodel.ini -o './results' -S
 3450  cd Projects/SB/Python
 3451  cd '/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170502_Feature/0502_Xy_set'
 3452  rm xy_train_nok_0106_0327_lbh14_lah30_ival14_t1_f10.csv
 3453  cd ../../Projects
 3454  cd TSTK_dashboard
 3455  Rscript
 3456  Rscript dashboard_script.R
 3457  defaults write org.rstudio.RStudio force.LANG en_US.UTF-8
 3458  defalts 
 3459  defaults
 3460  defaults help
 3461  vim /Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170502_Feature/0504_Xy_set/xy_train_nok_0106_0327_lbh14_lah30_ival14_t1_f10.csv.gz
 3462  vim '/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170502_Feature/0504_Xy_set_final/xy_train_nok_0106_0327_lbh14_lah3_ival14_t1_f10.csv.gz'
 3463  vim '/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170502_Feature/0504_Xy_set/xy_train_nok_0106_0327_lbh14_lah30_ival14_t1_f10.csv.gz'
 3464  python create_xy_feature.py
 3465  cd '/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170502_Feature/0504_Xy_set'
 3466  rm ./* 
 3467  .
 3468  cd ../0504_Xy_set_final
 3469  rm ./xy_train_eri_0106_0327_lbh14_lah30_ival14_t1_f10.csv.gz ./xy_train_nok_0106_0327_lbh14_lah14_ival14_t1_f10.csv.gz ./xy_train_nok_0106_0327_lbh14_lah1_ival14_t1_f10.csv.gz ./xy_train_nok_0106_0327_lbh14_lah30_ival14_t1_f10.csv.gz ./xy_train_nok_0106_0327_lbh14_lah3_ival14_t1_f10.csv.gz ./xy_train_nok_0106_0327_lbh14_lah5_ival14_t1_f10.csv.gz ./xy_train_nok_0106_0327_lbh14_lah7_ival14_t1_f10.csv.gz 
 3470  cd Documents/WebEx
 3471  npm install -g gifify
 3472  cd /Users/212339410/Box/GE Digital Data Science 2016/Utility/MOVtoGif
 3473  cd '/Users/212339410/Box/GE Digital Data Science 2016/Utility/MOVtoGif'
 3474  brew unlink gifsicle 
 3475  brew install giflossy
 3476  brew upgrade imagemagick
 3477  cd Box/GE\ Digital\ Data\ Science\ 2016/Utility/MOVtoGif
 3478  for file in ./\ndo\necho $file\ndone
 3479  for file in ./*\ndo\necho $file\ndone
 3480  for file in ./*.mov\ndo\necho $file\ndone
 3481  for file in ./*.mov\ndo\necho $file\necho $(basename "$file")\necho "${filename##*.}"\necho "${filename%.*}"\ndone
 3482  for file in ./*.\ndo\necho $file\necho $(basename "$file")\necho "${filename##*.}"\necho "${filename%.*}"\ndone
 3483  for file in ./*\ndo\necho $file\necho $(basename "$file")\necho "${filename##*.}"\necho "${filename%.*}"\ndone
 3484  for file in .*\ndo\necho $file\nfilename=$(basename "$file")\necho "${filename##*.}"\necho "${filename%.*}"\ndone
 3485  for file in ./*\ndo\necho $file\nfilename=$(basename "$file")\necho "${filename##*.}"\necho "${filename%.*}"\ndone
 3486  for file in ./*..*\ndo\necho $file\nfilename=$(basename "$file")\necho "${filename##*.}"\necho "${filename%.*}"\ndone
 3487  for file in ./*.*\ndo\necho $file\nfilename=$(basename "$file")\necho "${filename##*.}"\necho "${filename%.*}"\ndone
 3488  vim movie2gif.sh
 3489  sh ./movie2gif.sh
 3490  gifify ./movie/preprocessing_notebook.mov -o ./gif/preprocessing_notebook.gif --resize 800:-1
 3491  brew install imagemagick --with-fontconfig
 3492  brew install ffmpeg --with-libass --with-fontconfig
 3493  brew upgrade ffmpeg
 3494  pip install web.py
 3495  pip install web.py==0.40.dev0
 3496  mkdir wepy_test
 3497  mv wepy_test webpy_test
 3498  cd webpy_test
 3499  python code.py
 3500  python code.py 1235
 3501  vim index.html
 3502  vim code.py
 3503  git add doc/_static/link_documentation.png  doc/naming.rst
 3504  python update_schema.py ../../../Data/MBS2016/Original_flat/20170410_Internal_Log_formatted\ \(Ericsson\)/20170325_0610_SB_nodelist2000_out.log  ../Src/Schema/internalLog_v1.xml  ../Src/Schema/internalLog_v2.xml
 3505  python update_schema.py "/Users/212339410/Box/DSS Internal-Softbank/12 Working Directory for Ingestion/20170321_Ingestion/INOS/INOS_ERI_20170307.csv"  ../Src/Schema/inos_v3.xml  ../Src/Schema/inos_v4.xml
 3506  python update_schema.py ~/Box/DSS\ Internal-Softbank/12\ Working\ Directory\ for\ Ingestion/20170323_Ingestion/Input/nedb_.csv ../Src/Schema/nedb_v3.xml  ../Src/Schema/nedb_v3_mod.xml
 3507  python update_schema.py '../Src/ModData/NEDB_NOKERI_TQ.utf8_st_drop.csv' '../Src/Schema/nedb_summer_mod.xml'  '../Src/Schema/nedb_summer_mod_tmp.xml'
 3508  python update_schema.py '../Src/ModData/NEDB_NOKERI_TQ.utf8.csv' '../Src/Schema/nedb.xml'  '../Src/Schema/nedb_mod_tmp.xml'
 3509  python update_schema.py '../Src/UTF8_bom/Ingest/tte_20151201_20170201.csv' '../Src/Schema/tte2016_original_tmp.xml'  
 3510  python update_schema.py '../Src/UTF8_bom/Ingest/wot_20151201_20170201.csv' '../Src/Schema/wot2016_original.xml'  '../Src/Schema/wot2016_mod_tmp.xml'
 3511  python update_schema.py ../Src/ModData/BSW2016_2000.csv ../Src/Schema/bsw2016_v5.xml ../Src/Schema/bsw2016_v5_tmp.xml
 3512  python update_schema.py ../Src/ModData/BSW2016_2000.csv ../Src/Schema/bsw2016.xml ../Src/Schema/bsw2016_v5_tmp.xml
 3513  python update_schema.py ../Src/ModData/BSW2016_2000.csv ../Src/Schema/bsw0301.xml ../Src/Schema/bsw2016_v5_tmp.xml
 3514  python update_schema.py ../Src/ModData/bsw_0327_0410.csv ../Src/Schema/bsw0301.xml ../Src/Schema/bsw2016_v5_tmp.xml
 3515  python update_schema.py ../Src/ModData/bsw_0327_0410.csv ../Src/Schema/bsw2016.xml ../Src/Schema/bsw2016_v5_tmp.xml
 3516  git clone git@github.build.ge.com:212339410/GitPitchTest.git
 3517  cd GitPitchTest
 3518  vim PITCHME.yaml
 3519  git clone https://github.com/saltygk/GitPitchTest2.git
 3520  git clone git@github.com:saltygk/GitPitchTest.git
 3521  git clone git@github.com:saltygk/GitPitchTest.git gitpitchtest2
 3522  git clone https://github.com/saltygk/GitPitchTest.git GitPtchTest2
 3523  cp ./GitPitchTest/* ./GitPtchTest2
 3524  l ./GitPtchTest2
 3525  cd GitPtchTest2
 3526  git commit -a
 3527  /Users/212339410/anaconda2/envs/py35/bin/ipython_mac.command ; exit;
 3528  python optmodel.py 
 3529  uname -a
 3530  rpm
 3531  yum
 3532  head '~/Box/DSS\ Internal-Softbank/40\ Intermediate\ Results/20170502_Feature/0502_Xy_set_final/xy_train_nok_0106_0327_lbh14_lah1_ival14_t1_f10.csv.gz' | python optmodel.py 
 3533  head ~/Box/DSS\ Internal-Softbank/40\ Intermediate\ Results/20170502_Feature/0504_Xy_set_final/xy_train_eri_0106_0327_lbh14_lah14_ival14_t1_f10.csv.gz
 3534  head -n 5~/Box/DSS\ Internal-Softbank/40\ Intermediate\ Results/20170502_Feature/0504_Xy_set_final/xy_train_eri_0106_0327_lbh14_lah14_ival14_t1_f10.csv.gz
 3535  head -n 5 ~/Box/DSS\ Internal-Softbank/40\ Intermediate\ Results/20170502_Feature/0504_Xy_set_final/xy_train_eri_0106_0327_lbh14_lah14_ival14_t1_f10.csv.gz
 3536  vim  ~/Box/DSS\ Internal-Softbank/40\ Intermediate\ Results/20170502_Feature/0504_Xy_set_final/xy_train_eri_0106_0327_lbh14_lah14_ival14_t1_f10.csv.gz
 3537  head -n 5  ~/Box/DSS\ Internal-Softbank/40\ Intermediate\ Results/20170502_Feature/0504_Xy_set_final/xy_train_eri_0106_0327_lbh14_lah14_ival14_t1_f10.csv.gz | csvlook
 3538  file  ~/Box/DSS\ Internal-Softbank/40\ Intermediate\ Results/20170502_Feature/0504_Xy_set_final/xy_train_eri_0106_0327_lbh14_lah14_ival14_t1_f10.csv.gz | csvlook
 3539  zhead   ~/Box/DSS\ Internal-Softbank/40\ Intermediate\ Results/20170502_Feature/0504_Xy_set_final/xy_train_eri_0106_0327_lbh14_lah14_ival14_t1_f10.csv.gz | csvlook
 3540  zcat   ~/Box/DSS\ Internal-Softbank/40\ Intermediate\ Results/20170502_Feature/0504_Xy_set_final/xy_train_eri_0106_0327_lbh14_lah14_ival14_t1_f10.csv.gz | head 
 3541  zcat  '~/Box/DSS\ Internal-Softbank/40\ Intermediate\ Results/20170502_Feature/0504_Xy_set_final/xy_train_eri_0106_0327_lbh14_lah14_ival14_t1_f10.csv.gz' | head 
 3542  zcat
 3543  zcat ~/Box/DSS\ Internal-Softbank/40\ Intermediate\ Results/20170502_Feature/0504_Xy_set_final/xy_train_eri_0106_0327_lbh14_lah14_ival14_t1_f10.csv.gz | head
 3544  gzip -cd  '~/Box/DSS\ Internal-Softbank/40\ Intermediate\ Results/20170502_Feature/0504_Xy_set_final/xy_train_eri_0106_0327_lbh14_lah14_ival14_t1_f10.csv.gz' | head 
 3545  gzip -cd  ~/Box/DSS\ Internal-Softbank/40\ Intermediate\ Results/20170502_Feature/0504_Xy_set_final/xy_train_eri_0106_0327_lbh14_lah14_ival14_t1_f10.csv.gz | head 
 3546  gzip -cd  ~/Box/DSS\ Internal-Softbank/40\ Intermediate\ Results/20170502_Feature/0504_Xy_set_final/xy_train_eri_0106_0327_lbh14_lah14_ival14_t1_f10.csv.gz | head | python optmodel.py
 3547  gzip -cd  ~/Box/DSS\ Internal-Softbank/40\ Intermediate\ Results/20170502_Feature/0504_Xy_set_final/xy_train_eri_0106_0327_lbh14_lah14_ival14_t1_f10.csv.gz | head -n 1 | python optmodel.py
 3548  gzip -cd  ~/Box/DSS\ Internal-Softbank/40\ Intermediate\ Results/20170502_Feature/0504_Xy_set_final/xy_train_eri_0106_0327_lbh14_lah14_ival14_t1_f10.csv.gz | head -n 2 | python optmodel.py
 3549  gzip -cd  ~/Box/DSS\ Internal-Softbank/40\ Intermediate\ Results/20170502_Feature/0504_Xy_set_final/xy_train_eri_0106_0327_lbh14_lah14_ival14_t1_f10.csv.gz | head -n 2 | csvlook 
 3550  gzip -cd  ~/Box/DSS\ Internal-Softbank/40\ Intermediate\ Results/20170502_Feature/0504_Xy_set_final/xy_train_eri_0106_0327_lbh14_lah14_ival14_t1_f10.csv.gz | head -n 2 | csvcut -n
 3551  gzip -cd  ~/Box/DSS\ Internal-Softbank/40\ Intermediate\ Results/20170502_Feature/0504_Xy_set_final/xy_train_eri_0106_0327_lbh14_lah14_ival14_t1_f10.csv.gz | head | csvlook -n 
 3552  gzip -cd  ~/Box/DSS\ Internal-Softbank/40\ Intermediate\ Results/20170502_Feature/0504_Xy_set_final/xy_train_eri_0106_0327_lbh14_lah14_ival14_t1_f10.csv.gz | head | csvcut -n 
 3553  gzip -cd  ~/Box/DSS\ Internal-Softbank/40\ Intermediate\ Results/20170502_Feature/0504_Xy_set_final/xy_train_eri_0106_0327_lbh14_lah14_ival14_t1_f10.csv.gz | head | csvcut -n | head
 3554  gzip -cd  ~/Box/DSS\ Internal-Softbank/40\ Intermediate\ Results/20170502_Feature/0504_Xy_set_final/xy_train_eri_0106_0327_lbh14_lah14_ival14_t1_f10.csv.gz | head | csvcut -C '' | csvcut -n | head
 3555  gzip -cd  ~/Box/DSS\ Internal-Softbank/40\ Intermediate\ Results/20170502_Feature/0504_Xy_set_final/xy_train_eri_0106_0327_lbh14_lah14_ival14_t1_f10.csv.gz | head | csvcut -C  | csvcut -n | head
 3556  gzip -cd  ~/Box/DSS\ Internal-Softbank/40\ Intermediate\ Results/20170502_Feature/0504_Xy_set_final/xy_train_eri_0106_0327_lbh14_lah14_ival14_t1_f10.csv.gz | head | csvcut -C  , | csvcut -n | head
 3557  gzip -cd  ~/Box/DSS\ Internal-Softbank/40\ Intermediate\ Results/20170502_Feature/0504_Xy_set_final/xy_train_eri_0106_0327_lbh14_lah14_ival14_t1_f10.csv.gz | head | csvcut -C  , 'BSID' | csvcut -n | head
 3558  gzip -cd  ~/Box/DSS\ Internal-Softbank/40\ Intermediate\ Results/20170502_Feature/0504_Xy_set_final/xy_train_eri_0106_0327_lbh14_lah14_ival14_t1_f10.csv.gz | head | csvcut -C  , BSID | csvcut -n | head
 3559  gzip -cd  ~/Box/DSS\ Internal-Softbank/40\ Intermediate\ Results/20170502_Feature/0504_Xy_set_final/xy_train_eri_0106_0327_lbh14_lah14_ival14_t1_f10.csv.gz | head | csvcut -C  , BSID | head
 3560  gzip -cd  ~/Box/DSS\ Internal-Softbank/40\ Intermediate\ Results/20170502_Feature/0504_Xy_set_final/xy_train_eri_0106_0327_lbh14_lah14_ival14_t1_f10.csv.gz | head | csvcut -C  , BSID 
 3561  gzip -cd  ~/Box/DSS\ Internal-Softbank/40\ Intermediate\ Results/20170502_Feature/0504_Xy_set_final/xy_train_eri_0106_0327_lbh14_lah14_ival14_t1_f10.csv.gz | head | csvcut -C  , | csvcut -C BSID | head 
 3562  gzip -cd  ~/Box/DSS\ Internal-Softbank/40\ Intermediate\ Results/20170502_Feature/0504_Xy_set_final/xy_train_eri_0106_0327_lbh14_lah14_ival14_t1_f10.csv.gz | head | csvcut -C  , | csvcut -C BSID | csvcut -n
 3563  gzip -cd  ~/Box/DSS\ Internal-Softbank/40\ Intermediate\ Results/20170502_Feature/0504_Xy_set_final/xy_train_eri_0106_0327_lbh14_lah14_ival14_t1_f10.csv.gz | head | csvcut -C  , | csvcut -C BSID | csvcut -n | head
 3564  gzip -cd  ~/Box/DSS\ Internal-Softbank/40\ Intermediate\ Results/20170502_Feature/0504_Xy_set_final/xy_train_eri_0106_0327_lbh14_lah14_ival14_t1_f10.csv.gz | head | csvcut -C  , | csvcut -C BSID | python optmodel.py -target 'Prob_Urgent'
 3565  gzip -cd  ~/Box/DSS\ Internal-Softbank/40\ Intermediate\ Results/20170502_Feature/0504_Xy_set_final/xy_train_eri_0106_0327_lbh14_lah14_ival14_t1_f10.csv.gz | head | csvcut -C  , | csvcut -C BSID | python optmodel.py -t 'Prob_Urgent'
 3566  open model_selection.log
 3567  rm model_selection.log
 3568  gzip -cd  ~/Box/DSS\ Internal-Softbank/40\ Intermediate\ Results/20170502_Feature/0504_Xy_set_final/xy_train_eri_0106_0327_lbh14_lah14_ival14_t1_f10.csv.gz | head | csvcut -C  , | csvcut -C BSID | python optmodel.py -t 'Prob_Urgent' -o results_sb/  
 3569  open optmodel.log
 3570  gzip -cd  ~/Box/DSS\ Internal-Softbank/40\ Intermediate\ Results/20170502_Feature/0504_Xy_set_final/xy_train_eri_0106_0327_lbh14_lah14_ival14_t1_f10.csv.gz | csvcut -C  , | csvcut -C BSID | python optmodel.py -t 'Prob_Urgent' -o results_sb/  
 3571  gzip -cd  ~/Box/DSS\ Internal-Softbank/40\ Intermediate\ Results/20170502_Feature/0504_Xy_set_final/xy_train_eri_0106_0327_lbh14_lah14_ival14_t1_f10.csv.gz | head -n 100 |csvcut -C  , | csvcut -C BSID | python optmodel.py -t 'Prob_Urgent' -o results_sb/  
 3572  gzip -cd  ~/Box/DSS\ Internal-Softbank/40\ Intermediate\ Results/20170502_Feature/0504_Xy_set_final/xy_train_eri_0106_0327_lbh14_lah14_ival14_t1_f10.csv.gz | head -n 1000 |csvcut -C  , | csvcut -C BSID | python optmodel.py -t 'Prob_Urgent' -o results_sb/  
 3573  gzip -cd  ~/Box/DSS\ Internal-Softbank/40\ Intermediate\ Results/20170502_Feature/0504_Xy_set_final/xy_train_eri_0106_0327_lbh14_lah14_ival14_t1_f10.csv.gz | head -n 4000 |csvcut -C  , | csvcut -C BSID | python optmodel.py -t 'Prob_Urgent' -o results_sb/  
 3574  gzip -cd  ~/Box/DSS\ Internal-Softbank/40\ Intermediate\ Results/20170502_Feature/0504_Xy_set_final/xy_train_eri_0106_0327_lbh14_lah14_ival14_t1_f10.csv.gz | head -n 4000 |csvcut -C  , | csvcut -C BSID | python optmodel.py -t 'Prob_Urgent' -o results_sb/ -c optmodel_each.ini
 3575  cd Projects/SB/Src/ModData
 3576  cd 0508_feature
 3577  cd ../../../Python
 3578  python create_xy_feature.py 
 3579  pip install boxsdk[jwt]
 3580  pip install boxsdk
 3581  pip install boxsdkjwt
 3582  python functions/func_sec3_heatmap.py -o '' -f ''
 3583  ls /Users/212339410/anaconda2/envs/py35/bin/python
 3584  file /Users/212339410/anaconda2/envs/py35/bin/python
 3585  cd '/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170502_Feature/feature_ingredients'
 3586  l | grep lah1
 3587  l | grep lah1 | grep eri
 3588  l | grep lah1_ | grep eri
 3589  wc -l feature_eri_0313_0327_lbh14_lah1_ival14.csv.gz
 3590  wc -l feature_eri_0312_0326_lbh14_lah1_ival14.csv.gz
 3591  wc -l '/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170502_Feature/feature_ingredients/oss/df_feature_oss_eri_0313_0327_lbh14.csv.gz'
 3592  wc -l '/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170502_Feature/feature_ingredients/oss/df_feature_oss_eri_0312_0326_lbh14.csv.gz'
 3593  wc -l /Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170502_Feature/feature_ingredients/oss/df_feature_oss_nok_0313_0327_lbh14.csv.gz
 3594  wc -l '/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170502_Feature/feature_ingredients/oss/df_feature_oss_nok_0313_0327_lbh14.csv.gz\n'
 3595  wc -l '/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170502_Feature/feature_ingredients/oss/df_feature_oss_nok_0313_0327_lbh14.csv.gz'
 3596  wc - l ./oss/df_feature_oss_nok_0312_0326_lbh14.csv.gz
 3597  wc - l ./oss/df_feature_oss_nok_0311_0325_lbh14.csv.gz
 3598  wc -l '/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170424 Feature/0424_feature/feature_nok_0327_0410_lbh14_lah1_ival14.csv.gzip'
 3599  wc -l '/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170424 Feature/0424_feature/feature_nok_0328_0411_lbh14_lah1_ival14.csv.gzip'
 3600  pull
 3601  l;
 3602  sphinx-apidoc -o ./doc ./tstk
 3603  l tstk/visualization/plotly_docs.py
 3604  vim ./doc/conf.py
 3605  l ./doc
 3606  sphinx-build ./doc ../tstk.gh-pages
 3607  cd ../tstk.gh-pages
 3608  git add naming.html
 3609  git add .doctrees/naming.doctree
 3610  git add _modules/tstk/visualization/plotly_docs.html
 3611  git add _sources/naming.txt
 3612  git commit -m "Generated gh-pages for `git log master -1 --pretty=short \\n    --abbrev-commit`"
 3613  git commit -a -m "Generated gh-pages for `git log master -1 --pretty=short \\n    --abbrev-commit`"
 3614  cat ~/.zshrc | grep http
 3615  cd functions
 3616  vim splitstr.py
 3617  urllib2
 3618  cd /Users/212339410/anaconda2/envs/py35/lib/python3.5/site-packages/cufflinks
 3619  cd cufflinks
 3620  locate .Renviron
 3621  less ~/.Renviron
 3622  less ~/.Rapp.history
 3623  ls -al  ~/.rstudio-desktop/
 3624  vim ~/.Renviron
 3625  defaults write org.R-project.R force.LANG en_US.UTF-8
 3626  loca
 3627  local
 3628  locale
 3629  pip install webkit2png
 3630  which webkit2png
 3631  locate webkit2png
 3632  git clone https://github.com/adamn/python-webkit2png.git python-webkit2png
 3633  cd python-webkit2png
 3634  python ./webkit2png/scripts.py -h
 3635  python scripts/webkit2png -h
 3636  python webkit2png -h
 3637  python webkit2png/scripts.py
 3638  python webkit2png/scripts.py -h
 3639  git clone git@github.build.ge.com:212339410/configurations.git
 3640  cd configurations
 3641  git commit -m 'add requirements.txt'
 3642  pip freeze -f 
 3643  pip freeze -l
 3644  pip freeze -l > requirements_local.txt
 3645  pip freeze 
 3646  pip freeze --all
 3647  pip freeze --all wc
 3648  pip freeze --all wc -l
 3649  pip freeze --user | wc -l
 3650  pip freeze --user
 3651  pip freeze --all | wc -l
 3652  pip freeze  | wc -l
 3653  pip freeze -l  | wc -l
 3654  pip freeze -l --user  | wc -l
 3655  Rscript test_pythoninR.R
 3656  R CMD UNINSTALL rPython
 3657  R CMF -h
 3658  R CMD INSTALL rPython
 3659  R CMD INSTALL 'rPython'
 3660  R CMD INSTALL ~/Downloads/rPython_0.0-6.tar.gz
 3661  zls ~/Downloads/rPython_0.0-6.tar.gz
 3662  gls ~/Downloads/rPython_0.0-6.tar.gz
 3663  gls
 3664  pip uninstall pyper
 3665  pip install pyper
 3666  git checkout -b add_rmodel
 3667  ls /Users/212339410/anaconda2/envs/py35/* | grep site-package
 3668  ls -alr /Users/212339410/anaconda2/envs/py35/* | grep site-package
 3669  man ls
 3670  ls -ald /Users/212339410/anaconda2/envs/py35/* | grep site-package
 3671  ls -ald /Users/212339410/anaconda2/envs/py35/
 3672  ls -ald /Users/212339410/anaconda2/envs/py35/lib
 3673  ls -ald /Users/212339410/anaconda2/envs/py35/lib/
 3674  ls -ald /Users/212339410/anaconda2/envs/py35/lib/*
 3675  ls -ald /Users/212339410/anaconda2/envs/py35/*
 3676  locate site-package
 3677  ls -ald /Users/212339410/anaconda2/envs/py35/lib/python3.5/site-packages
 3678  ls -ald /Users/212339410/anaconda2/envs/py35/lib/python3.5/site-packages/*
 3679  python test_script/test_plotly_slider.py
 3680  cd Projects/SB/Python/test_script/
 3681  python test_plotly_slider.py
 3682  pip install cufflinks -upgrade
 3683  man pip
 3684  pip -h
 3685  pip install cufflinks --upgrade
 3686  git checkout add_rmodel
 3687  git push origin --delete add_rmodel
 3688  git branch -d add_rmodel
 3689  cd ~/Projects/SB/Python
 3690  pip install nbsphinx 
 3691  pip install IPython
 3692  cd SB/Notebook
 3693  sphinx.quickstart
 3694  python -m sphinx.quickstart
 3695  mkdir ../doc
 3696  mv source ../doc
 3697  mv build ../doc
 3698  mv Makefile ../doc
 3699  cd ../source
 3700  python -m 
 3701  python -m ./ ../build
 3702  python -m . ../build
 3703  python -m sphinx . ../build
 3704  python -m sphinx . ../build -b latex
 3705  pip install sphinx-autobuild 
 3706  sphinx-autobuild . ../build
 3707  cd Projects/SB
 3708  mv text_boxsdk.py ./Python/test_script
 3709  mv ./Python/test_visualize_optmodel.py ./Python/test_script
 3710  pip install freeze > requirements.txt 
 3711  pip freeze > requirements.txt 
 3712  git+https://github.build.ge.com/212339410/SB.git@master
 3713  sphinx-apidoc -o ./doc/source ./Python/process_alarm_names.py
 3714  ln -s ./Python/process_alarm_names.py ./doc/source/
 3715  sphinx-apidoc -o ./doc/source ./doc
 3716  sphinx-apidoc -o ./doc/source ./doc/source
 3717  git log --oneline --decorate
 3718  git log --pretty=%s  
 3719  git log --oneline --graph
 3720  cd Python/tstk/
 3721  vim .travis.tml
 3722  mv .travis.tml .travis.yml
 3723  git add .travis.yml
 3724  git commit -m 'add travis yml file for test purpose'
 3725  less .git/config
 3726  git checkout -b add_unittest
 3727  python tstk/tests/test_detect_ts.py -v
 3728  git add tstk/tests/test_detect_ts.py
 3729  git commit -m 'add unittest case for detect_ts.py and modifiy detect_ts for the dataframe with time series index, add examples'
 3730  git add tstk/preprocessing/detect_ts.py
 3731  git commit -m 'modifiy detect_ts for the dataframe with time series index'
 3732  git add tstk/tests/test_detect_ts.py 
 3733  git commit -m 'add two other test cases for multiple columns'
 3734  python -m unittest tstk/tests/test_detect_ts.py -v
 3735  python -m tstk
 3736  python -m tstk models
 3737  man python
 3738  pip install pandas_ml
 3739  python functions/func_sec3_heatmap.py
 3740  python functions/func_sec3_heatmap.py './figures' 'func_sec3_heatmap.png' 
 3741  chmod 664 ./figures/www/
 3742  chmod 664 ./figures/www/ge_monogram_primary_white_RGB.png
 3743  chmod 664 ./www/ge_monogram_primary_white_RGB.png
 3744  node -h
 3745  node
 3746  cd NodeJs_Udem
 3747  cd C6_LetsRunSomeJavascript/Starter
 3748  note app.js
 3749  which node
 3750  /usr/local/bin/node
 3751  pip install jedi
 3752  vim setup.py
 3753  cd ../OptModel.gh-pages
 3754  rm -rf OptModel.gh-pages
 3755  git clone https://github.build.ge.com/IndustrialDataScience/OptModel.git
 3756  git clone https://github.build.ge.com/IndustrialDataScience/OptModel.git OptModel.ghpages
 3757  cd OptModel.ghpages
 3758  cd ../OptModel
 3759  ls _build
 3760  vim conf.py
 3761  pip install sphinx-gallery
 3762  mkdir examples
 3763  cd ex
 3764  vim plot_exp.py
 3765  vim README.txt
 3766  cd ../doc
 3767  less '/Users/212339410/anaconda2/envs/py35/lib/python3.5/site-packages/sphinx_gallery/gen_rst.py'
 3768  make html -E
 3769  make html --
 3770  git clone https://github.com/janschulz/knitpy.git
 3771  python knitpy --to="all" -- examples\knitpy_overview.pymd
 3772  knitpy --to="all" -- examples\knitpy_overview.pymd
 3773  python knitpy.py 
 3774  knitpy --to="all"  examples\knitpy_overview.pymd
 3775  cd knitpy
 3776  python knitpy --to="all"  examples\knitpy_overview.pymd
 3777  pip install pypandac
 3778  pip install pypandoc
 3779  python knitpy.py --to="all"  ../examples/knitpy_overview.pymd
 3780  R -e "shiny:;runApp('test_app.R')"
 3781  R -e "shiny::runApp('TSTK_Template_App/app.R')"
 3782  cd Box/Analytics/R/NewmontDatVis
 3783  R -e "shiny::runApp('app.R')"
 3784  R -e "shiny::runApp('./')"
 3785  R -e "shiny:;runApp('../NewmontDatVis')"
 3786  R -e "shiny::runApp('../NewmontDatVis')"
 3787  ls  "/Library/Frameworks/R.framework/Resources/library"
 3788  ls  "/Library/Frameworks/R.framework/Resources/library" | grep
 3789  ls  "/Library/Frameworks/R.framework/Resources/library" | grep shiny
 3790  R =h
 3791  R -j
 3792  R -h
 3793  R -e '.Library'
 3794  ls  "/Users/212339410/anaconda2/envs/py35/lib/R/library"
 3795  R -e ".libPaths(); print('test')"
 3796  R -e ".libPaths( c( .libPaths(), \'/Library/Frameworks/R.framework/Resources/library\') ); .libPaths(); shiny::runApp(test_app.R)"
 3797  R -e ".libPaths( c( .libPaths(), '/Library/Frameworks/R.framework/Resources/library') ); .libPaths(); shiny::runApp(test_app.R)"
 3798  ls -l /Library/Frameworks/R.framework/Versions/
 3799  ls -l /Library/Frameworks/R.framework/Libraries
 3800  R -e ".libPaths( c( .libPaths(), \'/Library/Frameworks/R.framework/Resources/library\') ); .libPaths(); shiny::runApp(app_min.R)"
 3801  R -e ".libPaths( c( .libPaths(), '/Library/Frameworks/R.framework/Resources/library') ); .libPaths(); shiny::runApp(app_min.R)"
 3802  Rscript -e "library(methods)"
 3803  Rscript -e "library(methods)l shiny::runApp()"
 3804  Rscript -e "library(methods); shiny::runApp()"
 3805  Rscript -e "library(methods);  .libPaths( c( .libPaths(), '/Library/Frameworks/R.framework/Resources/library') ); .libPaths(); shiny::runApp()"
 3806  Rscript -e "library(methods);  .libPaths( c( .libPaths(), '/Library/Frameworks/R.framework/Resources/library') ); .libPaths(); shiny::runApp('shinyApp/', launch.browser=TRUE)"
 3807  Rscript -e "library(methods);  .libPaths( c( .libPaths(), '/Library/Frameworks/R.framework/Resources/library') ); .libPaths(); shiny::runApp('app_min.R', launch.browser=TRUE)"
 3808  Rscript -e "library(methods);  .libPaths( c( .libPaths(), '/Library/Frameworks/R.framework/Resources/library') ); .libPaths(); dyn.load('/Library/Java/JavaVirtualMachines/jdk1.8.0_101.jdk/Contents/Home/jre/lib/server/libjvm.dylib'); shiny::runApp('app_min.R', launch.browser=TRUE)"
 3809  Rscript -e "library(methods);  .libPaths( c( .libPaths(), '/Library/Frameworks/R.framework/Resources/library') ); .libPaths(); dyn.load('/Library/Java/JavaVirtualMachines/jdk1.8.0_101.jdk/Contents/Home/jre/lib/server/libjvm.dylib'); "
 3810  Rscript -e "library(methods);  .libPaths( c( .libPaths(), '/Library/Frameworks/R.framework/Resources/library') ); .libPaths(); dyn.load('/Library/Java/JavaVirtualMachines/jdk1.8.0_101.jdk/Contents/Home/jre/lib/server/libjvm.dylib'); shiny::runApp(appDir='app_min.R', launch.browser=TRUE)"
 3811  mkdir shinyR-test
 3812  cd shinyR-te
 3813  cd shinyR-test
 3814  vim app.R
 3815  Rscript -e "library(methods);  .libPaths( c( .libPaths(), '/Library/Frameworks/R.framework/Resources/library') ); .libPaths(); dyn.load('/Library/Java/JavaVirtualMachines/jdk1.8.0_101.jdk/Contents/Home/jre/lib/server/libjvm.dylib'); shiny::runApp(appDir='app.R', launch.browser=TRUE)"
 3816  Rscript -e "library(methods);  .libPaths( c( .libPaths(), '/Library/Frameworks/R.framework/Resources/library') ); .libPaths(); dyn.load('/Library/Java/JavaVirtualMachines/jdk1.8.0_101.jdk/Contents/Home/jre/lib/server/libjvm.dylib'); shiny::runApp(appDir='app.R', launch.browser=TRUE, port=8112)"
 3817  Rscript -e "library(methods);  .libPaths( c( .libPaths(), '/Library/Frameworks/R.framework/Resources/library') ); .libPaths(); dyn.load('/Library/Java/JavaVirtualMachines/jdk1.8.0_101.jdk/Contents/Home/jre/lib/server/libjvm.dylib'); shiny::runApp(appDir='./app.R', launch.browser=TRUE, port=8112)"
 3818  Rscript -e "library(methods);  .libPaths( c( .libPaths(), '/Library/Frameworks/R.framework/Resources/library') ); .libPaths(); dyn.load('/Library/Java/JavaVirtualMachines/jdk1.8.0_101.jdk/Contents/Home/jre/lib/server/libjvm.dylib'); shiny::runApp(appDir='app', launch.browser=TRUE, port=8112)"
 3819  Rscript -e 'app.R'
 3820  r apm
 3821  apm --version
 3822  r app.R
 3823  R CMD app.R
 3824  R CMD -e app.R
 3825  R -e app.R
 3826  Rscript -e "app.R"
 3827  Rscript -e "shiny::run('./app.R')"
 3828  Rscript -e "shiny::runApp('./app.R')"
 3829  R test_app.R
 3830  R app.R
 3831  Rscript -e app.R
 3832  Rscript  app.R
 3833  cd ~/Box/Analytics/Projects/TSTK_dashboard
 3834  R -e '.libs'
 3835  R -e '.libPaths'
 3836  R -e '.libPaths()'
 3837  R -e 'library(dygraph)'
 3838  R -e 'library(dygraphs)'
 3839  R -e "installed.packages()"
 3840  R -e "installed.packages()" | grep dygraph
 3841  R -e "shiny::runApp('test_app.R')"
 3842  R -e "test_app.R"
 3843  Rscript test_app.R
 3844  ls test_app.Rout
 3845  cat test_app.Rout
 3846  R CMD BATCH
 3847  cat test_*.Rout
 3848  R CMD BATCH -h
 3849  R CMD BATCH test_app.R
 3850  python gen_rmarkdown.py -o 'test_app.R'
 3851  bash -c 'R CMD BATCH test_app.R'
 3852  vim gen_rmarkdown.py
 3853  vimdiff temp-app.R test_app.R
 3854  cat test123.py
 3855  python test123.py
 3856  vim test123.py
 3857  python gen_rmarkdown.py -o 'testapp.R'
 3858  cd ../../R
 3859  git clone https://github.com/rstudio/rmarkdown-website.git
 3860  cd rmarkdown-website
 3861  cd ~/Projects/SB
 3862  cd Report
 3863  cd Softbank_report
 3864  Rscript -e "rmarkdown::render('SB_template.Rmd')"& open SB_template.pdf
 3865  cd ../SB/Report/Softbank_report
 3866  R CMD knit SB_template.Rmd
 3867  which R
 3868  /usr/local/bin/R
 3869  ls /usr/local/bin/R/
 3870  ls -al /usr/local/bin/R/
 3871  R CMD Sweave
 3872  R CMD knit
 3873  R CMD Knit
 3874  vim '/Library/Frameworks/R.framework/Resources/bin/Rcmd'
 3875  R CMD knit 
 3876  R CMD knitr 
 3877  cd ~/Projects/TSTK_dashboard
 3878  Rscript -e  "rmarkdown::render('temp-report.Rmd')
 3879  '''Rscript -e  "rmarkdown::render('{}')"'''.format(output_file_name)
 3880  Rscript -e  "rmarkdown::render('temp-report.Rmd')"
 3881  ''' c( .libPaths(), '/Library/Frameworks/R.framework/Resources/library') ); .libPaths(); Rscript -e  "rmarkdown::render('{}')"'''.format(output_file_name)
 3882  Rscript -e  "c( .libPaths(), \'/Library/Frameworks/R.framework/Resources/library\') ); .libPaths(); rmarkdown::render(\'temp-report.Rmd\')"
 3883  Rscript -e  "c( .libPaths(), '/Library/Frameworks/R.framework/Resources/library') ); .libPaths(); rmarkdown::render('temp-report.Rmd')"
 3884  Rscript -e  "c( .libPaths(), '/Library/Frameworks/R.framework/Resources/library') ; .libPaths(); rmarkdown::render('temp-report.Rmd')"
 3885  Rscript -e 'rmarkdown::render("temp-report.Rmd")'
 3886  Rscript -e ".libPaths()"
 3887  Rscript -e ".libPaths(); c( .libPaths(), '/Library/Frameworks/R.framework/Resources/library') ; .libPaths()"
 3888  Rscript -e  ".libPaths(c( .libPaths(), '/Library/Frameworks/R.framework/Resources/library')); .libPaths(); rmarkdown::render()"
 3889  Rscript -e  ".libPaths(c( .libPaths(), '/Library/Frameworks/R.framework/Resources/library')); .libPaths(); "
 3890  Rscript -e  ".libPaths(c( .libPaths(), '/Library/Frameworks/R.framework/Resources/library')); .libPaths();"
 3891  cat temp-report.Rmd.Rout
 3892  Rscript -e "rmarkdown::render('SB_template.Rmd')"
 3893  Rscript -e  ".libPaths(c( .libPaths(), '/Library/Frameworks/R.framework/Resources/library')); .libPaths(); rmarkdown::render('Softbank_report.Rmd')"
 3894  python gen_rmarkdown.py
 3895  python gen_rmarkdown.py -h
 3896  cd Projects/TSTK_dashboard
 3897  Rscript -e '.libPaths()[1]'
 3898  Rscript -e '.libPaths()[2]'
 3899  Rscript -e '.libPaths(.libPaths()12]); .libPaths()'
 3900  Rscript -e '.libPaths(.libPaths()[1]); .libPaths()'
 3901  Rscript -e '.libPaths(.libPaths()[2]); .libPaths()'
 3902  Rscript -e '.libPaths(.libPaths()[3]); .libPaths()'
 3903  Rscript -e '.libPaths()[3]; .libPaths(.libPaths()[3]); .libPaths()'
 3904  Rscript -e '.libPaths()[2]; .libPaths(.libPaths()[2]); .libPaths()'
 3905  Rscript -e '.libPaths()[1]; .libPaths(.libPaths()[1]); .libPaths()'
 3906  Rscript -e '.libPaths()[-1]; .libPaths(.libPaths()[1]); .libPaths()'
 3907  RMDFILE=temp-report.Rmd
 3908  RMDFILE=temp-report
 3909  Rscript -e "require(knitr); require(markdown); knit('$RMDFILE.rmd', '$RMDFILE.md'); markdownToHTML('$RMDFILE.md', '$RMDFILE.html', options=c('use_xhml'))"
 3910  Rscript -e "require(knitr); require(markdown); knit('$RMDFILE.rmd', '$RMDFILE.md'); markdownToHTML('$RMDFILE.md', '$RMDFILE.html')"
 3911  Rscript -e "require(knitr); require(markdown); knit('$RMDFILE.rmd', '$RMDFILE.md');"
 3912  cat temp-report.md
 3913  Rscript -e "require(knitr); require(markdown); rmarkdown::render('$RMDFILE.rmd');"
 3914  Rscript -e ".libPaths(c('/Library/Frameworks/R.framework/Resources/library', .libPaths()[1]));require(knitr); require(markdown); rmarkdown::render('$RMDFILE.rmd');"
 3915  python functions/func_sec3_heatmap.py ./figures func_sec3_heatmap.png
 3916  rm output.png 
 3917  cd figures
 3918  rm \[\].png
 3919  rm /['f
 3920  rm \[\'func_sec1_test_scatter_matrix\'\].png
 3921  rm output.png
 3922  python functions/func_sec1_test_scatter_matrix.py
 3923  R CMD -e functions/func_sec2_calendar.R
 3924  R CMD functions/func_sec2_calendar.R
 3925  Rscript functions/func_sec2_calendar.R
 3926  Rscript functions/func_sec2_calendar.R func_sec2_calendar.png
 3927  Rscript functions/func_sec2_calendar.R figures/func_sec2_calendar.png
 3928  Rscript -e  ".libPaths(c( .libPaths(), '/Library/Frameworks/R.framework/Resources/library')); .libPaths(); rmarkdown::render('temp-report.Rmd')"
 3929  Rscript -e  ".libPaths(c( .libPaths(), '/Library/Frameworks/R.framework/Resources/library')); .libPaths(); rmarkdown::render('temp-report.Rmd')"; open('temp-report.Rmd)
 3930  Rscript -e  ".libPaths(c( .libPaths(), '/Library/Frameworks/R.framework/Resources/library')); .libPaths(); rmarkdown::render('temp-report.Rmd')"; open('temp-report.Rmd')
 3931  Rscript -e  ".libPaths(c( .libPaths(), '/Library/Frameworks/R.framework/Resources/library')); .libPaths(); rmarkdown::render('temp-report.Rmd')"; open 'temp-report.Rmd'
 3932  open 'temp-report.Rmd'
 3933  open temp-report.pdf
 3934  python -m 'import pandas as pd; pd.__version__'
 3935  python -m 'import pandas as pd;'
 3936  git clone https://github.build.ge.com/IndustrialDataScience/SchindlerMSK.git
 3937  Rscript -e  ".libPaths(c( .libPaths(), '/Library/Frameworks/R.framework/Resources/library')); .libPaths(); rmarkdown::render('SB_template.Rmd')"
 3938  echo $LD_LIBRARY_PATH
 3939  set
 3940  which Rscript
 3941  cd Reporting_Toolkit
 3942  cd ../Reporting_Toolkit
 3943  Rscript -e  "installed.packages()"
 3944  vim store_packages.R
 3945  Rscript -e  store_packages.R ./store_packages.rda
 3946  Rscript -e  "store_packages.R" ./store_packages.rda
 3947  git clone https://github.build.ge.com/212339410/ReportingTK.git
 3948  cd ReportingT
 3949  cd ReportingTK
 3950  Rscript -e  ".libPaths(c( .libPaths(), '/Library/Frameworks/R.framework/Resources/library')); .libPaths(); rmarkdown::render('output/temp-report.Rmd')"; open output/temp-report.pdf
 3951  Rscript -e  ".libPaths(c( .libPaths(), '/Library/Frameworks/R.framework/Resources/library')); .libPaths(); rmarkdown::render('output/temp-report.Rmd')"
 3952  cd ../LazyReport
 3953  python gen_rmarkdown.py 
 3954  python lazyreport/gen_rmarkdown.py 
 3955  python lazyreport/gen_rmarkdown.py -h
 3956  python lazyreport/gen_rmarkdown.py -o ./chapters
 3957  python lazyreport/gen_rmarkdown.py -o /chapters/parents_report.rmd
 3958  python lazyreport/gen_rmarkdown.py -o ./chapters/parents_report.rmd
 3959  python lazyreport/gen_rmarkdown.py -o /chapters/parents_report.rmd -h
 3960  git push origin add_unittest
 3961  kl
 3962  cd ../Box/Development/Predix
 3963  git clone https://github.com/PredixDev/predix-webapp-starter.git
 3964  cd predix-
 3965  cd predix-webapp-starter
 3966  conda3
 3967  pip install dagobah
 3968  python -v
 3969  locate dagobahd.yml
 3970  ls ~/.dagobahd.yml
 3971  cat ~/.dagobahd.yml
 3972  vim /Users/212339410/dagobah/daemon/dagobahd.yml
 3973  sudo vim /Users/212339410/dagobah/daemon/dagobahd.yml
 3974  mkdir dagobah
 3975  cd dagobah
 3976  mkdir daemon
 3977  cd daemon
 3978  vim dagobahd.yml
 3979  dagobahd
 3980  man ssh-keygen
 3981  conda config --set ssl_verify false
 3982  conda create -n sch27 python=2.7 anaconda
 3983  cd Box/Analytics/R
 3984  git clone https://github.com/rich-iannone/pointblank.git
 3985  Rscript -e
 3986  R CMD -e 'devtools::install_github("rich-iannone/pointblank")'
 3987  R CMD 'devtools::install_github("rich-iannone/pointblank")'
 3988  Rscript -h
 3989  Rscript 
 3990  Rscript  'devtools::install_github("rich-iannone/pointblank")'
 3991  R CMD INSTALL devtools::install_github("rich-iannone/pointblank")
 3992  which bower
 3993  gulp -help
 3994  gulp compile:index
 3995  gulp --tasks
 3996  gulp compile:sass
 3997  gulp --task
 3998  conda --envs
 3999  conda -env
 4000  conda versions
 4001  enda envs
 4002  open lazyReport
 4003  open lazyreport.log
 4004  pip install stable-req.txt
 4005  cd ../Projects/LazyReport
 4006  python lazyreport/gen_rmarkdown.py -i ./lazyreport/test/data/test_report_config_all_brank_heading_level3.xlsx --child
 4007  git clone https://github.build.ge.com/IndustrialDataScience/SchindlerMSK.git SchindlerMSK.ghpages
 4008  l -al
 4009  echo 'doc/build/html' >> .gitignore
 4010  pip install sphinxcontrib-napoleon
 4011  pip install sphinx_rtd_theme
 4012  git remote add origin git git@github.build.ge.com:IndustrialDataScience/SchindlerMSK.git
 4013  git commit -a -m 'initial commit for gh-pages'
 4014  touch .nojekyll
 4015  git commit -a -m 'add .nojekyll
 4016  git commit -a -m 'add .nojekyll'
 4017  sphinx-apidoc -h
 4018  python lazyreport/gen_rmarkdown.py
 4019  jupyter notebook --NotebookApp.iopub_data_rate_limit=10000000 
 4020  cp FD_MSK006_v2.ipynb
 4021  cp FD_MSK006_v2.ipynb FD_MSK006_v2_hiro.ipynb
 4022  cat stable-req.txt | grep sklearn
 4023  cat stable-req.txt
 4024  pip install -r stable-req.txt
 4025  conda install --yes --file stable-req.txt
 4026  pip install -r stable-req.txt --no-index 
 4027  brew install unixodbc
 4028  brew install freetds
 4029  brew upgrade freetds
 4030  pip install --upgrade -r stable-req.txt
 4031  vim stable-req.txt
 4032  cd dss_msk_dev
 4033  pip install scikit-learn==0.18.1
 4034  python -m 'import sklearn; sklearn.__version__'
 4035  which python 
 4036  nh
 4037  ls &
 4038  bg
 4039  which jupyter
 4040  less .gitignore
 4041  Last login: Fri May 19 08:39:53 on console
 4042  (py35) 212339410@SFO1212339410M:~% pip install matplotlib -U
 4043  Retrying (Retry(total=4, connect=None, read=None, redirect=None)) after connection broken by 'NewConnectionError('<pip._vendor.requests.packages.urllib3.connection.VerifiedHTTPSConnection object at 0x1062d0c18>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known',)': /simple/matplotlib/
 4044  (py35) 212339410@SFO1212339410M:~% proxy
 4045  pip install matplotlib -U
 4046  python optmodel/optmodel.py
 4047  cd Projects/LazyReport
 4048  python optmodel.py -f ~/Box/DSS\ Internal-Softbank/40\ Intermediate\ Results/20170502_Feature/0502_Xy_set_final/xy_train_nok_0106_0327_lbh14_lah1_ival14_t1_f10.csv.gz  -t Prob_Urgent -c optmodel.ini -o './results_sb' -S
 4049  git commit -m 'resolved merge conflict'
 4050  cd Op
 4051  python optmodel.py -f ../data/boston.csv 
 4052  python optmodel.py -f ../data/boston.csv -t target -o ./results/demo_result_0519.csv
 4053  python optmodel.py -f ../data/boston.csv -t target -o ./results/
 4054  cd ~/Python/tstk
 4055  git checkout -b jenkinsfile-test
 4056  git checkout jenkinsfile-test
 4057  git remote --help
 4058  git remote set-url origin git@github.build.ge.com:212339410/tstk.git
 4059  git push origin jenkinsfile-test
 4060  cd Python/tstk
 4061  gs
 4062  python -m unittest tstk/tests/test_detect_ts.py
 4063  python lazyreport/gen_rmarkdown.py -i ./lazyreport/test/data/test_report_config_all_brank_heading_level3.xlsx
 4064  sphinx-apidoc -o ./doc/source ./dss_msk
 4065  sphinx-build ./doc/source ./doc/build
 4066  cd doc/build/
 4067  sphinx-apidoc -f -o source ../dss_msk/
 4068  cd source/ht
 4069  cat ../../../.gitignore
 4070  git commit -m 'doc update'
 4071  csvcut ~/Python/OptModel/data/boston.csv
 4072  csvcut -n ~/Python/OptModel/data/boston.csv
 4073  csvcut -C 'target' ~/Python/OptModel/data/boston.csv | csvlook
 4074  csvcut -c 'target' ~/Python/OptModel/data/boston.csv | csvlook
 4075  python optmodel/optmodel.py -f data/CombinedFeature_DoorTypeBinaryOnlyShort.csv 
 4076  python optmodel/optmodel.py -f data/CombinedFeature_DoorTypeBinaryOnlyShort.csv  -t label -i
 4077  python optmodel/optmodel.py -f data/boston.csv  -t label -c ./optmodel/optmodel.ini -o ./optmodel/results
 4078  csvlook ./data/boston.csv
 4079  csvcut -n ./data/CombinedFeature_DoorTypeBinaryOnlyShort.csv
 4080  csvcut -C 'label' ./data/CombinedFeature_DoorTypeBinaryOnlyShort.csv
 4081  csvcut -c 'label' ./data/CombinedFeature_DoorTypeBinaryOnlyShort.csv
 4082  csvcut -c 'label' ./data/CombinedFeature_DoorTypeBinaryOnlyShort.csv | csvlook
 4083  python optmodel/optmodel.py -f data/boston.csv  -t target -c ./optmodel/optmodel.ini -o ./optmodel/results
 4084  csvlook ./data/CombinedFeature_DoorTypeBinaryOnlyShort.csv
 4085  vim ./data/CombinedFeature_DoorTypeBinaryOnlyShort.csv
 4086  python optmodel/optmodel.py -f data/CombinedFeature_DoorTypeBinaryOnlyShort.csv  -t label -c ./optmodel/optmodel.ini -o ./optmodel/results
 4087  pip install matplotlib==1.4.3
 4088  cd doc/source
 4089  cd ../Generic
 4090  cd test_script
 4091  python command_pandas_ml.py -h
 4092  csvlook ../data/boston.csv
 4093  csvlook -n ../data/boston.csv
 4094  csvcut -n ../data/boston.csv
 4095  csvcut -r ../data/boston.csv
 4096  csvcut -b ../data/boston.csv
 4097  csvcut  ../data/boston.csv
 4098  csvcut  ../data/boston.csv | python command_pandas_ml.py --train 
 4099  csvcut  ../data/boston.csv | python command_pandas_ml.py --train  -t target 
 4100  csvcut  ../data/boston.csv | python command_pandas_ml.py --train  -t target | python command_pandas_ml.py --predict --test_data ../data/boston.csv
 4101  csvcut -C target ../data/boston.csv > ../data/boston_test.csv
 4102  csvcut  ../data/boston.csv | python command_pandas_ml.py --train  -t target | python command_pandas_ml.py --predict --test_data ../data/boston_test.csv
 4103  csvcut  ../data/boston.csv | python command_pandas_ml.py --train  -t target | python command_pandas_ml.py --predict --test_data ../data/boston_test.csv | python command_pandas_ml.py predicted_count
 4104  csvcut  ../data/boston.csv | python command_pandas_ml.py --train  -t target | python command_pandas_ml.py --predict --test_data ../data/boston_test.csv | 
 4105  csvcut  ../data/boston.csv | python command_pandas_ml.py --train  -t target | python command_pandas_ml.py --predict --test_data ../data/boston_test.csv 
 4106  csvcut  ../data/boston.csv | python command_pandas_ml.py --train  -t target | python command_pandas_ml.py --predict --test_data ../data/boston_test.csv | python command_pandas_ml.py --predicted_count
 4107  cf login -a https://api.system.asv-pr.ice.predix.io
 4108  cf s
 4109  cf delete-service uaatest-1
 4110  cf delete-service uaa-personal
 4111  cf -h 
 4112  cf -h  | grep unbind
 4113  cf unbind-service 
 4114  cf unbind-service predixseed
 4115  cf unbind-service predixseedvcf apps
 4116  cf app anomaly-detection-api
 4117  cf app test-0926-predix-nodejs-starter
 4118  cf app 
 4119  cf app dashboard_v1.R
 4120  cf env anomaly-detection-api
 4121  cf env test-0926-predix-nodejs-starter
 4122  cf unbind-service test-0926-predix-nodejs-starter
 4123  cf unbind-service test-0926-predix-nodejs-starter predix-uaa
 4124  cf envs anomaly-detection-api
 4125  cf delete app test-0926-predix-nodejs-starter
 4126  cf delete test-0926-predix-nodejs-starter
 4127  cf delete dashboard_v1.R
 4128  cf delete start_shiny_app.R
 4129  cf delete-service 
 4130  cf delete-service predixseed
 4131  git h
 4132  git add ../../../dss_msk/algorithms/__init__.py
 4133  git add ../../../dss_msk/models/__init__.py
 4134  git add ../../../dss_msk/preprocessing/__init__.py
 4135  git add ../../../dss_msk/tests/__init__.py
 4136  git commit -m 'add __init__.py for documentation'
 4137  git add ../../../dss_msk/algorithms/robust_distance.py
 4138  git add ../../../dss_msk/algorithms/steady_state.py
 4139  git add ../../../dss_msk/algorithms/sub_event_counter.py
 4140  git commit -m 'update copyright string to comment out using #'
 4141  git push master 
 4142  cd source
 4143  cd ../build
 4144  git checkout -b gh-pages
 4145  git -a -m 'update documentation'
 4146  git commit -a -m 'update documentation'
 4147  git remot eadd origin git@github.build.ge.com:IndustrialDataScience/SchindlerMSK.git
 4148  git remote add origin git@github.build.ge.com:IndustrialDataScience/SchindlerMSK.git
 4149  cd ~/Python/OptModel
 4150  python optmodel/optmodel.py -h
 4151  cd ~/Projects/SchindlerMSK/doc/build/html
 4152  git push -u origin gh-pages
 4153  git pull gh-pages origin
 4154  git add searchindex.js
 4155  cd doc/build
 4156  git commit -m 'update DOC'
 4157  git pull gh-pages gh-pages
 4158  git pull origin gh-pages
 4159  git git pull origin gh-pages --allow-unrelated-histories
 4160  git pull origin gh-pages --alow-unrelated-histories
 4161  git pull origin gh-pages --allow-unrelated-histories
 4162  rm .buildinfo
 4163  rm .git
 4164  rm -rf .git 
 4165  git clone git@github.build.ge.com:IndustrialDataScience/SchindlerMSK.git
 4166  cat CombinedFeature_DoorTypeBinaryOnlyShort.csv
 4167  cat CombinedFeature_DoorTypeBinaryOnlyShort1.csv
 4168  head -n 5 CombinedFeature_DoorTypeBinaryOnlyShort.csv | csvlook
 4169  mv CombinedFeature_DoorTypeBinaryOnlyShort1.csv CombinedFeature_DoorTypeMultiClassOnlyShort.csv
 4170  rm -rf SchindlerMSK
 4171  python lazyreport/gen_rmarkdown.py -i config/report_config.xlsx
 4172  python optmodel/optmodel.py --demo
 4173  python optmodel.py --demo
 4174  python optmodel.py --demo -S
 4175  git add ../optmodel/optmodel.ini ../optmodel/optmodel.pyl
 4176  cd optmodel
 4177  mkdir results
 4178  python optmodel.py -f ../data/CombinedFeature_DoorTypeMultiClassOnlyShort.csv -o results -S -t label
 4179  python optmodel.py -f ../data/CombinedFeature_DoorTypeMultiClassOnlyShort.csv -o results -S -t label -c ./optmodel_demo.ini
 4180  python optmodel.py -f ../data/CombinedFeature_DoorTypeMultiClassOnlyShort.csv -o results -S -t label -c ./optmodel_demo.ini -s accuracy
 4181  python optmodel.py -f ../data/boston_cat_input.csv -o results0523 -S -t label -c ./optmodel_demo.ini -s accuracy
 4182  python optmodel.py -f ../data/boston_cat_input.csv -o results0523 -S -t target -c ./optmodel_demo.ini -s accuracy
 4183  python optmodel.py -h
 4184  cd  ../data
 4185  python optmodel/optmodel.py --C target, target1, b, c
 4186  python optmodel/optmodel.py -C 'target, target1, b, c'
 4187  cd ./build/html
 4188  git clone git@github.build.ge.com:IndustrialDataScience/SchindlerMSK.git SchndlerMSK.gh-pages
 4189  cd SchndlerMSK.gh-pages
 4190  git checkout gh-pages
 4191  git commit -m 'update doc'
 4192  cd build/html
 4193  git clone git@github.build.ge.com:212339410/Test-Docs.git
 4194  cd Test-Docs
 4195  mkdir docs
 4196  pip install --upgrade
 4197  ase also report this if it was a user error, so that a better error message can be provided next time.
 4198  A bug report can be filed in the tracker at <https://github.com/sphinx-doc/sphinx/issues\>. Thanks!
 4199  make: *** [html] Error 1
 4200  (sch27) 212339410@SFO1212339410M:~l
 4201  cd ../../SchindlerMSK.gh-pages
 4202  git commit -m 'DOC update copy right'
 4203  cd SchindlerMSK/doc/source/
 4204  cd html
 4205  cp -a ./ ../../../../SchindlerMSK.gh-pages
 4206  cp -a ./. ../../../../SchindlerMSK.gh-pages
 4207  git log master -1 --pretty=short --abbrev-commit
 4208  l | grep Schindler
 4209  git add development/Notes/how_to_update_documents_with_sphinx.md
 4210  git commit -m 'DOC add development/Notes/how_to_update_documents_with_sphinx.md'
 4211  ssh -i /Users/212339410/Box/GE Digital Data Science 2016/10 Schindler Personal/AWS/DataScience.pem ubuntu@ec2-54-214-160-173.us-west-2.compute.amazonaws.com
 4212  ssh -i '/Users/212339410/Box/GE Digital Data Science 2016/10 Schindler Personal/AWS/DataScience.pem' ubuntu@ec2-54-214-160-173.us-west-2.compute.amazonaws.com
 4213  cd doc/
 4214  git commit -m 'DOC update'
 4215  git push origin gh-pages
 4216  git commit -m 'DOC add the link to how to update documentation'
 4217  cd ~/Projects/SchindlerMSK
 4218  cd LazyReport
 4219  cd ../SchindlerMSK.gh-pages
 4220  cd ../tstk
 4221  pip install PIL
 4222  docker ps
 4223  host
 4224  docker-machine ls
 4225  chkconfig
 4226  service
 4227  docker pull centos
 4228  docker pull centos:7
 4229  docker run -it centos:7 /bin/bash
 4230  locate pem
 4231  locate .pem
 4232  locate DataScience.pem
 4233  mv ~/Box/GE\ Digital\ Data\ Science\ 2016/10\ Schindler\ Personal/AWS/DataScience.pem ~/.ssh/
 4234  ssh -i ~/.ssh/DataScience.pem ubuntu@ec2-54-214-160-173.us-west-2.compute.amazonaws.com -L 8400:10.43.51.205:22
 4235  ssh -i ~/.ssh/DataScience.pem ubuntu@ip-10-43-51-90.us-west-2.compute.internal -L 8400:10.43.51.205:22
 4236  ssh -i ~/.ssh/DataScience.pem ubuntu@10.43.51.90 -L 8400:10.43.51.205:22
 4237  nxi_bastion
 4238  chmod 400 DataScience.pem
 4239  ssh -i ./DataScience.pem ubuntu@ec2-54-214-160-173.us-west-2.compute.amazonaws.com
 4240  ssh sch_hiro
 4241  vim config
 4242  nslookup 10.43.51.205
 4243  source activate sch
 4244  pip install --upgrade awscli
 4245  pip install --upgrade --user awscli
 4246  echo sch27
 4247  echo `sch27`
 4248  which aws
 4249  pip install awscli
 4250  brew install awsclie
 4251  brew install awscli
 4252  aws --version
 4253  aws s3
 4254  aws ec2
 4255  aws ec2 run-instances
 4256  defaults write com.apple.
 4257  defaults write com.apple.screencapture location /Users/212339410/Box/Analytics/Notes/Figures
 4258  killall SystemUIServer
 4259  cat ~/.local/bin
 4260  ssh -i 'DataScience.pem' ubuntu@ec2-54-214-160-173.us-west-2.compute.amazonaws.com -L 8400:10.43.51.205:22
 4261  cd ~/.ssh
 4262  brew install sshfs
 4263  osxfufe -v
 4264  osxfuse -v
 4265  fuse
 4266  cat co
 4267  cat config
 4268  sshfs ec2-54-214-160-173.us-west-2.compute.amazonaws.com
 4269  mkdir remote_mount
 4270  cd remote_mount
 4271  ubunt@sshfs ec2-54-214-160-173.us-west-2.compute.amazonaws.com /home/ubuntu ./
 4272  sshfs ubuntu@ec2-54-214-160-173.us-west-2.compute.amazonaws.com /home/ubuntu ./
 4273  sshfs ubuntu@ec2-54-214-160-173.us-west-2.compute.amazonaws.com: /home/ubuntu ./
 4274  sshfs ubuntu@ec2-54-214-160-173.us-west-2.compute.amazonaws.com: / ./
 4275  sshfs ubuntu@ec2-54-214-160-173.us-west-2.compute.amazonaws.com: / remote_mount
 4276  sshfs ubuntu@ec2-54-214-160-173.us-west-2.compute.amazonaws.com: / ~/remote_mount
 4277  sshfs ubuntu@ec2-54-214-160-173.us-west-2.compute.amazonaws.com: / ~/remote_mount/
 4278  sshfs ubuntu@ec2-54-214-160-173.us-west-2.compute.amazonaws.com: / 
 4279  sshfs ubuntu@ec2-54-214-160-173.us-west-2.compute.amazonaws.com: / -o IdentityFile=~/.ssh/DataScience.pem 
 4280  sshfs ubuntu@ec2-54-214-160-173.us-west-2.compute.amazonaws.com: / -o IdentityFile=~/.ssh/DataScience.pem
 4281  sshfs ubuntu@ec2-54-214-160-173.us-west-2.compute.amazonaws.com: /home/ubuntu -o IdentityFile=~/.ssh/DataScience.pem
 4282  sshfs ubuntu@ec2-54-214-160-173.us-west-2.compute.amazonaws.com: /home/ubuntu/ -o IdentityFile=~/.ssh/DataScience.pem
 4283  sshfs ubuntu@ec2-54-214-160-173.us-west-2.compute.amazonaws.com:/home/ubuntu/ -o IdentityFile=~/.ssh/DataScience.pem
 4284  sshfs ubuntu@ec2-54-214-160-173.us-west-2.compute.amazonaws.com:/home/ubuntu/ ~/remote_mount -o IdentityFile=~/.ssh/DataScience.pem
 4285  mkdir remote_centos
 4286  sshfs centos@localhost: /home/centos/ ~/remote_cnetos
 4287  sshfs centos@localhost:/home/centos/ ~/remote_cnetos
 4288  sshfs centos@localhost:/home/centos/ ~/remote_cnetos -o IdentityFile=~/.ssh/DataScience.pem
 4289  sshfs centos@localhost:/home/centos/ ~/remote_centos -o IdentityFile=~/.ssh/DataScience.pem
 4290  nslookup 10.43.51.90
 4291  sudo ssh -f centos@10.43.51.90 -L 22:ec2-54-214-160-173.us-west-2.compute.amazonaws.com:22 -N
 4292  su -
 4293  su - 
 4294  su
 4295  sudo ls
 4296  sudo 
 4297  sudo ssh sch_bastion
 4298  ssh -f centos@10.43.51.90 -L 22:ec2-54-214-160-173.us-west-2.compute.amazonaws.com:22 -N 
 4299  sudo ssh -f centos@10.43.51.90 -L 22:ec2-54-214-160-173.us-west-2.compute.amazonaws.com:22 -N 
 4300  ssh -i 'DataScience.pem' ubuntu@ec2-54-214-160-173.us-west-2.compute.amazonaws.com -L 8400:10.43.51.239:22
 4301  ssh -i '~/.ssh/DataScience.pem' ubuntu@ec2-54-214-160-173.us-west-2.compute.amazonaws.com -L 8400:10.43.51.239:22
 4302  ssh -i ~/.ssh/DataScience.pem ubuntu@ec2-54-214-160-173.us-west-2.compute.amazonaws.com -L 22:10.43.51.239:22
 4303  ssh -i ~/.ssh/DataScience.pem ubuntu@ec2-54-214-160-173.us-west-2.compute.amazonaws.com -L 8400:10.43.51.239:22
 4304  sshfs -p 8400 centos@10.43.51.239:/home/centos 
 4305  sshfs -p 8400 centos@10.43.51.239:/home/centos ~/remote_centos
 4306  cd Box/GE\ Digital\ Data\ Science\ 2016
 4307  cd docker
 4308  docker run 
 4309  docker -d
 4310  service docker start
 4311  osboot2docker up
 4312  boot2docker up
 4313  open -a docker
 4314  docker image
 4315  docker 
 4316  docker images
 4317  cd ~/Projects/LazyReport
 4318  git add doc/*
 4319  vim doc/source/index.rst
 4320  git add doc/source/index.rst
 4321  git commit -m 'add doc/ directory'
 4322  python ]
 4323  ssh sch_bastion -A
 4324  ssh -i 'DataScience.pem' ubuntu@ec2-54-214-160-173.us-west-2.compute.amazonaws.com -L 8400:10.43.51.90:22
 4325  ssh -i '~/.ssh/DataScience.pem' ubuntu@ec2-54-214-160-173.us-west-2.compute.amazonaws.com -L 8400:10.43.51.90:22
 4326  ssh -i ~/.ssh/DataScience.pem ubuntu@ec2-54-214-160-173.us-west-2.compute.amazonaws.com -L 8400:10.43.51.90:22
 4327  vim ~/.zshrc.pre-oh-my-zsh
 4328  ipfoncifg
 4329  cat ~/.ssh/config -a
 4330  ifconfig -all
 4331  scp sch_centos:/home/centos/.vimrc ~/Box/Analytics/Projects/
 4332  vim ~/Box/Analytics/Projects/
 4333  l ~/Box/Analytics/Projects/
 4334  vim ~/Box/Analytics/Projects/.vimrc
 4335  ssh sch_centos -L localhost:8888:localhost:8889 
 4336  ssh sch_centos -N -f -L localhost:8888:localhost:8889 
 4337  kill 41192
 4338  ps aux | grep localhost
 4339  ssh sch_centos -N -L localhost:8888:localhost:8889 
 4340  ssh sch_centos -N -L localhost:8888:localhost:8889 centos:10.43.51.90
 4341  ssh sch_centos -N -L localhost:8888:10.43.51.90:8889 sch_bastion
 4342  ssh  -L localhost:8888:10.43.51.90:8889 ec2-54-214-160-173.us-west-2.compute.amazonaws.com
 4343  ssh  -L localhost:8888:10.43.51.90:8889 ec2-54-214-160-173.us-west-2.compute.amazonaws.com -i ~/.ssh/DataScience.pem
 4344  ssh  -i ~/.ssh/DataScience.pem -L localhost:8888:10.43.51.90:8889 ec2-54-214-160-173.us-west-2.compute.amazonaws.com 
 4345  ssh  -i ~/.ssh/DataScience.pem -L localhost:8888:10.43.51.90:8889 ubuntu@ec2-54-214-160-173.us-west-2.compute.amazonaws.com 
 4346  ssh  -i ~/.ssh/DataScience.pem -L 13389:10.43.51.90:8889 centos@10.43.51.90
 4347  ssh  -i ~/.ssh/DataScience.pem -L localhost:13389:10.43.51.90:8889 centos@10.43.51.90
 4348  ssh sch_centos -N -L localhost:8888:10.43.51.90:88889 
 4349  ssh -i ~/.ssh/DataScience.pem -o 'ProxyCommand ssh sch_bastion -W %h:%p' -N -n -L localhost:8889:10.43.51.9:8889 centos@10.43.51.9
 4350  ssh -i ~/.ssh/DataScience.pem -o 'ProxyCommand ssh sch_bastion -W %h:%p' -N -n -L 127.0.0.1:8889:127.0.0.1:8889 10.43.51.9
 4351  ssh -i ~/.ssh/DataScience.pem -N -n -L 127.0.0.1:8889:127.0.0.1:8889 sch_bastion
 4352  ssh sch_centos -N -L localhost:8888:10.43.51.90:8889 
 4353  crontab -h
 4354  man crontab 
 4355  aws ec2 describe-instance-status 
 4356  aws ec2 describe-instance-status help
 4357  gzip
 4358  gzip -best 
 4359  gzip -best Document_D 
 4360  gzip --best Document_D 
 4361  tar 
 4362  man tar
 4363  tar -czf Document_D.tar.gz Document_D -o gzip:compression-level=9
 4364  cd Box\ Sync\ Backup
 4365  tar -czf Analtytics201612.tar.gz Analytics -o gzip:compression-level=9
 4366  man zip
 4367  rm Analtytics201612.tar.gz
 4368  rm Document_D.tar.gz
 4369  GZIP=-9 tar cvzf Document_D.tar.gz ./Document_D
 4370  rm -rf Document_D
 4371  diff -rq ./Analytics ~/Box/Analytics
 4372  diff  ./Analytics ~/Box/Analytics
 4373  diff -rq ./Analytics/Data ../Box/Analytics/Data
 4374  diff Analytics/Python/ ../Box/Analytics/Python
 4375  diff -qr dirA dirB | grep -v -e 'DS_Store' -e 'Thumbs' | 
 4376  open diffs_analytics.txt
 4377  rm -rf scikit-learn*
 4378  cd Analyticscd ..
 4379  cd Analytics
 4380  diff -qr ./Analytics ../Box/Analytics  | grep -v -e 'DS_Store' -e 'Thumbs' | sort > diffs_analytics.txt
 4381  mkdir Vim
 4382  git clone https://github.com/skilldrick/vim-exercises.git
 4383  cd vim-exercises
 4384  vim vim_ex.txt
 4385  pip install glumpy
 4386  pip install pyopengl
 4387  pip install triangle
 4388  pip install --upgrade cython
 4389  glxinfo
 4391  pip install vispy
 4392  pip install PyQt
 4393  brew install PyQt --with-python3
 4394  pip install flake8
 4395  brew install pyqt
 4396  brew upgrade pyqt
 4397  conda install bokeh
 4398  bokeh sampledata
 4399  pip install utils
 4400  env
 4401  vim crontab
 4402  crontab
 4403  cd anaconda
 4404  cd ../Box/Analytics/Projects
 4405  cd ../Development/Vim
 4406  mkdir crontask
 4407  cd crontask
 4408  cd Vim
 4409  rm crontask
 4410  rm crontask -rf
 4411  rm -rf crontask 
 4412  crontab -u 30 7 * * 1-5 aws ec2 stop-instances --instance-ids i-0b06ff9cce802df7b
 4413  man crontab
 4414  start-instances --instance-ids i-0b06ff9cce802df7b
 4415  ssh -i DataScience.pem  -N -n -L 127.0.0.1:8889:127.0.0.1:8889 centos@10.43.51.90 -Y âjupyter notebook --no-browser --port=8889 â
 4416  ssh -i  ~/.ssh/DataScience.pem  -N -n -L 127.0.0.1:8889:127.0.0.1:8889 sch_centos -Y # @bastion, portforwarding centos to bastion
 4417  sudo service ssh restart
 4418  ssh sch_bastion -v
 4419  grep sshd /etc/hosts.equiv
 4420  iptables -n -L -v
 4421  ifconfig -n
 4422  ifconfig 
 4423  ifconfig -L
 4424  ping 10.43.51.90
 4425  ssh nxi_bastion -vvv
 4426  netstat -anp | grep sshd
 4427  netstat -anp
 4428  netstat 
 4429  netstat  | grep sshd
 4430  sshd
 4431  ls ~/.ssh/
 4432  sudo dscacheutil -flushcache
 4433  ssh sch_bastion -vvv
 4434  ssh sch_bastion 
 4435  ssh -i ~/.ssh/DataScience.pem -N -n -L 127.0.0.1:8889:127.0.0.1:8889 sch_bastion -Y  # @local pc, portforwarding bastion to local 
 4436  :
 4437  * ssh -i ~/.ssh/DataScience.pem -N -n -L 127.0.0.1:8889:127.0.0.1:8889 sch_bastion -Y -o ServerAliveInterval=50  # @local pc, portforwarding bastion to local
 4438  ssh -i ~/.ssh/DataScience.pem -N -n -L 127.0.0.1:8889:127.0.0.1:8889 sch_bastion -Y -o ServerAliveInterval=50  # @local pc, portforwarding bastion to local
 4439  ssh sch_bastion "ssh -i  DataScience.pem  -N -n -L 127.0.0.1:8889:127.0.0.1:8889 centos@10.43.51.90 -Y  -o ServerAliveInterval=50"
 4440  ipython qtconsole
 4441  scp sch_centos:~/.vimrc ~/.vimrc.centos
 4442  vim ~/.vimrc.centos
 4443  cd Box/Development/Vim
 4444  curl https://raw.githubusercontent.com/Shougo/neobundle.vim/master/bin/install.sh > install.sh
 4445  sh ./install.sh
 4446  cp ~/.vimrc.centos ./
 4447  vim ~/.vimrc.swp
 4448  ls ~/
 4449  l ~/.vim*
 4450  mv ~/.vimrc.centos ./
 4451  vim ~/.vim
 4452  vim ~/.vimrc
 4453  aws ec2 stop-instances --instance-ids i-0b06ff9cce802df7b
 4454  calendar
 4455  ssh sch_centos 
 4456  ssh sch_centos -X
 4457  sudo echo "X11Forwarding yes" >> /etc/ssh/sshd_config
 4458  sudo service sshd restart
 4459  sudo launchctl stop com.openssh.sshd
 4460  sudo launchctl start com.openssh.sshd
 4461  ssh sch_centos -X -C
 4462  $PATH
 4463  echo $PATH | grep xauth
 4464  locate auth
 4465  locate xauth
 4466  launchtl unload ssh.plist
 4467  ssh sch_centos -Y -X -C
 4468  ssh sch_centos -Y -V
 4469  mv LazyReport AgileReport
 4470  rm lazyReport agilereport
 4471  mv lazyReport agilereport
 4472  vim /etc/ssh/sshd_config
 4473  sudo vim /etc/ssh/sshd_config
 4474  lanchtl
 4475  cd /System/Library/LaunchDaemons
 4476  launchctl unload ssh.plist
 4477  launchctl load ssh.plist
 4478  sudo launchctl unload  /System/Library/LaunchDaemons/ssh.plist 
 4479  sudo launchctl load -w /System/Library/LaunchDaemons/ssh.plist 
 4480  ssh sch_centos -Y -v
 4481  ssh -X -i ~/.ssh/DataScience.pem centos@10.43.51.90 'ssh sch_bastion -W %h:%p' 
 4482  ssh -X -i ~/.ssh/DataScience.pem centos@10.43.51.90 'ssh sch_bastion -W %h:%p' -v
 4483  ssh -X -i ~/.ssh/DataScience.pem centos@10.43.51.90 -o ProxyCommand='ssh sch_bastion -W %h:%p' -v
 4484  ssh -Y -i ~/.ssh/DataScience.pem centos@10.43.51.90 -o ProxyCommand='ssh sch_bastion -W %h:%p' -v
 4485  ssh sch_bastion -L 2022:10.43.51.90:22 &
 4486  ssh -X -p 2022 locahost
 4487  ssh -X -p 2022 127.0.0.1
 4488  ssh sch_centos -XC
 4489  man ps
 4490  kill 71705
 4491  kill 71704
 4492  ssh -i ~/.ssh/DataScience.pem -N -n -L 127.0.0.1:2022:127.0.0.1:22 sch_bastion -Y -o ServerAliveInterval=50  # @local pc, portforwarding bastion to local 
 4493  ssh -i ~/.ssh/DataScience.pem -N -n -L 127.0.0.1:2023:127.0.0.1:22 sch_bastion -Y -o ServerAliveInterval=50  # @local pc, portforwarding bastion to local 
 4494  ssh sch_bastion -L 2023:10.43.51.90:22 
 4495  ssh -i  ~/.ssh/DataScience.pem  -X -p 2023 localhost
 4496  ssh -i  ~/.ssh/DataScience.pem  -X -p 2023 127.0.0.1
 4497  ssh sch_bastion -L 2024:10.43.51.90:22 
 4498  ssh -i  ~/.ssh/DataScience.pem  -X -p 2024 127.0.0.1
 4499  l ~/.ssh/
 4500  cat ~/.ssh/known_hosts
 4501  ssh -L 127.0.0.1:2022:10.43.51.90 -i ~/.ssh/DataScience.pem centos@10.43.51.90 -o ProxyCommand='ssh sch_bastion -W %h:%p'
 4502  ssh -L 127.0.0.1:2022:10.43.51.90:22 -i ~/.ssh/DataScience.pem centos@10.43.51.90 -o ProxyCommand='ssh sch_bastion -W %h:%p'
 4503  ssh -L 127.0.0.1:2025:10.43.51.90:22 -i ~/.ssh/DataScience.pem centos@10.43.51.90 -o ProxyCommand='ssh sch_bastion -W %h:%p'
 4504  ssh -p 2025 -X 212339410@127.0.0.1
 4505  man ssh
 4506  ssh -p 2025 -X 127.0.0.1
 4507  ssh -Y sch_centos
 4508  ssh -Y sch_centos -v
 4509  xeyes
 4510  ssh -L 8999:10.43.51.90:22 sch_bastion 
 4511  ssh -X -p 9990 localhost
 4512  ssh -X -p 9990 127.0.0.1
 4513  ssh -L 8999:ec2-54-214-160-173.us-west-2.compute.amazonaws.com:22 sch_bastion 
 4514  vim ~/.ssh/known_hosts
 4515  ssh -X -p 8999 127.0.0.1
 4516  ssh -X -p 8999 127.0.0.1 -i ~/.ssh/DataScience.pem
 4517  ssh -i ~/.ssh/DataScience.pem -X -p 8999 127.0.0.1
 4518  ssh -i ~/.ssh/DataScience.pem -L 2022:ec2-54-214-160-173.us-west-2.compute.amazonaws.com:2022 ubuntu@ec2-54-214-160-173.us-west-2.compute.amazonaws.com
 4519  kill 74040
 4520  kill 84039
 4521  kill 74039
 4522  kill 74407
 4523  kill 74574
 4524  ssh -L 8999:ec2-54-214-160-173.us-west-2.compute.amazonaws.com:22 -i ~/.ssh/DataScience.pem ubuntu@ec2-54-214-160-173.us-west-2.compute.amazonaws.com
 4525  aws ec2 describe-instance-status      --instance-ids i-0b06ff9cce802df7b
 4526  aws configure --profile user2
 4527  asw s3 ls
 4528  echo $SHELL
 4529  complete -C '/user/local/bin/aws_completer' aws
 4530  which aws_completer
 4531  complete -C `which aws_completer` aws
 4532  complete
 4533  source /usr/local/bin/aws_zsh_completer.sh
 4534  aws ec2 help
 4535  aws ec2 describe-instance help
 4536  ping 4.2.2.2
 4537  curl http://www.google.com
 4538  sshfs sch_bastion
 4539  ecronta -l
 4540  ssh sch_centos -v
 4541  xclock
 4542  ssh sch_centos -Y
 4543  sshfs sch_centos:/home/centos/ /Users/212339410/remote_centos
 4544  cd Mount/remote_centos
 4545  git
 4546  mkdir test-repo
 4547  cd test-repo
 4548  nslookup 
 4549  nslookup ec2-54-214-160-173.us-west-2.compute.amazonaws.com
 4550  ssh sch_bastion -Y
 4551  ssh sch_centos
 4552  iwlist wlan0 scan
 4553  networksetup
 4554  networksetup -setairportnetwork en0 Internet 
 4555  ssh -i ~/.ssh/DataScience.pem -o ProxyCommand='ssh sch_bation'
 4556  ssh -i ~/.ssh/DataScience.pem -o ProxyCommand='ssh sch_bation' centos@10.43.72.230
 4557  ssh -i ~/.ssh/DataScience.pem -o ProxyCommand='ssh sch_bastion' centos@10.43.72.230
 4558  scp '/Users/212339410/Box/GE Digital Data Science 2016/10 Schindler Personal/docker/pds-msk-analytics-base-image.tar' sch_docker:
 4559  ssh sch_docker -vvv
 4560  scp -i ~/.ssh/DataScience.pem -o ProxyCommand='ssh sch_bastion -W %h:%p' '/Users/212339410/Box/GE Digital Data Science 2016/10 Schindler Personal/docker/pds-msk-analytics-base-image.tar' Centos@10.43.72.230:/home/centos/ 
 4561  scp -i ~/.ssh/DataScience.pem -o ProxyCommand='ssh sch_bastion -W %h:%p' '/Users/212339410/Box/GE Digital Data Science 2016/10 Schindler Personal/docker/pds-msk-analytics-base-image.tar' centos@10.43.72.230:/home/centos/ 
 4562  ssh sch_docker -v
 4563  cat ~/.ssh/known_hosts.old
 4564  locate deny.
 4565  ssh sch_docker
 4566  git clone git@github.build.ge.com:IndustrialDataScience/IndustrialImageAnalyticsToolkit.git
 4567  cd IndustrialImageAnalyticsToolkit
 4568  cd iitk
 4569  brew install tesseract
 4570  python ocrtables.py
 4571  ssh -i ~/.ssh/DataScience.pem -o ProxyCommand='ssh sch_bastion -W %h:%p' centos@10.43.72.230 
 4572  ssh sch_centos_hiro 'cat ~/.vimrc'
 4573  ssh sch_bastion âssh -i  DataScience.pem  -N -n -L 127.0.0.1:8889:127.0.0.1:8889 centos@10.43.51.90 -Y  -o ServerAliveInterval=50â
 4574  ssh sch_centos_hiro -X 'jupyter notebook'
 4575  ssh sch_centos_hiro -Y 'jupyter notebook'
 4576  scp sch_centos_siva:aws_run_fd_msk.py sch_centos_hiro:hiro/
 4577  ssh sch_centos_
 4578  ls ~/Library/Application\ Support/iTerm
 4579  vim ~/Library/Application\ Support/iTerm/version.txt
 4580  scp sch_centos_siva:aws_run_fd_msk.py Projects/SchindlerMSK/development/Pythonh
 4581  scp sch_centos_siva:aws_run_fd_msk.py sch_centos_hiro:Siva/
 4582  aws ec2 start-instances --instance-ids i-0b06ff9cce802df7b
 4583  man df
 4584  autofsd
 4585  locate mnt
 4586  locate mnt | mnt/
 4587  locate mnt | grep mnt/
 4588  locate mnt/
 4589  locate /mnt/
 4590  sudo mkdir /Volumes/sch_centos_hiro
 4591  sudo vim /etc/fstab.hd
 4592  locate fstab
 4593  ls /etc/fstab
 4594  vim /etc/fstab
 4595  sshfs#sch_centos_hiro:/ /mnt/sch_centos_hiro
 4596  sshfs sch_centos_hiro:/ /mnt/sch_centos_hiro
 4597  sshfs sch_centos_hiro:/ /Volumes/sch_centos_hiro
 4598  sudo sshfs sch_centos_hiro:/ /Volumes/sch_centos_hiro
 4599  sudo sshfs sch_centos_hiro /Volumes/sch_centos_hiro
 4600  sudo sshfs sch_centos_hiro: /Volumes/sch_centos_hiro
 4601  sshfs sch_centos:/home/centos/ /Volumes/sch_centos_hiro
 4602  sudo sshfs sch_centos:/home/centos/ /Volumes/sch_centos_hiro
 4603  sshfs sch_centos:/home/centos/ /Users/212339410/Mount/remote_centos
 4604  sudo sshfs sch_centos_hiro:/home/centos/ /Volumes/sch_centos_hiro
 4605  cd SchindlerEdgeAnalytics/
 4606  cp ../SchindlerMSK/dss_msk/ .
 4607  cp ../SchindlerMSK/dss_msk/ . -a
 4608  cp -a ../SchindlerMSK/dss_msk/ .
 4609  mkdir doc
 4610  rm main.pyc
 4611  cat .gitignore
 4612  /pyc
 4613  cat README_v011.md
 4614  mkdir seanalytics
 4615  cp -a algorithms seanalytics
 4616  mv -r features seanalytics/
 4617  man mv
 4618  rm -rf algorithms 
 4619  mv *.py seanalytics/
 4620  mv config_files seanalytics
 4621  mv models features seanalytics
 4622  mv preprocessing tests use_cases seanalytics
 4623  mv README_v011.md doc/realse_notesv011.md
 4624  vim use_case1.py
 4625  vim __init__.py
 4626  git add doc/
 4627  git add seanalytics/algorithms/
 4628  mkdir scripts
 4629  cd models
 4630  rm models
 4631  rm -rf models
 4632  rm -rf use_cases
 4633  rm main.py
 4634  cd config_files
 4635  vim DMDM_parameters_config.ini
 4636  cat config.ini
 4637  cat no_event_config.ini
 4638  vim dss_msk/main.py
 4639  vim __main__.py
 4640  cp ../SchindlerMSK/dss_msk
 4641  cp ../SchindlerMSK/dss_msk/__main__.py ./seanalytics/ 
 4642  cp ../SchindlerMSK/dss_msk/main.py ./seanalytics/ 
 4643  cp ../SchindlerMSK/dss_msk/__init__.py ./seanalytics/ 
 4644  mv tests ../
 4645  git -rf rm .
 4646  git -r  .
 4647  git rm --cached seanalytics/algorithms/DMDM_parameters_config.ini
 4648  git rm --cached seanalytics/algorithms/MSK006_results.csv
 4649  git rm --cached seanalytics/algorithms/align_old.py
 4650  git rm --cached seanalytics/features/features_README.md
 4651  mv seanalytics/features/features_README.md ./doc
 4652  cd doc
 4653  git rm --cached seanalytics/algorithms/config.ini
 4654  git rm --cached seanalytics/algorithms/no_event_config.ini
 4655  git rm --cached seanalytics/algorithms/*.ini
 4656  cd Projects/SchindlerA
 4657  git rm --cached seanalytics/algorithms/event_detection2.py
 4658  git rm --cached seanalytics/algorithms/no_event2.py
 4659  vim tests/test_README.md
 4660  git rm --cached tests/test_README.md
 4661  git commit -m 'initial setup'
 4662  git commit -m 'DOC modified reference in README.md'
 4663  gid add doc/features_README.md
 4664  git add doc/features_README.md
 4665  git commit -m 'add feature documents'
 4666  git clone git@github.build.ge.com:212339410/SchindlerEdgeAnalytics.git seanalytics-forked
 4667  mv test.txt doc
 4668  git commit -m 'test commit'
 4669  git pull --all
 4670  cd ../SchindlerEdgeAnalytics
 4671  git commit -m 'update README.md'
 4672  ssh -i ~/.ssh/DataScience.pem -N -n -L 127.0.0.1:8889:127.0.0.1:8889 sch_bastion -Y -o ServerAliveInterval=50 ssh sch_bastion 'ssh -i  DataScience.pem  -N -n -L 127.0.0.1:8889:127.0.0.1:8889 centos@10.43.51.90 -Y  -o ServerAliveInterval=50'
 4673  kill 37856
 4674  lsof -n -i4TCP:$PORT | grep LISTEN # Verified on macOS Sierra
 4675  lsof -n -iTCP:$PORT | grep LISTEN
 4676  netstat -p tcp | grep $POR
 4677  netstat -p tcp | grep $PORT
 4678  netstat -p tcp | grep 8889
 4679  netstat -p tcp
 4680  sphinx-quickstart -h
 4681  ssh  -N -n -L 127.0.0.1:8889:127.0.0.1:8889 sch_bastion -Y -o ServerAliveInterval=50
 4682  ssh  -N -n -L 127.0.0.1:8889:127.0.0.1:8889 sch_bastion -Y 
 4683  ssh  -Y -N -n -L 127.0.0.1:8889:127.0.0.1:8889 sch_bastion 
 4684  sudo chmod 600 /etc/auto_centos_hiro
 4685  pws
 4686  sshfs sch_centos_hiro:/home/centos/ /Users/212339410/Mount/remote_centos
 4687  man sshfs 
 4688  sshfs sch_centos_siva:/home/centos/ /Users/212339410/Mount/remote_centos_siva
 4689  ssh  -YNn -L 127.0.0.1:8889:127.0.0.1:8889 sch_bastion 
 4690  bokeh serve
 4691  pip install --upgrade bokeh
 4692  pip install bokeh 
 4693  mkdir bokeh_test
 4694  cd bo
 4695  cd bokeh_test
 4696  vim  download_sample_data.py
 4697  python download_sample_data.py
 4698  cd daily
 4699  ''' Create a simple stocks correlation dashboard.\nChoose stocks to compare in the drop down widgets, and make selections\non the plots to update the summary and histograms accordingly.\n.. note::\n    Running this example requires downloading sample data. See\n    the included `README`_ for more information.\nUse the ``bokeh serve`` command to run the example by executing:\n    bokeh serve stocks\nat your command prompt. Then navigate to the URL\n    http://localhost:5006/stocks\n.. _README: https://github.com/bokeh/bokeh/blob/master/examples/app/stocks/README.md\n'''
 4700  try:
 4701  except ImportError:
 4702  from os.path import dirname, join
 4703  from bokeh.io import curdoc
 4704  from bokeh.layouts import row, column
 4705  from bokeh.models import ColumnDataSource
 4706  from bokeh.models.widgets import PreText, Select
 4707  from bokeh.plotting import figure
 4708  Use the ``bo =Use the ``bokeh serve`` command to run the examni    bokeh serve stocks
 4709  at your command prompt. Then navigate che(at your command prompke    http://localhost:5006/stocks\n.. _README: ht t.. _README: https://github.com/ad'''\ntry:\n    from functools import lru_cache\nexcept ImportError:\n    # Python 2 doe,   oo   'o    # Python 2 doe,   oo   'o    # d    # Python 2 doee'    # Python 2 doe,   oo   'o    # Python 2 doe,   oo   'o    #at    # Python 2 doe,   oo   'o    # Python 2 doe:\n    # Python 2 doe,   oo   'o    # Python 2 doe,   oo   'o    # d    # Pythf1    # Python 2 doe, d    # Python 2 doe)\n    # Python 2 doe,   oo   'o  at    # Python 2 doe,   oo   'o  at    ns    # Python 2 doe,rn    # Python 2 doret    # Python 2 doe,   oo   'o  a re    # Python 2 doep w    # Python 2 doe,   oo  xt    # Python 2 doe,   oo   'o  at   ue    # Python 2 doe,   oo   'o  at    # PKE    # Python 2 doe,   oo   'o  at    # Python 2('    # Python 2 doe,   oo   'o  aup    # Python 2 doe,   oo   'o  at    # Python 2e=    # Python 2 doe,   oo   'o  at    # Python 2 doe,   oo   'o  at    ns    # Pythonta=dict(date=[], t1=[], t2=[], t1_returns=[], t2_returns=[]))
 4710  tools = 'pan,wheel_zoom,xbox_select,reset'
 4711  corr = figure(plot_width=350, plot_height=350,\n              tools='pan,wheel_zoom,box_select,reset')
 4712  corr.circle('t1_returns', 't2_returns', size=2, source=source,\n            selection_color="orange", alpha=0.6, nonselection_alpha=0.1, selection_alpha=0.4)
 4713  ts1 = figure(plot_width=900, plot_height=200, tools=tools, x_axis_type='datetime', active_drag="xbox_select")
 4714  ts1.line('date', 't1', source=source_static)
 4715  ts1.circle('date', 't1', size=1, source=source, color=None, selection_color="orange")
 4716  ts2 = figure(plot_width=900, plot_height=200, tools=tools, x_axis_type='datetime', active_drag="xbox_select")
 4717  ts2.x_range = ts1.x_range
 4718  ts2.line('date', 't2', source=source_static)
 4719  ts2.circle('date', 't2', size=1, source=source, color=None, selection_color="orange")
 4720  # set up callbacks
 4721  def ticker1_change(attrname, old, new):
 4722  bokeh serve 
 4723  cd Python/bokeh_test
 4724  netsat -vanp | grep 5006
 4725  netstat -vanp | grep 5006
 4726  netstat -vanp tcp | grep 5006
 4727  netstat -vanp tcp 
 4728  bokeh serve .
 4729  locate export_csv
 4730  which bokeh
 4731  mkdir stocks
 4732  mv daily download_sample_data.py stocks
 4733  mv main.py stocks
 4734  mkdir export_csv
 4735  cd export_csv
 4736  vim export_csv
 4737  vim export_csv.py
 4738  vim main.py
 4739  ssh sch_bastion 'ssh -i  DataScience.pem  -N -n -L 127.0.0.1:5006:127.0.0.1:5007 centos@10.43.51.90 -Y  -o ServerAliveInterval=50'
 4740  ssh -i ~/.ssh/DataScience.pem -N -n -L 127.0.0.1:5007:127.0.0.1:5007 sch_bastion -Y -o ServerAliveInterval=50
 4741  ssh sch_bastion 'ssh -i  DataScience.pem  -N -n -L 127.0.0.1:5007:127.0.0.1:5007 centos@10.43.51.90 -Y  -o ServerAliveInterval=50'
 4742  vim ~/.psswd-s3fs
 4743  echo  $ACCESS_KEY
 4744  ACCESS_KEY=AKIAJEWBXRC44KGNZV5Q
 4745  SECRET_KEY=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
 4746  echo $ACCESS_KEY:$SECRET_KEY > ~/.passwd-s3fs
 4747  mkdir -p ~/Mount/s3-ge-schindler-ds
 4748  brew install fuse
 4749  sudo brew cask install osxfuse
 4750  man s3fs
 4751  s3fs ge-schindler-ds 
 4752  brew install Caskroom/cask/osxfuse
 4753  brew install homebrew/fuse/s3fs
 4754  find Cellar
 4755  find remote
 4756  find remote_bastion
 4757  s3fs cat ~/.passwd-s3fs
 4758  brew link --force gettext
 4759  which fuse
 4760  which osxfuse
 4761  git clone --recursive -b support/osxfuse-3 git://github.com/osxfuse/osxfuse.git osxfuse
 4762  git clone --recursive -b support/osxfuse-3 https://github.com/osxfuse/osxfuse.git  osxfuse
 4763  ./build.sh
 4764  cd SchindlerMSK/development/Pythonh
 4765  mkdir watchdir
 4766  fswatch
 4767  cat watch
 4768  vim pywatch_test.py
 4769  python pywatch_test.py
 4770  cat watch.log
 4771  fswatch -o ./watchdir  | xargs -n1 
 4772  vim watchdir
 4773  vim change.sh
 4774  bash change.sh
 4775  man xargs
 4776  brew install fswatch
 4777  which sshfs
 4778  sshfs sch_centos_hiro:/home/centos/ /Users/212339410/Mount/remote_centos_hiro
 4779  pkgutil
 4780  sudo rm /usr/local/bin/sshfs
 4781  sudo rm /usr/local/share/man/man1/sshfs.1
 4782  sudo pkgutil --forget com.github.osxfuse.pkg.SSHFS
 4783  locate uninstall_osxfuse
 4784  cd osxfuse
 4785  ./build.sh -t distribution
 4786  ls /tmp/osxfuse/distribution
 4787  ./build.sh -v 5 -t distribution
 4788  sudo brew install autoconf automake libtool gettext
 4789  brew install autoconf automake libtool gettext
 4790  locate libosxfuse
 4791  locate sshfs
 4792  locate fuse
 4793  brew uninstall osxfuse
 4794  brew uninstall csk/osxfuse
 4795  brew -l
 4796  brew -h
 4797  brew list
 4798  rm /usr/local/var/homebrew/locks/osxfuse--3.5.8.dmg.incomplete.lock
 4799  rm -rf /usr/local/var/homebrew/locks/osxfuse--3.5.8.dmg.incomplete.lock
 4800  l /usr/local/var/homebrew/locks/osxfuse--3.5.8.dmg.incomplete.lock
 4801  l /usr/local/var/homebrew/locks/osxfuse
 4802  l /usr/local/var/homebrew/locks/
 4803  brew 
 4804  brew help
 4805  brew list osx
 4806  brew list Cask
 4807  brew list Cask.
 4808  brew list Cask/
 4809  brew cask search osxfuse
 4810  ssfs --version
 4811  sshfs --version
 4812  brew install cask/osxfuse
 4813  brew install cask osxfuse
 4814  brew search osxfuse
 4815  brew install caskroom/cask/osxfuse
 4816  brew install caskroom/cask/osxfuse -v
 4817  rm -rf /Library/Filesystems/osxfuse.fs/*
 4818  locate osxfuse
 4819  rm -rf /private/var/db/receipts/com.github.osxfuse.pkg*
 4820  ls /usr/local/Homebrew/Library/Homebrew/*osxfuse*
 4821  locate osxfuse | grep  /usr/local/Homebrew/Library/Homebrew/*osxfuse*
 4822  locate osxfuse | grep  /usr/local/Homebrew/Library/Homebrew/ | rm 
 4823  locate osxfuse | grep  /usr/local/Homebrew/Library/Homebrew/ | xargs -0 rm 
 4824  locate osxfuse | grep  /usr/local/Homebrew/Library/Homebrew/
 4825  locate osxfuse | grep  -l /usr/local/Homebrew/Library/Homebrew/* 
 4826  locate osxfuse | grep  --null /usr/local/Homebrew/Library/Homebrew/* 
 4827  locate osxfuse | grep   /usr/local/Homebrew/Library/Homebrew/ 
 4828  locate osxfuse | grep   /usr/local/Homebrew/Library/Homebrew/  | xargs -0 rm
 4829  brew install osxfuse
 4830  brew tap caskroom/cask
 4831  brew cask uninstall osxfuse --force --debug
 4832  brew cask install osxfuse -v 
 4833  cat /var/log/install.log
 4834  tail -n 1000 brew cask uninstall osxfuse --force --debug
 4835  tail -n 1000 /var/log/install.log
 4836  sshfs
 4837  sshfs sch_centos_hiro:/home/centos/ /Users/212339410/Mount/remote_centos_hiro -v
 4838  sshfs sch_centos_hiro:/home/centos/ /Users/212339410/Mount/remote_centos_hiro 
 4839  sshfs sch_centos_hiro:/home/centos/ /Users/212339410/Mount/remote_centos_hiro volname=remote_centos_hiro
 4840  s3fs
 4841  s3fs ge-schindler-ds ~/Mount/s3-ge-schindler-ds
 4842  s3fs ge-schindler-ds:FromPredixBasic/ ~/Mount/s3-ge-schindler-ds
 4843  s3fs ge-schindler-ds:/FromPredixBasic/ ~/Mount/s3-ge-schindler-ds
 4844  chmod 600 .passwd-s3fs
 4845  s3fs ge-schindler-ds:/FromPredixBasic/ /Users/212339410/Mount/s3-ge-schindler-ds -o passwd_file=.psswd-s3fs
 4846  which s3fs
 4847  s3fs ge-schindler-ds:/FromPredixBasic/ /Users/212339410/Mount/s3-ge-schindler-ds
 4848  fuser 8889/tcp -v
 4849  fuser 8889/tcp 
 4850  fuser 8889
 4851  fuser --all
 4852  fuser --a
 4853  fuser -a
 4854  man fuser
 4855  brew install golang
 4856  -e
   61  python qutil.py --input='/Users/212339410/Box Sync/Analytics/Data/BSF/S1.csv' cumsum
   62  head /Users/212339410/Box Sync/Analytics/Data/BSF/S1.csv
   63  head /Users/212339410/BoxÂ¥ Sync/Analytics/Data/BSF/S1.csv
   64  head()
   65  head -n 1 ../../Data/BSF/S1.csv
   66  python qutil.py --input='/Users/212339410/Box Sync/Analytics/Data/BSF/S1.csv' cumsum Tc_in
   67  python qutil.py --input='/Users/212339410/Box Sync/Analytics/Data/BSF/S1.csv' cumsum Tc_in --output 'output1012'
   68  less output1012
   69  cd Python/PB
   70  export http_proxy=https://proxy-src.research.ge.com:8080
   71  export https_proxy=$http_proxy
   72  less ~/.zshrc.old1012
   73  cp ~/.zshrc ~/.zshrcJP
   74  cp ~/.zshrc.old1012 ~/.zshrc
   75  rm ~/.zcompdump
   76  exec $SHELL -l
   77  less ~/.zshrcJP
   78  GEproxy
   79  cd PB
   80  pip install docopt
   81  python testxy.py
   82  python testxy.py -h
   83  python testxy.py -v
   84  python testxy.py test_code
   85  python testxy.py test_code x=1 y=3
   86  python testxy.py test_code 1 3
   87  git clone git@github.build.ge.com:IndustrialDataScience/tstk.git tstk.ghpages
   88  cd tstk.ghpages
   89  rm -rf tstk.ghpages
   90  cd TSkit
   91  git checkout --orphan
   92  git checkout --orphan gh-pages
   93  git rm -rf .
   94  git commit -m 'enable pages with gh-pages barnch and .nojekyll'
   95  echo "First commit" > index.html
   96  git commit -m 'test html'
   97  cd tstk.gh-pages/
   98  cd TSkit.io
   99  cd TSkit/
  100  sphinx-apidoc -o . ../tskit
  101  sphinx-build -a . ../../TSkit.io
  102  sphinx-build  . ../../TSkit.io
  103  sphinx-apidoc -o . ../tstk
  104  sphinx-apidoc -o . ../tstk/
  105  sphinx-build . ../../tstk.gh-pages
  106  git ci
  107  git ci -m "Generated gh-pages for `git log master -1 --pretty=short --abbrev-commit`" 
  108  git commit -m "Generated gh-pages for `git log master -1 --pretty=short --abbrev-commit`" 
  109  docker info
  110  docker start 3b93b4414591 
  111  docker start continuumio/anaconda1010
  112  docker run run
  113  docker run -i -t continuumio/anaconda1010:latest
  114  docker-machine ip default
  115  atom-rst-preview
  116  brew install pandoc
  117  apm install atom-rst-preview
  118  sudo docker run -d -p 8787:8787 rocker/rstudio
  119  docker ps -a
  120  docker stop continuumio/anaconda1010:latest
  121  docker stop agitated_lamarr
  122  docker rm <none>
  123  docker rm   6971cd34126d
  124  docker rm 
  125  docker rm --help
  126  docker rmi <none>
  127  docker rmi \<none\>
  128  docker rmi \<none\>/\<none\>
  129  docker rmi 6971cd34126d
  130  docker stop de07f67b0fb1
  131  docker stop 226a017b1c6b
  132  docker rmi 3b93b4414591
  133  docker rm continuumio/anaconda1010
  134  docker rmi -f 3b93b4414591
  135  docker rmi -f 0f2173f8e8d0
  136  docker stop rocker
  137  docker stop rocker/rstudio
  138  docker stop romantic_wing
  139  docker rmi 7e0be98eae06
  140  docker rmi 7e0be98eae06 -f
  141  tree ./*tstk
  142  tree ./tstk*
  143  tree ./tstk* n=2
  144  tree ./tstk* -L 2
  145  tree ./tstk* -L 0
  146  tree ./tstk* -L 1
  147  git checkout git@github.build.ge.com:IndustrialDataScience/tstk.git tstk.gh-pages2
  148  git clone git@github.build.ge.com:IndustrialDataScience/tstk.git tstk.gh-pages2
  149  cd tstk.gh-pages2
  150  cd tstk.gh-pages2/
  151  sphinx-build ./doc ../tstk.gh-pages2
  152  cd ../tstk.gh-pages2
  153  rm -rf tstk.gh-pages2
  154  sphinx-build -h
  155  git symbolic-ref -h
  156  git clean -h
  157  "Generated gh-pages for `git log master -1 --pretty=short \\n--abbrev-commit`" 
  158  echo "Generated gh-pages for `git log master -1 --pretty=short \\n--abbrev-commit`" 
  159  git log gh-pages
  160  ils
  161  export PYTHONPATH=$PYTHONPATH:/Users/212339410/Python/pymodule
  162  export PYTHONPATH=$PYTHONPATH:/Users/212339410
  163  git branch -h
  164  git branch models-pca
  165  git branch checkout models-pca
  166  git branch -D checkout
  167  git checkout models-pca
  168  py3
  169  brew install pkg-config
  170  brew install zeromq
  171  apm install hydrogen
  172  conda2 
  173  pip install h2o
  174  spyder --reset
  175  ipython --pylab
  176  spyder -h
  177  spyder --new
  178  brew test.py
  179  mkdir -p /Users/212339410/.local/lib/python3.5/site-packages
  180  brew info pyqt
  181  brew test pyqt
  182  spyder
  183  spyder 
  184  python -c 'import sip'
  185  python -c 'import PyQt5'
  186  python -c 'import sys;'
  187  . ./activate
  188  unset PYTHONPATH
  189  spyder --new-instance
  190  py35
  191  source activate rot
  192  ls /Users/212339410/Python/py
  193  ls /Users/212339410/Python/pymodule
  194  cd Python/pymodule/tstk
  195  touch pull-protext-test.txt
  196  git commit -m 'push protect test'
  197  cd tstk/tstk
  198  cd models/
  199  python hotellingT2.py
  200  git commit -m 'add working example command in docstring'
  201  git push origin models-pca
  202  python tstk/models/hotellingT2.py
  203  python tstk/tstk/models/hotellingT2.py
  204  rm iris_pcadata.pkl
  205  python -c 'import sys; print (sys.path)'
  206  cat ~/.local/lib/python3.5_back/site-packages/homebrew.pth 
  207  ln -s ~/Python/tstk/tstk ~/Python/pymodule
  208  unset $PYTHONPATH
  209  export PYTHONPATH=
  210  ln -s ~/Python/pymodule /Users/212339410/anaconda2/envs/py35/lib/python3.5/site-packages
  211  touch __init__.py
  212  cd iris/]
  213  cd iris/
  214  vim generte_iris.py
  215  cd '/Users/212339410/anaconda2/envs/py35/lib/python3.5/site-packages',
  216  ls -al tstk
  217  cd Users/212339410/anaconda2/envs/py35/lib/python3.5/site-packages'\nls \nq\n'
  218  cd Users/212339410/anaconda2/envs/py35/lib/python3.5/site-packages
  219  rm pymodule
  220  ls | grep pymodule
  221  export PYTHONPATH=~/Python/pymodule
  222  source ~/.bash_profile
  223  mkdir mymodule
  224  cd mymodule
  225  rm -r mymodule
  226  ls | mymodule
  227  ls | grep mymodule
  228  vim mymodules.pth
  229  import sys
  230  cd tstk/examples
  231  cd '/Users/212339410/anaconda2/envs/py35/lib/python3.5/site-packages'
  232  cat mymodules.pth
  233  conda1
  234  sphinx-apdoc -o ./doc ./tstk
  235  mkdir tstk.gh-pages.private
  236  python -c 'import sys; sys.path'
  237  python -c 'import sys; print sys.path'
  238  python -c 'import sys; print(sys.path)'
  239  python -c 'import sys; print(sys.path); import tstk'
  240  ssh -h 3.36.11.67 -u ctuser -p
  241  ssh 3.36.11.67 -u ctuser -p
  242  ssh 3.36.11.67 -l ctuser -p
  243  ssh 3.36.11.67 -l ctuser
  244  mysql 3.36.11.67 -u ctuser -p
  245  brew install mysql
  246  mysql -h 3.36.11.67 -u ctuser -p
  247  ssh 3.36.11.67 -l root
  248  vim GeSyslog.py
  249  sphinx-build ./doc ../tstk.gh-pages.private
  250  ls /usr/lib/j*
  251  which java_home
  252  cd /
  253  cd Users/212339410
  254  cd /Applications
  255  ;s
  256  cd /Library/Frameworks
  257  cd /usr/local
  258  mkdir bin
  259  vim .bash_profile
  260  mv /Applications/TOS_BD-macosx-cocoa -t ~/.local/bin/TOS_BD-20160704_1411-V6.2.1/
  261  mv /Applications/TOS_BD-macosx-cocoa ~/.local/bin/TOS_BD-20160704_1411-V6.2.1/
  262  mv /Applications/TOS_BD-macosx-cocoa.app ~/.local/bin/TOS_BD-20160704_1411-V6.2.1/
  263  git clone https://github.com/borismus/webvr-boilerplate.git
  264  cd Box\ Sync/Document_D/Development/WebApp/WebVR1
  265  pip install SimpleHTTPServer
  266  cd webvr-boilerplate
  267  python -m SimpleHTTPServer 8000
  268  python -m SimpleHTTPServer 8080
  269  git clone https://github.com/anjneymidha/webVR-Mars.git
  270  cd webVR-Mars
  271  python -m http.server 8080
  272  python -m http.server 8081
  273  python -m http.server 80800
  274  python -m http.server 808
  275  python -m http.server 8083
  276  cd Python/BoschKaggle/
  277  brew install wget
  278  wget http://www.cs.washington.edu/research/xmldatasets/data/SwissProt/SwissProt.xml
  279  wget http://www.cs.washington.edu/research/xmldatasets/data/SwissProt/SwissProt.xml -e use_proxy=yes 
  280  brew install caskroom/cask/brew-cask
  281  brew cask install mactex
  282  brew cask install texmaker
  283  which latex
  284  Ana
  285  cd Scripts
  286  git clone git@github.build.ge.com:212325745/AllisonTransmission.git
  287  cd Allison
  288  cd ServiceManual
  289  head 3245076_TS7149_217-10-19-32-en_US.xml
  290  head -v  3245076_TS7149_217-10-19-32-en_US.xml
  291  less  3245076_TS7149_217-10-19-32-en_US.xml
  292  vim  3245076_TS7149_217-10-19-32-en_US.xml
  293  man od
  294  head 3245076_TS7149_217-10-19-32-en_US.xml | od c
  295  head 3245076_TS7149_217-10-19-32-en_US.xml | less
  296  head 3245076_TS7149_217-10-19-32-en_US.xml | od -c
  297  cp StorageExplorer -t ~/.local/bin/
  298  mv StorageExplorer  ~/.local/bin/
  299  conda env export -f py35export.yml
  300  cat py35export.yml
  301  vim py35export.yml
  302  javac -version
  303  ls /Library/Java/JavaVirtualMachines
  304  man R
  305  sudo ln /Library/Java/JavaVirtualMachines/jdk1.8.0_101.jdk/Contents/Home/jre/lib/server/libjvm.dylib /usr/local/lib
  306  sudo  R CMD javareconf
  307  R CMD 
  308  R CMD javareconf
  309  brew install homebrew/dupes/gdb
  310  brew install gdb
  311  R -d
  312  java
  313  java -version
  314  R -d gdb
  315  java_homr
  316  java_home
  317  JAVA_HOME
  318  which java
  319  /usr/libexec/java_home
  320  cd Box\ Sync/Analytics/Projects/AllisonTransmission/data
  321  java -jar sqljdbc4.jar
  322  ls ~/Downloads/
  323  gzip -d sqljdbc_4.0.2206.100_enu.tar.gz
  324  tar -xf sqljdbc_4.0.2206.100_enu.tar
  325  cd ls
  326  ls -al /usr/local/lib/libjvm.dylib
  327  rm libjvm.dylib
  328  1. sudo ln -s $(/usr/libexec/java_home)/jre/lib/server/libjvm.dylib /usr/local/lib
  329  sudo ln -s $(/usr/libexec/java_home)/jre/lib/server/libjvm.dylib /usr/local/lib
  330  ls -al libjvm.dylib
  331  npminstall -g sql-cli
  332  npm install -g sql-cli
  333  mssql 
  334  cdd Box\ Sync
  335  cd data/Azure/St_Louis_Bus
  336  tree . -l 1
  337  tree . -l 0
  338  tree 
  339  tree . -L 1
  340  tree ./*csv -L 1 
  341  tree *csv -L 1 
  342  tree -L 1 -P *csv
  343  tree -P '*csv' -L 0
  344  tree -P '*csv' -L 1
  345  brew update && brew install unixODBC 
  346  wget 
  347  rm è·åçµæ­´æ¸.docx
  348  rm 11\ éè·_è»¢ç±ã«ã¤ãã¦.doc
  349  wget "http://cran.r-project.org/src/contrib/RODBC_1.3-10.tar.gz" 
  350  R CMD INSTALL RODBC_1.3-14.tar.gz
  351  brew install FreeTDS
  352  pip install pymsql
  353  pip install pymssqlÂ¨
  354  brew unlink freetds; brew install homebrew/versions/freetds091
  355  pip install pymssql
  356  pip install pyodbc
  357  less /usr/local/lib/libtdsodbc
  358  which freetds
  359  which homebrew
  360  ls /Library/Caches/C
  361  ssh alpidcdiha001v.cloud.ge.com -l 'sa'
  362  tsql -H '10.42.128.86' -p 1433 -U 'sa' -P 'datalake123'
  363  cd Box\ Sync/Analytics/Projects
  364  cd AllisonTransmission/data/
  365  cd Azure
  366  cd St_Louis_Bus
  367  tree
  368  tree -P *.csv 
  369  tree -P '*.csv '
  370  tree -P '*.csv'
  371  %doctest_mode
  372  which azure
  373  cd /Users/212339410/.local/bin/StorageExplorer
  374  echo $JAVA_HOME
  375  export PATH=$JAVA_HOME/bin:$PATH
  376  cd ~/Box\ Sync/Analytics/Projects/AllisonTransmission/data/Azure/St_Louis_Bus
  377  tree . -P '*csv'
  378  ls -al /usr/local/lib
  379  ls -al /usr/local/lib | grep libjvm
  380  mssql -s alpidcdiha001v.cloud.ge.com -u 'sa' -p 'datalake123' -d 'ATI_Data_Analysis' -e 
  381  conda create -n opencv numpy scipy scikit-learn matplotlib python=3
  382  source activate opencv
  383  conda install -c https://conda.binstar.org/menpo opencv3
  384  brew install imagemagick
  385  brew install ghostscript
  386  python3 -m pip install Pillow
  387  convert
  388  source .bash_profile
  389  whoami
  390  convert -h
  391  which convert
  392  cd /usr/local/bin/
  393  cd /Users/212339410/Box Sync/Analytics/Data/Hermle/Spindel
  394  cd /Users/212339410/BoxÂ¥ Sync/Analytics/Data/Hermle/Spindel
  395  cd /Users/212339410/Box\ Sync/Analytics/Data/Hermle/Spindel
  396  convert -density 300 Spindel_23005.pdf -quality 100 temp.jpg 
  397  convert -density 300 Spindel_23115.pdf -quality 100 temp2.jpg 
  398  cd Python/OpenCV
  399  git clone https://github.com/pannous/caffe-ocr.git 
  400  cd caffe-ocr
  401  bash train.sh
  402  brew install -vd snappy leveldb gflags glog szip lmdb
  403  brew tap homebrew/science
  404  brew install hdf5 opencv
  405  brew install --build-from-source --with-python -vd protobuf
  406  brew install --build-from-source -vd boost boost-python
  407  /usr/bin/cc --version
  408  caffe
  409  git clone https://github.com/pannous/tensorflow-ocr.git
  410  cd tensorflow-ocr
  411  import tensorflow
  412  source deactuvate 
  413  source activate OpenCV
  414  pip install -i https://pypi.anaconda.org/jjhelmus/simple tensorflow
  415  conda install -c jjhelmus tensorflow=0.10.0rc0
  416  pip3 install --upgrade https://storage.googleapis.com/tensorflow/mac/tensorflow-0.9.0-py3-none-any.whl
  417  pip install pip3
  418  which pip3
  419  python train_ocr_layer.py
  420  cd Box\ Sync/Analytics/Projects/AllisonTransmission/data/Azure/St_Louis_Bus/
  421  mkdir TalendTest
  422  cd TalendTest
  423  vim AddressFile.txt
  424  vim PersonFile.txt
  425  vim JoinedPersonAddressFile.txt
  426  cd Python/tstk/tstk/visualization
  427  vim plot.py
  428  cd ../../..
  429  cd Python/tstk/tstk/visualization/
  430  vim pca_plot.py
  431  ssh alpidcdiha001v.cloud.ge.com 
  432  ssh alpidcdiha001v.cloud.ge.com -l sa
  433  cd CRM_Data
  434  cd CRM_DATA
  435  head filterednew_dtc.rpt
  436  cd Box\ Sync/Analytics/Projects/AllisonTransmission/data/Azure/CRM_Data/CRM_DATA
  437  head filterednew_dtc.rpt -n 269
  438  head
  439  head -n 269 filterednew_dtc.rpt
  440  head -n 400  filterednew_dtc.rpt
  441  vim filterednew_dtc.rpt
  442  vim filteredactivityparty.rpt
  443  brew info mysql
  444  brew services start mysql
  445  ls /usr/local/Cellar/mysql/5.7.16/bin/mysql -u root -p
  446  ls /usr/local/Cellar/mysql/5.7.16/bin/mysql 
  447  which mysql
  448  ls al /usr/local/bin/mysql
  449  ls -al /usr/local/bin/mysql
  450  mysql -v
  451  mysqladmin -u root password 'sql'
  452  mysqladmin -u root -p create mytestdatabase
  453  mysql
  454  mysql -v -uroot -psql
  455  vim filteredactivitypointer_xml.txt
  456  cd lls
  457  mkdir MSSQL
  458  cd MSSQL
  459  vim createViewOpStLouis
  460  vim createViewOpStLouis.sql
  461  cd ../data/Azure
  462  vim test.txt 
  463  sed -i 's/[//g' test.txt
  464  sed -i 's/[/ /g' test.txt
  465  sed -i 's/\[/ /g' test.txt
  466  sed -i '.bak' 's/\[/ /g' test.txt
  467  sed -i '.bak' 's/\]//g' test.txt
  468  cat test.txt
  469  vim test.txt
  470  sed -i '.bak' 's/\[//g' BOM_SerialHeaders.csv
  471  sed -i '.bak' 's/\]//g' BOM_SerialHeaders.csv
  472  vim BOM_SerialHeaders.csv
  473  cd Box\ Sync/Analytics/Projects/AllisonTransmission/data/Azure/
  474  less BOM_SerialHeaders.csv.bak
  475  sed -i '.bak' 's/\//g' BOM_SerialHeaders.csv
  476  sed -i '.bak' 's/\"//g' BOM_SerialHeaders.csv
  477  unzip Updated_Claims_and_Service_Data.zip
  478  jar xvf Updated_Claims_and_Service_Data.zip
  479  man jar 
  480  docker pull microsoft/mssql-server-2014-express-windows
  481  cd Analytics/Projects/AllisonTransmission/data/Azure
  482  zip -FF Updated_Claims_and_Service_Data.zip --out fixedUpdate_Claims_Service.zip
  483  man zip 
  484  zip -fz Updated_Claims_and_Service_Data.zip --out fixedUpdate_Claims_Service.zip
  485  unzip fixedUpdate_Claims_Service.zip
  486  zip -FF fixedUpdate_Claims_Service.zip --out fixed_v2Update_Claims_Service.zip
  487  docker run -it -p 1433:1433 sqlexpress "powershell ./start"
  488  docker run -p 1433:1433 --env sa_password=datalake123 microsoft/mssql-server-2014-express-windows
  489  docker run -d -p 1433:1433 --env sa_password=datalake123 microsoft/mssql-server-2016-express-windows
  490  docker run -d -p 1433:1433 --env sa_password=Data_Lake_123 microsoft/mssql-server-2016-express-windows
  491  zip -fz Updated_Claims_and_Service_Data.zip --out fixedUpdate_Claims_Service.zipls
  492  cd ~/Box\ Sync/Analytics/Python
  493  cd pymodule
  494  pip install 2to2
  495  pip install 2to3
  496  python -c import 2to3
  497  python -c 'import 2to3'
  498  futurize 
  499  futurize --stage1 -w qutil.p
  500  man futurize
  501  futurize -h
  502  futurize --stage1 -w -v qutil.py
  503  scp
  504  scp joposch@sjc1ddpl07.crd.ge.com:/data0/datascience/ServiceDataFault.zip /Users/212339410/Box Sync/Analytics/Projects/AllisonTransmission/data/Azure/Updated_Claims_and_Service_Data
  505  scp joposch@sjc1ddpl07.crd.ge.com:/data0/datascience/ServiceDataFault.zip /Users/212339410/Box Sync/Analytics/Projects/AllisonTransmission/data/Azure/Updated_Claims_and_Service_Data/
  506  scp joposch@sjc1ddpl07.crd.ge.com:/data0/datascience/ServiceDataFault.zip /Users/212339410/Box\ Sync/Analytics/Projects/AllisonTransmission/data/Azure/Updated_Claims_and_Service_Data/
  507  scp joposch@sjc1ddpl07.crd.ge.com:/data0/datascience/ServiceData.zip /Users/212339410/Box\ Sync/Analytics/Projects/AllisonTransmission/data/Azure/Updated_Claims_and_Service_Data/
  508  scp joposch@sjc1ddpl07.crd.ge.com:/data0/datascience/ServiceDataStatic.zip /Users/212339410/Box\ Sync/Analytics/Projects/AllisonTransmission/data/Azure/Updated_Claims_and_Service_Data/
  509  scp joposch@sjc1ddpl07.crd.ge.com:/data0/datascience/ServiceDataHealthClutch.zip /Users/212339410/Box\ Sync/Analytics/Projects/AllisonTransmission/data/Azure/Updated_Claims_and_Service_Data/
  510  cd Box\ Sync/Analytics/ls
  511  cd Box\ Sync/Analytics/
  512  cd Projects/AllisonTransmission/data/Azure/Updated_Claims_and_Service_Data
  513  less CLAIMS_HEADER.csv
  514  vim CLAIMS_HEADER.csv
  515  less ServiceDataHealthClutch.csv
  516  less Claims_DTC.csv
  517  less ServiceDataStatic.csv
  518  ssh  joposch@sjc1ddpl07.crd.ge.com:/data0/datascience
  519  nslookup sjc1ddpl07.crd.ge.com
  520  ssh  joposch@sjc1ddpl07.crd.ge.com
  521  scp joposch@sjc1ddpl07.crd.ge.com:/data0/datascience/ServiceDataStaticDataDetail.zip /Users/212339410/Box\ Sync/Analytics/Projects/AllisonTransmission/data/Azure/Updated_Claims_and_Service_Data/
  522  scp joposch@sjc1ddpl07.crd.ge.com:/data0/datascience/ServiceDataHealthParameter.zip /Users/212339410/Box\ Sync/Analytics/Projects/AllisonTransmission/data/Azure/Updated_Claims_and_Service_Data/
  523  scp joposch@sjc1ddpl07.crd.ge.com:/data0/datascience/CLAIMS_DETAIL.zip /Users/212339410/Box\ Sync/Analytics/Projects/AllisonTransmission/data/Azure/Updated_Claims_and_Service_Data/
  524  ls Box\ Sync/Analytics/Projects/AllisonTransmission/data/Azure/Updated_Claims_and_Service_Data
  525  man unzip 
  526  unzip ServiceDataStaticDataDetail.zip -v 
  527  unzip ServiceDataStaticDataDetail.zip
  528  unzip CLAIMS_DETAIL.zip
  529  brew install p7zip
  530  7za x CLAIMS_DETAIL.zip
  531  7za x ServiceDataStaticDataDetail.zip
  532  7za x ServiceDataFaultDetail.zip
  533  head -n 1 ServiceDataStatic.csv
  534  head -n 2 ServiceDataStatic.csv
  535  head -n 3 ServiceDataStatic.csv
  536  vim ServiceDataHealthClutch.csv
  537  vim ServiceDataStatic.csv
  538  nslookup 10.42.128.86
  539  ping alpidcdiha001v.cloud.ge.com
  540  ping 10.42.128.86
  541  jupyter -v
  542  jupyter notebook -v
  543  jupyter notebook -version
  544  jupyter notebook -hjupyter-notebook cmd -h
  545  jupyter notebook -hjupyter-notebook cmd -h-help-all
  546  jupyter notebook -hjupyter-notebook cmd -help-all
  547  jupyter notebook -h
  548  jupyter-notebook -v
  549  jupyter-notebook -h
  550  jupyter-notebook --help-all
  551  jupyter-notebook 
  552  ; ~/.ssh
  553  ssh nxi_edge
  554  ssh nxi_jumpbox
  555  ssh -i ~/.ssh/id_rsa 212339410@52.35.107.124
  556  ssh -W
  557  ssh ssh -h
  558  man ssh-config
  559  ssh-config
  560  ssh  212339410@52.35.107.124
  561  vim ssh_config
  562  ssh_config
  563  man ssh_config
  564  ssh bastion
  565  ssh nxi_bastion1
  566  ssh -i ~/.ssh/github.build.ge.com_rsa 212339410@52.35.107.124
  567  cd Python/tstk/examples
  568  nxi_bastion2
  569  cat ~/.ssh/github.build.ge.com_rsa
  570  ssh nxi_bation
  571  ssh nxi_a
  572  nslookup 52.35.107.124
  573  cat .ssh/id_rsa
  574  ssh 52.35.107.124
  575  ssh 52.35.107.124 -i .ssh/id_rsa.pub
  576  ssh 52.35.107.124 
  577  chmod 600 ~/.ssh/id_rsa
  578  cat .ssh/id_rsa.pub
  579  man usermod
  580  ssh  -i .ssh/id_rsa 52.35.107.124
  581  ssh 52.35.107.124 -i .ssh/id_rsa
  582  ssh 52.35.107.124 -i .ssh/github.build.ge.com_rsa
  583  git checkout -b ysong/plotly origin/ysong/plotly
  584  vim doc/tstk.visualization.rst
  585  git add doc/tstk.visualization.rst
  586  git commit -m "resolve conflict"
  587  vim tstk/visualization/plot.py
  588  git add tstk/visualization/plot.py
  589  git add doc/tstk.visualization.rst 
  590  git commit -m "resolved conflicts"
  591  git merge --no-ff ysong/plotly
  592  cd Python/tstk.gh-pages
  593  spinx-apidoc
  594  commit -m 'update API documents based on realese'
  595  git commit -m 'update API documents based on realese'
  596  sphinx-apidoc -of ./doc ./tstk
  597  man sphinx-apidoc
  598  sphinx-apidoc
  599  sphinx-apidoc -f -E  -o ./doc ./tstk
  600  npm 
  601  npm help npm
  602  npm outdated 
  603  npm update oniguruma
  604  node-gyp rebuild
  605  apm uninstall markdown-preview
  606  ssh nxi_bastiion
  607  chown
  608  cd t
  609  rm -rf test 
  610  chown -R 212339410 test 
  611  chown -R 212339410. test 
  612  chown -R 700 test 
  613  branch 
  614  git clone git@github.build.ge.com:212339410/tstk.git
  615  cd ~/Box\ Sync/Analytics/Projects/AllisonTransmission/data
  616  echo /Library/Java/JavaVirtualMachines/jdk1.8.0_101.jdk/Contents/Home/jre/lib/server/libjvm.dylib
  617  l /usr/local/lib
  618  l /usr/local/lib | grep lib
  619  l /usr/local/lib | grep libjvm
  620  lls
  621  cd ModifiedData
  622  head SDFD.G5.csv
  623  ssh 10.42.50.159 -i  ~/.ssh/id_rsa
  624  ssh 10.42.50.159
  625  ssh -i ~/.ssh/github.build.ge.com_rsa.pub 10.42.50.159 
  626  ping 10.42.50.159
  627  ssh -t 52.35.107.124 ssh 10.42.50.159
  628  ssh -t 52.35.107.124 ssh -i ~/.ssh/id_rsa 10.42.50.159
  629  conda 2
  630  rm -rf tstk.gh-pages
  631  git clone git@github.build.ge.com:212339410/tstk.git tstk.gh-pages
  632  git commit -A
  633  git commit -m 'add notebook rst file'
  634  git fetch master
  635  git fetch mastergit push origin documentation
  636  git push documentation master
  637  git merge origin/master
  638  git git add -A
  639  git commit -m "Generated gh-pages for `git log master -1 --pretty=short \\ngit add -A\ngit commit -m "Generated gh-pages for `git log master -1 --pretty=short \\n    --abbrev-commit`" 
  640  git clone git@github.build.ge.com:IndustrialDataScience/tstk.git tstk.gh-pages.master
  641  cd tstk.gh-pages.master
  642  git push orign gh-pages
  643  git commit -m "Generated gh-pages for `git log master -1 --pretty=short \\n    --abbrev-commit`" 
  644  git clone https://github.build.ge.com/IndustrialDataScience/tstk.wiki.git tstk.wiki
  645  cd tstk.wiki
  646  pandoc --from-markdown --to-rst --output=Home.rst Home.md
  647  pandoc --from=markdown --to=rst --output=Home.rst Home.md
  648  pandoc --from=markdown --to=rst --output=API-Documentation.rst API-Documentation.md
  649  pandoc --from=markdown --to=rst --output=API-Development.rst Development.md
  650  rm development.html
  651  pip install tsfresh
  652  R 
  653  cat ~/.ssh/id_rsa
  654  cat ~/.ssh/id_rsa.ls
  655  rm Bepr0fessi0nal
  656  rm Bepr0fessi0nal*
  657  mkdir ~/.ssh/20161121
  658  cd ~/.ssh/20161121
  659  rm 20161121
  660  rm -rf 20161121
  661  ssh-keygen -t rsa -b 4096 
  662  cd ~/Library/Caches
  663  ls /Library/Java/JavaVirtualMachines/jdk1.8.0_101.jdk/Contents/Home/jre/lib/server/libjvm.dylib    
  664  cd Box\ Sync/Analytics/Projects/AllisonTransmission/data/Azure/St_Louis_Bus/DataElementLog
  665  head -n 000100_0
  666  head -n 10 000100_0
  667  kextstat | grep -v com.apple
  668  conda envs
  669  conda create 
  670  conda remove --name snowflakes
  671  conda create --name snowflake python3
  672  conda remove certifi
  673  conda update requests
  674  conda config --set ssl_verify False
  675  conda install certifi
  676  conda create --name snowflakes biopython
  677  conda create -n py3k python=3 anaconda
  678  source py3k 
  679  cd Box\ Sync/Analytics/Projects/AllisonTransmission/data/Azure/Updated_Claims_and_Service_Data
  680  vim ServiceDataHealthParameter.csv
  681  vim CLAIMS_DETAIL.csv
  682  head -n 100  CLAIMS_DETAIL.csv > CLAIMS_DETAIL100row.csv
  683  open CLAIMS_DETAIL100row.csv
  684  rm CLAIMS_DETAIL100row.csv
  685  pip install git+https://github.build.ge.com/IndustrialDataScience/tstk.git
  686  sudo pip install git+https://github.build.ge.com/IndustrialDataScience/tstk.git
  687  sp_msforeachtable 'Exec sp_spaceused [?]'
  688  sphinx-apidoc -f  -o ./doc ./tstkÂ¨diasjfoa
  689  cd Box\ Sync/Analytics/Data
  690  cd BHP\ \(shared\ zip\)
  691  unzip *.zip
  692  unzip \*.zip 
  693  unzip -h
  694  nzip -help
  695  unzip -help
  696  unzip -d
  697  find -name '*.zip'
  698  find ./ -name \*.zip 
  699  7z Live4DUserStory2-Inputs-BucketTraces.zip
  700  7z e  Live4DUserStory2-Inputs-BucketTraces.zip
  701  find 
  702  find -h
  703  find . 
  704  find .  "*.zip"
  705  find "./*.zip"
  706  find "./\*.zip"
  707  unzip "*.zip" -d "./Unzipped"
  708  7za for archive in *.zip; do 7z x -o"`basename \"$archive\" .zip`" "$archive"; done
  709  find . -name "*.7z" -type f| xargs -I {} 7z x {}
  710  find . -name
  711  find . -name "*.zip"
  712  find . -name "*.zip" -type f
  713  find . -name "*.zip" -type f|  xargs -I {} 7z x {}
  714  ls *.zip|awk -F'.zip' '{print "unzip "$0" -d "$1}'|sh
  715  cd Python/pymodule
  716  python -C 'import draft'
  717  python -c 'import draft'
  718  draft
  719  heatmap
  720  draft.py heatmap
  721  python draft.py heatmap 
  722  cd ~/Box\ Sync/Analytics/Data/BHP\ \(shared\)
  723  find . -r -name '*csv'
  724  ls ./*csv
  725  ls -r ./*csv
  726  ls -d ./*csv
  727  ls -d *csv
  728  ls -d '*csv'
  729  find . -name '*.csv' | python draft.py heatmap 
  730  python draft.py heatmap ./Live4DUserStory2-Inputs-BucketTraces/dippersERAug.csv
  731  cd Box\ Sync/Analytics/Data/BHP\ \(shared\)
  732  head -n 10000 ./Live4DUserStory2-Inputs-BucketTraces/dippersERAug.csv | ./Live4DUserStory2-Inputs-BucketTraces/dippersERAug10000row.csv
  733  head -n 10000 ./Live4DUserStory2-Inputs-BucketTraces/dippersERAug.csv > ./Live4DUserStory2-Inputs-BucketTraces/dippersERAug10000row.csv
  734  python draft.py heatmap ./Live4DUserStory2-Inputs-BucketTraces/dippersERAug10000row.csv
  735  head -n 10000 ./Live4DUserStory2-Inputs-BucketTraces/dippersERAug.csv > ./Live4DUserStory2-Inputs-BucketTraces/dippersERAug100000row.csv
  736  head -n 100000 ./Live4DUserStory2-Inputs-BucketTraces/dippersERAug.csv > ./Live4DUserStory2-Inputs-BucketTraces/dippersERAug100000row.csv
  737  python draft.py heatmap ./Live4DUserStory2-Inputs-BucketTraces/dippersERAug100000row.csv
  738  head -n 50000 ./Live4DUserStory2-Inputs-BucketTraces/dippersERAug.csv > ./Live4DUserStory2-Inputs-BucketTraces/dippersERAug50000row.csv
  739  head -n 50000 ./Live4DUserStory2-Inputs-BucketTraces/dippersERJun.csv > ./Live4DUserStory2-Inputs-BucketTraces/dippersERJun50000row.csv
  740  python draft.py heatmap ./Live4DUserStory2-Inputs-BucketTraces/dippersERJun50000row.csv
  741  python draft.py heatmap ./Live4DUserStory2-Inputs-BucketTraces/dippersERJun50000row.csv output='heatmap_dippersERJun50000'
  742  head -n 50000 ./Live4DUserStory2-Inputs-BucketTraces/dippersERJul.csv > ./Live4DUserStory2-Inputs-BucketTraces/dippersERJul50000row.csv
  743  python draft.py heatmap ./Live4DUserStory2-Inputs-BucketTraces/dippersERJul50000row.csv 'heatmap_dippersERJul50000'
  744  cd ~/Box\ Sync/Analytics/Python/tstk
  745  cat TSTK.egg-info
  746  cat TSTK.egg-info/PKG-INFO
  747  cat TSTK.egg-info/SOURCES.txt
  748  cat TSTK.egg-info/dependency_links.txt
  749  pip install git@github.build.ge.com:212339410/tstk.git
  750  source activate py3k
  751  pip install git+ssh://git@github.build.ge.com:212339410/tstk.git
  752  git config --global http.proxy https://proxy-src.research.ge.com:8080
  753  git config --global https.proxy https://proxy-src.research.ge.com:8080
  754  git config --get-all
  755  git config --get-all: *proxy*
  756  git config --get-all: "*proxy*"
  757  git config --get http.proxy
  758  git config --get https.proxy
  759  git clone -q
  760  git clone https://github.build.ge.com/212339410/tstk.git tstk.temp
  761  cat $HOME/.gitconfig
  762  git config --system --unset https.proxy
  763  git config --system --get https.proxy
  764  git config --system http.proxy https://proxy-src.research.ge.com:8080
  765  sudo git config --system http.proxy https://proxy-src.research.ge.com:8080
  766  sudo git config --system https.proxy https://proxy-src.research.ge.com:8080
  767  git config --system --get http.proxy
  768  sudo pip install git+https://github.build.ge.com/212339410/tstk.git
  769  git clone -q https://github.build.ge.com/212339410/tstk.git /private/tmp/pip-gh70niso-build
  770  sudo  git config --system --unset https.proxy
  771  sudo  git config --system --unset http.proxy
  772  sudo  git config --global  --unset http.proxy
  773  sudo  git config --global  --unset https.proxy
  774  git clone https://github.build.ge.com/IndustrialDataScience/tstk.git tmptstk
  775  pip freeze | tstk
  776  pip freeze | grep *tstk
  777  cd ~/.Trash/Â¥
  778  cd ~/.Trash/
  779  l |grep PCS
  780  unzip 
  781  unzip  -l PCS-DEMO.zip
  782  7z  i PCS-DEMO.zip
  783  7z 
  784  less /usr/local/etc/npmrc
  785  less /Users/212339410/.npm-init.js
  786  less /Users/212339410/.npm/http-proxy/1.15.1/package.tgz
  787  less /Users/212339410/.npm/http-proxy/1.15.1/package/package.json
  788  gyp
  789  brew install gyp
  790  npm install -g node-gyp
  791  npm config delete proxy
  792  atom-html-preview
  793  npm config --global edit 
  794  apm install atom-html-preview
  795  node-gyp
  796  node-gyp list
  797  apm install atom-html-preview --verbose
  798  chmod 400 ~/.ssh/20161121.pub
  799  chmod 400 ~/.ssh/20161121
  800  ls ~/.ssh/config
  801  BHP\ \(shared\)
  802  cd Live4DUserStory2-Inputs-BucketTraces
  803  cp draft.py -t ~/Box\ Sync/Analytics/Projects/AllisonTransmission/Python
  804  cp draft.py ~/Box\ Sync/Analytics/Projects/AllisonTransmission/Python
  805  cd Projects/AllisonTransmission/Python
  806  vim draft.py
  807  python draft.py heatmap ../cache/StLouisDELRRoverlap.spread.csv
  808  vim qutil.py
  809  cal 2016
  810  pip install tstk
  811  pip install git+https://github.build.ge.com/212339410/tstk.git
  812  ls ~/anaconda/envs/py3k/lib/python3.5/site-packages/tstk
  813  less  ~/anaconda/envs/py3k/lib/python3.5/site-packages/tstk/__init__.py
  814  deactivate py3k
  815  sphinxcontrib
  816  pip install sphinxcontriib-httpdomain2
  817  sphinx-build
  818  pip install sphinxcontrib
  819  cd Box\ Sync/Analytics/Python/tstk.gh-pages.master
  820  git push origin gh-pages'
  821  cd ../../../Python/tstk/examples
  822  git clone https://github.build.ge.com/212339410/tstk.git tstk.feature_importance
  823  git branch models/feature_importance
  824  git add tstk/models/feature_importance.py
  825  git commit -m 'add feature_imporance'
  826  git origin models/feature_importance
  827  cd ../Box\ Sync
  828  cd Analytics/Python
  829  pip install rpy2
  830  ls /Users/212339410/anaconda2/envs/py35/lib/python3.5/site-packages/tstk-0.1-py3.5.egg/tstk/cd ..
  831  git commit -m 'fix the installation bug'
  832  git commit -m 'update gitignore'
  833  pip install pip install git+https://github.build.ge.com/212339410/tstk.git
  834  git config edit
  835  git clone -q https://github.build.ge.com/212339410/tstk.git tstk.setup
  836  rm -rf tstk.setup
  837  git commit -m 'update fork'
  838  git remove
  839  git rm  examples/.ipynb_checkpoints/
  840  git rm  -r examples/.ipynb_checkpoints/
  841  git commit -m 'remove ipython checkpoints'
  842  checkout pip/setup.py
  843  git push origin pip/setup.py
  844  git checkout remotes/IndustrialDataScience/revert-24-revert-22-pip/setup.py
  845  git -A 
  846  git commit -m 'fix bug for pip install'
  847  git push origin remotes/IndustrialDataScience/revert-24-revert-22-pip/setup.py
  848  cat .git/HEAD
  849  git branch -d HEAD detached from IndustrialDataScience/revert-24-revert-22-pip/setup.py
  850  git branch -d IndustrialDataScience/revert-24-revert-22-pip/setup.py
  851  git checkout pip/setup.py
  852  git checkout -b 212339410-pip/setup.py master
  853  git pull https://github.build.ge.com/212339410/tstk.git pip/setup.py
  854  git merge --no-ff 212339410-pip/setup.py
  855  git merge
  856  git push -u origin 212339410-pip/setup.py
  857  cd tstk.feature_importance
  858  git pull master origin
  859  git commit -m 'update'
  860  git add remote upstream https://github.build.ge.com/212339410/tstk
  861  git remote add upstream https://github.build.ge.com/212339410/tstk
  862  less setup.py
  863  git pull upstream origin
  864  git rebase origin/master
  865  git push origin models/feature_importance
  866  cd ../tstk.feature_importance
  867  git checkout models/feature_importance
  868  cd ../tstk/models
  869  git clone https://github.build.ge.com/212339410/tstk.git
  870  git clone https://github.build.ge.com/212339410/tstk.git tstk.documentation
  871  git commit -m 'modify index and remove sub/modules titles'
  872  ls ./Box\ Sync/Analytics/Projects/AllisonTransmission/data
  873  cd  ./Box\ Sync/Analytics/Projects/AllisonTransmission/data
  874  rm test.py
  875  python show_summary.py 
  876  which tstk
  877  git clone https://github.build.ge.com/IndustrialDataScience/tstk.git tstk.ids.master
  878  plot_heatmap
  879  echo $path
  880  plot_heatmap.py
  881  sudo plot_heatmap.py
  882  python plot_heatmap.py
  883  plot_overlap.py 
  884  which plot_overlap.py 
  885  sudo  plot_overlap.py 
  886  cd ~/Box\ Sync/Analytics/Projects/AllisonTransmission/data/ModifiedData
  887  python plot_heatmap.py StLouisDELRRoverlap.spread_15GGB2711C1179661.csv StLouisDELRRoverlap.spread_15GGB2711C1179661_heatmap.html
  888  python "heatmap_plot.py"
  889  python /Users/212339410/Box Sync/Analytics/Python/tstk.ids.master/utils/plot_heatmap.py
  890  python '/Users/212339410/Box Sync/Analytics/Python/tstk.ids.master/utils/plot_heatmap.py'
  891  for x in 1, 2, 3; do echo x;\ndone
  892  for x in 1, 2, 3; do echo $x;\ndone
  893  for x in 1 2 3; do echo $x;\ndone
  894  for (i = 0, i <42; i++); do echo $i; done
  895  for (i = 0; i <42; i++); do echo $i; done
  896  for ((i = 0; i <42; i++)); do echo $i; done
  897  python '/Users/212339410/Box Sync/Analytics/Python/tstk.ids.master/utils/plot_heatmap.py' StLouisDELRRoverlap.spread_15GGB2711C1179661.csv StLouisDELRRoverlap.spread_15GGB2711C1179661_heatmap.html
  898  python '/Users/212339410/Box Sync/Analytics/Python/tstk.ids.master/utils/plot_stacked.py' StLouisDELRRoverlap.spread_15GGB2711C1179661.csv StLouisDELRRoverlap.spread_15GGB2711C1179661_heatmap.html
  899  python '/Users/212339410/Box Sync/Analytics/Python/tstk.ids.master/utils/plot_overlap.py' StLouisDELRRoverlap.spread_15GGB2711C1179661.csv StLouisDELRRoverlap.spread_15GGB2711C1179661_overlap.html
  900  python '/Users/212339410/Box Sync/Analytics/Python/tstk.ids.master/utils/show_summary.py' StLouisDELRRoverlap.spread_15GGB2711C1179661.csv StLouisDELRRoverlap.spread_15GGB2711C1179661_summary.html
  901  head -n 10 StLouisDELRRoverlap_15GGB2711C1179661.csv
  902  git fetch ipstream
  903  git merge 
  904  git pull master documentation
  905  git pull ls documentation
  906  git rebase master 
  907  git add doc/modules.rst
  908  vim doc/in
  909  git commit -m 'resolved conflict between master and documentation'
  910  git origin documentation
  911  git add
  912  git commit -m 'resolved conflict doc/moduels.rst'
  913  rm -rf examples/.ipynb_checkpoints
  914  git commit -m 'delete checkpoints for ipython'
  915  vim doc/modules.rst
  916  vim doc/example.rst
  917  vim doc/glossary.rst
  918  vim  doc/index.rst
  919  ls  examples/
  920  vim doc/AnomalyDetectionHotellingsT2.rst
  921  vim doc/PrincipalComponentAnalysis.rst
  922  vim doc/intro.rst
  923  vim examples/PrincipalComponentAnalysis.ipynb
  924  cd tstk.ids.master
  925  sphinx-build ./doc ../tstk.gh-pages.master
  926  cd ../tstk.gh-pages.master
  927  cd ../tstk.ids.master
  928  ln tstk.ids.master ./pymodule/tstk
  929  ln tstk.ids.master/ ./pymodule/tstk
  930  ln -s tstk.ids.master pymodule/tstk
  931  l pymodule/
  932  ln -s tstk.ids.master/tstk pymodule/tstk
  933  ln -s tstk.ids.master/tstk ./pymodule/
  934  ln -sf tstk.ids.master/tstk pymodule/tstk
  935  ln -sf "tstk.ids.master/tstk" pymodule/tstk
  936  ln -s ./tstk.ids.master/tstk ./pymodule
  937  sudo ln -s ./tstk.ids.master/tstk ./pymodule 
  938  sudo ln -s Python/tstk.ids.master/tstk Python/pymodule 
  939  cp Python/tstk.ids.master/tstk\ alias -t Python/pymodule
  940  cp Python/tstk.ids.master/tstk\ alias Python/pymodule
  941  pip install tstk 
  942  cd /Users/212339410/Box Sync/Analytics/Projects/AllisonTransmission/data/ModifiedData
  943  cd '/Users/212339410/Box Sync/Analytics/Projects/AllisonTransmission/data/ModifiedData'/Users/212339410/Box Sync/Analytics/Projects/AllisonTransmission/data/ModifiedDatauu
  944  cd '~/Box Sync/Analytics/Projects/AllisonTransmission/data/ModifiedData'
  945  cd '/Users/212339410/Box Sync/Analytics/Projects/AllisonTransmission/data/ModifiedData'
  946  git clone https://github.build.ge.com/IndustrialDataScience/tstk.git tstkIdsMaster
  947  python plot_heatmap.py aggregate_15GGB2711C1179661.csv aggregate_15GGB2711C1179661_heatmap.html
  948  python plot_stacked.py aggregate_15GGB2711C1179661.csv aggregate_15GGB2711C1179661_stacked.html
  949  pip uninstall tstk 
  950  cd AllisonTables
  951  head ServiceDataFault2016.csv
  952  head -n 2 ServiceData.csv
  953  head -n 2 ServiceDataFault.csv
  954  head -n 2 ServiceDataFaultDetail.csv
  955  head -n 2 ServiceDataHealthClutch.csv
  956  head -n 2 ServiceDataHealthParameter.csv
  957  cd Box\ Sync/Analytics/Projects/AllisonTransmission/cache
  958  scp 
  959  scp ~/Box\ Sync/Analytics/Projects/AllisonTransmission/cache/Allison_select\ \*\ from\ StLouisDELRRoverlap\ where\ vin\ =\ \'15GG*
  960  ls ~/Box\ Sync/Analytics/Projects/AllisonTransmission/cache/Allison_select\ \*\ from\ StLouisDELRRoverlap\ where\ vin\ =\ \'15GG*
  961  scp ~/Box\ Sync/Analytics/Projects/AllisonTransmission/cache/Allison_select\ \*\ from\ StLouisDELRRoverlap\ where\ vin\ =\ \'15GG* joposch@sjc1ddpl07.crd.ge.com:/data0/datascience/StLouisOperation
  962  less ~/.bashrc
  963  ln -s /Users/212339410/BoxÂ¥ Sync/Analytics/Python/tstk.ids.master/tstk tstk
  964  git clone https://github.build.ge.com/IndustrialDataScience/tstk.git
  965  export PYTHONPATH=~/test/tstk
  966  mv test testÂ¥ me
  967  mv test test\ me
  968  export PYTHONPATH="~/test me/test"
  969  echo $PYTONPATH
  970  export PYTHONPATH=~/test\ me/test
  971  export PYTHONPATH=~/test\ me/tstk
  972  ssh alpidcdiha001v.cloud.ge.com
  973  ssh alpidcdiha001v.cloud.ge.com -l sapm2
  974  nslookup alpidcdiha001v.cloud.ge.com
  975  mssql -s alpidcdiha001v.cloud.ge.com -u 'geds' -p 'Ang3l123' -d 'ATI_Data_Analysis' -e 
  976  ssh nxi_bastion2
  977  ssh nxi_hdp_edge2
  978  cp ~/Box\ Sync/Analytics/Projects/AllisonTransmission/data/sqljdbc4.jar /Library/Java/Extensions
  979  sudo  cp ~/Box\ Sync/Analytics/Projects/AllisonTransmission/data/sqljdbc4.jar /Library/Java/Extensions
  980  ls /Library/Java/Extensions
  981  cd ~/Box\ Sync/Analytics/R/AngelTrain/cache
  982  plot_heat.py Channel_VehicleID81.csv
  983  echo $TSTK_PATH
  984  vim ~/.zshrc]
  985  export PYTHONPATH=$PYTHONPATH:$TSTK_PATH/tstk
  986  git clone https://github.build.ge.com/IndustrialDataScience/tstk.git tstk.master
  987  sl -s tstk.master ~/test
  988  ln -s tstk.master ~/test
  989  ls -al test\ me/
  990  cd ~/test
  991  cd tstk.
  992  rm tstk.master
  993  ln -s /Users/212339410/Box\ Sync/Analytics/Python/tstk.master ~/test
  994  mkdir mypymodule
  995  ln -s /Users/212339410/Box\ Sync/Analytics/Python/tstk.master ~/mypymodule
  996  cd mypymodule
  997  ln -s ~/mypymodule/tstk.master/utils/plot_* ~/Box\ Sync/Analytics/R/AngelTrain/cache
  998  ls ~/mypymodule
  999  cd tstk.master
 1000  python -c import tstk
 1001  python plot_heatmap.py Channel_VehicleID81.csv Channel_VehicleID81_heatmap.html
 1002  vim plot_heatmap.py
 1003  pipi unintall tstk
 1004  ls /Users/212339410/mypymodule/tstk.master/
 1005  pyhton
 1006  cd ~/anaconda2/envs
 1007  cd bin
 1008  ./python
 1009  cd env
 1010  cd envs
 1011  cd py35
 1012  cd /Users/212339410/mypymodule/tstk.master/tstk
 1013  export PYTHONPATH=/Users/212339410/mypymodule/tstk.master/
 1014  (py35) 212339410@SFO1212339410M:
 1015  cd /usr/lib/python2.7
 1016  which pyhton
 1017  cd pack
 1018  cd python3.5/
 1019  cd site-packages
 1020  cd .local/o
 1021  cd .local
 1022  cd /usr/lib
 1023  cd pyh
 1024  cd python2.
 1025  cd /usr/local/lib
 1026  cd python2.7/site-packages/
 1027  cd ~/.local
 1028  cd site
 1029  cd python3.5
 1030  cd site-packages/
 1031  ls tstk*
 1032  ls -al tstk.egg-link
 1033  cat tstk.egg-link
 1034  cd /Users/212339410/Box Sync/Analytics/Python/tstk
 1035  cd "/Users/212339410/Box Sync/Analytics/Python/tstk"
 1036  mv tstk.egg-link tstk.egg-link.back1
 1037  mv tstk.egg-link.back1 ~
 1038  cd anaconda2
 1039  cd lib
 1040  cd python2.7
 1041  locate -u
 1042  locate -U
 1043  slocate- u
 1044  slocate -u
 1045  slocate
 1046  locate
 1047  locate help
 1048  sudo launchctl load -w /System/Library/LaunchDaemons/com.apple.locate.plist
 1049  cd '~/Box Sync/Analytics/R/AngelTrain/cache'
 1050  cd '/Box Sync/Analytics/R/AngelTrain/cache'
 1051  cd Box\ Sync/Analytics/R/AngelTrain/cache
 1052  python plot_heatmap.py Channel_VehicleID81_2000.csv Channel_VehicleID81_2000_heatmap.csv
 1053  python plot_heatmap.py Channel_VehicleID81_4000.csv Channel_VehicleID81_4000_heatmap.csv
 1054  scp ~/Box\ Sync/Analytics/Projects/AllisonTransmission/cache/Allison_A.X.RData joposch@sjc1ddpl07.crd.ge.com:/data0/datascience/StLouisOperation
 1055  scp ~/Box\ Sync/Analytics/Projects/AllisonTransmission/cache/Allison_A.X.SD.RData joposch@sjc1ddpl07.crd.ge.com:/data0/datascience/StLouisOperation
 1056  scp ~/Box\ Sync/Analytics/Projects/AllisonTransmission/cache/Allison_A.X.SDFM.RData  joposch@sjc1ddpl07.crd.ge.com:/data0/datascience/StLouisOperation
 1057  ssh sjc1ddpl07.crd.ge.com -l joposch
 1058  an locate
 1059  man locate
 1060  rm -rf test\ me
 1061  cd  /Users/212339410/mypymodule/tstk.master
 1062  python plot_heatmap.py Channel_VehicleID81_18col.csv Channel_VehicleID81_18col_heatmap.csv
 1063  python -c 'import tstk'
 1064  less ~/.local/lib/python3.5_back/site-packages/homebrew.pth
 1065  locate .local 
 1066  locate .local | grep py
 1067  locate pth | grep py
 1068  python plot_heatmap.py Channel_VehicleID81_18col.csv Channel_VehicleID81_18col_heatmap.html
 1069  less /Users/212339410/anaconda2/envs/py35/lib/python3.5/site-packages/mymodules.pth
 1070  less /Users/212339410/anaconda2/lib/python2.7/site-packages/Sphinx.pth
 1071  less /usr/local/Cellar/protobuf/3.1.0/libexec/lib/python2.7/site-packages/protobuf-3.1.0-py2.7-nspkg.pth
 1072  less /Users/212339410/anaconda2/envs/py35/lib/python3.5/site-packages/setuptools.pth
 1073  less /Users/212339410/anaconda2/envs/GEDS-conda/lib/python3.5/site-packages/aeosa.pth
 1074  vim /Users/212339410/anaconda2/envs/py35/lib/python3.5/site-packages/mymodules.pth
 1075  echo $PYTHONPATH
 1076  less /Applications/PyCharm CE.app/Contents/helpers/pydev/pydevconsole.py
 1077  less '/Applications/PyCharm CE.app/Contents/helpers/pydev/pydevconsole.py'
 1078  less '/Users/212339410/.local/lib/python3.5_back/site-packages/homebrew.pth'
 1079  vim '/Users/212339410/.local/lib/python3.5_back/site-packages/homebrew.pth;
 1080  vim '/Users/212339410/.local/lib/python3.5_back/site-packages/homebrew.pth'
 1081  locate .pth
 1082  cat ~/.ssh/20161121.pub
 1083  curl https://api.box.com/oauth2/token \\n-d 'grant_type=authorization_code&code={your_code}&client_id=2q495hks9hstlr9g3ewciiksgjcv3mmu\n&client_secret=m8uptGO74BMhgNwlptKXuhkDKzoZE3MW' \\n-X POST
 1084  GET https://account.box.com/api/oauth2/authorize\?response_type\=code\&client_id\= 2q495hks9hstlr9g3ewciiksgjcv3mmu&state=security_token%YXBwbGUzMkoNCg==
 1085  curl https://account.box.com/api/oauth2/authorize\?response_type\=code\&client_id\= 2q495hks9hstlr9g3ewciiksgjcv3mmu&state=security_token%YXBwbGUzMkoNCg==
 1086  cd Data/AngelTrain
 1087  touch sample.txt
 1088  head -n 10 ChannelValue_Vehicle1.csv
 1089  vim ChannelValue_Vehicle1.csv
 1090  python plot_heatmap.py Channel_VehicleID81_18col_4000row.csv Channel_VehicleID81_18col_4000row_heatmap.html
 1091  sl
 1092  ln -s '/Users/212339410/Box Sync/Analytics/Python/tstk.ids.master_modified' '/Users/212339410/anaconda2/envs/py35/lib/python3.5/site-packages/tstk'
 1093  ls -al /Users/212339410/anaconda2/envs/py35/lib/python3.5/site-packages/ | grep tstk
 1094  ln -s '/Users/212339410/Box Sync/Analytics/Python/tstk.ids.master_modified/tstk' '/Users/212339410/anaconda2/envs/py35/lib/python3.5/site-packages/tstk'
 1095  cd /Users/212339410/Box Sync/Analytics/Data/AngelTrain
 1096  cd '/Users/212339410/Box Sync/Analytics/Data/AngelTrain'
 1097  7z
 1098  7z -e ChannelValue_Vehicle*
 1099  7z e ChannelValue_Vehicle*
 1100  7z e *.zip
 1101  for f in *.zip \ndo \n7z e "$f" & \ndone 
 1102  l;s
 1103  git clone https://github.build.ge.com/IndustrialDataScience/tstk.git tstk.documentation
 1104  cd tstk.documentation
 1105  rm AnomalyDetectionHotellingsT2.py
 1106  rm PrincipalComponentAnalysis.py
 1107  rm PCADemo.ipynb
 1108  sphinx-apidoc -f  -o ./doc ./tstk
 1109  cd tstk/visualization
 1110  cd P
 1111  ln -s AngelTrain ../Python/AngelTrain
 1112  ln -s AngelTrain /Users/212339410/Box\ Sync/Analytics/Python/AngelTrain
 1113  ln -s /Users/212339410/Box\ Sync/Analytics/Projects/AngelTrain /Users/212339410/Box\ Sync/Analytics/Python/AngelTrain
 1114  cp AngelTrain/* AngelTrain2/
 1115  ls -al AngelTrain2
 1116  cd AngelTrain/
 1117  cd Box\ Sync/Analytics
 1118  ln -s /Users/212339410/Box\ Sync/Analytics/Projects /Users/212339410/
 1119  locate jabber-config.xml
 1120  locate jabber
 1121  vim /Applications/Cisco Jabber.app/Contents/Resources/jabber-config-defaults.xml
 1122  vim '/Applications/Cisco Jabber.app/Contents/Resources/jabber-config-defaults.xml'
 1123  vim '/Applications/Cisco Jabber.app/Contents/MacOS/jabber-config-defaults.xml'
 1124  locate jabber | xml
 1125  locate jabber | grep xml
 1126  locate myjabber
 1127  vim /Users/212339410/Library/Application\ Support/Cisco/Unified\ Communications/Jabber/CSF/Config/jabberLocalConfig.xml
 1128  vim /Users/212339410/Library/Application\ Support/Cisco/Unified\ Communications/Jabber/CSF/Config/service-location.xml
 1129  git rm cache/*
 1130  git rm result/*
 1131  git commit -m 'remove result and cache'
 1132  git initi
 1133  git commit -m 'modifygitignore'
 1134  git clone https://github.build.ge.com/212339410/AngelTrain.git AngelTrain2
 1135  cp -r AngelTrain/* AngelTrain2/
 1136  rm -rf AngelTrain
 1137  cd AngelTrain
 1138  cd ../Python/tstk.documentation
 1139  git commit -help
 1140  git comomit --short
 1141  git commit --short
 1142  git fetch remote
 1143  git fetch remote upstream
 1144  git branch -help
 1145  git pull -all
 1146  git fetch origin 
 1147  git remote add upstream git@github.build.ge.com:212339410/tstk.git
 1148  git pull updtream master
 1149  git push origin documentation 
 1150  git checkout documentatioin
 1151  git checkout remotes/origin/documentation
 1152  git branch documentation
 1153  git checkout  documentation
 1154  git push origin documentation
 1155  git commit -m 'rm unnecessary files'
 1156  git push -u origin documentation
 1157  git pull upstream master
 1158  cd ../../../Projects/ls
 1159  cd ../../../Projects/
 1160  cd AllisonTransmission/cache
 1161  python plot_heatmap.py Allison_StLouis_Aggregate_5VIN_X.Y.csv Allison_StLouis_Aggregate_5VIN_X.Y_heatmap.html
 1162  python ../Python/plot_stacked.py Allison_StLouis_Aggregate_5VIN_X.Y.csv Allison_StLouis_Aggregate_5VIN_X.Y_stacked.html
 1163  python ../Python/plot_stacked.py ../cache/Allison_StLouis_Aggregate_5VIN_X.Y.csv Allison_StLouis_Aggregate_5VIN_X.Y_stacked.html
 1164  python plot_stacked.py ../data/ModifiedData/Allison_StLouis_Aggregate_5VIN_X.Y.csv Allison_Stlouis_Aggregate_5VIN_X.Y_stacked.html
 1165  grep 'py' $(find tstk)
 1166  find
 1167  man ind
 1168  find Allison
 1169  find RR
 1170  find /User
 1171  find App
 1172  grep 'R' $(find App)
 1173  grep 'R' App/app.R 
 1174  cp plot_* ~/Box\ Sync/Analytics/Data/MBS2016
 1175  cd ~/Box\ Sync/Analytics/Data/MBS2016/
 1176  find *csv
 1177  find ./*csv
 1178  find ./ -name '*.csv'
 1179  find ./  '*.csv'
 1180  find -path ./ 
 1181  find -path ./  *.csv
 1182  find ./ -path '*.csv'
 1183  find ./ -iname "*.csv"
 1184  find . -iname "*.csv"
 1185  find  -iname "*.csv"
 1186  find  -iname pwd "*.csv"
 1187  find  -iname $(pwd) "*.csv"
 1188  find  $(pwd) -iname "*.csv"
 1189  find  '$(pwd)' -iname "*.csv"
 1190  find  \'$(pwd)\' -iname "*.csv"
 1191  $(pwd)
 1192  $('pwd')
 1193  for i_file in $(find *.csv);\ndo \necho i_file\ndone
 1194  for i_file in $(find *.csv);\ndo \necho $i_file\ndone
 1195  for i_file in $(find ./ -path *.csv);\ndo \necho $i_file\ndone
 1196  for i_file in $(find ./ -path '*.csv');\ndo \necho $i_file\ndone
 1197  python plot_heatmap.py ./Data\ form\ 20160721\ to\ 20160803/BSW/BSW_20160721_20160803.csv BSW_20160721_20160803_heatmap.html
 1198  python plot_heatmap.py ./Data\ form\ 20160721\ to\ 20160803/INOS/NSN1.20160721_20160723.csv   NSN1.20160721_20160723_heatmap.html
 1199  cd ../R/MBS2016/Python
 1200  python plot_stacked.py ../Src/dataset_BSW.csv dataset_BSQ_stacked.html
 1201  python plot_heatmap.py ../Src/dataset_BSW.csv dataset_BSQ_stacked.html
 1202  python plot_heatmap.py ../Src/dataset_OSS.csv dataset_OSS_heatmap.html
 1203  python plot_heatmap.py ../Src/dataset_WOT.csv dataset_WOT_heatmap.html
 1204  python plot_heatmap.py ../Src/dataset_TTE.csv dataset_TTE_heatmap.html
 1205  python plot_heatmap.py ../Src/dataset_WOTrow.csv  dataset_WOT_400row_heatmap.html
 1206  python plot_stacked.py ../Src/dataset_WOT_400row.csv   dataset_WOT_400row_stacked.html
 1207  python plot_stacked.py ../Src/dataset_TTE.csv   dataset_TTE_stacked.html
 1208  python plot_stacked.py ../Src/dataset_BSW.csv dataset_BSW_stacked.html
 1209  python plot_stacked.py ../Src/dataset_OSS_400row.csv dataset_OSS_400row_stacked.html
 1210  python plot_heatmap.py ../Src/dataset_OSS_400row.csv dataset_OSS_400row_heatmap.html
 1211  python plot_heatmap.py ../Src/dataset_INOS_400row.csv dataset_INOS_400row_heatmap.html
 1212  cd ~/mypymodule
 1213  rm ./mypymodule
 1214  rm -r ./mypymodule
 1215  cd ~/Box\ Sync/Analytics/Python/tstk.ids.master_modified/
 1216  gir branch
 1217  cd ../R
 1218  git clone https://github.build.ge.com/212325745/AllisonTransmission.git AllisonTransmission1212
 1219  cd AllisonTransmission1212
 1220  git add R/LoadHelper.R
 1221  git commit -m 'resolved the conflict in LoadHelper.R'
 1222  python plot_heatmap.py ../data/ModifiedData/Allison_StLouis_Aggregate_5VIN_X.Y_1212.csv ../result/Allison_StLouis_Aggregate_5VIN_X.Y_1212_heatmap.html
 1223  python plot_heatmap.py ../data/ModifiedData/Allison_StLouis_Aggregate_5VIN_X.Y_1212.csv ../result/Allison_StLouis_Aggregate_5VIN_X.Y_1212_heatmap2.html
 1224  python plot_heatmap.py ../data/ModifiedData/Allison_StLouis_Aggregate_5VIN_X.Y_1212.csv ../result/Allison_StLouis_Aggregate_5VIN_X.Y_1212_heatmap3.html
 1225  python plot_heatmap.py ../data/ModifiedData/Allison_StLouis_Aggregate_5VIN_X.Y_1212.csv ../result/Allison_StLouis_Aggregate_5VIN_X.Y_1212_heatmap4.html
 1226  head -n 2000 ../data/ModifiedData/StLouisDELRRoverlap.spread_15GGB271XC1179674.csv > ../data/ModifiedData/StLouisDELRRoverlap.spread_15GGB271XC1179674_2000row.csv
 1227  python plot_heatmap.py ../data/ModifiedData/StLouisDELRRoverlap.spread_15GGB271XC1179674_2000row.csv ../data/ModifiedData/StLouisDELRRoverlap.spread_15GGB271XC1179674_2000row_heatmap.html
 1228  mssql -s alpidcdiha001v.cloud.ge.com -u 'sa' -p 'datalake123' -d 'ATI_Data_Analysis' -e
 1229  python plot_heatmap.py ../data/ModifiedData/StLouisDELRRoverlap.spread_15GGB271XC1179674_2000row.csv ../data/ModifiedData/StLouisDELRRoverlap.spread_15GGB271XC1179674_2000row_heatmap2.html
 1230  python plot_heatmap.py ../data/ModifiedData/Allison_StLouis_StLouisDELRRoverlap.spread_15GGB2713C1179662_2000.csv  ../data/ModifiedData/StLouisDELRRoverlap.spread_15GGB2713C1179662_2000row_heatmap2.html
 1231  python plot_heatmap.py ../data/ModifiedData/Allison_StLouis_StLouisDELRRoverlap.spread_15GGB2713C1179662_4000.csv  ../data/ModifiedData/StLouisDELRRoverlap.spread_15GGB2713C1179662_4000row_heatmap.html
 1232  python plot_heatmap.py ../data/ModifiedData/Allison_StLouis_StLouisDELRRoverlap.spread_15GGB2713C1179662_40000.csv  ../data/ModifiedData/StLouisDELRRoverlap.spread_15GGB2713C1179662_40000row_heatmap.html
 1233  cd Box\ Sync/Analytics/Projects/AllisonTransmission/data/ModifiedData
 1234  python plot_heatmap.py StLouisDELRRoverlap.spread_15GGB271XC1179674_2000row.csv StLouisDELRRoverlap.spread_15GGB271XC1179674_2000row_heatmap2.html
 1235  vim StLouisDELRRoverlap.spread_15GGB271XC1179674_2000row.csv
 1236  python plot_heatmap.py Allison_StLouis_StLouisDELRRoverlap.spread_15GGB2713C1179662_2000.csv Allison_StLouis_StLouisDELRRoverlap.spread_15GGB2713C1179662_2000_heatmap.html
 1237  python plot_heatmap.py Allison_StLouis_StLouisDELRRoverlap.spread_15GGB2713C1179662_4000.csv Allison_StLouis_StLouisDELRRoverlap.spread_15GGB2713C1179662_4000_heatmap.html
 1238  git clone git@github.build.ge.com:212487744/Deep-Learning-LSTM-Survival.git
 1239  ls -al /Users/212339410/anaconda2/envs/py35/lib/python3.5/site-packages | tstk
 1240  ls -al /Users/212339410/anaconda2/envs/py35/lib/python3.5/site-packages | grep tstk
 1241  git@github.build.ge.com:IndustrialDataScience/tstk.git tstk.ids.master
 1242  git clone git@github.build.ge.com:IndustrialDataScience/tstk.git TSTK
 1243  git clone git@github.build.ge.com:IndustrialDataScience/tstk.git tstk.master
 1244  ln -s '/Users/212339410/Box Sync/Analytics/Python/tstk.master/tstk' '/Users/212339410/anaconda2/envs/py35/lib/python3.5/site-packages/tstk'
 1245  cd ../Projects/AllisonTransmission/
 1246  cd R
 1247  git clone https://github.build.ge.com/212325745/AllisonTransmission.git AllisonTmp
 1248  cd AllisonTmp
 1249  git checkout -b hiroaki/st-louis origin/hiroaki/st-louis
 1250  git merge master
 1251  git commit -m 'resoleved conflict'
 1252  git merge --no-ff hiroaki/st-louis
 1253  open R/LoadHelper2016.R
 1254  atom R/LoadHelper2016.R
 1255  git add R/LoadHelper2016.R
 1256  git commit -m 'remove the conflict part'
 1257  git revert c33634abd634d63107c9cc3aed79fd1d3e0b0ab3
 1258  rm -rf AllisonTmp
 1259  git clone git@github.build.ge.com:212325745/AllisonTransmission.git AllisonTmp
 1260  git revert -h
 1261  git revert c33634a
 1262  cd Projects/AllisonTmp
 1263  cd result
 1264  cd plots
 1265  js
 1266  mssql -s alpidcdiha001v.cloud.ge.com -u 'geds' -p 'Ang3l123' -e 
 1267  git log --oneline
 1268  git checkout hiroaki/st-louis
 1269  uname
 1270  bash -v
 1271  pip install pyxley
 1272  git clone https://github.com/realpython/ultimate-flask-front-end.git
 1273  cd ultimate-flask-front-end
 1274  sh run.sh
 1275  cd Projects/AllisonTransmission
 1276  cd Python/ultimate-flask-front-end
 1277  npm install -g bower
 1278  bower init 
 1279  npm init 
 1280  npm install --save-dev bower
 1281  vim /Library/Preferences/com.box.sync.plist
 1282  sudo vim /Library/Preferences/com.box.sync.plist
 1283  unlink ~/anaconda2/envs/py35/lib/python3.5/site-packages/tstk
 1284  ln -s ~/anaconda2/envs/py35/lib/python3.5/site-packages /Users/212339410/Box/Analytics/Python/tstk/tstk
 1285  ln
 1286  ls -al ~/anaconda2/envs/py35/lib/python3.5/site-packages/ | \ngrep
 1287  ls -al ~/anaconda2/envs/py35/lib/python3.5/
 1288  ln -s ~/anaconda2/envs/py35/lib/python3.5/site-packages/tstk /Users/212339410/Box/Analytics/Python/tstk/tstk
 1289  ls -al ~/anaconda2/envs/py35/lib/python3.5/site-packages | grep tstk
 1290  ls -al ~/anaconda2/envs/py35/lib/python3.5/site-packages | grep /Users
 1291  ls -al ~/anaconda2/envs/py35/lib/python3.5/site-packages 
 1292  ls /Users/212339410/Python
 1293  ls -al /Users/212339410/Box/Analytics/Python/tstk
 1294  ls -al /Users/212339410/anaconda/envs/py3k
 1295  ls -al /Users/212339410/anaconda2/envs/py35/lib/python3.5/site-packages/
 1296  ls -al ~/anaconda2/envs/py35/lib/python3.5/site-packages/tstk
 1297  ls -al ~/anaconda2/envs/py35/lib/python3.5/site-packages/ | grep tstk
 1298  rm Projects
 1299  rm Python
 1300  ln -s ./Box/Analytics/Projects/ Projects
 1301  cd App
 1302  git clone https://github.com/cldougl/plotly-shiny.git
 1303  cd ../../Box\ Sync\ Backup
 1304  cd Analytics/Projects/AllisonTransmission/
 1305  cp .gitignore ~/Box/Analytics/Projects/AllisonTransmission/.gitignore
 1306  cd Box/Analytics/Projects/AllisonTransmission
 1307  cp .git ~/Box/Analytics/Projects/AllisonTransmission
 1308  cp .git ~/Box/Analytics/Projects/AllisonTransmission/.git
 1309  cp -r .git ~/Box/Analytics/Projects/AllisonTransmission/.git
 1310  cd ~/Box/Analytics/Projects/AllisonTransmission
 1311  defaults write com.apple.finder AppleShowAllFiles YES
 1312  mssql -s alpidcdiha001v.cloud.ge.com -u 'dssb' -d 'SB' -e -t 20000
 1313  cd ~/Box\ Sync\ Backup/Analytics/Projects/AllisonTransmission/data/Azure/Updated_Claims_and_Service_Data
 1314  vim ServiceDataStatic_mod.csv
 1315  iconv
 1316  find ./ -type f -exec echo 
 1317  find ./ -type f -exec echo '{}'
 1318  find ./ -type f
 1319  find ./ -type f | grep .csv
 1320  find ./ -regex '.csv'  
 1321  find ./
 1322  find ./ -pattern '.csv'
 1323  find ./ -pattern '*.csv'
 1324  find ./ -pattern ''*.csv''
 1325  find ./ -regex ''*.csv''
 1326  find ./ -r -regex ''*.csv''
 1327  man find 
 1328  find ./  -E ''*.csv''
 1329  find ./ -type f -exec cp -i '{}' ./ ';'
 1330  for files in ./\ndo \necho "$files"\ndone
 1331  for files in ./*.csv\ndo \necho "$files" done\n\n
 1332  ln -s ./ ~/Box/Analytics/R/MBS2016/Src
 1333  ln -s ../ ~/Box/Analytics/R/MBS2016/Src
 1334  ln -s ../ ~/Box/Analytics/R/MBS2016/Src/Original_flat
 1335  ln -s ~/Box/Analytics/Data/MBS2016/Original_flat  ~/Box/Analytics/R/MBS2016/Src/Original_flat
 1336  ln -s ~/Box/Analytics/Data/MBS2016/UTF8_flat  ~/Box/Analytics/R/MBS2016/Src/UTF8_flat
 1337  for files in ./*.csv\ndo\niconv -f SHIFT_JIS -t UTF-8 '$files' '$files' \necho "$files"\ndone
 1338  for files in ./*.csv\ndo\nfile -I "$files" \necho "$files"\ndone
 1339  for files in ./*.csv\ndo\niconv -f SHIFT_JIS -t UTF-8 '$files' >  '$files' \necho "$files"\ndone
 1340  for files in ./*.csv\ndo\niconv -f SHIFT_JIS -t UTF-8 '$files' -o  '${files%.csv}.utf8.csv' \necho "$files"\ndone
 1341  iconv --list
 1342  iconv --list | grep shift
 1343  iconv --list | grep JIS
 1344  iconv --list | grep SHIFT_JIS
 1345  iconv --list | grep UTF8
 1346  find . -name '*.csv'
 1347  find . -name '*.csv' -exec iconv --verbose -f SHIFT_JIS -t UTF-8 -o {} {} \;
 1348  iconv --help
 1349  iconv -h
 1350  find . -exec ls '{}' 
 1351  find . -exec ls '{}' \;
 1352  find . -exec ls '{}' \+
 1353  find ./ -exec ls '{}' \+
 1354  find ./ -name '*csv' -exec ls '{}' \+
 1355  find -name '*csv' -exec ls '{}' \+
 1356  find . -name '*csv' -exec ls '{}' \+
 1357  find . -name '*csv' -exec echo '{}'  \+
 1358  find . -name '*csv' -exec echo '{file%}.txt'  \;
 1359  find . -name '*csv' -exec echo '{file%.csv}.txt'  \;
 1360  find . -name '*csv' -exec echo '{}.txt'  \;
 1361  find . -name '*csv' -exec echo '{1}.txt'  \;
 1362  find . -name '*csv' -exec echo {}.txt  \;
 1363  for files in ./*.csv\ndo \necho {$files}.csv \ndone
 1364  for files in ./*.csv\ndo \necho '$files'.csv \ndone
 1365  for files in ./*.csv\ndo \necho $files.csv \ndone
 1366  for files in ./*.csv\ndo \necho $files%.csv \ndone
 1367  for files in ./*.csv\ndo \necho ./$files%.csv \ndone
 1368  for files in ./*.csv\ndo\niconv -f SHIFT_JIS -t UTF-8 '$files' -o  '${files%.csv}.utf8.csv'\necho "$files"
 1369  for files in ./*.csv\ndo\niconv -f SHIFT_JIS -t UTF-8 '$files' > ../UTF8_flat/'${files}.utf8.csv'\necho "$files"\ndone
 1370  man iconv
 1371  iconv -f SHIFT_JIS -t UTF-8 ./BSW_20160721_20160803.csv
 1372  iconv -f SHIFT_JIS -t UTF-8 ./BSW_20160721_20160803.csv > ./BSW_20160721_20160803.utf8.csv
 1373  iconv -f SHIFT_JIS -t UTF-8 ./BSW_20160721_20160803.csv >> BSW_20160721_20160803.utf8.csv
 1374  iconv -c -f SHIFT_JIS -t UTF-8 ./BSW_20160721_20160803.csv >> BSW_20160721_20160803.utf8.csv
 1375  for files in ./*.csv\ndo\nECHO "{$files%csv}"\ndone
 1376  for files in ./*.csv\ndo\nECHO "$files%csv"\ndone
 1377  for files in ./*.csv\ndo\nECHO "$files%%csv"\ndone
 1378  for files in ./*.csv\ndo\nECHO "${files%%*}"\ndone
 1379  for files in ./*.csv\ndo\nECHO "${files%*}"\ndone
 1380  for files in ./*.csv\ndo\nECHO "${files%csv}"\ndone
 1381  for files in ./*.csv\ndo\nECHO "${files##*/%csv}"\ndone
 1382  for files in ./*.csv\ndo\nECHO "${files##*./%csv}"\ndone
 1383  for files in ./*.csv\ndo\nECHO "${files##}"\ndone
 1384  for files in ./*.csv\ndo\nECHO "${%%files%%.csv}"\ndone
 1385  for files in ./*.csv\ndo\nECHO "${%%/files%%.csv}"\ndone
 1386  for files in ./*.csv\ndo\nECHO "${files%Â¥\.csv}"\ndone
 1387  for files in ./*.csv\ndo\nECHO "${files#\.\/%%.csv}"\ndone
 1388  for files in ./*.csv\ndo\nECHO "${files#\.\/ %%.csv}"\ndone
 1389  for files in ./*.csv\ndo\nECHO "${files%%.csv}"\ndone
 1390  for files in ./*.csv\ndo\nECHO "${files##./}"\ndone
 1391  for files in ./*.csv\ndo\nECHO "${files##./%%.csv}"\ndone
 1392  vim shiftjis2utf8.sh
 1393  sh shiftjis2utf8.sh
 1394  cd Projects/AllisonTransmission/
 1395  cd /Users/212339410/Box/Analytics/Data/MBS2016/SBtoGE Data 12282016_0103_0950/Data from 20161201 to 20161227
 1396  cd '/Users/212339410/Box/Analytics/Data/MBS2016/SBtoGE Data 12282016_0103_0950/Data from 20161201 to 20161227'
 1397  head OSS_20160720_20160803.utf8.csv > OSS_header.csv
 1398  mssql -s alpidcdiha001v.cloud.ge.com -u 'dssb' -d 'SB' -e -t 20000 -p
 1399  ln -s ~/Box/Analytics/R/MBS2016 ~/Box/Analytics/Projects
 1400  cd Src/
 1401  cd UTF8_flat
 1402  head OSS_20160720_20160803.utf8.csv
 1403  head OSS_20160720_20160803.utf8.csv -n 1
 1404  head -n 1 OSS_20160720_20160803.utf8.csv 
 1405  head -n 2 OSS_20160720_20160803.utf8.csv 
 1406  head -n 2 OSS_20160804_20160817.utf8.csv
 1407  head -n 2 OSS_20160720_20160803.csv
 1408  head -n 2 OSS_20160804_20160817.csv
 1409  head -n `0 OSS_20160804_20160817.csv > OSS_20160804_20160817_n10.csv
 1410  head -n 10 OSS_20160804_20160817.csv > OSS_20160804_20160817_n10.csv
 1411  head -n 10 OSS_20160720_20160803.csv > OSS_20160720_20160803_n10.csv
 1412  cd AllisonTransmission
 1413  git push origin hiroaki/st-louis
 1414  mssql -s alpidcdiha001v.cloud.ge.com -u 'dssb' -p 'b@nklak3_123' -d 'SB' -e -t 20000
 1415  cd /Users/212339410/Box/Analytics/R/MBS2016/MBS2016.Rproj
 1416  cd '/Users/212339410/Box/Analytics/R/MBS2016/MBS2016.Rproj'
 1417  cd '/Users/212339410/Box/Analytics/R/MBS2016/'
 1418  git-log
 1419  vim SB.Rproj
 1420  git clone git@github.build.ge.com:212339410/SB.git
 1421  git git remote -v
 1422  git remote add origin git@github.build.ge.com:212339410/SB.git
 1423  add Python
 1424  git add Report/
 1425  git rm --cached Report/ReportForMBSDescriptiveAnalysis.Rmd Report/ReportForMBSDescriptiveAnalysis.html
 1426  mssql -s alpidcdiha001v.cloud.ge.com -u 'dssb' -p 'b@nklak3_123' -d 'SB' -e
 1427  man mssql
 1428  mssql -help
 1429  .quit
 1430  cd Src/UTF8_flat
 1431  head -n 5 OSS_20160720_20160803.utf8.csv
 1432  cd ../cache
 1433  touch void.txt
 1434  git branch --set-upstream-to=origin/master master
 1435  git pull origin master --allow-unrelated-histories
 1436  git commit -m 'resolve the conflict'
 1437  ls'\nls\n'
 1438  for files in ./*.csv\ndo\necho "$files"\ndone
 1439  for files in ./Original_flat/*.csv\ndo\necho "$files"\ndone
 1440  for files in ./Original_flat/*.csv\ndo\necho "$files"\necho â${files%.csv}.utf8.csv'\ndone\n'\n'
 1441  for files in ./Original_flat/*.csv\ndo\necho "$files"\necho â${files%.csv}.utf8.csvâ\ndone
 1442  man gsub
 1443  gsub
 1444  for files in ./Original_flat/*.csv\ndo\necho "$files"\necho â${files%.csv}.utf8.csvâ | sed âs/Original_flat/UTF8_flat2â\ndone
 1445  for files in ./Original_flat/*.csv\ndo\necho "$files"\necho â${files%.csv}.utf8.csvâ | sed âs/Original_flat/UTF8_flat2/â\ndone
 1446  echo 'orig' | sed 's/o/aaa/'
 1447  for files in ./Original_flat/*.csv\ndo\necho "$files"\necho â${files%.csv}.utf8.csvâ | sed âs+Original_flat+UTF8_flat2+â\ndone
 1448  for files in ./Original_flat/*.csv\ndo\necho '$files'\necho '${files%.csv}.utf8.csv' | sed 's+Original_flat+UTF8_flat2+'\ndone
 1449  for files in ./Original_flat/*.csv\ndo\necho "$files"\necho â${files%.csv}.utf8.csvâ | sed 's+Original_flat+UTF8_flat2+'\ndone
 1450  sl -s Box/Analytics/Data/MBS2016/UTF8_bom Projects/SB/Src/
 1451  ln -s Box/Analytics/Data/MBS2016/UTF8_bom Projects/SB/Src/
 1452  ln -s Box/Analytics/Data/MBS2016/UTF8_bom Box/Analytics/Projects/SB/Src
 1453  python ../Python/convert_utf8.py Original_flat/BSW_20160721_20160803.csv UTF8_bom/test.csv
 1454  vim Original_flat/BSW_20160721_20160803.csv
 1455  sed -i 's/Original_flat/UTF8_flat/'
 1456  sed -i 's/Original_flat/UTF8_flat/' 'Original_flat'
 1457  sed -i 's+Original_flat+UTF8_flat+' 'Original_flat'
 1458  sed -i 's+Original_flat+UTF8_flat+' Original_flat
 1459  echo day | sed s/day/night/
 1460  var=`echo day | sed s/day/night/`
 1461  for files in ./Original_flat/*.csv\ndo\necho "$files"\n\nnew_file_name=`echo ${files%.csv}.utf8.csvâ | sed 's+Original_flat+UTF8_flat2+'`\n#python  convert_utf8.py â$files' '$new_file_name'\n\ndone
 1462  for files in ./Original_flat/*.csv\ndo\necho "$files"\n\nnew_file_name=`echo ${files%.csv}.utf8.csvâ | sed 's+Original_flat+UTF8_flat2+'`\n#python  convert_utf8.py â$files' '$new_file_name'\necho "$new_file_name"\ndone
 1463  ls ../
 1464  ls ../Python/
 1465  for files in ./Original_flat/*.csv\ndo\necho "$files"\n\nnew_file_name=`echo ${files%.csv}.utf8.csvâ | sed 's+Original_flat+UTF8_bom+'`\npython  ../Python/convert_utf8.py â$files' '$new_file_name'\necho "$new_file_name"\ndone\n
 1466  for files in ./Original_flat/*.csv\ndo\necho "$files"\n\nnew_file_name=`echo ${files%.csv}.utf8.csvâ | sed 's+Original_flat+UTF8_bom+'`\npython  ../Python/convert_utf8.py '$files' '$new_file_name'\necho "$new_file_name"\ndone
 1467  for files in ./Original_flat/*.csv\ndo\necho "$files"\nnew_file_name=`echo ${files%.csv}.utf8.csvâ | sed 's+Original_flat+UTF8_bom+'`\npython  ../Python/convert_utf8.py "$files" "$new_file_name"\necho "$new_file_name"\ndone
 1468  locate command
 1469  ln -s /Users/212339410/Box/Analytics/Data/
 1470  l /
 1471  ln -s /Users/212339410/Box/Analytics/Data/MBS2016/UTF8_bom /Users/212339410/Box/Analytics/Projects/SB/Src
 1472  ln -s /Users/212339410/Box/Analytics/Data/MBS2016/UTF8_bom /Users/212339410/Box/Analytics/Projects/SB/Src 
 1473  ln -s /Users/212339410/Box/Analytics/Data/MBS2016/UTF8_bom /Users/212339410/Box/Analytics/Projects/SB/Src/
 1474  ln -s /Users/212339410/Documents/Analytics/Data/MBS2016/UTF8_bom /Users/212339410/Documents/Analytics/Projects/SB/Src/
 1475  history  | grep cp
 1476  history  | grep flat
 1477  cd Data/MBS2016/
 1478  for files in ./Original_flat/*.csv\ndo\necho "$files"\nnew_file_name=`echo ${files%.csv}.utf8.csv | sed 's+Original_flat+UTF8_bom+'`\n# python  ../Python/convert_utf8.py "$files" "$new_file_name"\necho "$new_file_name"\ndone
 1479  ln -s /Users/212339410/Documents/Analytics/Data/MBS2016/Original_flat /Users/212339410/Documents/Analytics/Projects/SB/Src 
 1480  for files in ./Original_flat/*.csv\ndo\necho "$files"\nnew_file_name=`echo ${files%.csv}.utf8.csv | sed 's+Original_flat+UTF8_bom+'`\npython  ../Python/convert_utf8.py "$files" "$new_file_name"\necho "$new_file_name"\ndone
 1481  python ../../../Projects/SB/Python/convert_utf8.py BSW_20160804_20160816.csv BSW_20160804_20160816.utf8.csv
 1482  ssh nxi_bastion
 1483  ssh nxi_hdp_edge
 1484  files=BSW_ERI_20151201_20170201.csv
 1485  echo new_file_name
 1486  echo n$ew_file_name
 1487  files=./Original_flat/BSW_ERI_20151201_20170201.csv
 1488  files=./Original_flat/BSW_NOK_20151201_20170201.csv
 1489  new_file_name=`echo ${files%.csv}.utf8.csv | sed 's+Original_flat+UTF8_bom+'`
 1490  python  ../Python/convert_utf8.py "$files" "$new_file_name"
 1491  cat BSW2016_bom.csv| wc -l
 1492  cd OSS_
 1493  tar -zxvf OSSRC_20170116.tar.gz
 1494  tar -zxvf OSSRC_20170116.tar.gzfor files in ./*tar.gz
 1495  tar -zxvf $files
 1496  for files in ./*tar.gz\ndo\necho $files\n# tar -zxvf $files\ndone
 1497  for files in ./*.tar.gz\ndo\necho $files\n# tar -zxvf $files\ndone
 1498  for files in ./*.tar.gz\ndo\necho $files\ntar -zxvf $files\ndone
 1499  rm *tar.gz
 1500  vim Tokyo_RAN_OSS-RC_01_20170116.txt
 1501  cd OSS_Nok
 1502  tar -xzf OSS_NOK_20170127-aa.tgz
 1503  for files in ./*.tgz\ndo\necho $files\ntar -zxvf $files\ndone
 1504  echo $file
 1505  new=${file/.csv}
 1506  echo $new
 1507  cut -d '-' -f 2
 1508  cut -d '-' -f 2 <<< $files
 1509  head $files
 1510  cd Pr
 1511  cd Documents/Analytics
 1512  Projects
 1513  scd ..
 1514  cd UTF8_bom/
 1515  head Tokyo_RAN_OSS-RC_01_20170116.txt -n 100 > Tokyo_RAN_OSS-RC_01_20170116_100.txt
 1516  head -n 100 Tokyo_RAN_OSS-RC_01_20170116.txt > Tokyo_RAN_OSS-RC_01_20170116_100.txt
 1517  man vim
 1518  cp OSS_NOK_20170117-aa.csv OSS_NOK_20170117-aa.bk.csv
 1519  cd Documents/Analytics/Projects/SB/Src
 1520  head Tokyo_RAN_OSS-RC_01_20170116.txt
 1521  cp Tokyo_RAN_OSS-RC_01_20170116_100.txt Tokyo_RAN_OSS-RC_01_20170116_100.backup.txt
 1522  sed -i '2~2 s/^M//' Tokyo_RAN_OSS-RC_01_20170116_100.txt
 1523  sed -i "2~2 s/^M//" Tokyo_RAN_OSS-RC_01_20170116_100.txt >> Tokyo_RAN_OSS-RC_01_20170116_100_mod.txt
 1524  sed 
 1525  man sed
 1526  sed -n '1~2!p' file
 1527  sed -n '1~2!p' Tokyo_RAN_OSS-RC_01_20170116_100.txt
 1528  sed -n '1,2!p' Tokyo_RAN_OSS-RC_01_20170116_100.txt
 1529  sed -i "2,2 s/^M//" Tokyo_RAN_OSS-RC_01_20170116_100.txt >> Tokyo_RAN_OSS-RC_01_20170116_100_mod.txt
 1530  sed -i "2,2 s/^M//" 'Tokyo_RAN_OSS-RC_01_20170116_100.txt' >> 'Tokyo_RAN_OSS-RC_01_20170116_100_mod.txt'
 1531  sed -i "2,2 s/^M//" 'Tokyo_RAN_OSS-RC_01_20170116_100.txt' 
 1532  sed 's/^M//2'  'Tokyo_RAN_OSS-RC_01_20170116_100.txt' >> 'Tokyo_RAN_OSS-RC_01_20170116_100_mod.txt'
 1533  sed 's/\r$//2' Tokyo_RAN_OSS-RC_01_20170116_100.txt >> Tokyo_RAN_OSS-RC_01_20170116_100_mod.txt
 1534  vim Tokyo_RAN_OSS-RC_01_20170116_100_mod.txt
 1535  sed -i 's/\r$//g2' Tokyo_RAN_OSS-RC_01_20170116_100.txt >> Tokyo_RAN_OSS-RC_01_20170116_100_mod.txt
 1536  sed -i 's/\r$//2g' Tokyo_RAN_OSS-RC_01_20170116_100.txt >> Tokyo_RAN_OSS-RC_01_20170116_100_mod.txt
 1537  sed -i 's/\r$//2' Tokyo_RAN_OSS-RC_01_20170116_100.txt >> Tokyo_RAN_OSS-RC_01_20170116_100_mod.txt
 1538  sed -i 's/\r$//g' Tokyo_RAN_OSS-RC_01_20170116_100.txt 
 1539  sed 's/\r$//2' Tokyo_RAN_OSS-RC_01_20170116_100.txt 
 1540  sed 's/\r$//2g' Tokyo_RAN_OSS-RC_01_20170116_100.txt 
 1541  sed 's/\r//2' Tokyo_RAN_OSS-RC_01_20170116_100.txt 
 1542  sed 's/\r//g' Tokyo_RAN_OSS-RC_01_20170116_100.txt 
 1543  head  Tokyo_RAN_OSS-RC_01_20170116_100.txt 
 1544  head  Tokyo_RAN_OSS-RC_01_20170116_100_mod.txt 
 1545  head Tokyo_RAN_OSS-RC_01_20170116_100_mod.txt cd ..
 1546  head Tokyo_RAN_OSS-RC_01_20170116_100.mod.csv
 1547  vim Tokyo_RAN_OSS-RC_01_20170116_100.mod.csv
 1548  s
 1549  cd ../../UTF8_bom
 1550  cd OSS_Eri
 1551  head Tokyo_RAN_OSS-RC_01_20170117.mod.mod.txt
 1552  vim Tokyo_RAN_OSS-RC_01_20170117.mod.mod.txt
 1553  rm Tokyo_RAN_OSS-RC_01_20170117.mod.mod.txt Tokyo_RAN_OSS-RC_01_20170117.mod.txt
 1554  rm Tokyo_RAN_OSS-RC_01_20170116_100.mod.txt
 1555  vim Tokyo_RAN_OSS-RC_01_20170116_100.mod.txt
 1556  vim Tokyo_RAN_OSS-RC_01_20170116_100.txt
 1557  python read_ericsson.py
 1558  python read_ericsson.py >> /Users/212339410/Documents/Analytics/Data/MBS2016/UTF8_bom/OSS_Eri/Tokyo_RAN_OSS-RC_01_20170116_100.mod.txt
 1559  l ../Src/UTF8_bom/OSS_Eri/*.txt
 1560  l ../Src/UTF8_bom/OSS_Eri/*.txtfor files in ../Src/UTF8_bom/OSS_Eri/*.txt
 1561  new_file_name=`echo ${files%.csv}.mod.csv`
 1562  echo $new_file_name
 1563  #python read_ericsson.py $files >> $new_file_name
 1564  for files in ../Src/UTF8_bom/OSS_Eri/*.txt\ndo\necho $files\nnew_file_name=`echo ${files%.csv}.mod.csv`\necho $new_file_name\n#python read_ericsson.py $files >> $new_file_name\ndone
 1565  for files in ../Src/UTF8_bom/OSS_Eri/*.txt\ndo\necho $files\nnew_file_name=`echo ${files%.txt}.mod.txt`\necho $new_file_name\n#python read_ericsson.py $files >> $new_file_name\ndone
 1566  for files in ../Src/UTF8_bom/OSS_Eri/*.txt\ndo\necho $files\nnew_file_name=`echo ${files%.txt}.mod.txt`\necho $new_file_name\npython read_ericsson.py $files >> $new_file_name\ndone
 1567  ls Documents/Analytics/Projects/SB/
 1568  cd Documents/Analytics/Projects/SB/
 1569  cd UTF8_bom/OSS_Eri
 1570  wc -l Tokyo_RAN_OSS-RC_01_20170117.mod.txt
 1571  wc -l Tokyo_RAN_OSS-RC_01_20170117.txt
 1572  vim Tokyo_RAN_OSS-RC_01_20170117.mod.txt
 1573  wc -l Tokyo_RAN_OSS-RC_01_20170118.mod.txt
 1574  wc -l Tokyo_RAN_OSS-RC_01_20170118.txt
 1575  dc Python
 1576  cp ../../Src/UTF8_bom/OSS_Eri/Tokyo_RAN_OSS-RC_0* .
 1577  rm *.mod.txt
 1578  rm Tokyo_RAN_OSS-RC_01_20170116_100.txt
 1579  python read_ericsson.py test/Tokyo_RAN_OSS-RC_01_20170116.txt
 1580  vim read_ericsson.py
 1581  python read_ericsson.py test/Tokyo_RAN_OSS-RC_01_20170116.txthistory
 1582  vim Tokyo_RAN_OSS-RC_01_20170117.txt
 1583  vim ../Src/UTF8_bom/OSS_Eri/Tokyo_RAN_OSS-RC_01_20170118.txt
 1584  vim ../Src/UTF8_bom/OSS_Eri/Tokyo_RAN_OSS-RC_01_20170119.txt
 1585  vim ../Src/UTF8_bom/OSS_Eri/Tokyo_RAN_OSS-RC_01_20170120.txt
 1586  vim ../Src/UTF8_bom/OSS_Eri/Tokyo_RAN_OSS-RC_01_20170121.txt
 1587  /bin/bash
 1588  vim read_ericsson_step2.py
 1589  tail -f output_processed.csv
 1590  df -h
 1591  brew mc
 1592  cd /Users/212339410/Documents/Analytics/Projects/SB/Src/Original_flat
 1593  vim OSS_20160707_20160721.csv
 1594  for files in ./; do echo $files done\n\n;\n;\ndone
 1595  for files in ./; do; echo $files; done
 1596  for files in ./OSS_NOK*\ndo\necho $files\ndone
 1597  for files in ./OSS_NOK*\ndo\necho $files\nnew_file_name=`echo $files | sed 's+Original_flat+UTF8_bom+'`\n#ln -s $files $new_file_name\ndone
 1598  cd ../../../Data/MBS2016/
 1599  for files in ./OSS_NOK*\ndo\n#echo $files\nnew_file_name=`echo $files | sed 's+Original_flat+UTF8_bom+'`\necho $new_file_name\n#ln -s $files $new_file_name\ndone
 1600  for files in ./Original_flat/OSS_NOK*\ndo\n#echo $files\nnew_file_name=`echo $files | sed 's+Original_flat+UTF8_bom+'`\necho $new_file_name\n#ln -s $files $new_file_name\ndone
 1601  for files in ./Original_flat/OSS_NOK*\ndo\n#echo $files\nnew_file_name=`echo $files | sed 's+Original_flat+UTF8_bom+'`\necho $new_file_name\nln -s $files $new_file_name\ndone
 1602  for files in ./Original_flat/OSS_NOK*\ndo\n#echo $files\nnew_file_name=`echo $files | sed 's+Original_flat+UTF8_bom+'`\necho $new_file_name\nln $files $new_file_name\ndone
 1603  vim UTF8_bom/OSS_NOK_20170117-aa.csv
 1604  cp OSS_NOK_20170117-aa.csv OSS_NOK_20170117-aa.bak.csv
 1605  head -n 1 OSS_NOK_20170117-aa.bak.csv
 1606  sed -i '' '/SQL>/d' ./OSS_NOK_20170117-aa.csv
 1607  wc -l OSS_NOK_20170117-aa.bak.csv
 1608  wc -l OSS_NOK_20170117-aa.csv
 1609  vim OSS_NOK_20170117-ab.csv
 1610  cd ../../Projects/SB/Python/
 1611  python read_OSS_nok.py ../Src/Original_flat/OSS_NOK_20170117-aa.csv
 1612  python read_OSS_nok.py ../Src/Original_flat/OSS_NOK_20170117-aa.csv 
 1613  for files in ../Src/Original_flat/OSS_NOK_*\ndo\n#echo $files\nnew_file_name=`echo $files | sed 's+Original_flat+UTF8_bom+'`\necho $new_file_name\n#python read_OSS_nok.py $files >> $new_file_name\ndone
 1614  for files in ../Src/Original_flat/OSS_NOK_*\ndo\n#echo $files\nnew_file_name=`echo $files | sed 's+Original_flat+UTF8_bom+'`\necho $new_file_name\npython read_OSS_nok.py $files >> $new_file_name\nfjiasdj\nj\nendone\ndone
 1615  cd /Users/212339410/Documents/Analytics/Projects/SB/Src/UTF8_bom
 1616  vim OSS_NOK_20170117-aa.csv
 1617  for files in ../Src/Original_flat/OSS_NOK_*\ndo\n#echo $files\nnew_file_name=`echo $files | sed 's+Original_flat+UTF8_bom+'`\necho $new_file_name\npython read_OSS_nok.py $files >> $new_file_name\ndone
 1618  vim OSS_NOK_20170127-aa.csv
 1619  for files in ../Src/Original_flat/OSS_NOK_*\ndo\n#echo $files\nnew_file_name=`echo $files | sed 's+Original_flat+UTF8_bom+'`\necho $new_file_name\npython read_OSS_nok.py $files > $new_file_name\ndone
 1620  cd Documents/Analytics/Projects/SB/Src/
 1621  vim OSS_20170117_20170127.csv
 1622  cd Box/Analytics\ \(user\ 227311313\)
 1623  cd ../Documents/Analytics
 1624  pip install https://github.com/chokkan/simstring.git
 1625  sudo geproxy;sudo pip install simstring
 1626  sudo pip install simstring
 1627  conda help
 1628  pyenv
 1629  conda create -n python2 python=2.7 anaconda
 1630  source activate python2
 1631  alias py2 source deactivate python2
 1632  py2
 1633  man alias
 1634  cd /Users/212339410/Documents/Analytics/Projects/SB/Python
 1635  gunzip -c simstring-1.0.tar.gz| tar xopft - 
 1636  gunzip -c simstring-1.0.tar.gz | tar xopf - 
 1637  tar -zxvf simstring-1.0.tar.gz
 1638  ls simstring-1.0
 1639  cd simstring-1.0/frontend
 1640  make 
 1641  l ./frontend
 1642  cd ./frontend
 1643  vim ../include/simstring/memory_mapped_file.h
 1644  cd swig/python
 1645  ./prepare.sh
 1646  python setup.py build_ext
 1647  pip install simstring
 1648  pip install jupyter-themes
 1649  pip install git+https://github.com/dunovank/jupyter-themes.git
 1650  jupyter-theme -l
 1651  jt -t onedork
 1652  conda install scikit-learn
 1653  vim /Users/212339410/Downloads/Tokyo_RAN_OSS-RC_05_20170125.txt
 1654  pip uninstall tstk
 1655  ls -al /Users/212339410/anaconda/envs/py3k/lib/python3.5/site-packages/
 1656  ls -al /Users/212339410/anaconda/envs/py3k/lib/python3.5/site-packages/ | grep tstk
 1657  rm -r /Users/212339410/anaconda/envs/py3k/lib/python3.5/site-packages/tstk/*
 1658  rm -r /Users/212339410/anaconda/envs/py3k/lib/python3.5/site-packages/tstk/
 1659  rm -r /Users/212339410/anaconda/envs/py3k/lib/python3.5/site-packages/tstk
 1660  rm /Users/212339410/anaconda/envs/py3k/lib/python3.5/site-packages/tstk-0.1-py3.5.egg-info
 1661  rm -r /Users/212339410/anaconda/envs/py3k/lib/python3.5/site-packages/tstk-0.1-py3.5.egg-info
 1662  cd ~/Documents/Analytics/Projects/SB/
 1663  git clone git@github.build.ge.com:IndustrialDataScience/tstk.git tstk
 1664  cd Documents/
 1665  cd Analytics/Python/
 1666  ls -al 
 1667  locate python35
 1668  ln -s /Users/212339410/Documents/Analytics/Python/tstk /Users/212339410/anaconda/envs/py3k/lib/python3.5/site-packages/tstk 
 1669  vim test.py
 1670  python test.py
 1671  l /Users/212339410/anaconda/envs/py3k/lib/python3.5/site-packages/
 1672  l /Users/212339410/anaconda/envs/py3k/lib/python3.5/site-packages/ | grep
 1673  rm /Users/212339410/anaconda/envs/py3k/lib/python3.5/site-packages/tstk
 1674  l /Users/212339410/anaconda/envs/py3k/lib/python3.5/site-packages/ | grep tstk
 1675  ln -s /Users/212339410/Documents/Analytics/Python/tstk/tstk /Users/212339410/anaconda/envs/py3k/lib/python3.5/site-packages/tstk 
 1676  deactivate l /Users/212339410/anaconda/envs/py3k/lib/python3.5/site-packages/tstk
 1677  pip install pdfkit
 1678  jt -t onedork -T 
 1679  zsh 
 1680  zsh -h
 1681  -h
 1682  update
 1683  pip install hide_code
 1684  jt
 1685  jt -t chesterish
 1686  jt -h
 1687  jt -r 
 1688  jt -l
 1689  jt -t onedork -T
 1690  cd Documents/Analytics/Projects/SB/Python
 1691  pip install Orange
 1692  sudo pip install Orange
 1693  head OSS_20170117_20170127.csv
 1694  cd Documents/Analytics/Projects/SB
 1695  wc -l BSW2016_bom.csv
 1696  ls -al BSW2016_bom.csv
 1697  ls -al BSW*
 1698  ln -s -F /Users/212339410/Documents/Analytics/Projects/ /Users/212339410/Projects
 1699  ls -al ` locate tstk`
 1700  ln -s /Users/212339410/anaconda2/envs/py35/lib/python3.5/site-packages/tstk 
 1701  ln -s /Users/212339410/Documents/Analytics/Python/tstk/tstk /Users/212339410/anaconda2/envs/py35/lib/python3.5/site-packages/tstk 
 1702  cd ./Python
 1703  git clone https://github.com/litaotao/IPython-Dashboard.git 
 1704  cd IPython-Dashboard
 1705  pip install CofigParser
 1706  pip install configparser==3.3.0.post2
 1707  pip install configparser==3.5.0b2
 1708  pip install mysqlclient
 1709  dash-server
 1710  sudo nosetest --with-coverage --cover-package=dashboard
 1711  pip install ipython-dashboard
 1712  rm -rf OSS_Eri
 1713  cd ModData/
 1714  wc -l OSS_Nokia_bsid_20160707_20170127.csv
 1715  mssql -s alpidcdiha001v.cloud.ge.com -u 'dsfaâ -p 'villa1hermosa9' -d 'FemsaAlpla'
 1716  mssql -s alpidcdiha001v.cloud.ge.com -u 'dsfa' -p 'villa1hermosa9' -d 'FemsaAlplaa'
 1717  mssql -s alpidcdiha001v.cloud.ge.com -u 'dsfa' -p 'villa1hermosa9' -d 'FemsaAlpla'
 1718  mssql -s alpidcdiha001v.cloud.ge.com -u 'dsfa' -p '123' -d 'FemsaAlplaa'
 1719  mssql -s alpidcdiha001v.cloud.ge.com -u 'dsfa' -p '123' -d 'FemsaAlpha'
 1720  mssql -s alpidcdiha001v.cloud.ge.com -u 'dsfa' -p 'villa1hermosa9' -d 'FemsaAlpha'
 1721  python update_schema.py '../Src/ModData/BSW2016.csv' '../Src/Schema/bsw2016.xml' '../Src/Schema/bsw2016_mod.xml'
 1722  python update_schema.py '../Src/ModData/BSW2016.csv' '../Src/Schema/bsw2016_v1.xml' '../Src/Schema/bsw2016_mod.xml'
 1723  conda install -c r ipython-notebook r-irkernel
 1724  python update_schema.py '../Src/ModData/NEDB_NOKERI_TQ.utf8.csv' '../Src/Schema/nedb.xml'
 1725  python update_schema.py '../Src/ModData/NEDB_NOKERI_TQ.utf8.csv' '../Src/Schema/nedb.xml' '' ''
 1726  python update_schema.py '../Src/ModData/NEDB_NOKERI_TQ.utf8.csv' '../Src/Schema/nedb.xml' 
 1727  brew install pkg-config libffi openssl python
 1728  env LDFLAGS="-L$(brew --prefix openssl)/lib" CFLAGS="-I$(brew --prefix openssl)/include" pip install cryptography
 1729  fabmanager
 1730  pip install flask-appbuilder
 1731  fabmanager create-app
 1732  cd superset_demo
 1733  fabmanager create-admin
 1734  fabmanager run
 1735  rm -rf superset_demo
 1736  abmanager create-admin --app superset
 1737  pip install cryptography-1.5
 1738  pip install cryptography
 1739  pip install cryptography 1.5
 1740  pip install cryptography==1.5.0
 1741  pip install cryptography==1.7.2
 1742  ls -al ~/ | superset
 1743  ls -al ~/ | grep superset
 1744  locate sqlite
 1745  locate sqlite | superset.db
 1746  locate sqlite | grep superset.db
 1747  locate sqlite | grep superset
 1748  locate sqlite | grep db
 1749  locate sqlite | grep .db
 1750  locate sqlite | grep *\.db
 1751  locate sqlite | grep \.db
 1752  locate sqlite | grep ".db"
 1753  locate sqlite | grep ls
 1754  ls /opt/
 1755  fabmanager create-admin --app superset
 1756  pip uninstall superset
 1757  pip install --upgrade setuptools pip
 1758  pip install superset
 1759  pip install superset --upgrade
 1760  super
 1761  superset db upgrade
 1762  superset load_examples
 1763  superset init
 1764  jupyter
 1765  jupyter no
 1766  mv Untitled.ipynb preprocessing_oss_nok.ipynb
 1767  cd Ingest
 1768  ls / | grep OSS
 1769  ls /
 1770  rm OSS_NOK_20170207-aa._cl.csv
 1771  mv Untitled.ipynb preprocessing_tte.ipynb
 1772  cd ../Src/Original_flat/
 1773  head -n 1 OSS_20160707_20160721.csv
 1774  ld
 1775  cd ~/Box
 1776  ln -s Box "Box Sync"
 1777  cd Box\ Sync
 1778  cd "~/Box Sync"
 1779  cd ~/Box\ Sync
 1780  split
 1781  cp ../BSW_20160707_20160721.csv .
 1782  split BSW_20160707_20160721.csv 
 1783  man split
 1784  man -l 100 split
 1785  split -l 100 BSW_20160707_20160721.csv
 1786  cat xa* > bsw.csv
 1787  cat bsw.csv
 1788  tail bsw.csv
 1789  cl
 1790  rm -rf test
 1791  mv *.ipynb ../Notebook
 1792  add Bash/
 1793  git add Bash/
 1794  git add Notebook/
 1795  git push master origin 
 1796  cf cd builspxka
 1797  cf cd builspacks
 1798  cd ../R/App
 1799  https://github.com/pivotalsoftware/superzip.git
 1800  git clone https://github.com/pivotalsoftware/superzip.git 
 1801  cf buildpacks
 1802  mv preprocessing_oss_nok.ipynb preprocessing_oss_nok_20170128_20170220.ipynb
 1803  cp preprocessing_oss_nok_20170128_20170220.ipynb preprocessing_oss_nok_20170104 -20170127.ipynb
 1804  cp preprocessing_oss_nok_20170128_20170220.ipynb preprocessing_oss_nok_20170104-20170127.ipynb
 1805  cd UTF8_bom/Ingest
 1806  cd OSS_Nok_20170128_20170220
 1807  rm OSS_NOK_20170207-aa.csv OSS_NOK_20170207-aa_cl.csv OSS_NOK_20170207-ab.csv OSS_NOK_20170207-ab_cl.csv OSS_NOK_20170207-ac.csv OSS_NOK_20170207-ac_cl.csv OSS_NOK_20170207-ad.csv OSS_NOK_20170207-ad_cl.csv OSS_NOK_20170207-ae.csv OSS_NOK_20170207-ae_cl.csv OSS_NOK_20170207-af.csv OSS_NOK_20170207-af_cl.csv OSS_NOK_20170207-ag.csv OSS_NOK_20170207-ag_cl.csv OSS_NOK_20170207-ah.csv OSS_NOK_20170207-ah_cl.csv OSS_NOK_20170207-ai.csv OSS_NOK_20170207-ai_cl.csv OSS_NOK_20170217-aa.csv OSS_NOK_20170217-aa_cl.csv OSS_NOK_20170217-ab.csv OSS_NOK_20170217-ab_cl.csv OSS_NOK_20170217-ac.csv OSS_NOK_20170217-ac_cl.csv OSS_NOK_20170217-ad.csv OSS_NOK_20170217-ad_cl.csv OSS_NOK_20170217-ae.csv OSS_NOK_20170217-ae_cl.csv OSS_NOK_20170217-af.csv OSS_NOK_20170217-af_cl.csv OSS_NOK_20170217-ag.csv OSS_NOK_20170217-ag_cl.csv OSS_NOK_20170217-ah.csv OSS_NOK_20170217-ah_cl.csv OSS_NOK_20170217-ai.csv OSS_NOK_20170217-ai_cl.csv 
 1808  cp preprocessing_oss_nok_20170104-20170127.ipynb preprocessing_wot_20151201_20170201.ipynb
 1809  head -n 1 /Users/212339410/Documents/Analytics/Data/MBS2016/UTF8_bom/WOT_20160707_20160721.utf8.csv
 1810  rm preprocessing_wot.ipynb
 1811  mv TTE_ERI_20151201_20170201.utf8.csv ./Ingest/
 1812  mv TTE_NOK_20151201_20170201.utf8.csv ./Ingest
 1813  cp preprocessing_wot_20151201_20170201.ipynb preprocessing_tte_20151201_20170201.ipynb
 1814  cp preprocessing_wot_20151201_20170201.ipynb preprocessing_bsw_20151201_20170201.ipynb
 1815  cp Projects/SB/Notebook/preprocessing_oss_nok_20170104_20170127.ipynb 
 1816  cd Projects/SB/Notebook/
 1817  cp preprocessing_oss_nok_20170104_20170127.ipynb preprocessing_oss_nok_20160707_20160828.ipynb
 1818  python update_schema.py '../Src/UTF8_bom/Ingest/tte_20151201_20170201.csv' '../Src/Schema/tte2016_original.xml'  
 1819  python update_schema.py '../Src/UTF8_bom/Ingest/tte_20151201_20170201.csv' '../Src/Schema/tte2016_original.xml'  '../Src/Schema/tte2016_mod.xml'
 1820  ls '~/Box Sync/'
 1821  l ~/Box\ Sync/DSS\ Internal-Softbank/12\ Working\ Directory\ for\ Ingestion/20170306_Ingestion/
 1822  python update_schema.py '../Src/UTF8_bom/Ingest/wot_20151201_20170201.csv' '../Src/Schema/wot2016_original.xml'  '../Src/Schema/wot2016_mod.xml'
 1823  python update_schema.py '/Users/212339410/Box/DSS Internal-Softbank/12 Working Directory for Ingestion/20170306_Ingestion/Output/wot_20151201_20170201.csv' '../Src/Schema/wot2016_original.xml'  '../Src/Schema/wot2016_mod.xml'
 1824  ln /Users/212339410/Box/DSS\ Internal-Softbank/12\ Working\ Directory\ for\ Ingestion 
 1825  ln -s /Users/212339410/Box/DSS\ Internal-Softbank/12\ Working\ Directory\ for\ Ingestion /Users/212339410/SB_ingest_working_directory
 1826  cd OSS_Nok_20160707_20160828
 1827  head -n 2 OSS_20160707_20160721.utf8.csv
 1828  rm _mod.xml 
 1829  python update_schema.py '/Users/212339410/Box/DSS Internal-Softbank/12 Working Directory for Ingestion/20170306_Ingestion/Output/oss_nok_all_20160707_20160828.csv' '../Src/Schema/oss_nok_original.xml'  '../Src/Schema/oss_nok_mod.xml'
 1830  cd Projects/SB/P
 1831  python update_schema.py '/Users/212339410/Box/DSS Internal-Softbank/12 Working Directory for Ingestion/20170306_Ingestion/Output/oss_nok_all_20170128_20170220.csv' '../Src/Schema/oss_nok_mod_v2.xml'  '../Src/Schema/oss_nok_mod_v3.xml'
 1832  cd R/App/
 1833  grproxy
 1834  cf env dashboard_v1.R
 1835  cd cf-r-shiny-app-master
 1836  cf push dashboard_v1.R -m 500m -k 1000m -b http://github.com/alexkago/cf-buildpack-r.git
 1837  cf push start_shiny_app.R -m 500m -k 1000m -b http://github.com/alexkago/cf-buildpack-r.git
 1838  head -n 2 bsw_20151201_20170201.csv
 1839  wc -l oss_nok_all_20160707_20160828.csv
 1840  wc -l oss_nok_all_20170104_20170127.csv
 1841  wc -l oss_nok_all_20170128_20170220.csv
 1842  python update_schema.py '/Users/212339410/Box/DSS Internal-Softbank/12 Working Directory for Ingestion/20170306_Ingestion/Output/oss_nok_all_20170128_20170220.csv' '../Src/Schema/oss_nok_mod_v3.xml'  '../Src/Schema/oss_nok_mod_v4.xml'
 1843  python update_schema.py '/Users/212339410/Box/DSS Internal-Softbank/12 Working Directory for Ingestion/20170306_Ingestion/Output/oss_nok_all_20170104_20170127.csv' '../Src/Schema/oss_nok_mod_v4.xml'  '../Src/Schema/oss_nok_mod_v4.xml'
 1844  ln -s /Users/212339410/anaconda/envs/py3k/lib/python3.5/site-packages/tstk
 1845  ln -s /Users/212339410/Documents/Analytics/Python/tstk/tstk /Users/212339410/anaconda/envs/py3k/lib/python3.5/site-packages/tstk
 1846  ls -al /Users/212339410/anaconda/envs/py3k/lib/python3.5/site-packages/tstk
 1847  ls -al /Users/212339410/anaconda2/envs/py35/lib/python3.5/site-packages/tstk/
 1848  ls -al /Users/212339410/anaconda2/envs/py35/lib/python3.5/site-packages/tstk
 1849  ls -al /Users/212339410/anaconda2/envs/py35/lib/python3.5/site-packages/tstk/visualization
 1850  less /Users/212339410/anaconda2/envs/py35/lib/python3.5/site-packages/tstk/visualization/plot.py
 1851  git getch
 1852  python update_schema.py '../Src/ModData/NEDB_NOKERI_TQ.utf8.csv' '../Src/Schema/nedb.xml'  '../Src/Schema/nedb_mod.xml'
 1853  python update_schema.py '../Src/UTF8_bom/NEDB_BASE_INFO.20160721085837_2.utf8.csv' '../Src/Schema/nedb.xml'  '../Src/Schema/nedb_mod.xml'
 1854  python update_schema.py '../Src/UTF8_bom/NEDB_BASE_INFO.20160721085837_2.utf8.csv' '../Src/Schema/nedb_summer.xml'  '../Src/Schema/nedb_mod.xml'
 1855  vim update_schema.py
 1856  vim ../Src/Schema/nedb_summer.xml
 1857  python update_schema.py '../Src/UTF8_bom/NEDB_BASE_INFO.20160721085837_2.utf8.csv' '../Src/Schema/nedb_summer.xml'  '../Src/Schema/nedb_summer_mod.xml'
 1858  python update_schema.py '../Src/ModData/NEDB_NOKERI_TQ.utf8_st_drop.csv' '../Src/Schema/nedb_summer_mod.xml'  '../Src/Schema/nedb_summer_mod_v2.xml'
 1859  head -n 2 ../Src/ModData/NEDB_NOKERI_TQ.utf8_st_drop.csv
 1860  vim ../Src/ModData/NEDB_NOKERI_TQ.utf8_st_drop.csv
 1861  python convert_utf8.py ../Src/ModData/NEDB_NOKERI_TQ.utf8_st_drop.csv ../Src/ModData/NEDB_NOKERI_TQ.utf8_st_drop.utf8.csv
 1862  vim ../Src/ModData/NEDB_NOKERI_TQ.utf8.csv
 1863  vim "../Src/ModData/NEDB_NOKERI_TQ.utf8.csv"
 1864  python update_schema.py '../Src/ModData/NEDB_NOKERI_TQ.utf8.csv' '../Src/Schema/nedb_jan.xml'  '../Src/Schema/nedb_jan_mod.xml'
 1865  cd /Users/212339410
 1866  cd ./.atom/storage/
 1867  vim application.json
 1868  ls al
 1869  ls -al .
 1870  cp preprocessing_oss_nok_20170128_20170220.ipynb preprocessing_oss_nok_20170214_20170227.ipynb
 1871  cp preprocessing_bsw_20151201_20170201.ipynb preprocessing_bsw_20170201_20170301.ipynb
 1872  cd Box/DSS\ Internal-Softbank/11\ Processed\ Dataset/201702_OSS\ \(Jan\)
 1873  vim OSS_Ericsson_processed_20171116_20170127.csv
 1874  cd Box/DSS\ Internal-Softbank
 1875  cd 11\ Processed\ Dataset/201702_OSS\ \(Jan\)
 1876  head OSS_Ericsson_processed_20171116_20170127.csv
 1877  head OSS_Ericsson_processed_20171116_20170127.csv -> tmp.csv
 1878  head OSS_Ericsson_processed_20171116_20170127.csv > tmp.csv
 1879  cd ../Python/
 1880  python update_schema.py ~/Box/DSS\ Internal-Softbank/12\ Working\ Directory\ for\ Ingestion/20170310_Ingestion/Output/bsw_20170201_20170301.csv '../Src/Schema/bsw0301.xml'  '../Src/Schema/bsw0301_mod.xml'
 1881  cd ../../12\ Working\ Directory\ for\ Ingestion
 1882  cd 20170310_Ingestion
 1883  cd ../../20170306_Ingestion/Output
 1884  vim bsw_20151201_20170201.csv
 1885  cd ../../20170310_Ingestion/
 1886  vim bsq
 1887  vim bsw_20170201_20170301.csv
 1888  python update_schema.py ~/Box/DSS\ Internal-Softbank/12\ Working\ Directory\ for\ Ingestion/20170310_Ingestion/Output/bsw_20170201_20170301.csv '../Src/Schema/bsw2016_v4_mod_tmap.xml'  '../Src/Schema/bsw2016_v5.xml'
 1889  wc -l oss_nok_all_20170214_20170227.csv
 1890  vim ../../20170310_Ingestion/Output
 1891  git push pull
 1892  ls ~/Projects
 1893  ls -al ~/Projects
 1894  vim ../../20170310_Ingestion/Output/OSS_Eri_20170128_20170220.csv
 1895  vim ../Output/oss_nok_all_20170214_20170227.csv
 1896  head -n 10000 OSS_Eri_20170128_20170220.csv > ~/Projects/SB/Src/ModData/oss_eri_20170128_10000.csv
 1897  vim OSS_Eri_20170128_20170220.csv
 1898  cd Original_flat
 1899  cd 20170209_20170215_YM_logs
 1900  md5 OSS_Eri_20170221_20170228.csv
 1901  md5 OSS_Eri_20170128_20170220.csv
 1902  pip install XlsxWriter
 1903  git commit -m 'merge changes for preprocessing_oss_eri'
 1904  cp feature_nok_0314.csv feature_nok_0314_bom.csv
 1905  vim feature_nok_0314_bom.csv
 1906  cd ~/212339410@SFO1212339410M:~/Projects/SB/Src/Original_flat/
 1907  head -n 10000 20170209_0610_SB_nodelist2000.log > internal_log_sample.txt
 1908  head -n 100000 20170209_0610_SB_nodelist2000.log > internal_log_sample.txt
 1909  grep readclock internal_log_sample.txt
 1910  grep temp internal_log_sample.txt
 1911  cp NEDB_BASE_INFO.20160721085837_2.csv wc -l NEDB_BASE_INFO.20160721085837_2.csv
 1912  wc -l NEDB_BASE_INFO.20160721085837_2.csv
 1913  wc -l NEDB_ERI_T.csv
 1914  wc -l NEDB_NOK_Q.csv
 1915  cd ../ModData
 1916  open bsw_prob_area_eri.csv
 1917  vim bsw_prob_area_eri.csv
 1918  vim bsw_prob_area_eri_2017.csv
 1919  head SB_internal_log_201702.csv
 1920  head -n 1 SB_internal_log_201702.csv
 1921  vim 20170209_20170215_SB_logs_0309_1929/20170209_20170215_SB_logs/20170209_0610_SB_nodelist2000.log
 1922  cd ../Src/Original_flat/20170309_Internal\ Logs
 1923  cd 20170209_20170215_SB_logs_0309_1929/20170209_20170215_SB_logs
 1924  head -n 2 SB_internal_log_201702.csv
 1925  head -n 13 SB_internal_log_201702.csv
 1926  cls
 1927  open SB_internal_log_201702_fuigettemp.csv
 1928  open SB_internal_log_201702_mbs_PC145U.csv 
 1929  cf 
 1930  head  feature_eri_0315.csv
 1931  head  feature_eri_raw2_0315.csv
 1932  head  feature_eri_raw_0315.csv
 1933  head  -n 1 feature_eri_raw_0315.csv
 1934  head  -n 2 feature_eri_raw_0315.csv
 1935  date; fc read
 1936  readlink
 1937  date; ls
 1938  vim 20170209_0610_YM_nodelist1000.log
 1939  grep sfp 20170209_0610_YM_nodelist1000.log > 20170209_0610_YM_nodelist1000_sfp.log
 1940  less 20170209_0610_YM_nodelist1000_sfp.log
 1941  grep temp 20170209_0610_YM_nodelist1000.log > 20170209_0610_YM_nodelist1000_sfp.log
 1942  less 20170209_0610_YM_nodelist1000_temp.log
 1943  grep temp 20170209_0610_YM_nodelist1000.log > 20170209_0610_YM_nodelist1000_temp.log
 1944  grep clock 20170209_0610_YM_nodelist1000.log > 20170209_0610_YM_nodelist1000_clock.log
 1945  less 20170209_0610_YM_nodelist1000_clock.log
 1946  rm 20170209_0610_YM_nodelist1000_clock.log
 1947  zip df_event_oss.csv
 1948  grep 33720 BSW2016.csv 
 1949  grep 33720 BSW2016.csv  > tmp.csv
 1950  vim tmp.csv
 1951  open(tmp.csv)
 1952  open tmp.csv
 1953  python3
 1954  vim 20170208_1355_SB_nodelist100.log
 1955  vim RESURLT_20170208_1355_SB_nodelist100.log
 1956  vim prob_area_nok_after0317.csv
 1957  vim prob_area_nok_before0317.csv
 1958  less select_\*_from_event_view_oss_nok.csv
 1959  cd Projects/SB/Src/Original_flat
 1960  cd 20170309_Internal\ Logs
 1961  cd 20170209_20170215_SB_logs_0309_1929
 1962  cd 20170209_20170215_SB_logs
 1963  vim 20170209_0610_SB_nodelist2000.log
 1964  grep fault 20170209_0610_SB_nodelist2000.log 
 1965  grep fault 20170209_0610_SB_nodelist2000.log  | grep 34428
 1966  cd ../20170209_20170215_YM_logs
 1967  vim 20170209_0610_YM_nodelist1000_sfp.log
 1968  vim run.sh
 1969  rm run.sh
 1970  cd ../../20170309_Internal\ Logs
 1971  python confmat_mbs_nok.py 
 1972  python confmat_mbs_nok.py -h
 1973  python confmat_mbs_nok.py
 1974  python preprocessing_nok.py 
 1975  cd ../Src/Original_flat/20170309_Internal\ Logs/20170209_20170215_SB_logs_0309_1929/20170209_20170215_SB_logs
 1976  mv training_nok.py eval_model_nok.py
 1977  python eval_model_nok.py 
 1978  mv training_nok.py eval_model_nok.py --save_flag True --use_cache True --input_file ../Src/ModData/feature__nok_0320.csv
 1979  python eval_model_nok.py --save_flag True --use_cache True --input_file ../Src/ModData/feature__nok_0320.csv
 1980  cd Projects/SB/cache
 1981  head select_\*_from_bswn.csv
 1982  head -n 1 select_\*_from_bswn.csv
 1983  head -n 2 select_\*_from_bswn.csv
 1984  head -n 2 select_\*_from_event_view_bswn.csv
 1985  head -n 2 select_\*_from_event_view_oss_nok.csv
 1986  python preprocessing_nok.py
 1987  python preprocessing_nok.py -sample_days 30 -o test
 1988  python preprocessing_nok.py --sample_days 30 --o test
 1989  cd Projects/SB/Src/Original_flat/
 1990  less RESURLT_20170208_1355_SB_nodelist100.log
 1991  python preprocessing_nok.py -h
 1992  cd Projects/SB/Src
 1993  cd ModData
 1994  less df_event_uh_proc_test_nok_lbh14_lah1_0321.csv
 1995  python eval_model_nok.py -i "../Src/ModData/feature_select_nok_0321.csv" -o "test" 
 1996  python eval_model_nok.py --i "../Src/ModData/feature_select_nok_0321.csv" --o "test" 
 1997  python eval_model_nok.py --i "../Src/ModData/feature_nok_0321.csv" --o "test" 
 1998  python eval_model_nok.py --i "../Src/ModData/feature_select_nok_0321.csv" --o "test" --target Urgent
 1999  python eval_model_nok.py --i "../Src/ModData/feature_select_nok_0321.csv" --o "test" --target 'Urgent'
 2000  python eval_model_nok.py --i "../Src/ModData/feature_select_nok_0321.csv" --o "test" --target 'Prob_Urgent'
 2001  ls | grep df
 2002  python eval_model_nok.py --i "../Src/ModData/feature_select_nok_0321.csv" --o "../Src/ModData/" --target 'Prob_Urgent'
 2003  python eval_model_nok.py --i "../Src/ModData/feature_select_nok_0321.csv" --o "../Src/ModData/select_" --target 'Prob_Urgent'
 2004  python eval_model_nok.py --i "../Src/ModData/feature_nok_0321.csv" --o "../Src/ModData/" --target 'Prob_Urgent' --n_healthy '500, 1000'
 2005  open ../Src/ModData/df_result_rf_nok_0321.csv
 2006  python eval_model_nok.py --i "../Src/ModData/feature_nok_0321.csv" --o "../Src/ModData/" --target 'Prob_Urgent' --n_healthy '500, 1000, 3000, 5000' 
 2007  python eval_model_nok.py -h
 2008  python eval_model_nok.py --i "../Src/ModData/feature_nok_0321.csv" --o "../Src/ModData/" --target 'Prob_Urgent' --n_healthy '500, 1000, 3000' --h  
 2009  python eval_model_nok.py --i "../Src/ModData/feature_nok_0321.csv" --o "../Src/ModData/" --target 'Prob_Urgent' --n_healthy '500, 1000, 3000'   
 2010  python eval_model_nok.py --i "../Src/ModData/feature_nok_0321.csv" -o "../Src/ModData/" -target 'Prob_Urgent' --n_healthy '500, 1000, 3000'   
 2011  python eval_model_nok.py -i "../Src/ModData/feature_nok_0321.csv" -o "../Src/ModData/" --target 'Prob_Urgent' --n_healthy '500, 1000, 3000'  --n_unhealthy '500'
 2012  python eval_model_nok.py -i "../Src/ModData/feature_nok_0321.csv" -o "../Src/ModData/" --target 'Prob_Urgent' --n_healthy '500, 1000, 3000'   
 2013  python eval_model_nok.py -i "../Src/ModData/feature_nok_0321.csv" -o "../Src/ModData/" --target 'Prob_Urgent' --n_healthy '500, 1000, 3000'   --top_problem "'', 'FSME', 'Rectifier', 'FSMF', 'Breaker', 'FRGP', 'FXDA', 'BATT', 'FeederCable', 'OpticalCable', 'non-urgent', 'Others'"
 2014  less ../Src/ModData/feature_eri_v2_0316.csv
 2015  python eval_model_nok.py -i "../Src/ModData/feature_eri_v2_0316.csv" -o "../Src/ModData/" --target 'Prob_Urgent' --n_healthy '500, 1000, 3000'   --top_problem "'', 'DUW', 'DUL', 'AC', 'RRUS', 'Rectifier', 'RUS', 'BATT', 'DUS', 'OpticalCable', 'è¨ç»åé»ã«ä¼´ãé»æºä¾çµ¦',"
 2016  python eval_model_nok.py -i "../Src/ModData/feature_eri_v2_0316.csv" -o "../Src/ModData/" --target 'Prob_Urgent' --n_healthy '500, 1000, 3000'   --top_problem "'', 'DUW', 'DUL', 'AC', 'RRUS', 'Rectifier', 'RUS', 'BATT', 'DUS', 'OpticalCable', 'è¨ç»åé»ã«ä¼´ãé»æºä¾çµ¦'"
 2017  python eval_model_nok.py -i "../Src/ModData/feature_eri_v2_0316.csv" -o "../Src/ModData/" --target 'Prob_Urgent' --n_healthy '500, 1000, 3000'   --top_problem " 'DUW', 'DUL', 'RRUS', 'Rectifier', 'RUS', 'BATT', 'DUS', 'OpticalCable''"
 2018  python eval_model_nok.py -i "../Src/ModData/feature_eri_v2_0316.csv" -o "../Src/ModData/" --target 'Prob_Urgent' --n_healthy '500, 1000, 3000'   --top_problem " 'DUW', 'DUL', 'RRUS', 'Rectifier', 'RUS', 'BATT', 'DUS', 'OpticalCable'"
 2019  python eval_model_nok.py -i "../Src/ModData/feature_eri_v2_0316.csv" -o "../Src/ModData/" --target 'Prob_Urgent' --n_healthy '500, 1000, 3000'   --top_problem " 'DUW', 'DUL', 'RRUS', 'Rectifier', 'RUS', 'BATT', 'DUS', 'OpticalCable', 'Urgent'"
 2020  cp ../Src/ModData/feature_eri_v2_0316.csv ../Src/ModData/feature_eri_0321.csv
 2021  vim ../Src/ModData/feature_eri_0321.csv
 2022  python eval_model_nok.py -i "../Src/ModData/feature_eri_0321.csv" -o "../Src/ModData/" --target 'Prob_Urgent' --n_healthy '500, 1000, 3000'   --top_problem " 'DUW', 'DUL', 'RRUS', 'Rectifier', 'RUS', 'BATT', 'DUS', 'OpticalCable', 'Urgent'" --vendor 'eri'
 2023  python eval_model_nok.py -i "../Src/ModData/feature_eri_0321.csv" -o "../Src/ModData/" --target 'Prob_Urgent' --n_healthy '500, 1000, 3000'   --top_problem " 'DUW', 'DUL', 'RRUS', 'Rectifier', 'RUS', 'BATT', 'DUS', 'OpticalCable', 'Urgent'"
 2024  python eval_model_nok.py -i "../Src/ModData/feature_nok_0321.csv" -o "../Src/ModData/" --target 'Prob_Urgent' --n_healthy '500, 1000, 3000'   --top_problem "'FSME', 'Rectifier', 'FSMF', 'Breaker', 'FRGP', 'FXDA', 'BATT', 'FeederCable', 'OpticalCable', 'Urgent', 'Others'"
 2025  python eval_model_nok.py -i "../Src/ModData/feature_select_nok_0321.csv" -o "../Src/ModData/select" --target 'Prob_Urgent' --n_healthy '500, 1000, 3000'   --top_problem "'FSME', 'Rectifier', 'FSMF', 'Breaker', 'FRGP', 'FXDA', 'BATT', 'FeederCable', 'OpticalCable', 'Urgent', 'Others'"
 2026  mv eval_model_nok.py eval_model_rf.py
 2027  python eval_model.py -i "../Src/ModData/feature_nok_0321.csv" -o "../Src/ModData/" --target 'Prob_Urgent' --n_healthy '500, 1000, 3000'   --top_problem "'FSME', 'Rectifier', 'FSMF', 'Breaker', 'FRGP', 'FXDA', 'BATT', 'FeederCable', 'OpticalCable', 'Urgent', 'Others'"
 2028  python eval_model_rf.py -i "../Src/ModData/feature_nok_0321.csv" -o "../Src/ModData/" --target 'Prob_Urgent' --n_healthy '500, 1000, 3000'   --top_problem "'FSME', 'Rectifier', 'FSMF', 'Breaker', 'FRGP', 'FXDA', 'BATT', 'FeederCable', 'OpticalCable', 'Urgent', 'Others'"
 2029  python eval_model_dt.py -i "../Src/ModData/feature_nok_0321.csv" -o "../Src/ModData/" --target 'Prob_Urgent' --n_healthy '500, 1000, 3000'   --top_problem "'FSME', 'Rectifier', 'FSMF', 'Breaker', 'FRGP', 'FXDA', 'BATT', 'FeederCable', 'OpticalCable', 'Urgent', 'Others'"
 2030  python eval_model_dt.py -i "../Src/ModData/feature_select_nok_0321.csv" -o "../Src/ModData/select_" --target 'Prob_Urgent' --n_healthy '500, 1000, 3000'   --top_problem "'FSME', 'Rectifier', 'FSMF', 'Breaker', 'FRGP', 'FXDA', 'BATT', 'FeederCable', 'OpticalCable', 'Urgent', 'Others'"
 2031  onalData:_id_,_unitId_,_portNo_,_cause_-()_(Mixed_Mode)_The_alarm_is_currently_toggling.","Link_Stability@Temporary_not_in_operation,_additionalData:_id_,_unitId_,_portNo_,_cause_BER_LOS()","Link_Stability@Temporary_not_in_operation,_additionalData:_id_,_unitId_,_portNo_,_cause_BER_LOS()_(Mixed_Mode)","Link_Stability@Temporary_not_in_operation,_additionalData:_id_,_unitId_,_portNo_,_cause_BER_LOS()_(Mixed_Mode)_The_alarm_is_currently_toggling.","Link_Stability@Temporary_not_in_operation,_additionalData:_id_,_unitId_,_portNo_,_cause_BER_LOS()_The_alarm_is_currently_toggling.","Link_Stability@Temporary_not_in_operation,_additionalData:_id_,_unitId_,_portNo_,_cause_RAI_received()_(Mixed_Mode)","Link_Stability@Temporary_not_in_operation,_additionalData:_id_,_unitId_,_portNo_,_cause_RAI_received()_(Mixed_Mode)_The_alarm_is_currently_toggling.","Link_Stability@Temporary_not_in_operation,_additionalData:_id_,_unitId_,_portNo_,_cause_SFP_LOS()_(Mixed_Mode)","Link_Stability@Temporary_not_in_operation,_additionalData:_id_,_unitId_,_portNo_,_cause_SFP_not_connected()_(Mixed_Mode)","Link_Stability@Temporary_not_in_operation,_additionalData:_id_,_unitId_,_portNo_,_cause_version/revision_mismatch()_(Mixed_Mode)","Link_Stability@Temporary_not_in_operation,_additionalData:_id_,_unitId_,_portNo_,_cause_version/revision_mismatch()_(Mixed_Mode)_The_alarm_is_currently_toggling.",Link_Stability@This_alarm_is_generated_by_FM_Kernel_to_Clear_the_uncorrelated_alarm,"Link_Stability@Unexpected_values_on_Control_Words_in_CPRI_protocol,_additionalData:_id_,_unitId_,_portNo","Link_Stability@Unexpected_values_on_Control_Words_in_CPRI_protocol,_additionalData:_id_,_unitId_,_portNo_(Mixed_Mode)","Link_Stability@Unexpected_values_on_Control_Words_in_CPRI_protocol,_additionalData:_id_,_unitId_,_portNo_(Mixed_Mode)_The_alarm_is_currently_toggling.","Link_Stability@Unexpected_values_on_Control_Words_in_CPRI_protocol,_additionalData:_id_,_unitId_,_portNo_The_alarm_is_currently_toggling.",Loss_of_Sync_over_CPRI_Connections@,Loss_of_Sync_over_CPRI_Connections@This_alarm_is_generated_by_FM_Kernel_to_Clear_the_uncorrelated_alarm,MBSNo_XXXX_,MixedModeLicenseNotValid@,MixedModeLicenseNotValid@This_alarm_is_generated_by_FM_Kernel_to_Clear_the_uncorrelated_alarm,MsmmRadioPhaseNotAligned@,MsmmRadioPhaseNotAligned@This_alarm_is_generated_by_FM_Kernel_to_Clear_the_uncorrelated_alarm,NGSM_Configuration_Fault@More_than_one_node_has_the_same_Node_Priority,NGSM_Configuration_Fault@This_alarm_is_generated_by_FM_Kernel_to_Clear_the_uncorrelated_alarm,NTP_System_Time_Sync_Fault@NTP_sync_alarm,NTP_System_Time_Sync_Fault@NTP_sync_fault,NTP_System_Time_Sync_Fault@This_alarm_is_generated_by_FM_Kernel_to_Clear_the_uncorrelated_alarm,NbirLicenseNotValid@,NbirLicenseNotValid@This_alarm_is_generated_by_FM_Kernel_to_Clear_the_uncorrelated_alarm,No_Connection@(Mixed_Mode),No_Connection@AlarmResourceId:NoProcessing,No_Connection@AlarmResourceId:NoProcessing_eriAlarmNObjResourceId,No_Connection@Contact_lost_with_resource_object,No_Connection@This_alarm_is_generated_by_FM_Kernel_to_Clear_the_uncorrelated_alarm,No_Connection@Timeout:_Failed_to_get_anuConnectIndication,No_Connection@eriAlarmNObjResourceId:,None@AlarmResourceId:NoProcessing,None@AlarmResourceId:NoProcessing_eriAlarmNObjResourceId,None@This_alarm_is_generated_by_FM_Kernel_to_Clear_the_uncorrelated_alarm,"None@eriAlarmNObjMoreAdditionalText:No_SFP_plugged_in,_additionalData:_id_,_unitId_,",Number_of_HW_Entities_Mismatch@AlarmResourceId:NoProcessing,Number_of_HW_Entities_Mismatch@This_alarm_is_generated_by_FM_Kernel_to_Clear_the_uncorrelated_alarm,Number_of_HW_Entities_Mismatch@eriAlarmNObjResourceId:,PLMN_Service_Degraded@PLMN_mcc:_mnc:,PLMN_Service_Degraded@This_alarm_is_generated_by_FM_Kernel_to_Clear_the_uncorrelated_alarm,PLMN_Service_Degraded@eriAlarmNObjResourceId:,PLMN_Service_Unavailable@AlarmResourceId:NoProcessing,PLMN_Service_Unavailable@PLMN_mcc:_mnc:,"PLMN_Service_Unavailable@PLMN_mcc:_mnc:,_PLMN_mcc:_mnc:",PLMN_Service_Unavailable@PLMN_mcc:_mnc:_The_alarm_is_currently_toggling.,PLMN_Service_Unavailable@This_alarm_is_generated_by_FM_Kernel_to_Clear_the_uncorrelated_alarm,PLMN_Service_Unavailable@eriAlarmNObjResourceId:,Packet_Server_Availability_Fault@,"PciConflictDetected@PCI_Alarm,_the_same_PCI_value_has_been_reused_for_cells_that_could_interfere_with_each_other.[name:_--,_cgi:","PciConflictDetected@PCI_Alarm,_the_same_PCI_value_has_been_reused_for_cells_that_could_interfere_with_each_other.[name:_--,_cgi:_////,","PciConflictDetected@PCI_Alarm,_the_same_PCI_value_has_been_reused_for_cells_that_could_interfere_with_each_other.[name:_--,_cgi:_////,_eNodeB:","PciConflictDetected@PCI_Alarm,_the_same_PCI_value_has_been_reused_for_cells_that_could_interfere_with_each_other.[name:_--,_cgi:_////,_eNodeB:_]",Power_Failure_Left_Slot@,RF_Reflected_Power_High@Reflected_power_too_high_[_A_],Remote_IP_Address_Unreachable@,Remote_IP_Address_Unreachable@This_alarm_is_generated_by_FM_Kernel_to_Clear_the_uncorrelated_alarm,Resource_Activation_Timeout@,Resource_Activation_Timeout@Cyclic_Resetup_detected,Resource_Activation_Timeout@Cyclic_Resetup_detected_The_alarm_is_currently_toggling.,Resource_Activation_Timeout@Resource_Activation_Timeout,Resource_Activation_Timeout@This_alarm_is_generated_by_FM_Kernel_to_Clear_the_uncorrelated_alarm,Resource_Allocation_Failure@Configuration_not_valid_due_to_Channel_Bandwidth_license_shortage,Resource_Allocation_Failure@Failed_to_allocate_path,Resource_Allocation_Failure_Service_Degraded@Insufficient_RF_power_Hardware_Activation_Codes,Resource_Allocation_Failure_Service_Degraded@Insufficient_RF_power_in_radio_equipment,Resource_Allocation_Failure_Service_Degraded@Insufficient_RF_power_in_radio_equipment_The_alarm_is_currently_toggling.,Resource_Allocation_Failure_Service_Degraded@No_RF_power_Hardware_Activation_Code_available,Resource_Allocation_Failure_Service_Degraded@No_RF_power_available_in_radio_equipment,Resource_Allocation_Failure_Service_Degraded@No_RF_power_available_in_radio_equipment_The_alarm_is_currently_toggling.,Resource_Allocation_Failure_Service_Degraded@This_alarm_is_generated_by_FM_Kernel_to_Clear_the_uncorrelated_alarm,Resource_Configuration_Failure@Max_PUCCH_PRB_Pairs_per_BBM_exceeded.,SFP_Not_Present@AlarmResourceId:NoProcessing,SFP_Not_Present@This_alarm_is_generated_by_FM_Kernel_to_Clear_the_uncorrelated_alarm,"SFP_Not_Present@eriAlarmNObjMoreAdditionalText:No_SFP_plugged_in,_additionalData:_id_,_unitId","SFP_Not_Present@eriAlarmNObjMoreAdditionalText:No_SFP_plugged_in,_additionalData:_id_,_unitId_,","SFP_Not_Present@eriAlarmNObjMoreAdditionalText:No_SFP_plugged_in,_additionalData:_id_,_unitId_,_portNo",SFP_Not_Present@eriAlarmNObjResourceId:,"SFP_Stability_Problem@SFP_VCC-LOW_alarm,_additionalData:_id_,_unitId_,_portNo","SFP_Stability_Problem@SFP_VCC-LOW_alarm,_additionalData:_id_,_unitId_,_portNo_(Mixed_Mode)",SFP_Stability_Problem@This_alarm_is_generated_by_FM_Kernel_to_Clear_the_uncorrelated_alarm,SW_Error@This_alarm_is_generated_by_FM_Kernel_to_Clear_the_uncorrelated_alarm,SW_Error@eriAlarmNObjResourceId:,Service_Degraded@,Service_Degraded@Cyclic_Resetup_detected,Service_Degraded@Cyclic_Resetup_detected_The_alarm_is_currently_toggling.,Service_Degraded@Failed_to_allocate_path!,Service_Degraded@Failed_to_allocate_path!_The_alarm_is_currently_toggling.,Service_Degraded@InconsistentConfiguration,"Service_Degraded@Requested_TX-power_is_not_available_totalPowermW_=_.,_availPowermW_=_.",Service_Degraded@The_alarm_is_currently_toggling.,Service_Degraded@This_alarm_is_generated_by_FM_Kernel_to_Clear_the_uncorrelated_alarm,Service_Unavailable@,Service_Unavailable@Failed_to_allocate_path!,Service_Unavailable@Failed_to_allocate_path!_The_alarm_is_currently_toggling.,Service_Unavailable@InconsistentConfiguration,Service_Unavailable@InconsistentConfiguration_The_alarm_is_currently_toggling.,Service_Unavailable@The_alarm_is_currently_toggling.,Service_Unavailable@This_alarm_is_generated_by_FM_Kernel_to_Clear_the_uncorrelated_alarm,Service_Unavailable@eriAlarmNObjResourceId:,SfpModuleHwError@This_alarm_is_generated_by_FM_Kernel_to_Clear_the_uncorrelated_alarm,Subrack_Power_Configuration_Fault@Conflicting_power_configuration,Sync_Reference_PDV_Problem@,Sync_Reference_PDV_Problem@This_alarm_is_generated_by_FM_Kernel_to_Clear_the_uncorrelated_alarm,SystemUndervoltage@This_alarm_is_generated_by_FM_Kernel_to_Clear_the_uncorrelated_alarm,TU_Oscillator_Temperature_Fault@,TemperatureExceptionalTakenOutOfService@Temp_fault,TemperatureExceptionalTakenOutOfService@This_alarm_is_generated_by_FM_Kernel_to_Clear_the_uncorrelated_alarm,TemperatureSensorFailure@,TemperatureSensorFailure@The_alarm_is_currently_toggling.,UlCeGracePeriodExpired@LicencedCapacity=,UlCeGracePeriodExpired@This_alarm_is_generated_by_FM_Kernel_to_Clear_the_uncorrelated_alarm,"UlCeGracePeriodExpiring@LicencedCapacity=,CapacityLimitEnforceDate=--","UlCeGracePeriodStarted@problem "'FSME', 'Rectifier', 'FSMF', 'Breaker', 'FRGP', 'FXDA', 'BATT', 'FeederCable', 'OpticalCable', 'Urgent', 'Others'"
 2032  Namespace(i='../Src/ModData/feature_nok_0321.csv', n_healthy='500, 1000, 3000', n_unhealthy='400', o='../Src/ModData/', save_flag=True, str_today=None, str_vendor='nok', target='Prob_Urgent', threshold='0.5', top_problem="'FSME', 'Rectifier', 'FSMF', 'Breaker', 'FRGP', 'FXDA', 'BATT', 'FeederCable', 'OpticalCable', 'Urgent', 'Others'", use_cache=True)
 2033  #####################################
 2034  Prob_Urgent 500 400 0.5
 2035  Prob_Urgent 1000 400 0.5
 2036  Dropped cols: ['problem_area', 'Prob_FSME', 'Prob_Rectifier', 'Prob_FSMF', 'Prob_Breaker', 'Prob_FRGP', 'Prob_FXDA', 'Prob_BATT', 'Prob_FeederCable', 'Prob_OpticalCable', 'Prob_Urgent', 'Prob_Others']
 2037  target columns: Prob_Urgent
 2038  Prob_Urgent 3000 400 0.5
 2039  Dropped cols: ['problem_area', 'Prob_FSME', 'Prob_Rectifier', 'Prob_FSMF', 'Prob_Breaker', 'Prob_FRGP', 'PrDropped cols: ['problem_area', 'Probable', 'Prob_OpticalCable', 'Prob_Urgent', 'Prob_Others']Dropped cols: ['probleUrgent
 2040  RF training and test for target variable: [Prob_Urgent]
 2041  CV iteration: (0)
 2042  CV iteration: (1)
 2043  CV iteration: (2)
 2044  CV iteration: (3)
 2045  CV iteration: (4)
 2046  result is saved: ../Src/ModData/df_result_rf_nok_0result is saved: ../Src/Mo##result is saved: ../Src/ModData/df_result_rf_nok_0resu##################
 2047  (py35) 212339410@SFO1212339410M:~/Projects/SB/Python% python eval_model_rf.py -i "../Src/ModData/feature_nok_0321.csv" -o "../Src/ModData/" --target 'Prob_Urgent' --n_healthy '500, 1000, 3000'   --top_problem "'FSME', 'Rectifier', 'FSMF', 'Breaker', 'FRGP', 'FXDA', 'BATT', 'FeederCable', 'OpticalCable', 'Urgent', 'Others'"
 2048  Namespace(i='../Src/ModData/feature_nok_0321.csv', n_healthy='500, 1000, 3000', n_unhealthy='400', o='../Src/ModData/', save_flag=True, str_today=None, target='Prob_Urgent', threshold='0.5', top_problem="'FSME', 'Rectifier', 'FSMF', 'Breaker', 'FRGP', 'FXDA', 'BATT', 'FeederCable', 'OpticalCable', 'Urgent', 'Others'", use_cache=True, vendor='')
 2049  loading feature from: ../Src/ModData/feature_nok_0321.csv
 2050  (row, col) = (11521, 303)
 2051  feature cols: Index(['alarm_database_upload_in_progress',\n       'atm_interface_disabled_on_slot_port', 'atm_vcrdi_on_slot_port_vpi_vci',\n       'atm_vp       'atm_vp       'atm_vp       'atm_vp       'atm_vp       'atm_vp       'atm_vp       'atm_vp       'atm_vp  ction_local_management_connection',\n       'axc_rebooted_due_to_power_off_power_off',\n       'axc_rebooted_due_to_software_trigger_software_trigger',\n       'base_station_connectivity_degraded_transport_layer_connection_failure_in_s_interface',\n       'base_station_connectivity_degraded_transport_layer_connection_failure_in_x       'base_station_connectivity_degraded_transport_layer_connection_failure_in_x       'base_station_connectivity_degraded_transport_layer_connection_failure_in_x       'base_station_connectivity_degraded_transport_layer_connection_failure_in_x       'base_station_connectivity_degraded_transport_layer_connection_failure_in_x       'base_sta400       'base_station_connectivity_degraded_transport_layec       'baProb_FSMF', 'Prob_Breaker', 'Prob_FRGP', 'Prob_FXDA', 'Prob_BATT', 'Prob_FeederCable', 'Prob_OpticalCable', 'Prob_Urgent', 'Prob_Others']\ntarget columns: Prob_Urgent\nRF training and test for target variable: [Prob_Urgent]\nCV iteration: (0)\nCV iteration: (1)\nCV iteration: (2)\nCV iteration: (3)\nCV iteration: (4)\nProb_Urgent 1000 400 0.5\nDropped cols: ['problem_area', 'Prob_FSME', 'Prob_Rectifier', 'Prob_FSMF', 'Prob_Breaker', 'Prob_FRGP', 'Prob_FXDA', 'Prob_BATT', 'Prob_FeederCable', 'Prob_OpticalCable', 'Prob_Urgent', 'Prob_Others']\ntarget columns: Prob_Urgent\nRF training and test for target variable: [Prob_Urgent]\nCV iteration: (0)\nCV iteration: (1)\nCV iteration: (2)\nCV iteration: (3)\nCV iteration: (4)\nProb_Urgent 3000 400 0.5\nDropped cols: ['problem_area', 'Prob_FSME', 'Prob_Rectifier', 'Prob_FSMF', 'Prob_Breaker', 'Prob_FRGP', 'Prob_FXDA', 'Prob_BATT', 'Prob_FeederCable', 'Prob_OpticalCable', 'Prob_Urgent', 'Prob_Others']\ntarget columns: Prob_Urgent\nRF training and test for target variable: [Prob_Urgent]\nCV iteration: (0)\nCV iteration: (1)\nCV iteration: (2)\nCV iteration: (3)\nCV iteration: (4)\nTraceback (most recent call last):\n  File "/Users/212339410/anaconda2/envs/py35/lib/python3.5/site-packages/pandas/indexes/base.py", line 1945, in get_loc\n    return self._engine.get_loc(key)\n  File "pandas/index.pyx", line 137, in pandas.index.IndexEngine.get_loc (pandas/index.c:4154)\n  File "pandas/index.pyx", line 159, in pandas.index.IndexEngine.get_loc (pandas/index.c:4018)\n  File "pandas/hashtable.pyx", line 675, in pandas.hashtable.PyObjectHashTable.get_item (pandas/hashtable.c:12368)\n  File "pandas/hashtable.pyx", line 683, in pandas.hashtable.PyObjectHashTable.get_item (pandas/hashtable.c:12322)\nKeyError: 'F N_te'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File "eval_model_rf.py", line 147, in <module>\n    input_file=args.i, output_file=args.o)\n  File "eval_model_rf.py", line 109, in eval_model\n    df_result['Total_event_te'] = df_result['TP_te'] + df_result['F N_te']\n  File "/Users/212339410/anaconda2/envs/py35/lib/python3.5/site-packages/pandas/core/frame.py", line 1997, in __getitem__\n    return self._getitem_column(key)\n  File "/Users/212339410/anaconda2/envs/py35/lib/python3.5/site-packages/pandas/core/frame.py", line 2004, in _getitem_column\n    return self._get_item_cache(key)\n  File "/Users/212339410/anaconda2/envs/py35/lib/python3.5/site-packages/pandas/core/generic.py", line 1350, in _get_item_cache\n    values = self._data.get(item)\n  File "/Users/212339410/anaconda2/envs/py35/lib/python3.5/site-packages/pandas/core/internals.py", line 3290, in get\n    loc = self.items.get_loc(item)\n  File "/Users/212339410/anaconda2/envs/py35/lib/python3.5/site-packages/pandas/indexes/base.py", line 1947, in get_loc\n    return self._engine.get_loc(self._maybe_cast_indexer(key))\n  File "pandas/index.pyx", line 137, in pandas.index.IndexEngine.get_loc (pandas/index.c:4154)\n  File "pandas/index.pyx", line 159, in pandas.index.IndexEngine.get_loc (pandas/index.c:4018)\n  File "pandas/hashtable.pyx", line 675, in pandas.hashtable.PyObjectHashTable.get_item (pandas/hashtable.c:12368)\n  File "pandas/hashtable.pyx", line 683, in pandas.hashtable.PyObjectHashTable.get_item (pandas/hashtable.c:12322)\nKeyError: 'F N_te'\n(py35) 212339410@SFO1212339410M:~/Projects/SB/Python% python eval_model_dt.py -i "../Src/ModData/feature_nok_0321.csv" -o "../Src/ModData/" --target 'Prob_Urgent' --n_healthy '500, 1000, 3000'   --top_problem "'FSME', 'Rectifier', 'FSMF', 'Breaker', 'FRGP', 'FXDA', 'BATT', 'FeederCable', 'OpticalCable', 'Urgent', 'Others'"\nNamespace(i='../Src/ModData/feature_nok_0321.csv', n_healthy='500, 1000, 3000', n_unhealthy='400', o='../Src/ModData/', save_flag=True, str_today=None, target='Prob_Urgent', threshold='0.5', top_problem="'FSME', 'Rectifier', 'FSMF', 'Breaker', 'FRGP', 'FXDA', 'BATT', 'FeederCable', 'OpticalCable', 'Urgent', 'Others'", use_cache=True, vendor='')\nloading feature from: ../Src/ModData/feature_nok_0321.csv\n(row, col) = (11521, 303)\nfeature cols: Index(['alarm_database_upload_in_progress',\n       'atm_interface_disabled_on_slot_port', 'atm_vcrdi_on_slot_port_vpi_vci',\n       'atm_vpais_on_slot_port_vpi', 'atm_vprdi_on_slot_port_vpi',\n       'axc_in_maintenance_mode_due_to_local_management_connection_local_management_connection',\n       'axc_rebooted_due_to_power_off_power_off',\n       'axc_rebooted_due_to_software_trigger_software_trigger',\n       'base_station_connectivity_degraded_transport_layer_connection_failure_in_s_interface',\n       'base_station_connectivity_degraded_transport_layer_connection_failure_in_x_interface',\n       ...\n       'Prob_Others', 'Prob_FSME', 'Prob_OpticalCable', 'Prob_Rectifier',\n       'Prob_BATT', 'Prob_FRGP', 'Prob_FeederCable', 'Prob_FXDA', 'Prob_FSMF',\n       'Prob_Breaker'],\n      dtype='object', length=303)\n#####################################\n  Training Model\n#####################################\nProb_Urgent 500 400 0.5\nDropped cols: ['problem_area', 'Prob_FSME', 'Prob_Rectifier', 'Prob_FSMF', 'Prob_Breaker', 'Prob_FRGP', 'Prob_FXDA', 'Prob_BATT', 'Prob_FeederCable', 'Prob_OpticalCable', 'Prob_Urgent', 'Prob_Others']\ntarget columns: Prob_Urgent\nRF training and test for target variable: [Prob_Urgent]\nCV iteration: (0)\nCV iteration: (1)\nCV iteration: (2)\nCV iteration: (3)\nCV iteration: (4)\nProb_Urgent 1000 400 0.5\nDropped cols: ['problem_area', 'Prob_FSME', 'Prob_Rectifier', 'Prob_FSMF', 'Prob_Breaker', 'Prob_FRGP', 'Prob_FXDA', 'Prob_BATT', 'Prob_FeederCable', 'Prob_OpticalCable', 'Prob_Urgent', 'Prob_Others']\ntarget columns: Prob_Urgent\nRF training and test for target variable: [Prob_Urgent]\nCV iteration: (0)\nCV iteration: (1)\nCV iteration: (2)\nCV iteration: (3)\nCV iteration: (4)\nProb_Urgent 3000 400 0.5\nDropped cols: ['problem_area', 'Prob_FSME', 'Prob_Rectifier', 'Prob_FSMF', 'Prob_Breaker', 'Prob_FRGP', 'Prob_FXDA', 'Prob_BATT', 'Prob_FeederCable', 'Prob_OpticalCable', 'Prob_Urgent', 'Prob_Others']\ntarget columns: Prob_Urgent\nRF training and test for target variable: [Prob_Urgent]\nCV iteration: (0)\nCV iteration: (1)\nCV iteration: (2)\nCV iteration: (3)\nCV iteration: (4)\nTraceback (most recent call last):\n  File "/Users/212339410/anaconda2/envs/py35/lib/python3.5/site-packages/pandas/indexes/base.py", line 1945, in get_loc\n    return self._engine.get_loc(key)\n  File "pandas/index.pyx", line 137, in pandas.index.IndexEngine.get_loc (pandas/index.c:4154)\n  File "pandas/index.pyx", line 159, in pandas.index.IndexEngine.get_loc (pandas/index.c:4018)\n  File "pandas/hashtable.pyx", line 675, in pandas.hashtable.PyObjectHashTable.get_item (pandas/hashtable.c:12368)\n  File "pandas/hashtable.pyx", line 683, in pandas.hashtable.PyObjectHashTable.get_item (pandas/hashtable.c:12322)\nKeyError: 'F N_te'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File "eval_model_dt.py", line 147, in <module>\n    input_file=args.i, output_file=args.o)\n  File "eval_model_dt.py", line 109, in eval_model\n    df_result['Total_event_te'] = df_result['TP_te'] + df_result['F N_te']\n  File "/Users/212339410/anaconda2/envs/py35/lib/python3.5/site-packages/pandas/core/frame.py", line 1997, in __getitem__\n    return self._getitem_column(key)\n  File "/Users/212339410/anaconda2/envs/py35/lib/python3.5/site-packages/pandas/core/frame.py", line 2004, in _getitem_column\n    return self._get_item_cache(key)\n  File "/Users/212339410/anaconda2/envs/py35/lib/python3.5/site-packages/pandas/core/generic.py", line 1350, in _get_item_cache\n    values = self._data.get(item)\n  File "/Users/212339410/anaconda2/envs/py35/lib/python3.5/site-packages/pandas/core/internals.py", line 3290, in get\n    loc = self.items.get_loc(item)\n  File "/Users/212339410/anaconda2/envs/py35/lib/python3.5/site-packages/pandas/indexes/base.py", line 1947, in get_loc\n    return self._engine.get_loc(self._maybe_cast_indexer(key))\n  File "pandas/index.pyx", line 137, in pandas.index.IndexEngine.get_loc (pandas/index.c:4154)\n  File "pandas/index.pyx", line 159, in pandas.index.IndexEngine.get_loc (pandas/index.c:4018)\n  File "pandas/hashtable.pyx", line 675, in pandas.hashtable.PyObjectHashTable.get_item (pandas/hashtable.c:12368)\n  File "pandas/hashtable.pyx", line 683, in pandas.hashtable.PyObjectHashTable.get_item (pandas/hashtable.c:12322)\nKeyError: 'F N_te'\n(py35) 212339410@SFO1212339410M:~/Projects/SB/Python% python eval_model_dt.py -i "../Src/ModData/feature_nok_0321.csv" -o "../Src/ModData/" --target 'Prob_Urgent' --n_healthy '500, 1000, 3000'   --top_problem "'FSME', 'Rectifier', 'FSMF', 'Breaker', 'FRGP', 'FXDA', 'BATT', 'FeederCable', 'OpticalCable', 'Urgent', 'Others'"\nNamespace(i='../Src/ModData/feature_nok_0321.csv', n_healthy='500, 1000, 3000', n_unhealthy='400', o='../Src/ModData/', save_flag=True, str_today=None, target='Prob_Urgent', threshold='0.5', top_problem="'FSME', 'Rectifier', 'FSMF', 'Breaker', 'FRGP', 'FXDA', 'BATT', 'FeederCable', 'OpticalCable', 'Urgent', 'Others'", use_cache=True, vendor='')\nloading feature from: ../Src/ModData/feature_nok_0321.csv\n(row, col) = (11521, 303)\nfeature cols: Index(['alarm_database_upload_in_progress',\n       'atm_interface_disabled_on_slot_port', 'atm_vcrdi_on_slot_port_vpi_vci',\n       'atm_vpais_on_slot_port_vpi', 'atm_vprdi_on_slot_port_vpi',\n       'axc_in_maintenance_mode_due_to_local_management_connection_local_management_connection',\n       'axc_rebooted_due_to_power_off_power_off',\n       'axc_rebooted_due_to_software_trigger_software_trigger',\n       'base_station_connectivity_degraded_transport_layer_connection_failure_in_s_interface',\n       'base_station_connectivity_degraded_transport_layer_connection_failure_in_x_interface',\n       ...\n       'Prob_Others', 'Prob_FSME', 'Prob_OpticalCable', 'Prob_Rectifier',\n       'Prob_BATT', 'Prob_FRGP', 'Prob_FeederCable', 'Prob_FXDA', 'Prob_FSMF',\n       'Prob_Breaker'],\n      dtype='object', length=303)\n#####################################\n  Training Model\n#####################################\nProb_Urgent 500 400 0.5\nDropped cols: ['problem_area', 'Prob_FSME', 'Prob_Rectifier', 'Prob_FSMF', 'Prob_Breaker', 'Prob_FRGP', 'Prob_FXDA', 'Prob_BATT', 'Prob_FeederCable', 'Prob_OpticalCable', 'Prob_Urgent', 'Prob_Others']\ntarget columns: Prob_Urgent\nRF training and test for target variable: [Prob_Urgent]\nCV iteration: (0)\nCV iteration: (1)\nCV iteration: (2)\nCV iteration: (3)\nCV iteration: (4)\nProb_Urgent 1000 400 0.5\nDropped cols: ['problem_area', 'Prob_FSME', 'Prob_Rectifier', 'Prob_FSMF', 'Prob_Breaker', 'Prob_FRGP', 'Prob_FXDA', 'Prob_BATT', 'Prob_FeederCable', 'Prob_OpticalCable', 'Prob_Urgent', 'Prob_Others']\ntarget columns: Prob_Urgent\nRF training and test for target variable: [Prob_Urgent]\nCV iteration: (0)\nCV iteration: (1)\nCV iteration: (2)\nCV iteration: (3)\nCV iteration: (4)\nProb_Urgent 3000 400 0.5\nDropped cols: ['problem_area', 'Prob_FSME', 'Prob_Rectifier', 'Prob_FSMF', 'Prob_Breaker', 'Prob_FRGP', 'Prob_FXDA', 'Prob_BATT', 'Prob_FeederCable', 'Prob_OpticalCable', 'Prob_Urgent', 'Prob_Others']\ntarget columns: Prob_Urgent\nRF training and test for target variable: [Prob_Urgent]\nCV iteration: (0)\nCV iteration: (1)\nCV iteration: (2)\nCV iteration: (3)\nCV iteration: (4)\nresult is saved: ../Src/ModData/df_result_rf__0322.csv\n#####################################\n  End of process\n#####################################\n(py35) 212339410@SFO1212339410M:~/Projects/SB/Python% python eval_model_dt.py -i "../Src/ModData/feature_select_nok_0321.csv" -o "../Src/ModData/select_" --target 'Prob_Urgent' --n_healthy '500, 1000, 3000'   --top_problem "'FSME', 'Rectifier', 'FSMF', 'Breaker', 'FRGP', 'FXDA', 'BATT', 'FeederCable', 'OpticalCable', 'Urgent', 'Others'"\nNamespace(i='../Src/ModData/feature_select_nok_0321.csv', n_healthy='500, 1000, 3000', n_unhealthy='400', o='../Src/ModData/select_', save_flag=True, str_today=None, target='Prob_Urgent', threshold='0.5', top_problem="'FSME', 'Rectifier', 'FSMF', 'Breaker', 'FRGP', 'FXDA', 'BATT', 'FeederCable', 'OpticalCable', 'Urgent', 'Others'", use_cache=True, vendor='')\nloading feature from: ../Src/ModData/feature_select_nok_0321.csv\n(row, col) = (11521, 22)\nfeature cols: Index(['base_station_connectivity_degraded_transport_layer_connection_failure_in_s_interface',\n       'base_station_notification_difference_between_bts_master_clock_and_reference_frequency',\n       'base_station_notification_peer_system_module_connection_lost',\n       'external_al_ac_v_input_failure', 'external_al_air_conditioner_failure',\n       'external_al_m_canceller_failure',\n       'failure_in_wcdma_wbts_om_connection', 'synchronization_lost', 'Urgent',\n       'Semi urgent', 'problem_area', 'Prob_FSME', 'Prob_Rectifier',\n       'Prob_FSMF', 'Prob_Breaker', 'Prob_FRGP', 'Prob_FXDA', 'Prob_BATT',\n       'Prob_FeederCable', 'Prob_OpticalCable', 'Prob_Others', 'Prob_Urgent'],\n      dtype='object')\n#####################################\n  Training Model\n#####################################\nProb_Urgent 500 400 0.5\nDropped cols: ['problem_area', 'Prob_FSME', 'Prob_Rectifier', 'Prob_FSMF', 'Prob_Breaker', 'Prob_FRGP', 'Prob_FXDA', 'Prob_BATT', 'Prob_FeederCable', 'Prob_OpticalCable', 'Prob_Urgent', 'Prob_Others']\ntarget columns: Prob_Urgent\nRF training and test for target variable: [Prob_Urgent]\nCV iteration: (0)\nCV iteration: (1)\nCV iteration: (2)\nCV iteration: (3)\nCV iteration: (4)\nProb_Urgent 1000 400 0.5\nDropped cols: ['problem_area', 'Prob_FSME', 'Prob_Rectifier', 'Prob_FSMF', 'Prob_Breaker', 'Prob_FRGP', 'Prob_FXDA', 'Prob_BATT', 'Prob_FeederCable', 'Prob_OpticalCable', 'Prob_Urgent', 'Prob_Others']\ntarget columns: Prob_Urgent\nRF training and test for target variable: [Prob_Urgent]\nCV iteration: (0)\nCV iteration: (1)\nCV iteration: (2)\nCV iteration: (3)\nCV iteration: (4)\nProb_Urgent 3000 400 0.5\nDropped cols: ['problem_area', 'Prob_FSME', 'Prob_Rectifier', 'Prob_FSMF', 'Prob_Breaker', 'Prob_FRGP', 'Prob_FXDA', 'Prob_BATT', 'Prob_FeederCable', 'Prob_OpticalCable', 'Prob_Urgent', 'Prob_Others']\ntarget columns: Prob_Urgent\nRF training and test for target variable: [Prob_Urgent]\nCV iteration: (0)\nCV iteration: (1)\nCV iteration: (2)\nCV iteration: (3)\nCV iteration: (4)\nresult is saved: ../Src/ModData/select_df_result_rf__0322.csv\n#####################################\n  End of process\n#####################################\n(py35) 212339410@SFO1212339410M:~/Projects/SB/Python% python eval_model_dt.py -i "../Src/ModData/feature_select_nok_0321.csv" -o "../Src/ModData/select_" --target 'Prob_Urgent' --n_healthy '500, 1000, 3000'   --top_problem "'FSME', 'Rectifier', 'FSMF', 'Breaker', 'FRGP', 'FXDA', 'BATT', 'FeederCable', 'OpticalCable', 'Urgent', 'Others'" -exp True\nNamespace(exp='True', i='../Src/ModData/feature_select_nok_0321.csv', n_healthy='500, 1000, 3000', n_unhealthy='400', o='../Src/ModData/select_', save_flag=True, str_today=None, target='Prob_Urgent', threshold='0.5', top_problem="'FSME', 'Rectifier', 'FSMF', 'Breaker', 'FRGP', 'FXDA', 'BATT', 'FeederCable', 'OpticalCable', 'Urgent', 'Others'", use_cache=True, vendor='')\nloading feature from: ../Src/ModData/feature_select_nok_0321.csv\n(row, col) = (11521, 22)\nfeature cols: Index(['base_station_connectivity_degraded_transport_layer_connection_failure_in_s_interface',\n       'base_station_notification_difference_between_bts_master_clock_and_reference_frequency',\n       'base_station_notification_peer_system_module_connection_lost',\n       'external_al_ac_v_input_failure', 'external_al_air_conditioner_failure',\n       'external_al_m_canceller_failure',\n       'failure_in_wcdma_wbts_om_connection', 'synchronization_lost', 'Urgent',\n       'Semi urgent', 'problem_area', 'Prob_FSME', 'Prob_Rectifier',\n       'Prob_FSMF', 'Prob_Breaker', 'Prob_FRGP', 'Prob_FXDA', 'Prob_BATT',\n       'Prob_FeederCable', 'Prob_OpticalCable', 'Prob_Others', 'Prob_Urgent'],\n      dtype='object')\n#####################################\n  Training Model\n#####################################\nProb_Urgent 500 400 0.5\nDropped cols: ['problem_area', 'Prob_FSME', 'Prob_Rectifier', 'Prob_FSMF', 'Prob_Breaker', 'Prob_FRGP', 'Prob_FXDA', 'Prob_BATT', 'Prob_FeederCable', 'Prob_OpticalCable', 'Prob_Urgent', 'Prob_Others']\ntarget columns: Prob_Urgent\nRF training and test for target variable: [Prob_Urgent]\nCV iteration: (0)\nCV iteration: (1)\nCV iteration: (2)\nCV iteration: (3)\nCV iteration: (4)\nProb_Urgent 1000 400 0.5\nDropped cols: ['problem_area', 'Prob_FSME', 'Prob_Rectifier', 'Prob_FSMF', 'Prob_Breaker', 'Prob_FRGP', 'Prob_FXDA', 'Prob_BATT', 'Prob_FeederCable', 'Prob_OpticalCable', 'Prob_Urgent', 'Prob_Others']\ntarget columns: Prob_Urgent\nRF training and test for target variable: [Prob_Urgent]\nCV iteration: (0)\nCV iteration: (1)\nCV iteration: (2)\nCV iteration: (3)\nCV iteration: (4)\nProb_Urgent 3000 400 0.5
 2052  conda install -c conda-forge pydotplus
 2053  python eval_model_rf.py -i "../Src/ModData/feature_nok_0321.csv" -o "../Src/ModData/" --target 'Prob_Urgent' --n_healthy '500, 1000, 3000'   --top_problem "'FSME', 'Rectifier', 'FSMF', 'Breaker', 'FRGP', 'FXDA', 'BATT', 'FeederCable', 'OpticalCable', 'Urgent', 'Others'" --vendor 'nok'
 2054  python eval_model.py -i "../Src/ModData/feature_eri_0321.csv" -o "../Src/ModData/" --target 'Prob_Urgent' --n_healthy '500, 1000, 3000'   --top_problem " 'DUW', 'DUL', 'RRUS', 'Rectifier', 'RUS', 'BATT', 'DUS', 'OpticalCable', 'Urgent'" --vendor 'eri\n'
 2055  python eval_model.py -i "../Src/ModData/feature_eri_0321.csv" -o "../Src/ModData/" --target 'Prob_Urgent' --n_healthy '500, 1000, 3000'   --top_problem " 'DUW', 'DUL', 'RRUS', 'Rectifier', 'RUS', 'BATT', 'DUS', 'OpticalCable', 'Urgent'" --vendor 'eri'\n'\n'
 2056  python eval_model.py -i "../Src/ModData/feature_eri_0321.csv" -o "../Src/ModData/" --target 'Prob_Urgent' --n_healthy '500, 1000, 3000'   --top_problem " 'DUW', 'DUL', 'RRUS', 'Rectifier', 'RUS', 'BATT', 'DUS', 'OpticalCable', 'Urgent'" --vendor "eri"
 2057  python eval_model_rf.py -i "../Src/ModData/feature_eri_0321.csv" -o "../Src/ModData/" --target 'Prob_Urgent' --n_healthy '500, 1000, 3000'   --top_problem " 'DUW', 'DUL', 'RRUS', 'Rectifier', 'RUS', 'BATT', 'DUS', 'OpticalCable', 'Urgent'" --vendor "eri"
 2058  brew install graphviz
 2059  python eval_model_dt.py -i "../Src/ModData/feature_select_nok_0321.csv" -o "../Src/ModData/select_" --target 'Prob_Urgent' --n_healthy '500, 1000, 3000'   --top_problem "'FSME', 'Rectifier', 'FSMF', 'Breaker', 'FRGP', 'FXDA', 'BATT', 'FeederCable', 'OpticalCable', 'Urgent', 'Others'" -exp True
 2060  python eval_model_dt.py -i "../Src/ModData/feature_nok_0321.csv" -o "../Src/ModData/" --target 'Prob_Urgent' --n_healthy '500, 1000, 3000'   --top_problem "'FSME', 'Rectifier', 'FSMF', 'Breaker', 'FRGP', 'FXDA', 'BATT', 'FeederCable', 'OpticalCable', 'Urgent', 'Others'" -exp True
 2061  python eval_model_dt.py -i "../Src/ModData/feature_nok_0321.csv" -o "../Src/ModData/" --target 'Prob_Urgent' --n_healthy '500, 1000, 3000'   --top_problem "'FSME', 'Rectifier', 'FSMF', 'Breaker', 'FRGP', 'FXDA', 'BATT', 'FeederCable', 'OpticalCable', 'Urgent', 'Others'" -exp True --vendor 'nok'
 2062  python eval_model_dt.py -i "../Src/ModData/feature_select_nok_0321.csv" -o "../Src/ModData/select_" --target 'Prob_Urgent' --n_healthy '500, 1000, 3000'   --top_problem "'FSME', 'Rectifier', 'FSMF', 'Breaker', 'FRGP', 'FXDA', 'BATT', 'FeederCable', 'OpticalCable', 'Urgent', 'Others'" -exp True --vendor 'nok'
 2063  python eval_model_dt.py -i "../Src/ModData/feature_eri_0321.csv" -o "../Src/ModData/" --target 'Prob_Urgent' --n_healthy '500, 1000, 3000'   --top_problem " 'DUW', 'DUL', 'RRUS', 'Rectifier', 'RUS', 'BATT', 'DUS', 'OpticalCable', 'Urgent'" --vendor "eri"
 2064  python eval_model_dt.py -i "../Src/ModData/feature_eri_0321.csv" -o "../Src/ModData/" --target 'Prob_Urgent' --n_healthy '500, 1000, 3000'   --top_problem " 'DUW', 'DUL', 'RRUS', 'Rectifier', 'RUS', 'BATT', 'DUS', 'OpticalCable', 'Urgent'" --vendor "eri" --exp True
 2065  python eval_model_dt.py -i "../Src/ModData/feature_eri_0321.csv" -o "../Src/ModData/" --target 'Prob_Urgent' --n_healthy '500, 1000, 3000'   --top_problem " 'DUW', 'DUL', 'RRUS', 'Rectifier', 'RUS', 'BATT', 'DUS', 'OpticalCable', 'Urgent'" --vendor "eri" -exp True
 2066  python eval_model_dt.py -i "../Src/ModData/feature_eri_0321.csv" -o "../Src/ModData/" --target 'Prob_Urgent' --n_healthy '500, 1000, 3000'   --top_problem " 'DUW', 'DUL', 'RRUS', 'Rectifier', 'RUS', 'BATT', 'DUS', 'OpticalCable', 'Others','Prob_L2SW', 'Urgent'" --vendor "eri" -exp True
 2067  python eval_model_dt.py -i "../Src/ModData/feature_eri_0321.csv" -o "../Src/ModData/" --target 'Prob_Urgent' --n_healthy '500, 1000, 3000'   --top_problem " 'DUW', 'DUL', 'RRUS', 'Rectifier', 'RUS', 'BATT', 'DUS', 'OpticalCable', 'Others','L2SW', 'Urgent'" --vendor "eri" -exp True
 2068  python eval_model_dt.py -i "../Src/ModData/feature_eri_0321.csv" -o "../Src/ModData/" --target 'Prob_Urgent' --n_healthy '500, 1000, 3000'   --top_problem " 'DUW', 'DUL', 'RRUS', 'Rectifier', 'RUS', 'BATT', 'DUS', 'OpticalCable', 'Others','L2SW', 'æ¬é', 'FeederCable','Urgent'" --vendor "eri" -exp True
 2069  python eval_model_dt.py -i "../Src/ModData/feature_eri_0321.csv" -o "../Src/ModData/setcolor_" --target 'Prob_Urgent' --n_healthy '1000'   --top_problem " 'DUW', 'DUL', 'RRUS', 'Rectifier', 'RUS', 'BATT', 'DUS', 'OpticalCable', 'Others','L2SW', 'æ¬é', 'FeederCable','Urgent'" --vendor "eri" -exp True
 2070  python eval_model_dt.py -i "../Src/ModData/feature_eri_0321.csv" -o "../Src/ModData/setcolor_" --target 'Prob_Urgent' --n_healthy '500 1000 3000'   --top_problem " 'DUW', 'DUL', 'RRUS', 'Rectifier', 'RUS', 'BATT', 'DUS', 'OpticalCable', 'Others','L2SW', 'æ¬é', 'FeederCable','Urgent'" --vendor "eri" -exp True
 2071  python eval_model_dt.py -i "../Src/ModData/feature_eri_0321.csv" -o "../Src/ModData/setcolor_" --target 'Prob_Urgent' --n_healthy '500, 1000, 3000'   --top_problem " 'DUW', 'DUL', 'RRUS', 'Rectifier', 'RUS', 'BATT', 'DUS', 'OpticalCable', 'Others','L2SW', 'æ¬é', 'FeederCable','Urgent'" --vendor "eri" -exp True
 2072  python eval_model_rf.py -i "../Src/ModData/feature_nok_0321.csv" -o "../Src/ModData/select_" --target 'Prob_Urgent, Prob_FSME, Prob_FSMF, Prob_FXDA' --n_healthy '1000'   --top_problem " 'DUW', 'DUL', 'RRUS', 'Rectifier', 'RUS', 'BATT', 'DUS', 'OpticalCable', 'Others','L2SW', 'æ¬é', 'FeederCable','Urgent'" --vendor "eri" -exp True
 2073  python eval_model_dt.py -i "../Src/ModData/feature_eri_0321.csv" -o "../Src/ModData/setcolor_" --target 'Prob_Urgent' --n_healthy '500, 1000, 3000'   --top_problem " 'DUW', 'DUL', 'RRUS', 'Rectifier', 'RUS', 'BATT', 'DUS', 'OpticalCable', 'Others','L2SW', 'æ¬é', 'FeederCable','Urgent'" --vendor "eri" -exp True --plot_imp True
 2074  ls /Library/LaunchAgents
 2075  ls /Library/LaunchDaemons
 2076  l ~/Library/LaunchAgents
 2077  vim ~/Library/LaunchAgents/org.virtualbox.vboxwebsrv.plist
 2078  pip install percol
 2079  percol /var/log/syslog
 2080  man percol
 2081  percol_cd_history
 2082  zle -l
 2083  which -a echo
 2084  ../Notebook
 2085  pip install plotly --upgrade
 2086  cp preprocessing_bsw_20151201_20170201.ipynb preprocessing_nedb_201607.ipynb
 2087  cd Input
 2088  wc -l NEDB_BASE_INFO.20160721085837_2.utf8.csv
 2089  ls | wc -l 
 2090  ls | wc -l
 2091  wc -l $(ls)
 2092  man wc
 2093  awk -F ',' 'print NF; exit' NEDB_BASE_INFO.20160721085837_2.utf8.csv
 2094  head -1 NEDB_
 2095  head -1 NEDB_BASE_INFO.20160721085837_2.utf8.csv| tr ',' '\n' | wc -l
 2096  head -1 $(ls) | tr ',' '\n' | wc -l
 2097  wc man awk
 2098  man awk
 2099  awk -F ',' '' NEDB_BASE_INFO.20160721085837_2.utf8.csv
 2100  awk -F ',' '{print NF; exit}' NEDB_BASE_INFO.20160721085837_2.utf8.csv
 2101  awk -F ',' '{print NF; exit}' NEDB_ERI_T.utf8.csv
 2102  awk -F ',' '{print NF; exit}' NEDB_NOK_Q.utf8.csv
 2103  awk -F ',' '{print NF; exit}' NEDB_NOKERI_TQ.utf8_st_drop.csv
 2104  python feature_create.py --vendor 'nok' --lb 14 --la 1 --nh 500 --nu 400 --sample_days 40 --save True --cache True --i '../Src/ModData/' --o '../Src/ModData/test'
 2105  python feature_create.py --vendor 'nok' --lb 14 --la 1 --nh 500 --nu 400 --sample_days 40 --save True --cache True --i '../Src/ModData/' --o '../Src/ModData/'
 2106  pip install --upgrade gensim
 2107  jumanpp
 2108  brew install jumanpp
 2109  pip install six
 2110  tar xvf pyknp-0.3.tar.gz
 2111  cd pyknp-0.3
 2112  rm pyknp-0.3.tar.gz
 2113  rm -rf pyknp-0.3
 2114  sudo rm -rf pyknp-0.3
 2115  jumanpp -v
 2116  mkdir knp
 2117  cd knp
 2118  wget http://nlp.ist.i.kyoto-u.ac.jp/nl-resource/knp/knp-4.16.tar.bz2
 2119  knp
 2120  cd Src/ModData/
 2121  vim df_dt_result_rf_eri_0322.csv
 2122  vim df_event_uh_proc_test_nok_lbh14_lah1_0321.csv
 2123  vim df_event_uh_proc_test_nok_lbh14_lah1_0323.csv
 2124  vim df_event_uh_proc_nok_lbh14_lah1_0323.csv
 2125  python feature_create.py --vendor 'nok' --lb 14 --la 1 --nh 500 --nu 400 --sample_days 40 --save True --cache False  --o '../Src/ModData/'
 2126  vim NEDB_NOKERI_TQ.utf8.csv
 2127  python eval_model_dt.py -i "../Src/ModData/feature_eri_0321.csv" -o "../Src/ModData/setcolor_" --target 'Prob_Urgent, DUW, DUL, RRUS, BATT, Rectifier, L2SW' --n_healthy '1000'   --top_problem " 'DUW', 'DUL', 'RRUS', 'Rectifier', 'RUS', 'BATT', 'DUS', 'OpticalCable', 'Others','L2SW', 'æ¬é', 'FeederCable','Urgent'" --vendor "eri" -exp True --plot_imp True
 2128  python eval_model_dt.py -i "../Src/ModData/feature_eri_0321.csv" -o "../Src/ModData/setcolor_" --target 'Prob_Urgent, DUW, DUL, RRUS, BATT, Rectifier, L2SW' --n_healthy '1000'   --top_problem " 'DUW', 'DUL', 'RRUS', 'Rectifier', 'RUS', 'BATT', 'DUS', 'OpticalCable', 'Others','L2SW', 'æ¬é', 'FeederCable','Urgent'" --vendor "eri" -exp True --export_tree True
 2129  python eval_model_dt.py -i "../Src/ModData/feature_eri_0321.csv" -o "../Src/ModData/setcolor_" --target 'Prob_Urgent, DUW, DUL, RRUS, BATT, Rectifier, L2SW' --n_healthy '1000'   --top_problem " 'DUW', 'DUL', 'RRUS', 'Rectifier', 'RUS', 'BATT', 'DUS', 'OpticalCable', 'Others','L2SW', 'æ¬é', 'FeederCable','Urgent'" --vendor "eri" -exp True --exp True
 2130  python eval_model_dt.py -i "../Src/ModData/feature_eri_0321.csv" -o "../Src/ModData/setcolor_" --target 'Prob_Urgent, DUW, DUL, RRUS, BATT, Rectifier, L2SW' --n_healthy '1000'   --top_problem " 'DUW', 'DUL', 'RRUS', 'Rectifier', 'RUS', 'BATT', 'DUS', 'OpticalCable', 'Others','L2SW', 'æ¬é', 'FeederCable','Urgent'" --vendor "eri" -exp True 
 2131  python eval_model_dt.py -i "../Src/ModData/feature_eri_0321.csv" -o "../Src/ModData/setcolor_" --target 'Prob_Urgent, Prob_DUW, Prob_DUL, Pro_RRUS, Prob_BATT, Prob_Rectifier, Prob_L2SW' --n_healthy '1000'   --top_problem " 'DUW', 'DUL', 'RRUS', 'Rectifier', 'RUS', 'BATT', 'DUS', 'OpticalCable', 'Others','L2SW', 'æ¬é', 'FeederCable','Urgent'" --vendor "eri" -exp True 
 2132  python eval_model_dt.py -i "../Src/ModData/feature_eri_0321.csv" -o "../Src/ModData/setcolor_" --target 'Prob_Urgent, Prob_DUW, Prob_DUL, Prob_RRUS, Prob_BATT, Prob_Rectifier, Prob_L2SW' --n_healthy '1000'   --top_problem " 'DUW', 'DUL', 'RRUS', 'Rectifier', 'RUS', 'BATT', 'DUS', 'OpticalCable', 'Others','L2SW', 'æ¬é', 'FeederCable','Urgent'" --vendor "eri" -exp True 
 2133  python eval_model_dt.py -i "../Src/ModData/feature_eri_0321.csv" -o "../Src/ModData/setcolor_" --target 'Prob_Urgent, Prob_DUW, Prob_DUL, Pro_RRUS, Prob_RUS, Prob_L2SW' --n_healthy '1000'   --top_problem " 'DUW', 'DUL', 'RRUS', 'Rectifier', 'RUS', 'BATT', 'DUS', 'OpticalCable', 'Others','L2SW', 'æ¬é', 'FeederCable','Urgent'" --vendor "eri" -exp True 
 2134  python eval_model_dt.py -i "../Src/ModData/feature_eri_0321.csv" -o "../Src/ModData/setcolor_" --target 'Prob_Urgent, Prob_DUW, Prob_DUL, Prob_RRUS, Prob_RUS, Prob_L2SW' --n_healthy '1000'   --top_problem " 'DUW', 'DUL', 'RRUS', 'Rectifier', 'RUS', 'BATT', 'DUS', 'OpticalCable', 'Others','L2SW', 'æ¬é', 'FeederCable','Urgent'" --vendor "eri" -exp True 
 2135  python eval_model_dt.py -i "../Src/ModData/feature_eri_0321.csv" -o "../Src/ModData/setcolor_" --target 'Prob_Urgent, Prob_DUW, Prob_DUL, Prob_RRUS, Prob_RUS' --n_healthy '1000'   --top_problem " 'DUW', 'DUL', 'RRUS', 'Rectifier', 'RUS', 'BATT', 'DUS', 'OpticalCable', 'Others','L2SW', 'æ¬é', 'FeederCable','Urgent'" --vendor "eri" -exp True 
 2136  python eval_model_dt.py -i "../Src/ModData/feature_eri_0321.csv" -o "../Src/ModData/setcolor_" --target "Prob_Urgent, Prob_DUW, Prob_DUL, Prob_RRUS, Prob_RUS" --n_healthy '1000'   --top_problem "'DUW', 'DUL', 'RRUS', 'Rectifier', 'RUS', 'BATT', 'DUS', 'OpticalCable', 'Others','L2SW', 'æ¬é', 'FeederCable','Urgent'" --vendor "nok" -exp True 
 2137  python eval_model_dt.py -i "../Src/ModData/feature_nok_0321.csv" -o "../Src/ModData/setcolor_" --target 'Prob_Urgent, Prob_FSME, Prob_Rectifier, Prob_FSMF', Prob_Breaker, Prob_FRGP, Prob_FXDA, Prob_BATT, Prob_FeederCable, prob_OpticalCable, Prob_Others' --n_healthy '1000'   --top_problem "'FSME', 'Rectifier', 'FSMF', 'Breaker', 'FRGP', 'FXDA', 'BATT', 'FeederCable', 'OpticalCable', 'Others', 'non-urgent'"  --vendor "nok" -exp True --plot_imp True
 2138  python eval_model_dt.py -i "../Src/ModData/feature_nok_0321.csv" -o "../Src/ModData/setcolor_" --target "Prob_Urgent, Prob_FSME, Prob_Rectifier, Prob_FSMF, Prob_Breaker, Prob_FRGP, Prob_FXDA, Prob_BATT, Prob_FeederCable, prob_OpticalCable, Prob_Others" --n_healthy '1000'   --top_problem "'FSME', 'Rectifier', 'FSMF', 'Breaker', 'FRGP', 'FXDA', 'BATT', 'FeederCable', 'OpticalCable', 'Others', 'non-urgent'"  --vendor "nok" -exp True --plot_imp True
 2139  python eval_model_dt.py -i "../Src/ModData/feature_nok_0321.csv" -o "../Src/ModData/setcolor_" --target "Prob_Urgent, Prob_FSME, Prob_Rectifier, Prob_FSMF, Prob_Breaker, Prob_FRGP, Prob_FXDA, Prob_BATT, Prob_FeederCable, prob_OpticalCable, Prob_Others" --n_healthy '1000'   --top_problem "'FSME', 'Rectifier', 'FSMF', 'Breaker', 'FRGP', 'FXDA', 'BATT', 'FeederCable', 'OpticalCable', 'Others', 'non-urgent'"  --vendor "nok" -exp True 
 2140  python eval_model_dt.py -i "../Src/ModData/feature_nok_0321.csv" -o "../Src/ModData/setcolor_" --target "Prob_Urgent, Prob_FSME, Prob_Rectifier, Prob_FSMF, Prob_Breaker, Prob_FRGP, Prob_FXDA, Prob_BATT, Prob_FeederCable, prob_OpticalCable, Prob_Others" --n_healthy '1000'   --top_problem "'FSME', 'Rectifier', 'FSMF', 'Breaker', 'FRGP', 'FXDA', 'BATT', 'FeederCable', 'OpticalCable', 'Others', 'Urgent'"  --vendor "nok" -exp True 
 2141  python eval_model_dt.py -i "../Src/ModData/feature_nok_0321.csv" -o "../Src/ModData/setcolor_" --target "Prob_Urgent, Prob_FSME, Prob_Rectifier, Prob_FSMF, Prob_Breaker, Prob_FRGP, Prob_FXDA, Prob_BATT, Prob_FeederCable, prob_OpticalCable, Prob_Others" --n_healthy '1000  --vendor "nok" -exp True 
 2142  python eval_model_dt.py -i "../Src/ModData/feature_nok_0321.csv" -o "../Src/ModData/setcolor_" --target "Prob_Urgent, Prob_FSME, Prob_Rectifier, Prob_FSMF, Prob_Breaker, Prob_FRGP, Prob_FXDA, Prob_BATT, Prob_FeederCable, prob_OpticalCable, Prob_Others" --n_healthy '1000'  --vendor "nok" -exp True 
 2143  python eval_model_dt.py -i "../Src/ModData/feature_eri_0321.csv" -o "../Src/ModData/setcolor_" --target "Prob_Urgent, Prob_DUW, Prob_DUL, Prob_RRUS, Prob_RUS" --n_healthy '1000'   --top_problem "'DUW', 'DUL', 'RRUS', 'Rectifier', 'RUS', 'BATT', 'DUS', 'OpticalCable', 'Others','L2SW', 'æ¬é', 'FeederCable','Urgent'" --vendor "eri" -exp True 
 2144  locate docker
 2145  locate docker | grep agent
 2146  locate docker | grep deamon
 2147  locate docker | grep start
 2148  less launchctl list launchctl list 
 2149  launchctl list 
 2150  launchctl list | pulse
 2151  lanchctl remove com.docker.docker.238112
 2152  launchctl remove com.docker.docker.238112
 2153  launchctl list | grep docker
 2154  launchctl remove net.pulsesecure.pulsetray
 2155  launchctl list | grep pulse
 2156  launchctl list | grep net.pulsesecure.Pulse-Secure.259872
 2157  launchctl remove net.pulsesecure.Pulse-Secure.259872
 2158  launchctl list | grep jetbeans
 2159  launchctl list | grep pycharm
 2160  launchctl list | grep toolbox
 2161  python eval_model_dt.py -i "../Src/ModData/feature_nok_0321.csv" -o "../Src/ModData/setcolor_" --target "Prob_Urgent, Prob_FSME, Prob_Rectifier, Prob_FSMF, Prob_Breaker, Prob_FRGP, Prob_FXDA, Prob_BATT, Prob_FeederCable, Prob_OpticalCable, Prob_Others" --n_healthy '1000'  --vendor "nok" -exp True 
 2162  python eval_model_rf.py -i "../Src/ModData/feature_nok_0321.csv" -o "../Src/ModData/RF_10tree_0324/" --target "Prob_Urgent, Prob_FSME, Prob_Rectifier, Prob_FSMF, Prob_Breaker, Prob_FRGP, Prob_FXDA, Prob_BATT, Prob_FeederCable, Prob_OpticalCable, Prob_Others" --n_healthy '1000'  --vendor "nok" -exp True  --exp True
 2163  mkdir ../Src/ModData/RF_10tree_0324
 2164  python eval_model_rf.py -i "../Src/ModData/feature_nok_0321.csv" -o "../Src/ModData/RF_10tree_0324/" --target "Prob_Urgent, Prob_FSME, Prob_Rectifier, Prob_FSMF, Prob_Breaker, Prob_FRGP, Prob_FXDA, Prob_BATT, Prob_FeederCable, Prob_OpticalCable, Prob_Others" --n_healthy '1000'  --vendor "nok" -exp True  -exp True
 2165  python eval_model_rf.py -i "../Src/ModData/feature_nok_0321.csv" -o "../Src/ModData/RF_10tree_0324/" --target "Prob_Urgent, Prob_FSME, Prob_Rectifier, Prob_FSMF, Prob_Breaker, Prob_FRGP, Prob_FXDA, Prob_BATT, Prob_FeederCable, Prob_OpticalCable, Prob_Others" --n_healthy '1000'  --vendor "nok" -exp True  -exp True 
 2166  python eval_model_rf.py -i "../Src/ModData/feature_nok_0321.csv" -o "../Src/ModData/RF_10tree_0324/" --target "Prob_Urgent, Prob_FSME, Prob_Rectifier, Prob_FSMF, Prob_Breaker, Prob_FRGP, Prob_FXDA, Prob_BATT, Prob_FeederCable, Prob_OpticalCable, Prob_Others" --n_healthy '1000'  --vendor "nok" -exp True  
 2167  python eval_model_rf.py -i "../Src/ModData/feature_nok_0321.csv" -o "../Src/ModData/RF_10tree_0324/" --target "Prob_Urgent" --n_healthy '1000'  --vendor "nok" -exp True  
 2168  python eval_model_rf.py -i "../Src/ModData/feature_eri_0321.csv" -o "../Src/ModData/RF_10tree_0324/" --target "Prob_Urgent" --n_healthy '1000'  --vendor "eri" -exp True  
 2169  pip install jython
 2170  python reduce_dimension.py
 2171  python reduce_dimension.py -h
 2172  ls ../Src/ModData
 2173  python reduce_dimension.py -i ../Src/ModData/testfeature_nok_lbh14_lah1_0323.csv
 2174  python reduce_dimension.py -i ../Src/ModData/testfeature_nok_lbh14_lah1_0323.csv -plot 1 -o 'test_dim_reduction0325' 
 2175  python -c 'import qutils; qutils.col('../Src/ModData/testfeature_nok_lbh14_lah1_0323.csv')'
 2176  python -c "import qutils; qutils.col('../Src/ModData/testfeature_nok_lbh14_lah1_0323.csv')"
 2177  python qutils -col -i ../Src/ModData/feature_eri_0321.csv
 2178  python qutils.py -col -i ../Src/ModData/feature_eri_0321.csv
 2179  python qutils.py -col '' -i ../Src/ModData/feature_eri_0321.csv
 2180  python qutils.py -col 1 -i ../Src/ModData/feature_eri_0321.csv
 2181  python reduce_dimension.py -i ../Src/ModData/testfeature_nok_lbh14_lah1_0323.csv -plot 1 -o 'test_dim_reduction0325'  -drop ''problem_area'\n 'Prob_BATT' 'Prob_DUL' 'Prob_DUS' 'Prob_DUW' 'Prob_FeederCable'\n 'Prob_L2SW' 'Prob_OpticalCable' 'Prob_Others' 'Prob_RRU' 'Prob_RRUS'
 2182  python reduce_dimension.py -i ../Src/ModData/testfeature_nok_lbh14_lah1_0323.csv -plot 1 -o 'test_dim_reduction0325'  -drop "'problem_area'\n 'Prob_BATT' 'Prob_DUL' 'Prob_DUS' 'Prob_DUW' 'Prob_FeederCable'\n 'Prob_L2SW' 'Prob_OpticalCable' 'Prob_Others' 'Prob_RRU' 'Prob_RRUS'"
 2183  python reduce_dimension.py -i ../Src/ModData/feature_eri_0321.csv  -plot 1 -o 'test_dim_reduction0325'  -drop "'problem_area' 'Prob_BATT' 'Prob_DUL' 'Prob_DUS' 'Prob_DUW' 'Prob_FeederCable' 'Prob_L2SW' 'Prob_OpticalCable' 'Prob_Others' 'Prob_RRU' 'Prob_RRUS'"
 2184  python reduce_dimension.py -n_comp 20 -i ../Src/ModData/feature_eri_0321.csv  -plot 1 -o 'test_dim_reduction0325'  -drop "'problem_area' 'Prob_BATT' 'Prob_DUL' 'Prob_DUS' 'Prob_DUW' 'Prob_FeederCable' 'Prob_L2SW' 'Prob_OpticalCable' 'Prob_Others' 'Prob_RRU' 'Prob_RRUS'"
 2185  less com.jamfsoftware.jamf.agent.plist
 2186  less net.pulsesecure.pulsetray.plist 
 2187  vim net.pulsesecure.pulsetray.plist
 2188  sudo vim net.pulsesecure.pulsetray.plist
 2189  less org.macosforge.xquartz.startx.plist
 2190  vim com.docker.vmnetd.plist
 2191  vim net.pulsesecure.
 2192  vim net.pulsesecure.AccessService.plist
 2193  vim net.pulsesecure.UninstallPulse.plist
 2194  vim com.jetbrains.toolbox.plist
 2195  cd /System/Library/LaunchAgents
 2196  ../LaunchDaemons
 2197  cd ~/Library/LaunchAgents
 2198  vim com.google.keystone.agent.plist
 2199  cd test/Â¥
 2200  cd test/
 2201  l seq-test.txt
 2202  less seq-test.txt
 2203  vim seq-test.txt
 2204  cd /Library/Launch
 2205  cd /Library/LaunchAgents
 2206  cd ../LaunchDaemons
 2207  cd ~/Projects/SB/
 2208  cd ~/Documents/Analytics/Python/tstk
 2209  locate git/contrib/
 2210  locate git | grep contrib
 2211  locate git | grep completion
 2212  source ~/.zshrch
 2213  locate git-completion.zsh
 2214  brew install bash-completion
 2215  curl -o ~/.git-prompt.sh \\n    https://raw.githubusercontent.com/git/git/master/contrib/completion/git-prompt.sh
 2216  source ~/.git-prompt.sh
 2217  echo $ZSH
 2218  cd ../Python/tstk
 2219  PROMPT
 2220  PROMPT='%n'
 2221  PROMPT='%n@'
 2222  PROMPT='%n@%m'
 2223  PROMPT='%n@%#'
 2224  PROMPT='%n@%m%#'
 2225  PROMPT='%n@%m%# '
 2226  python -c 'import plotly'
 2227  PROMPT hiro
 2228  PROMPT=hiro
 2229  git noproxy
 2230  vim ../Src/ModData/testfeature_nok_lbh14_lah1_0323.csv
 2231  python feature_create.py --vendor 'nok' --lb 14 --la 1 --nh 500 --nu 400 --sample_days 40 --save False --cache False  --o '../Src/ModData/'
 2232  vim ../Src/ModData/df_event_uh_proc_nok_lbh14_lah1_0323.csv
 2233  head -n 100  ../Src/ModData/df_event_uh_proc_nok_lbh14_lah1_0323.csv > tmp100.csv
 2234  open tmp100.csv
 2235  head -n 100  ../Src/ModData/df_event_uh_proc_nok_lbh14_lah1_0323.csv
 2236  python feature_create.py --vendor 'nok' --lb 14 --la 1 --nh 500 --nu 400 --sample_days 40 --save False --cache True  --o '../Src/ModData/'
 2237  head ../Src/ModData/feature_nok_lbh14_lah1_0326.csv
 2238  head '../Src/ModData/feature_nok_lbh14_lah1_0326.csv'
 2239  open temp.csv
 2240  head Src/ModData/df_event_uh_proc_nok_lbh14_lah1_0326.csv > temp.csv
 2241  less Src/ModData/df_event_uh_proc_nok_lbh14_lah1_0326.csv
 2242  vim ../Src/ModData/feature_nok_lbh14_lah1_0326.csv
 2243  pip install stop_words
 2244  pip install brew install jython
 2245  brew install jython
 2246  mv -r ~/Downloads/UCanAccess-4.0.1-bin ~/.local/bin/
 2247  mv  ~/Downloads/UCanAccess-4.0.1-bin ~/.local/bin/
 2248  mv ~/Downloads/ERI_YM_20170212.accdb ~/Projects/SB/Src/Original_flat/
 2249  cp preprocessing_oss_nok_20170214_20170227.ipynb preprocessing_oss_nok_20170327.ipynb
 2250  mv preprocessing_oss_nok_20170327.ipynb preprocessing_oss_nok_20170221_20170317.ipynb
 2251  ls prep*
 2252  cd 20170327_Ingestion/Input/
 2253  vim BSW_
 2254  vim BSW_ERI_20170301_20170327.csv
 2255  python feature_create.py --vendor 'nok' --lb 14 --la 1 --nh 500 --nu 400 --sample_days 40 --save True  --cache True  --o '../Src/ModData/'
 2256  vim ../Src/ModData/feature_nok_lbh14_lah1_0327.csv
 2257  python feature_create.py --vendor 'eri' --lb 14 --la 1 --nh 500 --nu 400 --sample_days 40 --save True  --cache True  --o '../Src/ModData/'
 2258  vim ../Src/ModData/feature_eri_lbh14_lah1_0327.csv
 2259  mkdir ../Src/ModData/feature0327
 2260  cd ../Src/ModData/feature0327
 2261  python feature_create.py --vendor 'eri' --lb 7 --la 1 --nh 500 --nu 400 --sample_days 40 --save True  --cache True  --o '../Src/ModData/feature0327/'
 2262  python feature_create.py --vendor 'eri' --lb 7 --la 8 --nh 500 --nu 400 --sample_days 40 --save True  --cache True  --o '../Src/ModData/feature0327/'
 2263  python feature_create.py --vendor 'nok' --lb 7 --la 1 --nh 500 --nu 400 --sample_days 40 --save True  --cache True  --o '../Src/ModData/feature0327'
 2264  pip install ConfigParser
 2265  pythohn model_selection.py -h
 2266  python model_selection.py --ifeat ../Src/ModData/feature0327/feature_eri_lbh7_lah1_0327.csv --conf config.ini --t Prob_Urgent --o ../Src/ModData/model_select
 2267  python model_selection.py --ifeat ../Src/ModData/feature0327/feature_eri_lbh7_lah1_0327.csv --iconf config.ini --t Prob_Urgent --o ../Src/ModData/model_select
 2268  python model_selection.py --ifeat ../Src/ModData/feature0327/feature_eri_lbh7_lah1_0327.csv --iconf config.ini --t Prob_Urgent --o ../Src/ModData/model_select --loglevel info --olog 'model_selection.log'
 2269  python model_selection.py --ifeat ../Src/ModData/feature0327feature_nok_lbh7_lah1_target_Urgent_0327.csv --iconf config.ini --t Prob_Urgent --o ../Src/ModData/model_select --loglevel info --olog 'model_selection.log'
 2270  python model_selection.py --ifeat ../Src/ModData/feature0327feature_nok_lbh7_lah1_target_Urgent_0327.csv --iconf config.ini --t Prob_Urgent --o ../Src/ModData/model_select --loglevel info --olog 'model_selection.log' --t Urgent
 2271  python model_selection.py --ifeat ../Src/ModData/feature0327feature_nok_lbh7_lah1_target_Urgent_0327.csv --iconf config.ini --t Prob_Urgent --o ../Src/ModData/model_select --loglevel info --olog 'model_selection.log' --t Prob_Urgent
 2272  python model_selection.py --ifeat ../Src/ModData/feature0327feature_nok_lbh7_lah1_target_Urgent_0327.csv --iconf config.ini --t Prob_Urgent --o ../Src/ModData/model_select --loglevel info --olog 'model_selection.log' 
 2273  python model_selection.py --ifeat ../Src/ModData/feature0327feature_nok_lbh7_lah1_target_Urgent_0327.csv --iconf config.ini --t Prob_Urgent --o ../Src/ModData/model_select --loglevel info --olog 'model_selection.log' --report 'std'
 2274  python model_selection.py --ifeat ../Src/ModData/feature0327feature_nok_lbh7_lah1_target_Urgent_0327.csv --iconf config.ini --t Prob_Urgent --o ../Src/ModData/model_select0328 --loglevel info --olog 'model_selection.log' --report 'std'
 2275  python model_selection.py --ifeat ../Src/ModData/feature0327feature_nok_lbh7_lah1_target_Urgent_0327.csv --iconf config.ini --t Prob_Urgent --o ../Src/ModData/model_select0328 --loglevel info --olog 'model_selection.log' --report 'csv'
 2276  python model_selection.py --ifeat ../Src/ModData/feature0327feature_nok_lbh7_lah1_target_Urgent_0327.csv --iconf config.ini --t Prob_Urgent --o ../Src/ModData/model_select0328.csv --loglevel info --olog 'model_selection.log' --report 'csv'
 2277  python model_selection.py --ifeat ../Src/ModData/feature0327feature_nok_lbh7_lah1_target_Urgent_0327.csv --iconf config.ini --t Prob_Urgent --o ../Src/ModData/model_select_batch_dt_0328.csv --loglevel info --olog 'model_selection.log' --report 'csv'
 2278  python model_selection.py --ifeat ../Src/ModData/feature_nok_lbh14_lah1_Prob_Urgent0327.csv --iconf config.ini --t Prob_Urgent --o ../Src/ModData/model_select_batch_dt_nok_0328.csv --loglevel info --olog 'model_selection.log' --report 'csv'
 2279  python model_selection.py --ifeat ../Src/ModData/feature_nok_lbh14_lah1_Prob_Urgent0327.csv --iconf config.ini --t Prob_Urgent --o ../Src/ModData/model_select_batch_dt_use_feature_nok_lbh14_lah1_Prob_Urgent0327.csv --loglevel info --olog 'model_selection.log' --report 'csv'
 2280  python model_selection.py --ifeat ../Src/ModData/feature_nok_lbh14_lah1_Prob_Urgent0327.csv --iconf config.ini --t Prob_Urgent --o ../Src/ModData/model_select_batch_use_feature_nok_lbh14_lah1_Prob_Urgent0327.csv --loglevel info --olog 'model_selection.log' --report 'csv'
 2281  python model_selection.py --ifeat ../Src/ModData/feature_nok_lbh14_lah1_Prob_Urgent0327.csv --iconf config.ini --t Prob_Urgent --o ../Src/ModData/model_select_batch_use_feature_nok_lbh14_lah1_Prob_Urgent0327.csv --loglevel info --olog 'model_selection.log' --report 'csv' --scoring 'precision'
 2282  python model_selection.py --ifeat ../Src/ModData/feature0327feature_nok_lbh7_lah1_target_Urgent_0327.csv --iconf config.ini --t Prob_Urgent --o ../Src/ModData/model_select_batch_0328.csv --loglevel info --olog 'model_selection.log' --report 'csv'
 2283  python eval_model_dt.py -i ../Src/ModData/feature_nok_lbh14_lah1_Prob_Urgent0327.csv  -o "../Src/ModData/setcolor_" --target "Prob_Urgent, Prob_FSME, Prob_Rectifier, Prob_FSMF, Prob_Breaker, Prob_FRGP, Prob_FXDA, Prob_BATT, Prob_FeederCable, Prob_OpticalCable, Prob_Others" --n_healthy '1000'  --vendor "nok" -exp True 
 2284  python eval_model_dt.py -i ../Src/ModData/feature_nok_lbh14_lah1_Prob_Urgent0327.csv  -o "../Src/ModData/best_dt_" --target Prob_Urgent --n_healthy '1000'  --vendor "nok" -exp True 
 2285  python feature_create.py --vendor 'eri' --lb 14 --la 1 --nh 2000 --nu 600 --sample_days 40 --save True  --cache True  --o '../Src/ModData/'
 2286  python feature_create.py --vendor 'nok' --lb 14 --la 1 --nh 2000 --nu 600 --sample_days 40 --save True  --cache True  --o '../Src/ModData/'
 2287  python eval_model_dt.py -i ../Src/ModData/feature_nok_lbh14_lah1_Prob_Urgent0328.csv  -o "../Src/ModData/best_dt_" --target Prob_Urgent --n_healthy 10000 --n_unhealthy 10000  --vendor "nok" -exp True 
 2288  pip install pypyodbc
 2289  jython
 2290  locate UcanAccess
 2291  locate ucanaccess
 2292  ls ~/.local/bin
 2293  ls ~/.local/bin/UCanAccess-4.0.1-bin
 2294  vz
 2295  python load_internal_log.py
 2296  echo $CLASSPATH
 2297  vim ../Src/Original_flat/ERI_YM_20170212.accdb
 2298  ln -s ~/.local/bin/UCanAccess-4.0.1-bin/loader/ucanload.jar ~/Documents/Driver/ucanload.jar
 2299  ln 
 2300  unlink
 2301  unlink ~/Documents/Driver/ucanload.jar
 2302  cp ~/.local/bin/UCanAccess-4.0.1-bin/loader/ucanload.jar ~/Documents/Driver/ucanload.jar
 2303  cd Projects/SB/Notebook
 2304  cd 20170306_Ingestion/Output
 2305  cd 20170310_Ingestion/Output
 2306  gzip -c OSS_Eri_20170116_20170127.csv > OSS_Eri_20170116_20170127.gz
 2307  jython load_internal_log.py
 2308  echo geproxy
 2309  cd SB_box_ingest_working_dir
 2310  cd 20170306_Ingestion
 2311  gzip -c oss_*
 2312  .tables 
 2313  for file in ./\necho $file 
 2314  echo $file 
 2315  for i in ./ \ndo \necho $i\ndone
 2316  for files in ./\ndo\necho $files\ndone
 2317  l .
 2318  for files in ./*\ndo\necho $files\ndone
 2319  echo $files
 2320  echo $files.gz
 2321  source convert_gz.sh
 2322  cd ../../Talend\ Working\ Directory
 2323  l ./*
 2324  vim convert_gz.sh
 2325  cd ../20170323_Ingestion/Input
 2326  for f in ./*\ndo\ngzip -c $f > ../../Talend\ Working\ Directory/$f.gz\ndone
 2327  cd ../../20170310_Ingestion/Output
 2328  gzip -c bsw_20170201_20170301.csv > bsw_20170201_20170301.csv.gz
 2329  cd 20170323_Ingestion
 2330  gzip -c nedb_2016summer_2017TQ.csv > ../../Talen_Working_Directory/nedb_2016summer_2017TQ.csv.gz
 2331  locate odbc
 2332  locate .freetds
 2333  ls /usr/local/etc/freetds.conf
 2334  tsql -S MSSQL_SB -U dssb -P b@nklak3_123
 2335  brew uninstall greetds
 2336  brew uninstall freetds
 2337  brew install freetds --with-unixodbc
 2338  less /usr/local/etc/freetds.conf
 2339  tsql -S [mssqlServerAlpha]
 2340  tsql
 2341  tsql -H
 2342  less /Library/ODBC/odbcinst.ini
 2343  isql alpidcdiha001v dssb b@nklak3_123
 2344  isql alpidcdiha001v dssb b@nklak3_123 -v
 2345  vim /Library/ODBC/odbcinst.ini 
 2346  vim /Library/ODBC/odbc.ini
 2347  less /Library/ODBC/odbc.ini
 2348  isql mssqlServerAlpha dssb b@nklak3_123
 2349  l .odbc.ini
 2350  l /usr/local/Cellar/freetds/1.00.27/etc
 2351  vim /usr/local/Cellar/freetds/1.00.27/etc/freetds.conf
 2352  l /usr/local/Cellar/unixodbc/2.3.4/etc/
 2353  l /usr/local/Cellar/unixodbc/2.3.4/etc/odbc.ini
 2354  cp /usr/local/Cellar/unixodbc/2.3.4/etc/odbc.ini /usr/local/Cellar/unixodbc/2.3.4/etc/odbc.ini.bak
 2355  rm /usr/local/Cellar/unixodbc/2.3.4/etc/odbc.ini
 2356  ln -s  /Library/ODBC/odbc.ini /usr/local/Cellar/unixodbc/2.3.4/etc/odbc.ini /usr/local/Cellar/unixodbc/2.3.4/etc/odbc.ini
 2357  ln -s /Library/ODBC/odbc.ini /usr/local/Cellar/unixodbc/2.3.4/etc/odbc.ini
 2358  cp /usr/local/Cellar/unixodbc/2.3.4/etc/odbcinst.ini /usr/local/Cellar/unixodbc/2.3.4/etc/odbcinst.ini.bak
 2359  rm /usr/local/Cellar/unixodbc/2.3.4/etc/odbcinst.ini
 2360  ln -s /Library/ODBC/odbcinst.ini /usr/local/Cellar/unixodbc/2.3.4/etc/odbcinst.ini
 2361  source  /usr/local/Cellar/unixodbc/2.3.4/etc/odbc.ini
 2362  less /usr/local/Cellar/unixodbc/2.3.4/etc/odbc.ini
 2363  less /Users/212339410/.odbc.ini
 2364  vim ~/.odbc.ini
 2365  tsql -S mssqlServerAlpha  -U dssb -P b@nklak3_123 
 2366  isql mssqlServerAlpha  dssb b@nklak3_123 -vodbcinst
 2367  odbcinst
 2368  odbcinst -l
 2369  cat /usr/local/Cellar/unixodbc/2.3.4/etc/ODBCDataSources
 2370  less /usr/local/Cellar/unixodbc/2.3.4/etc/ODBCDataSources
 2371  cat /usr/local/Cellar/unixodbc/2.3.4/etc/odbc.ini
 2372  isql
 2373  isql help
 2374  isql 
 2375  l /usr/local/Cellar/unixodbc/2.3.4/etc/odbcinst.ini
 2376  cd 20170306_Ingestion/Input
 2377  python preprocessing_bsw.py -h
 2378  tsql -C
 2379  odbcinst -j
 2380  python preprocessing_bsw.py --i ~/SB_box_ingest_working_dir/20170310_Ingestion/Input/BSW_ERI_20170201_0301.csv  --o ./ --dfrom 20170201 --dto 20170301
 2381  python preprocessing_bsw.py --i ~/SB_box_ingest_working_dir/20170310_Ingestion/Input/  --o ./ --dfrom 20170201 --dto 20170301
 2382  brew unlink midnight-commander
 2383  brew install mc
 2384  l *gz
 2385  vim /Users/212339410/.odbc.ini
 2386  cp /usr/local/Cellar/unixodbc/2.3.4/etc/odbc.ini /usr/local/Cellar/unixodbc/2.3.4/etc/odbc.bak0330.ini
 2387  vim /usr/local/etc/freetds.conf
 2388  isql mssqlServerAlpha  dssb b@nklak3_123 -v
 2389  vim /usr/local/Cellar/unixodbc/2.3.4/etc/odbc.ini
 2390  vim /usr/local/Cellar/unixodbc/2.3.4/etc/odbcinst.ini
 2391  unlink Projects
 2392  cd Data
 2393  ln -s /Users/212339410/Box/Analytics/Projects /Users/212339410/Projects
 2394  SB
 2395  cd Src
 2396  pip install upgrade
 2397  brew install gcc --without-multilib    
 2398  mkdir ~/tmp20170331
 2399  locate tstk
 2400  l /Users/212339410/anaconda/envs/py3k/lib/python3.5/site-packages/tstk
 2401  unlink /Users/212339410/anaconda/envs/py3k/lib/python3.5/site-packages/tstk
 2402  unlink /Users/212339410/anaconda2/envs/py35/lib/python3.5/site-packages/tstk
 2403  ln -s /Users/212339410/anaconda2/envs/py35/lib/python3.5/site-packages/tstk /Users/212339410/Box/Analytics/Python/tstk/tstk
 2404  l /Users/212339410/Box/Analytics/Python/tstk/tstk
 2405  unlink /Users/212339410/Box/Analytics/Python/tstk/tstk
 2406  l /Users/212339410/anaconda2/envs/py35/lib/python3.5/site-packages/tstk
 2407  l /Users/212339410/anaconda2/envs/py35/lib/python3.5/site-packages/
 2408  l /Users/212339410/anaconda2/envs/py35/lib/python3.5/site-packages/ | grep tstk
 2409  ln -s /Users/212339410/Box/Analytics/Python/tstk/tstk /Users/212339410/anaconda2/envs/py35/lib/python3.5/site-packages/tstk
 2410  /Users/212339410/Box/Analytics/Python/tstk/
 2411  unlink tstk
 2412  python feature_create.py --vendor 'nok' --lb 14 --la $n --nh 2000 --nu 600 --sample_days 40 --save True  --cache True  --o ~/tmp20170331/feature
 2413  python feature_create.py --vendor 'nok' --lb 14 --la 1 --nh 2000 --nu 600 --sample_days 40 --save True  --cache True  --o ~/tmp20170331/feature
 2414  {1..5}
 2415  for i in {1..5}\ndo \necho i\ndone
 2416  for i in {1..5}\ndo \necho $i\ndone
 2417  seq 1 10
 2418  echo `seq 1 10`
 2419  echo `seq 1 10 -s`
 2420  echo `seq -s 1 10 `
 2421  echo `seq -s ,  1 10 `
 2422  python feature_create.py --vendor 'nok' --lb 14 --la "2,3,4,5,6,7,8,9,10,11,12,13,14,15"  --nh 2000 --nu 600 --sample_days 40 --save True  --cache True  --o ~/tmp20170331/
 2423  python feature_create.py --vendor 'nok' --lb 14 --la `seq -s , 2 15 `  --nh 2000 --nu 600 --sample_days 40 --save True  --cache True  --o ~/tmp20170331/
 2424  superset
 2425  superset runserver
 2426  history | grep 
 2427  hisotry | grep fabmanager
 2428  history | grep password
 2429  history | grep superset
 2430  history 
 2431  history | 8283
 2432  history | grep 8283
 2433  for n in `seq 1 15`;\ndo \npython feature_create.py --vendor 'nok' --lb 14 --la $n --nh 2000 --nu 600 --sample_days 40 --save True  --cache True  --o ~/tmp20170331/feature\ndone
 2434  sudo pip install xgboost
 2435  sudo -
 2436  sudo
 2437  mkdir xgboost
 2438  git clone --recursive https://github.com/dmlc/xgboost 
 2439  export CC = gcc
 2440  cp make/config.mk .
 2441  vim make/config.mk
 2442  cp make/config.mk ./config.mk
 2443  python model_selection.py --ifeat ../Src/ModData/feature_nok_lbh14_lah1_Prob_Urgent0327.csv --iconf config.ini --t Prob_Urgent --o ../Src/ModData/model_select_batch_use_feature_nok_lbh14_lah1_Prob_Urgent0327.csv --loglevel info --olog 'model_selection.log' --report 'csv' --score 'precision'
 2444  l ../Src/ModData/feature_
 2445  l ../Src/ModData/feature_n*
 2446  python model_selection.py --ifeat ../Src/ModData/0328/feature_nok_lbh14_lah1_Prob_Urgent0327.csv --iconf config.ini --t Prob_Urgent --o ../Src/ModData/model_select_batch_use_feature_nok_lbh14_lah1_Prob_Urgent0327.csv --loglevel info --olog 'model_selection.log' --report 'csv' --score 'precision'
 2447  cd ~/Box/Analytics/Py
 2448  cd ~/Box/Analytics/Python
 2449  git commit -m 'fix detect_ts.py'
 2450  git push origin pr/36
 2451  git push -f origin 78f42c322546f748e9f8556044ed7db9de8daf63:pr/36
 2452  git checkout -- examples/Plotting using tstk.visualization.plot.ipynb
 2453  git checkout -- 
 2454  git branch -d pr/36
 2455  git branch -D pr/36
 2456  git fetch -all
 2457  git branch -vv
 2458  git remote show remote
 2459  git remote prune refs/remotes/origin/pr/36 stale
 2460  git fetch tstk
 2461  git tag
 2462  git fetch origin /remotes/origin/plot_update
 2463  git fetch origin origin/plot_update
 2464  git fetch origin plot_update
 2465  rm tstk/preprocessing/util.py
 2466  rm tstk/visualization/plotlyjs.py
 2467  git checkout -b plot_update origin/plot_update
 2468  branch
 2469  cp preprocessing_bsw_20170201_20170301.ipynb preprocessing_bsw_20170301_20170327.ipynb
 2470  python feature_create.py --vendor 'nok' --lb 14 --la 1  --nh 2000 --nu 600 --sample_days 40 --save True  --cache True  --o ../Src/ModData/0402/
 2471  python feature_create_v2.py --vendor 'nok' --lb 14 --la 1  --nh '' --nu 1000 --sample_days 40 --save True  --cache True  --o ../Src/ModData/0402/
 2472  seq 1, 15
 2473  for n in `seq 1 15`\ndo \necho $n\ndone
 2474  for n in `seq 1 15`\ndo \necho filename_lbh$n\ndone
 2475  for n in `seq 1 15`\ndo\necho model_selection.py --ifeat ../Src/ModData/feature_0331/featurefeature_nok_lbh14_lah$n_0331.csv  --iconf config.ini --t Prob_Urgent --o ../Src/ModData/model_select_batch_use_feature_nok_lbh14_lah$n_Prob_Urgent0327.csv --loglevel info --olog 'model_selection.log' --report 'csv' --score 'precision'\ndone
 2476  for n in `seq 1 15`\ndo\necho âmodel_selection.py --ifeat ../Src/ModData/feature_0331/featurefeature_nok_lbh14_lah$n_0331.csv  --iconf config.ini --t Prob_Urgent --o ../Src/ModData/model_select_batch_use_feature_nok_lbh14_lah$n_Prob_Urgent0327.csv --loglevel info --olog 'model_selection.log' --report 'csv' --score 'precisionââ\ndone\n
 2477  for n in `seq 1 15`\ndo\npython model_selection.py --ifeat ../Src/ModData/feature_0331/featurefeature_nok_lbh14_lah$n_0331.csv  --iconf config.ini --t Prob_Urgent --o ../Src/ModData/model_select_batch_use_feature_nok_lbh14_lah$n_Prob_Urgent0331.csv --loglevel info --olog 'model_selection.log' --report 'csv' --score 'precisionâ\ndone\n
 2478  for n in 'seq 1 15'\ndo\npython model_selection.py --ifeat ../Src/ModData/feature_0331/featurefeature_nok_lbh14_lah$n_0331.csv  --iconf config.ini --t Prob_Urgent --o ../Src/ModData/model_select_batch_use_feature_nok_lbh14_lah$n_Prob_Urgent0331.csv --loglevel info --olog 'model_selection.log' --report 'csv' --score 'precisionâ\ndone\n
 2479  for n in `seq 1 15`\ndo\npython model_selection.py --ifeat ../Src/ModData/feature_0331/featurefeature_nok_lbh14_lah$n_0331.csv  --iconf config.ini --t Prob_Urgent --o ../Src/ModData/model_select_batch_use_feature_nok_lbh14_lah$n_Prob_Urgent0331.csv --loglevel info --olog 'model_selection.log' --report 'csv' --score 'precision'\ndone
 2480  for n in `seq 1 2` \ndo \necho $n\ndone
 2481  for n in `ls ./*` \ndo \necho $n\ndone
 2482  for n in `ls ./*.py` \ndo \necho $n\ndone
 2483  for n in `ls ./*.py` \ndo \ngrep $n\ndone
 2484  for n in {1..10}\ndo \necho $n\ndone
 2485  for n in {1..10}\ndo \necho $n $\ndone
 2486  for n in {1..10}\ndo \necho '$n' $\ndone
 2487  for n in {1..10}\ndo \necho "$n" $\ndone
 2488  for n in `seq 1 15`\ndo\npython model_selection.py --ifeat ../Src/ModData/feature_0331/featurefeature_nok_lbh14_lah{$n}_0331.csv  --iconf config.ini --t Prob_Urgent --o ../Src/ModData/model_select_batch_use_feature_nok_lbh14_lah{$n}_Prob_Urgent0331.csv --loglevel info --olog 'model_selection.log' --report 'csv' --score 'precision'\ndone
 2489  for n in `seq 1 15`\ndo\necho $n; \npython model_selection.py --ifeat ../Src/ModData/feature_0331/featurefeature_nok_lbh14_lah$n_0331.csv  --iconf config.ini --t Prob_Urgent --o ../Src/ModData/model_select_batch_use_feature_nok_lbh14_lah$n_Prob_Urgent0331.csv --loglevel info --olog 'model_selection.log' --report 'csv' --score 'precision'\ndone
 2490  for $n in {1..10}
 2491  for n in {1..10}\ndo\necho $n+$n\ndone
 2492  for n in {1..3}\ndo\necho $n; \npython model_selection.py --ifeat ../Src/ModData/feature_0331/featurefeature_nok_lbh14_lah$n_0331.csv  --iconf config.ini --t Prob_Urgent --o ../Src/ModData/model_select_batch_use_feature_nok_lbh14_lah$n_Prob_Urgent0331_ --loglevel info --olog 'model_selection.log' --report 'csv' --score 'precision'\ndone
 2493  for n in {1..3}\ndo\necho lbh$n_0331; \n#python model_selection.py --ifeat ../Src/ModData/feature_0331/featurefeature_nok_lbh14_lah$n_0331.csv  --iconf config.ini --t Prob_Urgent --o ../Src/ModData/model_select_batch_use_feature_nok_lbh14_lah$n_Prob_Urgent0331_ --loglevel info --olog 'model_selection.log' --report 'csv' --score 'precision'\n\ndone
 2494  for n in {1..3}\ndo\necho lbh${n}_0331; \n#python model_selection.py --ifeat ../Src/ModData/feature_0331/featurefeature_nok_lbh14_lah$n_0331.csv  --iconf config.ini --t Prob_Urgent --o ../Src/ModData/model_select_batch_use_feature_nok_lbh14_lah$n_Prob_Urgent0331_ --loglevel info --olog 'model_selection.log' --report 'csv' --score 'precision'\n\ndone
 2495  for n in {1..3}\ndo\necho lbh${n}_0331; \n#python model_selection.py --ifeat ../Src/ModData/feature_0331/featurefeature_nok_lbh14_lah${n}_0331.csv  --iconf config.ini --t Prob_Urgent --o ../Src/ModData/model_select_batch_use_feature_nok_lbh14_lah${n}_Prob_Urgent0331_ --loglevel info --olog 'model_selection.log' --report 'csv' --score 'precision'\n\ndone
 2496  for n in {1..3}\ndo\necho lbh${n}_0331; \npython model_selection.py --ifeat ../Src/ModData/feature_0331/featurefeature_nok_lbh14_lah${n}_0331.csv  --iconf config.ini --t Prob_Urgent --o ../Src/ModData/model_select_batch_use_feature_nok_lbh14_lah${n}_Prob_Urgent0331_ --loglevel info --olog 'model_selection.log' --report 'csv' --score 'precision'\n\ndone
 2497  for n in {1..15}\ndo\necho lbh${n}_0331; \npython model_selection.py --ifeat ../Src/ModData/feature_0331/featurefeature_nok_lbh14_lah${n}_0331.csv  --iconf config.ini --t Prob_Urgent --o ../Src/ModData/model_select_batch_use_feature_nok_lbh14_lah${n}_Prob_Urgent0331_ --loglevel info --olog 'model_selection.log' --report 'csv' --score 'precision'\n\ndone
 2498  cd ../Src/ModData/feature_0331
 2499  cd ../Src/ModData/feature_0331\necho lbh${n}_0331; \npython model_selection.py --ifeat ../Src/ModData/feature_0331/featurefeature_nok_lbh14_lah${n}_0331_Prob_Urgent.csv  --iconf config.ini --t Prob_Urgent --o ../Src/ModData/model_select_batch_use_feature_nok_lbh14_lah${n}_Prob_Urgent0331_ --loglevel info --olog 'model_selection.log' --report 'csv' --score 'precision'\n\ndone
 2500  for n in {1..15}\ndo\necho lbh${n}_0331; \npython model_selection.py --ifeat ../Src/ModData/feature_0331/featurefeature_nok_lbh14_lah${n}_0331_Prob_Urgent.csv  --iconf config.ini --t Prob_Urgent --o ../Src/ModData/model_select_batch_use_feature_nok_lbh14_lah${n}_Prob_Urgent0331_ --loglevel info --olog 'model_selection.log' --report 'csv' --score 'precision'\n\ndone
 2501  for n in {1..1}\ndo\necho lbh${n}_0331; \npython model_selection.py --ifeat ../Src/ModData/feature_0331/featurefeature_nok_lbh14_lah${n}_0331_Prob_Urgent.csv  --iconf config.ini --t Prob_Urgent --o ../Src/ModData/model_select_batch_use_feature_nok_lbh14_lah${n}_Prob_Urgent0331_ --loglevel info --olog 'model_selection.log' --report 'csv' --score 'precision'\n\ndone
 2502  cp model_selection.py ./xgboost/model_selection.py
 2503  rm ./xgboost/model_selection.py
 2504  mv model_selection.py ./xgboost/model_selection.py
 2505  ln -s ./xgboost/model_selection.py model_selection.py
 2506  ln -s mbs.py ./xgboost/mbs.py
 2507  ln -s mbs.py /xgboost/mbs.py
 2508  ln -s `pwd`/mbs.py `pwd`/xgboost/mbs.py
 2509  ln -s `pwd`/qutils.py `pwd`/xgboost/qutils.py
 2510  ln -s `pwd`/process_alarm_names.py `pwd`/xgboost//process_alarm_names.py
 2511  ln -s `pwd`/process_alarm_names_eri.py `pwd`/xgboost/process_alarm_names_eri.py
 2512  ln -s /Users/212339410/Projects/SB/Python/process_pro.py `pwd`/xgboost/process_problem_areas.py
 2513  ln -s /Users/212339410/Projects/SB/Python/process_problem_areas.py `pwd`/xgboost/process_problem_areas.py
 2514  ln -s /Users/212339410/Projects/SB/Python/plot_v1.py `pwd`/xgboost/plot_v1.py
 2515  for n in {1..1}\ndo\necho lbh${n}_0331; \npython model_selection.py --ifeat ../Src/ModData/feature_0331/featurefeature_nok_lbh14_lah${n}_0331_Prob_Urgent.csv  --iconf config.ini --t Prob_Urgent --o ../Src/ModData/model_select_batch_use_feature_nok_lbh14_lah${n}_Prob_Urgent0331_ --loglevel info --olog 'model_selection.log' --report 'csv' --score 'accuracy'\n\ndone
 2516  python feature_create_v2.py --vendor 'nok' --lb 14 --la 1  --nh '' --nu 1000 --sample_days 40 --save True  --cache True  --o ../Src/ModData/0403/
 2517  for n in {1..10}\ndo\necho lbh${n}_0331; \npython model_selection.py --ifeat ../Src/ModData/feature_0331/featurefeature_nok_lbh14_lah${n}_0331_Prob_Urgent.csv  --iconf config.ini --t Prob_Urgent --o ../Src/ModData/model_select_batch_use_feature_nok_lbh14_lah${n}_Prob_Urgent0331_ --loglevel info --olog 'model_selection.log' --report 'csv' --score 'accuracy'\n\ndone
 2518  for n in {1..10}\ndo\necho lbh${n}_0331; \npython model_selection.py --ifeat ../Src/ModData/feature_0331/featurefeature_nok_lbh14_lah${n}_0331_Prob_Urgent.csv  --iconf config.ini --t Prob_Urgent --o ../Src/ModData/model_select_batch_use_feature_nok_lbh14_lah${n}_Prob_Urgent0331.csv --loglevel info --olog 'model_selection.log' --report 'csv' --score 'accuracy'\n\ndone
 2519  mkdir ../Src/ModData/model_selection_0403
 2520  for n in {1..10}\ndo\necho lbh${n}_0331; \npython model_selection.py --ifeat ../Src/ModData/feature_0331/featurefeature_nok_lbh14_lah${n}_0331_Prob_Urgent.csv  --iconf config.ini --t Prob_Urgent --o ../Src/ModData/model_selection_0403/model_select_feature_nok_lbh14_lah${n}_Prob_Urgent0331.csv --loglevel info --olog 'model_selection.log' --report 'csv' --score 'accuracy'\n\ndone
 2521  locate xgboost
 2522  locate -h
 2523  man locate 
 2524  /usr/libexec/locate.updatedb
 2525  python predict_top_prob.py 
 2526  python predict_top_prob.py  --t Prob_Urgent --i ''
 2527  python predict_top_prob.py  --t Prob_Urgent --i '' --o ../Src/ModData/results_predict_top10.csv
 2528  for n in {1..1}\ndo\necho lbh${n}_0331; \npython model_selection.py --ifeat ../Src/ModData/feature_0331/featurefeature_nok_lbh14_lah${n}_0331_Prob_Urgent.csv  --iconf config.ini --t Prob_Urgent --o ../Src/ModData/model_selection_0403/model_select_feature_nok_lbh14_lah${n}_Prob_Urgent0331.csv --loglevel info --olog 'model_selection.log' --report 'csv' --score 'accuracy'\n\ndone
 2529  for n in {1..15}\ndo\necho lbh${n}_0331; \npython model_selection.py --ifeat ../Src/ModData/feature_0331/featurefeature_nok_lbh14_lah${n}_0331_Prob_Urgent.csv  --iconf config.ini --t Prob_Urgent --o ../Src/ModData/model_selection_0403/model_select_feature_nok_lbh14_lah${n}_Prob_Urgent0331.csv --loglevel info --olog 'model_selection.log' --report 'csv' --score 'accuracy'\n\ndone
 2530  for n in {2..15}\ndo\necho lbh${n}_0331; \npython model_selection.py --ifeat ../Src/ModData/feature_0331/featurefeature_nok_lbh14_lah${n}_0331_Prob_Urgent.csv  --iconf config.ini --t Prob_Urgent --o ../Src/ModData/model_selection_0403/model_select_feature_nok_lbh14_lah${n}_Prob_Urgent0331.csv --loglevel info --olog 'model_selection.log' --report 'csv' --score 'accuracy'\n\ndone
 2531  python model_selection.py --ifeat ../Src/ModData/0403/feature_nok_lbh14_lah1_Prob_Urgent_0403.csv --iconf config.ini --t Prob_Urgent --o ../Src/ModData/model_select_use_week_feature_nok_lbh14_lah1_Prob_Urgent0403.csv --loglevel info --olog 'model_selection.log' --report 'csv' --score 'precision'
 2532  python model_selection.py --ifeat ../Src/ModData/0403/feature_nok_lbh14_lah1_Prob_Urgent_0403.csv --iconf config.ini --t Prob_Urgent --o ../Src/ModData/model_select_use_week_feature_nok_lbh14_lah1_Prob_Urgent0403.csv --loglevel info --olog 'model_selection.log' --report 'csv' --score 'accracy'
 2533  python model_selection.py --ifeat ../Src/ModData/0403/feature_nok_lbh14_lah1_Prob_Urgent_0403.csv --iconf config.ini --t Prob_Urgent --o ../Src/ModData/model_select_use_week_feature_nok_lbh14_lah1_Prob_Urgent0403.csv --loglevel info --olog 'model_selection.log' --report 'csv' --score 'accuracy'
 2534  python eval_model_rf.py -i "../Src/ModData/0403/feature_nok_lbh14_lah1_Prob_Urgent_0403.csv" -o "../Src/ModData/week_model_" --target "Prob_Urgent" --n_healthy '1000'  --vendor "nok" -exp True  
 2535  python eval_model_rf.py -i "../Src/ModData/0403/feature_nok_lbh14_lah1_Prob_Urgent_0403.csv" -o "../Src/ModData/week_model_" --target "Prob_Urgent" --n_healthy '1000'  --n_unhealthy '1000'  --vendor "nok" -exp True  
 2536  python eval_model_dt.py -i "../Src/ModData/0403/feature_nok_lbh14_lah1_Prob_Urgent_0403.csv" -o "../Src/ModData/week_model_" --target "Urgent" --n_healthy '1000'  --n_unhealthy '1000'  --vendor "nok" -exp True  
 2537  python eval_model_dt.py -i "../Src/ModData/0403/feature_nok_lbh14_lah1_Prob_Urgent_0403.csv" -o "../Src/ModData/week_model_" --target "Prob_Urgent" --n_healthy '1000'  --n_unhealthy '1000'  --vendor "nok" -exp True  
 2538  python eval_model_dt.py -i "../Src/ModData/0403/feature_nok_lbh14_lah1_Prob_Urgent_0403.csv" -o "../Src/ModData/week_model_" --target "Prob_Urgent" --n_healthy '1000'  --n_unhealthy '10000'  --vendor "nok" -exp True  
 2539  python eval_model_dt.py -i "../Src/ModData/0403/feature_nok_lbh14_lah1_Prob_Urgent_0403.csv" -o "../Src/ModData/week_model_" --target "Prob_Urgent" --n_healthy '01000'  --n_unhealthy '10000'  --vendor "nok" -exp True  
 2540  python eval_model_rf.py -i "../Src/ModData/0403/feature_nok_lbh14_lah1_Prob_Urgent_0403.csv" -o "../Src/ModData/week_model_" --target "Prob_Urgent" --n_healthy '01000'  --n_unhealthy '10000'  --vendor "nok" -exp True  
 2541  python eval_model_rf.py -i "../Src/ModData/0403/feature_nok_lbh14_lah1_Prob_Urgent_0403.csv" -o "../Src/ModData/week_model_" --target "Prob_Urgent" --n_healthy '01000'  --n_unhealthy '10000'  --vendor "nok" -exp True  --top_problem Urgent
 2542  python feature_create.py --vendor 'nok' --lb 14 --la 1  --nh '' --nu 1000 --sample_days 40 --save True  --cache True  --o ../Src/ModData/0402/
 2543  python feature_create.py --vendor 'nok' --lb 14 --la 1  --nh '' --nu 1000 --sample_days 40 --save True  --cache True  --o ../Src/ModData/0402/gil
 2544  python pip install flake8
 2545  flake8 ./
 2546  flake8 ../../../Python/tstk/tstk
 2547  rm preprocessing_oss_nok_20170327.ipynb
 2548  git rm void.txt
 2549  cd Box/Analytics/Python/tstk
 2550  python feature_create_v2.py --vendor 'nok' --lb 14 --la 1  --nh '' --nu 1000 --sample_days 40 --save True  --cache True  --o ../Src/ModData/0404_dataset/ --dfrom '2016-07-07 07:27:46' --dto '2016-08-29 16:07:41'
 2551  python feature_create_v2.py --vendor 'nok' --lb 14 --la 1  --nh '' --nu 1000 --sample_days 40 --save True  --cache False  --o ../Src/ModData/0404_dataset/ --dfrom '2016-07-07 07:27:46' --dto '2016-08-29 16:07:41'
 2552  python update_schema.py ~/Box/DSS\ Internal-Softbank/12\ Working\ Directory\ for\ Ingestion/20170328_Ingestion/Output/Tokyo_RAN_OSS_Eri_20170322_20170327.csv  ../Src/Schema/oss_nok_mod_v4.xml  ../Src/Schema/oss_nok_v5.xml
 2553  vim ~/Box/DSS\ Internal-Softbank/12\ Working\ Directory\ for\ Ingestion/20170328_Ingestion/Output/Tokyo_RAN_OSS_Eri_20170322_20170327.csv
 2554  head -n 100  ~/Box/DSS\ Internal-Softbank/12\ Working\ Directory\ for\ Ingestion/20170328_Ingestion/Output/Tokyo_RAN_OSS_Eri_20170322_20170327.csv > tmp_oss_nok_0322.csv
 2555  open tmp_oss_nok_0322.csv
 2556  vim tmp_oss_nok_0322.csv
 2557  head -n 100  ~/Box/DSS\ Internal-Softbank/12\ Working\ Directory\ for\ Ingestion/20170310_Ingestion/Output/OSS_Eri_20170128_20170220.csv > tmp_oss_eri_0128.csv
 2558  mv tmp_oss_nok_0322.csv tmp_oss_eri_0322.csv
 2559  open tmp_oss_eri_0128.csv
 2560  python update_schema.py "/Users/212339410/Box/DSS Internal-Softbank/12 Working Directory for Ingestion/20170327_Ingestion/Output/oss_nok_all_20170221_20170317.csv"  ../Src/Schema/oss_nok_mod_v4.xml  ../Src/Schema/oss_nok_v5.xml
 2561  python feature_create_v2.py --vendor 'nok' --lb 14 --la 1  --nh 100 --nu 1000 --sample_days 40 --save True  --cache False  --o ../Src/ModData/0404_dataset/ --dfrom '2016-07-07 07:27:46' --dto '2016-08-29 16:07:41'
 2562  pip install --upgrade notebook
 2563  .schema BSWn
 2564  python feature_create_v2.py --vendor 'nok' --lb 14 --la 1  --nh 10 --nu 10 --sample_days 40 --save True  --cache False  --o ../Src/ModData/0404_dataset/ --dfrom '2016-07-07 07:27:46' --dto '2016-08-29 16:07:41'
 2565  python feature_create_v2.py --vendor 'nok' --lb 14 --la 1  --nh 10 --nu 10 --sample_days 40 --save True  --cache True  --o ../Src/ModData/0404_dataset/ --dfrom '2016-07-07 07:27:46' --dto '2016-08-29 16:07:41'
 2566  python feature_create_v2.py --vendor 'nok' --lb 14 --la 1  --nh ""  --nu "" --sample_days 40 --save True  --cache True  --o ../Src/ModData/0404_dataset_mass/ --dfrom '2016-07-07 00:00:00' --dto '2016-08-28 23:59:59'
 2567  seq 1 14
 2568  seq 1, 14
 2569  seq 1 2
 2570  for n in 1..14\necho $n
 2571  for n in {1..14}\necho $n
 2572  for n in {1..14}\ndo ;echo $n; done
 2573  sq -s, 1 14
 2574  seq -s, 1 14
 2575  python feature_create_v2.py --vendor 'nok' --lb "1,2,3,4,5,6,7,8,9,10,11,12,13,14" --la 1  --nh ""  --nu "" --sample_days 40 --save True  --cache True  --o ../Src/ModData/0404_dataset_mass/ --dfrom '2016-07-07 00:00:00' --dto '2016-08-28 23:59:59'
 2576  python feature_create_v2.py --vendor 'nok' --lb "1,2,3,4,5,6,7,8,9,10,11,12,13,14" --la 1  --nh ""  --nu "" --sample_days 40 --save True  --cache True  --o ../Src/ModData/0404_dataset_mass/ --dfrom '2017-01-04 00:00:00' --dto '2017-02-28 23:59:59'
 2577  python feature_create_v2.py --vendor 'nok' --lb 14 --la 1  --nh 10 --nu 10 --sample_days 40 --save True  --cache True  --o ../Src/ModData/0404_dataset/ --dfrom '2017-01-04 00:00:00' --dto '2017-02-28 23:59:59'
 2578  seq -s, 14 1
 2579  python feature_create_v2.py --vendor 'nok' --lb "1,2,3,4,5,6,7,8,9,10,11,12,13,14" --la 1  --nh ""  --nu "" --sample_days 40 --save True  --cache True  --o ../../../../DSS\ Internal-Softbank/40\ Intermediate\ Results/20170403\ Feature\ Set\ \(training\ vs\ validation\)/  --dfrom '2017-01-04 00:00:00' --dto '2017-02-28 23:59:59'
 2580  python feature_create_v2.py --vendor 'nok' --lb "14,13,12,11,10,9,8,7,6,5,4,3,2,1" --la 1  --nh ""  --nu "" --sample_days 20 --save True  --cache True  --o ../../../../DSS\ Internal-Softbank/40\ Intermediate\ Results/20170403\ Feature\ Set\ \(training\ vs\ validation\)/  --dfrom '2017-01-04 00:00:00' --dto '2017-02-28 23:59:59'
 2581  python feature_create_v2.py --vendor 'nok' --lb "14,12,10,8,6,4,2" --la 1  --nh ""  --nu "" --sample_days 40 --save True  --cache True  --o ../Src/ModData/0404_dataset_mass/ --dfrom '2016-07-07 00:00:00' --dto '2016-08-28 23:59:59'
 2582  python feature_create_v2.py --vendor 'eri' --lb 14 --la 1  --nh 10 --nu 10 --sample_days 40 --save True  --cache True  --o ../Src/ModData/0404_dataset/ --dfrom '2017-01-04 00:00:00' --dto '2017-02-28 23:59:59'
 2583  python feature_create_v2.py --vendor 'nok' --lb "14,12,10,8,6,4,2" --la 1  --nh ""  --nu "" --sample_days 20 --save True  --cache True  --o ../../../../DSS\ Internal-Softbank/40\ Intermediate\ Results/20170403\ Feature\ Set\ \(training\ vs\ validation\)/  --dfrom '2017-01-04 00:00:00' --dto '2017-02-28 23:59:59'
 2584  python feature_create_v2.py --vendor 'nok' --lb "14,12,10,8,6,4,2" --la 1  --nh ""  --nu "" --sample_days 40 --save True  --cache True  --o ../../../../DSS\ Internal-Softbank/40\ Intermediate\ Results/20170403\ Feature\ Set\ \(training\ vs\ validation\)/ --dfrom '2016-07-07 00:00:00' --dto '2016-08-28 23:59:59'
 2585  python feature_create_v2.py --vendor 'eri' --lb 14 --la 1  --nh 10 --nu 10 --sample_days 40 --save True  --cache True  --o ../Src/ModData/0404_dataset/ --dfrom '2017-01-16 00:00:00' --dto '2017-02-28 23:59:59'
 2586  python feature_create_v2.py --vendor 'eri' --lb 14 --la 1  --nh "" --nu "" --sample_days 10 --save True  --cache True  --o ../../../../DSS\ Internal-Softbank/40\ Intermediate\ Results/20170403\ Feature\ Set\ \(training\ vs\ validation\)/ --dfrom '2017-01-16 00:00:00' --dto '2017-03-12 23:59:59'
 2587  python feature_create_v2.py --vendor 'eri' --lb 14 --la 1  --nh "" --nu "" --sample_days 10 --save True  --cache True  --o ../../../../DSS\ Internal-Softbank/40\ Intermediate\ Results/20170403\ Feature\ Set\ \(training\ vs\ validation\)/ --dfrom '2017-02-13 00:00:00' --dto '2017-03-27 23:59:59'
 2588  python feature_create_v2.py --vendor 'eri' --lb 14 --la 1  --nh "" --nu "" --sample_days 10 --save True  --cache True  --o ../../../../DSS\ Internal-Softbank/40\ Intermediate\ Results/20170403\ Feature\ Set\ \(training\ vs\ validation\)/ --dfrom '2017-03-13 00:00:00' --dto '2017-03-27 23:59:59'
 2589  python feature_create_v2.py --vendor 'eri' --lb 14 --la 1  --nh "" --nu "" --sample_days 13 --save True  --cache True  --o ../../../../DSS\ Internal-Softbank/40\ Intermediate\ Results/20170403\ Feature\ Set\ \(training\ vs\ validation\)/ --dfrom '2017-03-13 00:00:00' --dto '2017-03-27 23:59:59'
 2590  python feature_create_v2.py --vendor 'eri' --lb "14, 8, 4" --la 1  --nh "" --nu "" --sample_days 13 --save True  --cache True  --o ../../../../DSS\ Internal-Softbank/40\ Intermediate\ Results/20170403\ Feature\ Set\ \(training\ vs\ validation\)/ --dfrom '2017-03-13 00:00:00' --dto '2017-03-27 23:59:59'
 2591  python feature_create_v2.py --vendor 'eri' --lb "14, 8, 4" --la 1  --nh "" --nu "" --sample_days 13 --save True  --cache True  --o ../../../../DSS\ Internal-Softbank/40\ Intermediate\ Results/20170403\ Feature\ Set\ \(training\ vs\ validation\)/ --dfrom '2017-03-11 00:00:00' --dto '2017-03-27 23:59:59'
 2592  python feature_create_v2.py --vendor 'nok' --lb "14,8,4" --la 1  --nh ""  --nu "" --sample_days 40 --save True  --cache True  --o ../../../../DSS\ Internal-Softbank/40\ Intermediate\ Results/20170403\ Feature\ Set\ \(training\ vs\ validation\)/ --dfrom '2017-03-01 00:00:00' --dto '2017-03-17 23:59:59'
 2593  python feature_create_v2.py --vendor 'nok' --lb "10" --la 1  --nh ""  --nu "" --sample_days 40 --save True  --cache True  --o ../../../../DSS\ Internal-Softbank/40\ Intermediate\ Results/20170403\ Feature\ Set\ \(training\ vs\ validation\)/ --dfrom '2017-03-01 00:00:00' --dto '2017-03-17 23:59:59'
 2594  python feature_create_v2.py --vendor 'nok' --lb "2, 6" --la 1  --nh ""  --nu "" --sample_days 40 --save True  --cache True  --o ../../../../DSS\ Internal-Softbank/40\ Intermediate\ Results/20170403\ Feature\ Set\ \(training\ vs\ validation\)/ --dfrom '2017-03-01 00:00:00' --dto '2017-03-17 23:59:59'
 2595  python feature_create_v2.py --vendor 'nok' --lb "12" --la 1  --nh ""  --nu "" --sample_days 40 --save True  --cache True  --o ../../../../DSS\ Internal-Softbank/40\ Intermediate\ Results/20170403\ Feature\ Set\ \(training\ vs\ validation\)/ --dfrom '2017-03-01 00:00:00' --dto '2017-03-17 23:59:59'
 2596  python feature_create_v2.py --vendor 'eri' --lb "14, 8, 4" --la 1  --nh "" --nu "" --sample_days 10 --save True  --cache True  --o ../../../../DSS\ Internal-Softbank/40\ Intermediate\ Results/20170403\ Feature\ Set\ \(training\ vs\ validation\)/ --dfrom '2017-01-16 00:00:00' --dto '2017-03-12 23:59:59'
 2597  python feature_create_v2.py --vendor 'eri' --lb "14, 8, 4" --la 1  --nh "" --nu "" --sample_days 40 --save True  --cache True  --o ../../../../DSS\ Internal-Softbank/40\ Intermediate\ Results/20170403\ Feature\ Set\ \(training\ vs\ validation\)/s40_ --dfrom '2017-01-16 00:00:00' --dto '2017-03-12 23:59:59'
 2598  python feature_create_v2.py --vendor 'nok' --lb 14  --la 0  --nh ""  --nu "" --sample_days 40 --save True  --cache True  --o ../../../../DSS\ Internal-Softbank/40\ Intermediate\ Results/20170403\ Feature\ Set\ \(training\ vs\ validation\)/ --dfrom '2017-03-01 00:00:00' --dto '2017-03-17 23:59:59'
 2599  less /Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170403 Feature Set (training vs validation)/feature_date170116_170312_eri_t0_lbh14_lah1_0405.csv
 2600  less '/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170403 Feature Set (training vs validation)/feature_date170116_170312_eri_t0_lbh14_lah1_0405.csv'
 2601  less '/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170403 Feature Set (training vs validation)/feature_date170311_170327_eri_t0_lbh14_lah1_0405.csv\n'
 2602  less '/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170403 Feature Set (training vs validation)/feature_date170311_170327_eri_t0_lbh14_lah1_0405.csv'
 2603  python feature_create_v2.py --vendor 'eri' --lb 14 --la 1Â  --nh 2000 --nu "" --sample_days 40 --save TrueÂ  --cache TrueÂ  --o ../../../../DSS\ Internal-Softbank/40\ Intermediate\ Results/20170403\ Feature\ Set\ \(training\ vs\ validation\)/s40_nh2000_ --dfrom '2017-03-11 00:00:00' --dto '2017-03-27 23:59:59'
 2604  python feature_create_v2.py --vendor 'eri' --lb '14' --la 1Â  --nh '2000' --nu "" --sample_days 40 --save TrueÂ  --cache TrueÂ  --o ../../../../DSS\ Internal-Softbank/40\ Intermediate\ Results/20170403\ Feature\ Set\ \(training\ vs\ validation\)/s40_nh2000_ --dfrom '2017-03-11 00:00:00' --dto '2017-03-27 23:59:59'
 2605  python feature_create_v2.py --vendor 'eri' --lb '14' --la 1Â  --nh '2000' --nu "" --sample_days 40  --save True --cache TrueÂ  --o ../../../../DSS\ Internal-Softbank/40\ Intermediate\ Results/20170403\ Feature\ Set\ \(training\ vs\ validation\)/s40_nh2000_ --dfrom '2017-03-11 00:00:00' --dto '2017-03-27 23:59:59'
 2606  python feature_create_v2.py --vendor 'eri' --lb '14' --la 1Â  --nh '2000' --nu "" --sample_days 40  --save True --cache True --o ../../../../DSS\ Internal-Softbank/40\ Intermediate\ Results/20170403\ Feature\ Set\ \(training\ vs\ validation\)/s40_nh2000_ --dfrom '2017-02-28 00:00:00' --dto '2017-03-27 23:59:59'
 2607  python feature_create_v2.py --vendor 'eri' --lb '14' --la 1Â  --nh '2000' --nu "" --sample_days 40  --save True --cache True --o ../../../../DSS\ Internal-Softbank/40\ Intermediate\ Results/20170403\ Feature\ Set\ \(training\ vs\ validation\)/s40_nh2000_ --dfrom '2017-03-11 00:00:00' --dto '2017-03-27 23:59:59'
 2608  python feature_create_v2.py --vendor 'eri' --lb '14' --la 1Â  --nh '2000' --nu "" --sample_days 40  --save True --cache True --o ../../../../DSS\ Internal-Softbank/40\ Intermediate\ Results/20170403\ Feature\ Set\ \(training\ vs\ validation\)/s40_nh2000_deep --dfrom '2017-03-11 00:00:00' --dto '2017-03-27 23:59:59'
 2609  python feature_create_v2.py --vendor 'eri' --lb '14' --la 1Â  --nh '2000' --nu "" --sample_days 40  --save True --cache True --o ../../../../DSS\ Internal-Softbank/40\ Intermediate\ Results/20170403\ Feature\ Set\ \(training\ vs\ validation\)/s40_nh2000_deep --dfrom '2017-02-28 00:00:00' --dto '2017-03-27 23:59:59'
 2610  cp preprocessing_oss_nok_20170221_20170317.ipynb preprocessing_oss_nok_20170317_20170327.ipynb
 2611  python feature_create_v2.py --vendor 'nok' --lb 14  --la 1  --nh ""  --nu "" --sample_days 40 --save True  --cache True  --o ../../../../DSS\ Internal-Softbank/40\ Intermediate\ Results/20170403\ Feature\ Set\ \(training\ vs\ validation\)/ --dfrom '2017-02-15 00:00:00' --dto '2017-03-17 23:59:59'
 2612  ln -s process_alarm_names_nok.py ./xgboost/process_alarm_names_nok.py
 2613  ln -s process_alarm_names_nok.py `pwd`/xgboost/process_alarm_names_nok.py
 2614  ln -s process_alarm_names_nok.py /Users/212339410/Box/Analytics/Projects/SB/Python/xgboost/process_alarm_names_nok.py
 2615  ln -s /Users/212339410/Box/Analytics/Projects/SB/Python/process_alarm_names_nok.py /Users/212339410/Box/Analytics/Projects/SB/Python/xgboost/process_alarm_names_nok.py
 2616  python model_selection.py --ifeat ../Src/ModData/feature_nok_0406.csv  --iconf config.ini --t Prob_Urgent --o ../Src/ModData/model_select_use_week_feature_nok_0406.csv --loglevel info --olog 'model_selection.log' --report 'csv' --score 'accracy'
 2617  python model_selection.py --ifeat ../Src/ModData/feature_nok_0406.csv  --iconf config.ini --t Prob_Urgent --o ../Src/ModData/model_select_use_week_feature_nok_0406.csv --loglevel info --olog 'model_selection.log' --report 'csv' --score 'accuracy'
 2618  python model_selection.py --ifeat ../Src/ModData/feature_nok_0406.csv  --iconf prediction_validation.ini --t Prob_Urgent --o ../Src/ModData/model_select_use_week_feature_nok_0406.csv --loglevel info --olog 'model_selection.log' --report 'csv' --score 'accuracy'
 2619  python model_selection.py --ifeat ../Src/ModData/feature_nok_0406.csv  --iconf prediction_validation.ini --t Prob_Urgent --o ../Src/ModData/0406_validation/model_select_use_feature_date170104_170228_nok_t0_lbh14_lah1_0405.csv --loglevel info --olog 'model_selection.log' --report 'csv' --score 'accuracy'
 2620  python feature_create_v2.py --vendor 'nok' --lb 14  --la 0  --nh ""  --nu "" --sample_days 40 --save True  --cache True  --o ../../../../DSS\ Internal-Softbank/40\ Intermediate\ Results/20170403\ Feature\ Set\ \(training\ vs\ validation\)/prediction_ --dfrom '2017-03-13 00:00:00' --dto '2017-03-27 23:59:59'
 2621  python feature_create_v2.py --vendor 'nok' --lb 14  --la 0  --nh ""  --nu "" --sample_days 40 --save True  --cache True  --o ../../../../DSS\ Internal-Softbank/40\ Intermediate\ Results/20170403\ Feature\ Set\ \(training\ vs\ validation\)/prediction_ --dfrom '2017-02-27 00:00:00' --dto '2017-03-27 23:59:59'
 2622  python model_selection.py --ifeat ../Src/ModData/feature_nok_0406.csv  --iconf prediction_validation.ini --t Prob_Urgent --o ../Src/ModData/0406_validation/model_select_prec_use_feature_date170104_170228_nok_t0_lbh14_lah1_0405.csv --loglevel info --olog 'model_selection.log' --report 'csv' --score 'precision'
 2623  python feature_create_v2.py --vendor 'nok' --lb 14  --la 0  --nh ""  --nu "" --sample_days 40 --save True  --cache True  --o ../../../../DSS\ Internal-Softbank/40\ Intermediate\ Results/20170403\ Feature\ Set\ \(training\ vs\ validation\)/prediction_ --dfrom '2017-01-31 00:00:00' --dto '2017-02-28 23:59:59'
 2624  python feature_create_v2.py --vendor 'nok' --lb 14  --la 0  --nh ""  --nu "" --sample_days 40 --save True  --cache True  --o ../Src/ModData/prediction_ --dfrom '2017-02-13 00:00:00' --dto '2017-02-28 23:59:59'
 2625  python model_selection.py --ifeat ../Src/ModData/feature_date170116_170312_eri_t0_lbh14_lah1_0405.csv  --iconf prediction_validation.ini --t Prob_Urgent --o ../Src/ModData/0406_validation/model_select_prec_use_feature_date170116_170312_eri_t0_lbh14_lah1_0405.csv --loglevel info --olog 'model_selection.log' --report 'csv' --score 'precision'
 2626  python eval_model_dt.py -i "../Src/ModData/prediction_feature_date170213_170228_nok_t0_lbh14_lah0_0406.csv" -o "../Src/ModData/model_" --target "Prob_Urgent" --n_healthy '1000'  --n_unhealthy '10000'  --vendor "nok" -exp True  
 2627  cd python-package
 2628  sudo python setup.py install
 2629  head -n 2000 OSS_Eri_20170116_20170127.csv > OSS_Eri_20170116_20170127_2000.csv
 2630  head -n 2000 BSW2016.csv > BSW2016_2000.csv
 2631  head -n 2000 NEDB_NOKERI_TQ.utf8.csv > NEDB_NOKERI_TQ_2000.csv
 2632  cd ~/Box/Analytics/Data/MBS2016
 2633  cd UTF8_
 2634  tail -n 2000 WOT_NOK_20151201_20170201.utf8.csv > WOT_NOK_20151201_20170201.utf8_2000.csv
 2635  cd ../Original_flat
 2636  tail -n 2000 NSN1.20160707_20160721.csv > NSN1.20160707_20160721_2000.csv
 2637  head -n 2000 WOT_NOK_20151201_20170201.utf8.csv > WOT_NOK_20151201_20170201.utf8_2000.csv
 2638  tail -n 2000 TTE_NOK_20151201_20170201.csv > TTE_NOK_20151201_20170201_2000.csv
 2639  cd ../UTF8_bom
 2640  head -n 2000 NEDB_NOK_Q.utf8.csv NEDB_NOK_Q.utf8_2000.csv
 2641  head -n 2000 NEDB_NOK_Q.utf8.csv > NEDB_NOK_Q.utf8_2000.csv
 2642  brew install ffmpeg
 2643  brew cask install x-quartz #dependency for gifsicle, only required for mountain-lion and above
 2644  brew cask install xquartz #dependency for gifsicle, only required for mountain-lion and above
 2645  which quartzz
 2646  which quartz
 2647  ln -s /Users/212339410/Box/Analytics/Python /Users/212339410/Python
 2648  rm abc.rst
 2649  git clone git@github.build.ge.com:IndustrialDataScience/tstk.git tstk.gh-pages
 2650  cd tstk.gh-pages
 2651  cd ~/Documents/WebEx
 2652  uninstall ffmpeg
 2653  brew uninstall ffmpeg
 2654  .tree
 2655  tree .
 2656  tree -l 2
 2657  tree -L 2
 2658  pip install git+ssh://github.build.ge.com/IndustrialDataScience/tstk.git
 2659  pip install pip
 2660  sudo pip install git+ssh://github.build.ge.com/IndustrialDataScience/tstk.git
 2661  git ls-remote
 2662  git remote add upstream git@github.build.ge.com:IndustrialDataScience/tstk.git
 2663  git checkout --track upstream/gh-pages
 2664  gut checkout master
 2665  brew install gifsicle
 2666  ffmpeg -i TSTK_documentation_clone_ghpages0410.mov -s 600x400 -pix_fmt rgb24 -r 10  -f gif - | gifsicle --optimize=3 --delay=3 > tstk_doc_clone_ghpages.gif 
 2667  ffmpeg -i TSTK_documentation_clone_ghpages0410.mov -s 800x600 -pix_fmt rgb8 -r 10  -f gif - | gifsicle --optimize=3 --delay=3 > tstk_doc_clone_ghpages.gif 
 2668  ffmpeg -i TSTK_documentation_clone_ghpages0410.mov -s 600x600 -r 10  -f gif - | gifsicle --optimize=3 --delay=3 > tstk_doc_clone_ghpages.gif 
 2669  ffmpeg -i TSTK_documentation_clone_ghpages0410.mov  -r 10  -f gif - | gifsicle --optimize=3 --delay=3 > tstk_doc_clone_ghpages.gif 
 2670  brew cask install xquartz
 2671  xquartz
 2672  brew install ffmpeg imagemagick gifsicle pkg-config
 2673  gem install screengif     
 2674  sudo gem install screengif     
 2675  screenfig
 2676  screengif
 2677  screengif -i TSTK_documentation_clone_ghpages0410.mov -o tstk_clone_ghpages.gif 
 2678  cd tstk/
 2679  mkdir images
 2680  cp tstk_doc_clone_ghpages.gif ~/Python/tstk/doc/images
 2681  cp tstk_doc_clone_ghpages.gif ~/Python/tstk/doc/images/tstk_doc_clone_ghpages.gif
 2682  cp tstk_clone_ghpages.gif ~/Python/tstk/doc/images/tstk_doc_clone_ghpages.gif
 2683  mkdir predix_studio
 2684  cd ~/Box/Analytics/Data
 2685  cd MBS2016
 2686  cd UTF8_bom
 2687  cd 20170328_Ingestion
 2688  head Tokyo_RAN_OSS_Eri_20170322_20170327.csv 
 2689  head -n 3000 Tokyo_RAN_OSS_Eri_20170322_20170327.csv > ~/Projects/SB/Python/predix_studio/Tokyo_RAN_OSS_Eri_20170322_20170327_3000.csv
 2690  cd ../../20170306_Ingestion
 2691  head -n 3000 bsw_20151201_20170201.csv > ~/Projects/SB/Python/predix_studio/bsw_20151201_20170201_3000.csv
 2692  head -n 3000 tte_20151201_20170201.csv > ~/Projects/SB/Python/predix_studio/tte_20151201_20170201_3000.csv
 2693  head -n 3000 wot_20151201_20170201.csv > ~/Projects/SB/Python/predix_studio/wot_20151201_20170201_3000.csv
 2694  cd ../20170310_Ingestion
 2695  cd INOS
 2696  gzip INOS_ERI_20170223.csv.gz | head -n 3000 > ~/Projects/SB/Python/predix_studio/INOS_ERI_20170223_3000.csv
 2697  gzip -d INOS_ERI_20170223.csv.gz | head -n 3000 > ~/Projects/SB/Python/predix_studio/INOS_ERI_20170223_3000.csv
 2698  cd predix_studio
 2699  gzip -d INOS_ERI_20170223.csv.gz  ~/Projects/SB/Python/predix_studio/INOS_ERI_20170223_3000.csv
 2700  gzip -d INOS_ERI_20170223.csv.gz  > ~/Projects/SB/Python/predix_studio/INOS_ERI_20170223_3000.csv
 2701  gzip -d INOS_ERI_20170223.csv.gz  > ~/Projects/SB/Python/predix_studio/
 2702  gzip -d INOS_ERI_20170223.csv.gz  > ~/Projects/SB/Python/predix_studio
 2703  gzip -d INOS_ERI_20170223.csv.gz  -C  ~/Projects/SB/Python/predix_studio
 2704  unzip INOS_ERI_20170223.csv.gz  -C  ~/Projects/SB/Python/predix_studio
 2705  man unzip
 2706  unzip INOS_ERI_20170223.csv -d ~/Projects/SB/Python/predix_studio
 2707  gunzip -d INOS_ERI_20170307.csv.gz -d ~/Projects/SB/Python/predix_studio
 2708  cp INOS_ERI_20170223.csv ~/Projects/SB/Python/predix_studio/INOS_ERI_20170223.csv
 2709  gunzip INOS_ERI_20170223.csv
 2710  gunzip -d INOS_ERI_20170223.csv
 2711  head -n 3000 INOS_ERI_20170223.csv > INOS_ERI_20170223_3000.csv
 2712  ../../20170327_Ingestion
 2713  cd ../../20170321_Ingestion
 2714  cd ../20170328_Ingestion
 2715  cd ../../20170310_Ingestion
 2716  mc
 2717  10
 2718  :10
 2719  Q
 2720  cd ../../20170323_Ingestion
 2721  cd Output
 2722  head -n 3000 nedb_2016summer_2017TQ.csv ~/Projects/SB/Python/predix_studio/nedB_3000.csv
 2723  head -n 3000 nedb_2016summer_2017TQ.csv > ~/Projects/SB/Python/predix_studio/nedB_3000.csv
 2724  pip install lifelines
 2725  screengif -i tstk_sphinx_build_gif.mov  -o tstk_sphinx_build.gif 
 2726  cp tstk_sphinx_build.gif ~/Python/tstk/doc/images/tstk_sphinx_build.gif
 2727  docker -h
 2728  docker start
 2729  docker start --help
 2730  docker start -i
 2731  docker pull pathtrk/opencv-python3
 2732  docker run pathtrk/opencv-python3
 2733  docker run pathtrk/opencv-python3; ipython
 2734  docker run pathtrk/opencv-python3 --version
 2735  docker run python  python --version
 2736  docker run python ls
 2737  docker run pathtrk/opencv-python3 ls
 2738  docker run pathtrk/opencv-python3 ipython
 2739  docker run pathtrk/opencv-python3 python --version
 2740  docker run images
 2741  docker run -i -t pathtrk/opencv-python3
 2742  docker run -i -t pathtrk/opencv-python3 /bin/bash 
 2743  run ln -s /usr/local/bin/python3.5 /usr/local/bin/python3
 2744  docker run ln -s /usr/local/bin/python3.5 /usr/local/bin/python3
 2745  python update_schema.py "/Users/212339410/Box/DSS Internal-Softbank/12 Working Directory for Ingestion/20170321_Ingestion/INOS/INOS_ERI_20170223.csv"  ../Src/Schema/inos_v1.xml  ../Src/Schema/inos_v2.xml
 2746  less "/Users/212339410/Box/DSS Internal-Softbank/12 Working Directory for Ingestion/20170321_Ingestion/INOS/INOS_ERI_20170223.csv"
 2747  cd Projects/SB/Py
 2748  cd ~/SB_box_ingest_working_dir/Talen_Working_Directory
 2749  vim INOS_ERI_20170223.csv
 2750  python model_selection.py --demo 
 2751  python feature_create_v2.py --vendor 'nok' --lb 14  --la 0  --nh 3000  --nu "" --sample_days 40 --save True  --cache True  --o ../Src/ModData/prediction_ --dfrom '2017-02-13 00:00:00' --dto '2017-02-28 23:59:59'
 2752  vim ~/SB_box_ingest_working_dir/Talen_Working_Directory/INOS_ERI_20170223.csv
 2753  vim ~/SB_box_ingest_working_dir/Talen_Working_Directory/INOS_ERI_20170223.csv.gz
 2754  pip install tpot
 2755  pip install xgboost
 2756  cd `which python`
 2757  cd /Users/212339410/anaconda2/envs/py35/lib/python3.5/site-packages
 2758  cd xgboost-0.6-py3.5.egg/
 2759  cd xgboost
 2760  ln -s ~/anaconda2/envs/py35/lib/python3.5/site-packages/xgboost-0.6-py3.5.egg/xgboost ~/anaconda2/envs/py35/lib/python3.5/site-packages/xgboost
 2761  pip install prettytable
 2762  python update_schema.py "/Users/212339410/Box/DSS Internal-Softbank/12 Working Directory for Ingestion/Archive_for_ingested_data/INOS_NOK_20170307.csv.gz"  ../Src/Schema/inos_v4.xml  ../Src/Schema/inos_v5.xml
 2763  python update_schema.py "/Users/212339410/Box/DSS Internal-Softbank/12 Working Directory for Ingestion/Archive_for_ingested_data/INOS_NOK_20170307.csv.csv"  ../Src/Schema/inos_v4.xml  ../Src/Schema/inos_v5.xml
 2764  iconv -c -f SHIFT_JIS -t UTF-8 ./BSW_ERI_20170327_20170411.csv >> ./BSW_ERI_20170327_20170411.utf8.csv
 2765  vim BSW_ERI_20170327_20170411.csv
 2766  open BSW_ERI_20170327_20170411.
 2767  vim BSW_ERI_20170327_20170411.utf8.csv
 2768  open BSW_ERI_20170327_20170411.utf8.csv
 2769  python update_schema.py "/Users/212339410/Box/DSS Internal-Softbank/12 Working Directory for Ingestion/Archive_for_ingested_data/INOS_NOK_20170307.csv"  ../Src/Schema/inos_v4.xml  ../Src/Schema/inos_v5.xml
 2770  git clone git@github.build.ge.com:IndustrialDataScience/AutoModel.git
 2771  cd AutoModel
 2772  mkdir automodel
 2773  cd automodel
 2774  rm -rf AutoModel
 2775  git clone git@github.build.ge.com:IndustrialDataScience/OptModel.git
 2776  mkdir optmodel
 2777  vim bsw_20170327_20170411.csv
 2778  vim bsw_20170327_20170411\ copy.csv
 2779  python model_selection.py 
 2780  python model_selection.py -h
 2781  python model_selection.py --demo
 2782  mv `ls *.log` ./log/
 2783  ls *.log
 2784  cd log
 2785  tpot ~/Downloads/mnist_train.csv -is , -target class -o tpot_exported_pipeline.py -g 5 -p 20 -cv 5 -s 42 -v 2
 2786  vagrant
 2787  rm Vagrantfile
 2788  mkdir DataScienceToolbox
 2789  cd DataScienceToolbox
 2790  vagrant init data-science-toolbox/data-science-at-the-command-line
 2791  Vagrant.configure(2) do |config|\n  config.vm.box = "data-science-toolbox/data-science-at-the-command-line"
 2792  vagrant login
 2793  cat Vagrantfile
 2794  vagrant up
 2795  man header
 2796  header
 2797  man head
 2798  csvlook ~/Downloads/mnist_train.csv
 2799  csvlook ~/Downloads/mnist_train.csv | less -S
 2800  csvcut -n NEDB_NOKERI_TQ.utf8.csv
 2801  csvcut -n bsw0412.csv
 2802  csvcut -n BSW2016_2000.csv
 2803  csvcut -c BSW_Company_name | head
 2804  head BSW2016_2000.csv | csvcut -c BSW_Work_hours | csvstat
 2805  head BSW2016_2000.csv | csvcut -c BSW_Rank_of_Work | csvstat
 2806  head BSW2016_2000.csv | csvstat
 2807  csvstat BSW2016_2000.csv
 2808  csvcut -c BSW_Company_name | csvlook
 2809  csvcut -c BSW_Company_name BSW2016_2000.csv| csvlook
 2810  head feature0327df_event_uh_proc_nok_lbh7_lah1_0327.csv| csvlook
 2811  head feature0327df_event_uh_proc_nok_lbh7_lah1_0327.csv| csvstat
 2812  csvcut -n feature0327df_event_uh_proc_nok_lbh7_lah1_0327.csv
 2813  head ~/Downloads/mnist_train.csv | csvlook
 2814  cd 0403_feature_nok
 2815  tpot feature_nok_lbh14_lah1_Prob_Urgent_0403.csv -target 'Prob_Urgent' -o tpot_exported_pipeline.py -g -p 20 -cv 5 -s 42 -v 2
 2816  tpot feature_nok_lbh14_lah1_Prob_Urgent_0403.csv -target 'Prob_Urgent' -o tpot_exported_pipeline.py -g 5 -p 20 -cv 5 -s 42 -v 2
 2817  head feature_nok_lbh14_lah1_Prob_Urgent_0403.csv | csvlook
 2818  head feature_nok_lbh14_lah1_Prob_Urgent_0403.csv | csvcut -n
 2819  head feature_nok_lbh14_lah1_Prob_Urgent_0403.csv | csvcut -n -C incident_id
 2820  csvcut -C incident_id -n 
 2821  csvcut -C incident_id feature_nok_lbh14_lah1_Prob_Urgent_0403.csv| tpot -is , -target Prob_Urgent -o tpot_exported_pipeline.py -g 5 -p 20 -cv 5 -s 42 -v 2
 2822  csvcut -C incident_id feature_nok_lbh14_lah1_Prob_Urgent_0403.csv
 2823  csvcut -C incident_id feature_nok_lbh14_lah1_Prob_Urgent_0403.csv > feature_nok_lbh14_lah1_Prob_Urgent_0403_tpot.csv
 2824  tpot feature_nok_lbh14_lah1_Prob_Urgent_0403_tpot.csv -is , -target Prob_Urgent -o tpot_exported_pipeline.py -g 5 -p 20 -cv 5 -s 42 -v 2
 2825  head feature_nok_lbh14_lah1_Prob_Urgent_0403.csv | csvcut -n 
 2826  cd ../0327_feature
 2827  cd ../0402_feature_nok
 2828  cd ../0331_feature
 2829  csvstat -n featurefeature_nok_lbh14_lah10_0331_Prob_Urgent.csv
 2830  csvstat featurefeature_nok_lbh14_lah10_0331_Prob_Urgent.csv -C incident_id > featurefeature_nok_lbh14_lah10_0331_Prob_Urgent_tpot.csv
 2831  csvcut featurefeature_nok_lbh14_lah10_0331_Prob_Urgent.csv -C incident_id > featurefeature_nok_lbh14_lah10_0331_Prob_Urgent_tpot.csv
 2832  tpot featurefeature_nok_lbh14_lah10_0331_Prob_Urgent_tpot.csv
 2833  vagrant ssh
 2834  awk
 2835  tpot mnist.csv.gz -is , -target class -o tpot_exported_pipeline.py -g 5 -p 20 -cv 5 -s 42 -v 2
 2836  csvstat -n mnist.csv
 2837  head mnist.csv
 2838  head mnist.csv -n 1
 2839  head -n 1 mnist.csv 
 2840  tpot mnist.csv -is \t  -target class -o tpot_exported_pipeline.py -g 5 -p 20 -cv 5 -s 42 -v 2
 2841  vim mnist.csv
 2842  tpot mnist.csv -is ,  -target class -o tpot_exported_pipeline.py -g 5 -p 20 -cv 5 -s 42 -v 2
 2843  mkdir test_script
 2844  mv test_* ./test_script
 2845  mv test_* > ./test_script
 2846  mv test_*.py > ./test_script
 2847  python feature_create_v3.py --vendor nok --dto 20170327 
 2848  python feature_create_v3.py --vendor nok --dto 20170327 --lb 14
 2849  python feature_create_v3.py --vendor nok --dto 20170327 -o ../Src/ModData/0413_feature
 2850  python feature_create_v3.py --vendor nok --dto 20170327 --o ../Src/ModData/0413_feature
 2851  python feature_create_v3.py --vendor nok --dto 20170327 --o ../Src/ModData/0413_feature/
 2852  python feature_create_v3.py --vendor nok --dto 20170327 --o ../Src/ModData/0413_feature/ 
 2853  python feature_create_v3.py --vendor nok --dto 20170327 --o ../Src/ModData/0413_feature/ --cache
 2854  seq {1..15}
 2855  seq 1..15
 2856  seq 115
 2857  seq 1 15
 2858  seq 20170301 20170327
 2859  for i in seq 20170301 20170327\ndo \necho $i\ndone
 2860  for i in {20170301.. 20170327} do \necho $i\ndone
 2861  `seq 20170301 20170327`
 2862  for i in `seq 20170301 20170327`\ndo \necho $i\ndone
 2863  for i in `seq 20170301 20170327`\ndo\necho $i\npython feature_create_v3.py --vendor nok --dto $i --cache\ndone
 2864  seq -g 20170301 20170327
 2865  seq --g 20170301 20170327
 2866  seq 1 5 | xargs -I {} date -d "2017030{}" +%Y%m%d
 2867  python feature_create_v3.py --vendor eri --dto 20170327 --o ../Src/ModData/0413_feature/ --cache
 2868  enddate=20170327
 2869  for (( date="$startdate"; date != enddate; )); do\n    dates+=( "$date" )\n    date="$(date --date="$date + 1 days" +'%Y%m%d')"\ndone
 2870  echo "${dates[@]}"
 2871  dates=()
 2872  for (( date="$startdate"; date != enddate; ))\ndo\ndates+=("$date")\ndate="$(date --date="$date + 1 days" +'%Y%m%d')"\ndone
 2873  find . -mtime $start -mtime $end
 2874  cd Projects/SB/Bash
 2875  date 20170120
 2876  date 20170120 -f %Y%m%d
 2877  date 20170120 -f yyyymmdd
 2878  date 20170120 -f yymmdd
 2879  gdate
 2880  todaty
 2881  yoday
 2882  today
 2883  date
 2884  sh run_feature_create_v3.sh
 2885  date -j -f '%d-%b-%Y' "22-Aug-2013" "+%s"
 2886  date -j -f '%Y%b%d' startdate "+%s"
 2887  date -j -f '%s' startdate "+%s"
 2888  startdate=170120
 2889  date -j -f '%s' startdate
 2890  date 032717
 2891  date 0327175013
 2892  date -j -vJulm -v20d -v1999y '+%A'
 2893  date -f ddMMyy 
 2894  date -f 101010
 2895  date "+DATE: %Y-%m-%d%nTIME: %H:%M:%S"
 2896  date "+DATE: %Y-%m-%d TIME: %H:%M:%S"
 2897  date "DATE: %Y-%m-%d TIME: %H:%M:%S"
 2898  +DATE
 2899  date "+%Y-%m-%d %H:%M:%S"
 2900  date "%Y-%m-%d %H:%M:%S"
 2901  date "+%Y%m%d"
 2902  echo $TIMESTAMP
 2903  for i in `seq 10`; do date '+%Y%m%d' -d ${i}; echo -e "\n\n\n"; done
 2904  for i in `seq 10`; do date '+%Y%m%d' -d ${i}day; echo -e "\n\n\n"; done
 2905  for i in `seq 100`; do date '+%Y/%m/%d (%a)' -d ${i}day; echo -e "\n\n\n"; done
 2906  date -d `1 day ago`
 2907  echo `date -v-1m` +"%Y%m%d"
 2908  echo `date -v-1m` +"%Y%m%d"`\ne\n[\n`
 2909  echo `date -v-1m +"%Y%m%d"`
 2910  for D in `seq 1 60`\ndo\necho  $(date -v-${D}d +%Y-%m-%d)\ndone
 2911  for D in `seq 1 80`\ndo\necho $(date -v-${D}d +%Y%m%d)\ndone
 2912  for D in `seq 1 90`\ndo\necho $(date -v-${D}d +%Y%m%d)\ndone
 2913  for D in `seq 1 85`\ndo\necho $(date -v-${D}d +%Y%m%d)\ndone
 2914  for D in `seq 1 10`\ndo\necho $(date -v-${D}d +%Y%m%d)\npython feature_create_v3.py --vendor eri --dto $(date -v-${D}d +%Y%m%d) --o ../Src/ModData/0413_feature/ --cache\ndone
 2915  for D in `seq 10 83`\ndo\necho $(date -v-${D}d +%Y%m%d)\npython feature_create_v3.py --vendor eri --dto $(date -v-${D}d +%Y%m%d) --o ../Src/ModData/0413_feature/ --cache\ndone
 2916  for D in `seq 1 83`\ndo\necho $(date -v-${D}d +%Y%m%d)\npython feature_create_v3.py --vendor nok --dto $(date -v-${D}d +%Y%m%d) --o ../Src/ModData/0413_feature/ --cache\ndone
 2917  cd 0413_feature
 2918  for f in ./*.csv\ndo\necho $f.gzip\n#gzip $f.gzip\ndone
 2919  for f in ./*.csv$\ndo\necho $f.gzip\n#gzip $f.gzip\ndone
 2920  ls | grep *.csv$
 2921  ls | grep *
 2922  ls | grep .
 2923  ls | grep .*.csv
 2924  ls | grep *.csv
 2925  ls | grep . *csv
 2926  ls | grep csv
 2927  ls | grep csv$
 2928  ls | grep csv$for f in `ls | grep csv$`
 2929  for f in `ls | grep csv$`\ndo\necho $f.gzip\n#gzip $f.gzip\ndone
 2930  gzip feature_nok_0106_0120_lbh14_lah1_ival14.csv -S .gzip
 2931  gzip feature_nok_0106_0120_lbh14_lah1_ival14.csv
 2932  gzip feature_nok_0106_0120_lbh14_lah1_ival14.csv --suffix ".gzip"
 2933  gzip feature_nok_0106_0120_lbh14_lah1_ival14.csv --suffix .gzip
 2934  for f in `ls | grep csv$`\ndo\necho $f.gzip\ngzip $f -S .gzip\ndone
 2935  for D in `seq 1 69`\ndo\necho $(date -v-${D}d +%Y%m%d)\npython feature_create_v3.py --vendor nok --dto $(date -v-${D}d +%Y%m%d) --lb 28 --o ../Src/ModData/0413_feature/ --cache\ndone
 2936  for D in `seq 1 69`\ndo\necho $(date -v-${D}d +%Y%m%d)\npython feature_create_v3.py --vendor eri --dto $(date -v-${D}d +%Y%m%d) --lb 28 --o ../Src/ModData/0413_feature/ --cache\ndone
 2937  drop view OSS_Count_table
 2938  ln -s /Users/212339410/Box/DSS\ Internal-Softbank/40\ Intermediate\ Results/20170414\ Feature/0414_feature /Users/212339410/Box/Analytics/Projects/SB/Src/ModData/0414_feature 
 2939  for D in `seq 2 72`\ndo\necho $(date -v-${D}d +%Y%m%d)\npython feature_create_v3.py --vendor eri --dto $(date -v-${D}d +%Y%m%d) --o ../Src/ModData/0414_feature/ --cache\ndone
 2940  for D in `seq 2 84`\ndo\necho $(date -v-${D}d +%Y%m%d)\npython feature_create_v3.py --vendor nok --dto $(date -v-${D}d +%Y%m%d) --o ../Src/ModData/0414_feature/ --cache\ndone
 2941  cp preprocessing_oss_nok_20170317_20170327.ipynb preprocessing_oss_nok_20170327_20170410.ipynb
 2942  for D in `seq 72 86`\ndo\necho $(date -v-${D}d +%Y%m%d)\npython feature_create_v3.py --vendor eri --dto $(date -v-${D}d +%Y%m%d) --o ../Src/ModData/0414_feature/ --cache\ndone
 2943  for D in `seq 85 86`\ndo\necho $(date -v-${D}d +%Y%m%d)\npython feature_create_v3.py --vendor nok --dto $(date -v-${D}d +%Y%m%d) --o ../Src/ModData/0414_feature/ --cache\ndone
 2944  for D in `seq 73 774`\ndo\necho $(date -v-${D}d +%Y%m%d)\npython feature_create_v3.py --vendor eri --dto $(date -v-${D}d +%Y%m%d) --o ../Src/ModData/0414_feature/ --cache\ndone
 2945  D=15
 2946  D=18
 2947  python feature_create_v3.py --vendor nok --dto $(date -v-${D}d +%Y%m%d) --o ../Src/ModData/0414_feature_duration/ --cache
 2948  for D in `seq 2 84`\ndo\necho $(date -v-${D}d +%Y%m%d)\npython feature_create_v3.py --vendor nok --dto $(date -v-${D}d +%Y%m%d) --o ../Src/ModData/0414_feature_duration/ --cache
 2949  for D in `seq 17 84`\ndo\necho $(date -v-${D}d +%Y%m%d)\npython feature_create_v3.py --vendor nok --dto $(date -v-${D}d +%Y%m%d) --o ../Src/ModData/0414_feature_duration/ --cache\ndone
 2950  for D in `seq 17 72`\ndo\necho $(date -v-${D}d +%Y%m%d)\npython feature_create_v3.py --vendor eri --dto $(date -v-${D}d +%Y%m%d) --o ../Src/ModData/0414_feature_duration/ --cache\ndone
 2951  python model_selection.py --ifeat ../../../../DSS\ Internal-Softbank/40\ Intermediate\ Results/20170414\ Feature/0414_Xy_set/Train_Validation_Test/Ericsson/xy_train_eri_0118_0312_lbh14_lah1_ival14_t1_f10.csv --iconf prediction_validation.ini --t Prob_Urgent --o ../Src/ModData/0414_model_selection/xy_train_eri_0118_0312_lbh14_lah1_ival14_t1_f10.csv  --loglevel info --olog 'model_selection.log' --report 'csv' --score 'precision'
 2952  python model_selection.py --ifeat ../../../../DSS\ Internal-Softbank/40\ Intermediate\ Results/20170414\ Feature/0414_Xy_set/Train_Validation_Test/Ericsson/xy_train_eri_0118_0312_lbh14_lah1_ival14_t1_f10.csv --iconf prediction_validation.ini --t Prob_Urgent --o ../Src/ModData/0414_model_selection/xy_train_eri_0118_0312_lbh14_lah1_ival14_t1_f10.csv  --loglevel info --olog 'model_selection.log' --report 'csv' --score 'f1'
 2953  python model_selection.py --ifeat ../../../../DSS\ Internal-Softbank/40\ Intermediate\ Results/20170414\ Feature/0414_Xy_set/Train_Validation_Test/Ericsson/xy_train_eri_0118_0312_lbh14_lah1_ival14_t1_f10.csv --iconf prediction_validation.ini --t Prob_Urgent --o ../Src/ModData/0414_model_selection/xy_train_eri_0118_0312_lbh14_lah1_ival14_t1_f10.csv  --loglevel info --olog 'model_selection.log' --report 'csv' --score 'accuracy'
 2954  python model_selection.py --ifeat ../../../../DSS\ Internal-Softbank/40\ Intermediate\ Results/20170414\ Feature/0414_Xy_set/Train_Validation_Test/Ericsson/xy_train_eri_0118_0312_lbh14_lah1_ival14_t1_f10.csv --iconf model_selection.ini --t Prob_Urgent --o ../Src/ModData/0414_model_selection/xy_train_eri_0118_0312_lbh14_lah1_ival14_t1_f10.csv  --loglevel info --olog 'model_selection.log' --report 'csv' --score 'precision'
 2955  python prediction_validation.py -h
 2956  python prediction_validation.py --ifeat /Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170414 Feature/0414_Xy_set/Train_Test/Ericsson/xy_test_eri_0328_0410.csv --model_dir ../Src/ModData/0414_model_selection/ --vendor nok --o 0414_test/0328_0410_ --target Prob_Urgent
 2957  python prediction_validation.py --ifeat '/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170414 Feature/0414_Xy_set/Train_Test/Ericsson/xy_test_eri_0328_0410.csv' --model_dir ../Src/ModData/0414_model_selection/ --vendor nok --o 0414_test/0328_0410_ --target Prob_Urgent
 2958  python prediction_validation.py --ifeat '/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170414 Feature/0414_Xy_set/Train_Test/Ericsson/xy_test_eri_0328_0410.csv' --model_dir ../Src/ModData/0414_model_selection/ --vendor nok --o 0414_test/0328_0410_ --t Prob_Urgent
 2959  OA
 2960  python prediction_validation.py --ifeat '/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170414 Feature/0414_Xy_set/Train_Test/Ericsson/xy_test_eri_0328_0410.csv' --model_dir ../Src/ModData/0414_model_selection/ --vendor nok --o ../Src/ModData/0414_test/0328_0410_ --t Prob_Urgent
 2961  python prediction_validation.py --ifeat '/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170414 Feature/0414_Xy_set/Train_Test/Ericsson/xy_test_eri_0328_0410.csv' --model_dir ../Src/ModData/0414_model_selection/ --vendor eri --o ../Src/ModData/0414_test/0328_0410_ --t Prob_Urgent
 2962  python prediction_validation.py --ifeat '/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170414 Feature/0414_Xy_set/Train_Validation_Test/Ericsson/xy_test_eri_0328_0410.csv' --model_dir ../Src/ModData/0414_model_selection/ --vendor eri --o ../Src/ModData/0414_test/0328_0410_ --t Prob_Urgent
 2963  python prediction_validation.py --ifeat '/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170414 Feature/0414_Xy_set/Train_Validation_Test/Ericsson/xy_validation_eri_0328_0410.csv' --model_dir ../Src/ModData/0414_model_selection/ --vendor eri --o ../Src/ModData/0414_test/0328_0410_validation_ --t Prob_Urgent
 2964  csvcut '/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170414 Feature/0414_Xy_set/Train_Validation_Test/Ericsson/xy_validation_eri_0328_0410.csv' Prob_Urgent
 2965  csvcut '/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170414 Feature/0414_Xy_set/Train_Validation_Test/Ericsson/xy_validation_eri_0328_0410.csv' -c Prob_Urgent
 2966  csvcut ''/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170414 Feature/0414_Xy_set/Train_Validation_Test/Ericsson/xy_validation_eri_0328_0410.csv'
 2967  csvcut '/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170414 Feature/0414_Xy_set/Train_Validation_Test/Ericsson/xy_validation_eri_0328_0410.csv' -n
 2968  csvcut -n "/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170414 Feature/0414_Xy_set/Train_Validation_Test/Ericsson/xy_validation_eri_0328_0410.csv"
 2969  csvcut "/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170414 Feature/0414_Xy_set/Train_Validation_Test/Ericsson/xy_validation_eri_0313_0327_lbh14_lah1_ival14.csv" -n
 2970  csvcut "/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170414 Feature/0414_Xy_set/Train_Validation_Test/Ericsson/xy_validation_eri_0313_0327_lbh14_lah1_ival14.csv" -C sum > ../Src/ModData/xy_validation_eri_0313_0327_lbh14_lah1_ival14.csv
 2971  python model_selection.py --ifeat ../../../../DSS\ Internal-Softbank/40\ Intermediate\ Results/20170414\ Feature/0414_Xy_set/Train_Validation_Test/Ericsson/xy_train_eri_0118_0312_lbh14_lah1_ival14_t1_f10.csv --iconf model_selection.ini --t Prob_Urgent --o ../Src/ModData/0414_model_selection/xy_train_eri_0118_0312_lbh14_lah1_ival14_t1_f10.csv  --loglevel info --olog 'model_selection.log'  --score 'accuracy'
 2972  python prediction_validation.py --ifeat '/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170414 Feature/0414_Xy_set/Train_Validation_Test/Ericsson/xy_validation_eri_0313_0327_lbh14_lah1_ival14.csv' --model_dir ../Src/ModData/0414_model_selection/ --vendor eri --o ../Src/ModData/0414_test/0328_0410_validation_ --t Prob_Urgent
 2973  python prediction_validation.py --ifeat '../Src/ModData/xy_validation_eri_0313_0327_lbh14_lah1_ival14.csv' --model_dir ../Src/ModData/0414_model_selection/ --vendor eri --o ../Src/ModData/0414_test/0328_0410_validation_ --t Prob_Urgent
 2974  python prediction_validation.py --ifeat '/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170414 Feature/0414_Xy_set/Train_Test/Ericsson/xy_test_eri_0328_0410.csv' --model_dir ../Src/ModData/0414_model_selection/ --vendor eri --o ../Src/ModData/0414_test/0328_0410_test_ --t Prob_Urgent
 2975  csvcut /Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170414 Feature/0414_Xy_set/Train_Test/Ericsson/xy_train_eri_0118_0327_lbh14_lah1_ival14_t1_f10.csv -n
 2976  csvcut -n "/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170414 Feature/0414_Xy_set/Train_Test/Ericsson/xy_train_eri_0118_0327_lbh14_lah1_ival14_t1_f10.csv"
 2977  csvcut -n "/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170414 Feature/0414_Classification_Results/pred_nok_Apr_14.csv"
 2978  csvcut -c [BSID, PROB] "/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170414 Feature/0414_Classification_Results/pred_nok_Apr_14.csv" > ~/Projects/SB/Src/ModData/pred_nok_0414_young.csv
 2979  csvcut -c BSID, PROB  "/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170414 Feature/0414_Classification_Results/pred_nok_Apr_14.csv" > ~/Projects/SB/Src/ModData/pred_nok_0414_young.csv
 2980  csvcut   "/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170414 Feature/0414_Classification_Results/pred_nok_Apr_14.csv" -c BSID, PROB  > ~/Projects/SB/Src/ModData/pred_nok_0414_young.csv
 2981  csvcut   "/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170414 Feature/0414_Classification_Results/pred_nok_Apr_14.csv" -c BSID,PROB  > ~/Projects/SB/Src/ModData/pred_nok_0414_young.csv
 2982  csvcut   "/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170414 Feature/0414_Classification_Results/pred_eri_Apr_14.csv" -c BSID,PROB  > ~/Projects/SB/Src/ModData/pred_eri_0414_young.csv
 2983  python prediction_validation.py --ifeat '/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170414 Feature/0414_Xy_set/Train_Test/Nokia/xy_test_nok_0328_0410.csv' --model_dir ../Src/ModData/0414_model_selection/ --vendor nok --o ../Src/ModData/0414_test/0328_0410_test_ --t Prob_Urgent
 2984  python model_selection.py --ifeat ../../../../DSS\ Internal-Softbank/40\ Intermediate\ Results/20170414\ Feature/0414_Xy_set/Train_Validation_Test/Nokia/xy_train_nok_0106_0312_lbh14_lah1_ival14_t1_f10.csv --iconf model_selection.ini --t Prob_Urgent --o ../Src/ModData/0414_model_selection/xy_train_nok_0106_0312_lbh14_lah1_ival14_t1_f10.csv
 2985  python model_selection.py --ifeat ../../../../DSS\ Internal-Softbank/40\ Intermediate\ Results/20170414\ Feature/0414_Xy_set/Train_Validation_Test/Ericsson/xy_train_eri_0118_0312_lbh14_lah1_ival14_t1_f10.csv --iconf model_selection.ini --t Prob_Urgent --o ../Src/ModData/0414_model_selection/xy_train_eri_0118_0312_lbh14_lah1_ival14_t1_f10.csv  --olog /log/model_selection_eri0414.log
 2986  python model_selection.py --ifeat ../../../../DSS\ Internal-Softbank/40\ Intermediate\ Results/20170414\ Feature/0414_Xy_set/Train_Validation_Test/Ericsson/xy_train_eri_0118_0312_lbh14_lah1_ival14_t1_f10.csv --iconf model_selection.ini --t Prob_Urgent --o ../Src/ModData/0414_model_selection/xy_train_eri_0118_0312_lbh14_lah1_ival14_t1_f10.csv  --olog ./log/model_selection_eri0414.log
 2987  python model_selection.py --ifeat ../../../../DSS\ Internal-Softbank/40\ Intermediate\ Results/20170414\ Feature/0414_Xy_set/Train_Validation_Test/Nokia/xy_train_nok_0106_0312_lbh14_lah1_ival14_t1_f10.csv --iconf model_selection.ini --t Prob_Urgent --o ../Src/ModData/0414_model_selection/xy_train_nok_0106_0312_lbh14_lah1_ival14_t1_f10.csv --score precision
 2988  python model_selection.py --ifeat ../../../../DSS\ Internal-Softbank/40\ Intermediate\ Results/20170414\ Feature/0414_Xy_set/Train_Validation_Test/Nokia/xy_train_nok_0106_0312_lbh14_lah1_ival14_t1_f10.csv --iconf model_selection.ini --t Prob_Urgent --o ../Src/ModData/0415_model_selection/xy_train_nok_0106_0312_lbh14_lah1_ival14_t1_f10.csv --score precision
 2989  python model_selection.py --ifeat ../../../../DSS\ Internal-Softbank/40\ Intermediate\ Results/20170414\ Feature/0414_Xy_set/Train_Validation_Test/Nokia/xy_train_nok_0106_0312_lbh14_lah1_ival14_t1_f10.csv --iconf model_selection.ini --t Prob_Urgent --o ../Src/ModData/0415_model_selection/xy_train_nok_0106_0312_lbh14_lah1_ival14_t1_f10.csv --score precision --olog ./log/model_selection_nok0415.log
 2990  python prediction_validation.py --ifeat '/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170414 Feature/0414_Xy_set/Train_Test/Nokia/xy_test_nok_0328_0410.csv' --model_dir ../Src/ModData/0415_model_selection/ --vendor nok --o ../Src/ModData/0414_test/precision_0328_0410_test_ --t Prob_Urgent
 2991  python prediction_validation.py --ifeat '/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170414 Feature/0414_Xy_set/Train_Validation_Test/Nokia/xy_validation_nok_0313_0327_lbh14_lah1_ival14.csv' --model_dir ../Src/ModData/0414_model_selection/ --vendor nok --o ../Src/ModData/0414_test/0328_0410_valid_ --t Prob_Urgent
 2992  python prediction_validation.py --ifeat '/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170414 Feature/0414_Xy_set/Train_Validation_Test/Nokia/xy_validation_nok_0313_0327_lbh14_lah1_ival14.csv' --model_dir ../Src/ModData/0415_model_selection/ --vendor nok --o ../Src/ModData/0414_test/0328_0410_valid_ --t Prob_Urgent
 2993  csvcut -n /Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170414 Feature/0414_Xy_set/Train_Validation_Test/Nokia/xy_validation_nok_0313_0327_lbh14_lah1_ival14.csv
 2994  csvcut -n "/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170414 Feature/0414_Xy_set/Train_Validation_Test/Nokia/xy_validation_nok_0313_0327_lbh14_lah1_ival14.csv"
 2995  python prediction_validation.py --ifeat ../Src/ModData/xy_validation_eri_0313_0327_lbh14_lah1_ival14.csv  --model_dir ../Src/ModData/0415_model_selection/ --vendor nok --o ../Src/ModData/0414_test/0328_0410_valid_ --t Prob_Urgent
 2996  python prediction_validation.py --ifeat ../Src/ModData/xy_validation_eri_0313_0327_lbh14_lah1_ival14.csv  --model_dir ../Src/ModData/0413_model_selection/ --vendor eri --o ../Src/ModData/0414_test/0328_0410_valid_ --t Prob_Urgent
 2997  python prediction_validation.py --ifeat ../Src/ModData/xy_validation_eri_0313_0327_lbh14_lah1_ival14.csv  --model_dir ../Src/ModData/0414_model_selection/ --vendor eri --o ../Src/ModData/0414_test/0328_0410_valid_ --t Prob_Urgent
 2998  csvcut -C sum "/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170414 Feature/0414_Xy_set/Train_Validation_Test/Nokia/xy_validation_nok_0313_0327_lbh14_lah1_ival14.csv" > ../Src/ModData/xy_validation_nok_0313_0327_lbh14_lah1_ival14.csv
 2999  python prediction_validation.py --ifeat ../Src/ModData/xy_validation_nok_0313_0327_lbh14_lah1_ival14.csv  --model_dir ../Src/ModData/0415_model_selection/ --vendor nok --o ../Src/ModData/0414_test/0328_0410_valid_ --t Prob_Urgent
 3000  python ./Python/my
 3001  python ./Python/Generic/myfunct.py
 3002  python ./Python/Generic/myprogram.py
 3003  python ./Python/Generic/convert_temp.py
 3004  python ./Python/Generic/udem/scrip1.py
 3005  python ./Python/Generic/udem/script6.py
 3006  python ./Python/Generic/udem/password_test.py
 3007  python ./Python/Generic/udem/script7.py
 3008  python calc_survival_time.py -h
 3009  python calc_survival_time.py --dfrom 20170101 --dto 20170410 -o ../Src/ModData/survival_time.csv
 3010  rm ../SB/Src/Schema/internalL_v1og.xml
 3011  cd SB/Python
 3012  cp model_selection.py ../../../Python/OptModel/optmodel/optmodel.py
 3013  cp ../Projects/SB/Python/model_selection.ini OptModel/optmodel/optmodel.ini
 3014  open demo_result_accuracy.csv
 3015  python OptModel/optmodel/optmodel.py --demo
 3016  cat optmodel/optmodel.ini
 3017  rm demo_result_accuracy.csv
 3018  cd OptModel/optmodel
 3019  python optmodel.py
 3020  cd Projects/SB/Src/ModData/
 3021  ks
 3022  git scheckout plot_update
 3023  git checkout plot_update
 3024  git checkout -b plot_update
 3025  cp doc/API-Documentation.rst ../Archive/tstk\ archive/
 3026  cp doc/index.rst ../Archive/tstk\ archive/
 3027  cp doc/tstk.rst ../Archive/tstk\ archive/
 3028  rm -rf tstk
 3029  git clone doc/API-Documentation.rst
 3030  git clone git@github.build.ge.com:IndustrialDataScience/tstk.git
 3031  vim '/Users/212339410/Box/DSS External-Softbank/21 Data from SB/20170329_Internal_Log_formatted (Ericsson)/20170209_0610_SB_nodelist2000_out.zip\n'
 3032  vim /Users/212339410/Box/DSS External-Softbank/21 Data from SB/20170329_Internal_Log_formatted (Ericsson)/20170209_0610_SB_nodelist2000_out.zip
 3033  vim "/Users/212339410/Box/DSS External-Softbank/21 Data from SB/20170329_Internal_Log_formatted (Ericsson)/20170209_0610_SB_nodelist2000_out.zip\n"
 3034  vim â/Users/212339410/Box/DSS External-Softbank/21 Data from SB/20170329_Internal_Log_formatted (Ericsson)/20170209_0610_SB_nodelist2000_out.zipâ
 3035  vim â/Users/212339410/Box/DSS External-Softbank/21 Data from SB/20170329_Internal_Log_formatted (Ericsson)/20170209_0610_SB_nodelist2000_out.zip"
 3036  vim "/Users/212339410/Box/DSS External-Softbank/21 Data from SB/20170329_Internal_Log_formatted (Ericsson)/20170209_0610_SB_nodelist2000_out.zip"
 3037  git checkout --track origin/plot_update
 3038  python calc_survival_time.py --dfrom 20170101 --dto 20170410 --o ../Src/ModData/survival_time.csv
 3039  cd ../Src/ModData/0418_internal_log
 3040  rm internal_log_SB_02010410_bsid32996.csv internal_log_SB_02010410_bsid33360.csv internal_log_SB_02010410_bsid33460.csv internal_log_SB_02010410_bsid34357.csv internal_log_SB_02010410_bsid34409.csv internal_log_SB_02010410_bsid34416.csv internal_log_SB_02010410_bsid34580.csv internal_log_SB_02010410_bsid35516.csv internal_log_SB_02010410_bsid35898.csv internal_log_SB_02010410_bsid36230.csv internal_log_SB_02010410_bsidPB210V.csv internal_log_SB_02010410_bsidPF394V.csv internal_log_SB_02010410_bsidPF402V.csv internal_log_SB_02010410_bsidSA110R.csv internal_log_SB_02010410_bsidSA263R.csv internal_log_SB_02010410_bsidSB107V.csv internal_log_SB_02010410_bsidSB171V.csv internal_log_SB_02010410_bsidSB687V.csv internal_log_SB_02010410_bsidSE111T.csv internal_log_SB_02010410_bsidSE408V.csv internal_log_SB_02010410_bsidSE612V.csv internal_log_SB_02010410_bsidSJ053U.csv internal_log_SB_02010410_bsidSJ765G.csv internal_log_SB_02010410_bsidSK294U.csv internal_log_SB_02010410_bsidSK870T.csv internal_log_SB_02010410_bsidSK939U.csv internal_log_SB_02010410_bsidSL526U.csv internal_log_SB_02010410_bsidSL622G.csv internal_log_SB_02010410_bsidSM682T.csv internal_log_SB_02010410_bsidWD260V.csv internal_log_SB_02010410_bsidWD795V.csv internal_log_SB_02010410_bsidWE429V.csv internal_log_SB_02010410_bsidWG043V.csv internal_log_SB_02010410_bsidWG561V.csv internal_log_SB_02010410_bsidWG571V.csv internal_log_SB_02010410_bsidWG954V.csv internal_log_SB_02010410_bsidWH020V.csv 
 3041  vim bsw_overlap_internallog.csv
 3042  gzip -9 internal_log_SB_02010410_bsw.csv
 3043  pip install ggplot
 3044  python subset_internal_log.py
 3045  wc -l /Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170418 Internal Log (subset by BSID, all command) & BSW 0201_0406/0418_internal_log/bsid_EM_unique.txt
 3046  vim '/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170418 Internal Log (subset by BSID, all command) & BSW 0201_0406/0418_internal_log/bsid_EM_unique.txt'
 3047  vim /Users/212339410/Box/DSS\ Internal-Softbank/40\ Intermediate\ Results/20170418\ Internal\ Log\ \(subset\ by\ BSID,\ all\ command\)\ \&\ BSW\ 0201_0406/0418_internal_log/bsw_overlap_internallog_EM.csv
 3048  vim '/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170418 Internal Log (subset by BSID, all command) & BSW 0201_0406/0418_internal_log/bsw_overlap_internallog_EM_Lsuffix.csv\n'
 3049  vim '/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170418 Internal Log (subset by BSID, all command) & BSW 0201_0406/0418_internal_log/bsw_overlap_internallog_EM_Lsuffix.csv'
 3050  vim new.py
 3051  rm new.py
 3052  git branch -b new_branch
 3053  cd Predix
 3054  rm -rf predix-seed
 3055  cd predix-seed  geproxy
 3056  git clone https://github.com/PredixDev/predix-seed.git  
 3057  git clone https://github.com/PredixDev/predix-seed.git
 3058  cd predix-seed
 3059  cd Box/Development
 3060  cd Predix/predix-seed
 3061  bom install gulp-cli -g
 3062  vim /Users/212339410/.npmrc
 3063  npm install gulp-cli g
 3064  vim test_um_unittest.py
 3065  vim unnecessary_math.py
 3066  python -m doctest -v unnecessary_math.py
 3067  cd visualization/
 3068  python -m doctest -v plot.py
 3069  python -m doctest -v pca_plot.py
 3070  cd ../preprocessing
 3071  python -m doctest -v align_ts.py
 3072  python -m doctest -v detect_ts.py
 3073  python -m doctest -v preprop.py
 3074  cd ../../../Generic
 3075  python test_um_unittest.py
 3076  python test_um_unittest.py -v
 3077  mkdir simple_example 
 3078  mv test_um_unittest.py simple_example/
 3079  mv unnecessary_math.py ./simple_example
 3080  python -m unittest discover simple_example
 3081  python -m unittest discover simple_example -v
 3082  python -m
 3083  man python 
 3084  pip install nose
 3085  nosetest -h
 3086  mkdir tests
 3087  python -m doctest optmodel.py
 3088  python -m doctest optmodel.py -v
 3089  git commit -m 'DOC update the doc to pass doctest'
 3090  pip install mdptoolbox
 3091  mkdir 0420_report_internallog
 3092  cd 0420_report_internallog
 3093  cd Projects/SB/Src/ModData/0420_report_internallog
 3094  ipython nbconvert Asset_view.ipynb --to rst
 3095  mkdir html_output
 3096  ls ./*.rst
 3097  ipython nbconvert *.ipynb --to rst
 3098  sphinx-build ./*.rst ./html_output
 3099  sphinx-build ./Asset_view.rst ./Overview.rst ./Variable_view.rst ./index.rst  ./html_output
 3100  pip install guzzle_sphinx_theme
 3101  cd html_output
 3102  cd embeded_reports
 3103  rm 0419_internal_log
 3104  ln -s /Users/212339410/Box/Analytics/Projects/SB/Src/ModData/0419_internal_log /Users/212339410/Projects/SB/Src/ModData/0420_report_internallog/html_output/embeded_reports/0419_internal_log
 3105  jupyter nbconvert --to html 
 3106  mkdir test_html_from_nb
 3107  jupyter nbconvert --to html Asset_view.ipynb Overview.ipynb Variable_view.ipynb --output ./test_html_from_nb
 3108  jupyter nbconvert --to html Asset_view.ipynb  --output ./test_html_from_nb
 3109  jupyter nbconvert --to rst *.ipynb
 3110  jupyter nbconvert --to rst *.ipynb --template output_toggle_html
 3111  jupyter nbconvert --to rst *.ipynb 
 3112  sphinx-build ./  ./html_output/
 3113  mkdir 0420_wd_internal_log
 3114  ln -s /Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170418 Internal Log (subset by BSID, all command) & BSW 0201_0406/0418_internal_log/bsw_overlap_internallog.csv /Users/212339410/Projects/SB/Src/ModData/0420_wd_internal_log/bsw_overlap_internallog.csv
 3115  ln -s '/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170418 Internal Log (subset by BSID, all command) & BSW 0201_0406/0418_internal_log/bsw_overlap_internallog.csv' /Users/212339410/Projects/SB/Src/ModData/0420_wd_internal_log/bsw_overlap_internallog.csv
 3116  ln -s '/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170418 Internal Log (subset by BSID, all command) & BSW 0201_0406/0418_internal_log/bsw_overlap_internallog_EM.csv  /Users/212339410/Projects/SB/Src/ModData/0420_wd_internal_log/bsw_overlap_internallog_EM.csv
 3117  ln -s '/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170418 Internal Log (subset by BSID, all command) & BSW 0201_0406/0418_internal_log/bsw_overlap_internallog_EM.csv'  /Users/212339410/Projects/SB/Src/ModData/0420_wd_internal_log/bsw_overlap_internallog_EM.csv
 3118  ln -s '/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170418 Internal Log (subset by BSID, all command) & BSW 0201_0406/0418_internal_log/bsw_overlap_internallog_EM_Lsuffix.csv'  /Users/212339410/Projects/SB/Src/ModData/0420_wd_internal_log/bsw_overlap_internallog_EM_Lsuffix.csv
 3119  ln -s "/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170418 Internal Log (subset by BSID, all command) & BSW 0201_0406/0418_internal_log/internal_log_YM_selected.csv.gz" /Users/212339410/Projects/SB/Src/ModData/0420_wd_internal_log/internal_log_YM_selected.csv.gz
 3120  ln -s "/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170418 Internal Log (subset by BSID, all command) & BSW 0201_0406/0418_internal_log/internal_log_SB_selected.csv.gz" /Users/212339410/Projects/SB/Src/ModData/0420_wd_internal_log/internal_log_SB_selected.csv.gz
 3121  ln -s '/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170418 Internal Log (subset by BSID, all command) & BSW 0201_0406/0418_internal_log/Internal_log_YM' /Users/212339410/Projects/SB/Src/ModData/0420_wd_internal_log/Internal_log_YM
 3122  ln -s '/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170418 Internal Log (subset by BSID, all command) & BSW 0201_0406/0418_internal_log/Internal_log_SB' /Users/212339410/Projects/SB/Src/ModData/0420_wd_internal_log/Internal_log_SB
 3123  cd 0420_wd_internal_log
 3124  gzip -9 internal_log_YM_02010410_bsw.csv
 3125  ln -s internal_log_SB_02010410_bsw.csv.gz /Users/212339410/Projects/SB/Src/ModData/0420_wd_internal_log/internal_log_SB_02010410_bsw.csv.gz
 3126  ln -s `pwd`internal_log_SB_02010410_bsw.csv.gz /Users/212339410/Projects/SB/Src/ModData/0420_wd_internal_log/internal_log_SB_02010410_bsw.csv.gz
 3127  ln -s `pwd`/internal_log_SB_02010410_bsw.csv.gz /Users/212339410/Projects/SB/Src/ModData/0420_wd_internal_log/internal_log_SB_02010410_bsw.csv.gz
 3128  ln -s `pwd`/internal_log_YM_02010410_bsw.csv.gz /Users/212339410/Projects/SB/Src/ModData/0420_wd_internal_log/internal_log_YM_02010410_bsw.csv.gz
 3129  vim /Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170418 Internal Log (subset by BSID, all command) & BSW 0201_0406/0418_internal_log/bsw_overlap_internallog.csv
 3130  vim '/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170418 Internal Log (subset by BSID, all command) & BSW 0201_0406/0418_internal_log/bsw_overlap_internallog.csv'
 3131  python descriptive_internal_log.py
 3132  git clone git@github.build.ge.com:IndustrialDataScience/OptModel.git OptModel.gh-pages
 3133  checkout gh-pages
 3134  rm -rf optmodel
 3135  rm README.md
 3136  ls Makefile
 3137  ls build
 3138  ls build/
 3139  sphinx-apidoc -o ./doc ./optmodel
 3140  cd ../../OptModel.gh-pages
 3141  ls source
 3142  cp ./* ../OptModel/doc
 3143  cp ./source/* ../OptModel/doc
 3144  cp -rf ./source/* ../OptModel/doc
 3145  sphinx-build ./doc ../OptModel.gh-pages/
 3146  rm make.bat
 3147  sphinx-apidoc -o . ../optmodel
 3148  sphinx-build . ../../OptModel.gh-pages
 3149  sphinx-apidoc -o . ../optmodel/optmodel.py
 3150  sphinx-apidoc -o . ../optmodel/
 3151  vim optmodel.rst
 3152  vim modules.rst
 3153  mkdir ../Src/ModData/0421_feature
 3154  brew install gawk
 3155  gawk
 3156  man date
 3157  date -j -f "%Y%m%d" "20151010"
 3158  cd ../bas
 3159  cd ../Bash
 3160  echo x
 3161  startd=$(date -j -f '%Y%m%d' "$x" +'%Y%m%d'); 
 3162  echo "$startd";
 3163  x="20151010";
 3164  echo $x
 3165  startd=$(date -j -f '%Y%m%d' "$x" +'%Y%m%d');
 3166  echo "$startd";startdate=20170115
 3167  # startd=$(date -j -f '%Y%m%d' "$startdate" +'%Y%m%d');
 3168  # endd=$(date -j -f '%Y%m%d' "$enddate" +'%Y%m%d');
 3169  endDateTs=$(date -j -f "%Y%m%d" $endd "+%s")
 3170  echo $currentDateTs
 3171  echo $endDateTs
 3172  while [ "$currentDateTs" -le "$endDateTs" ]\ndo\n  date=$(date -j -f "%s" $currentDateTs "+%Y-%m-%d")\n  echo $date\n  currentDateTs=$(($currentDateTs+$offset))\ndone
 3173  while [ "$startdateTs" -le "$enddateTs" ]\ndo\n  date=$(date -j -f "%s" $startdateTs "+%Y%m%d")\n  echo $date\n  startdateTs=$(($startdateTs+$offset))\ndone
 3174  ln -s /Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170421 Feature/0421_feature ../Src/ModData/0421_feature
 3175  cd Src/ModData
 3176  ln -s "/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170421 Feature/0421_feature" /Users/212339410/Box/Analytics/Projects/SB/Src/ModData/0421_feature
 3177  startdate=20170106
 3178  startdateTs=$(date -j -f "%Y%m%d" $startdate "+%s")
 3179  enddateTs=$(date -j -f "%Y%m%d" $enddate "+%s")
 3180  while [ "$startdateTs" -le "$enddateTs" ]\ndo\n  date=$(date -j -f "%s" $startdateTs "+%Y%m%d")\n  echo $date\n  python feature_create_v3.py --vendor nok --dto $date --o ../Src/ModData/0421_feature/ --cache\n\n  startdateTs=$(($startdateTs+$offset))\ndone
 3181  cd Notebook
 3182  cp preprocessing_wot_20151201_20170201.ipynb preprocessing_wot_20170201_20170301.ipynb
 3183  cp preprocessing_wot_20151201_20170301.ipynb preprocessing_wot_20170301_20170327.ipynb
 3184  startdate=20170129
 3185  gzip survival_time.csv
 3186  startdate=20170229
 3187  enddate=20170310
 3188  startdate=20170311
 3189  vim '/Users/212339410/Box/Analytics/Python/Generic/data/servo1.csv'
 3190  vim '/Users/212339410/Box/Analytics/Python/Generic/data/Servo_Guide_CSV.csv'
 3191  cd Projects/SB/
 3192  startdate=20170410
 3193  python feature_create_v3.py --vendor nok --dto 20170410 --o ../Src/ModData/0414_feature_duration/ --cache
 3194  python feature_create_v3.py --vendor nok --dto 20170410 --o ../Src/ModData/0421_feature/ --cache
 3195  python feature_create_v3.py --vendor eri --dto 20170410 --o ../Src/ModData/0421_feature/ --cache
 3196  python feature_create_v3.py --vendor eri --dto 20170411 --o ../Src/ModData/0421_feature/ --cache
 3197  python feature_create_v3.py --vendor nok --dto 20170411 --o ../Src/ModData/0421_feature/ --cache
 3198  python feature_create_v3.py --vendor nok --dto 20170411 --o ../Src/ModData/0421_feature/ 
 3199  cd Projects/SB/ms
 3200  while [ "$currentDateTs" -le "$endDateTs" ]\ndo\n  date=$(date -j -f "%s" $currentDateTs "+%Y%m%d")\n  echo $date\n  currentDateTs=$(($currentDateTs+$offset))\ndone
 3201  while [ "$currentDateTs" -le "$endDateTs" ]\ndo\n  date=$(date -j -f "%s" $currentDateTs "+%Y%m%d")\n  echo $date\n  python feature_create_v3.py --vendor nok --dto $date --o ../Src/ModData/0421_feature/ --cache\n\n  currentDateTs=$(($currentDateTs+$offset))\ndone
 3202  python feature_create_v3.py --vendor eri --dto 20170411 --o ../Src/ModData/0421_feature/ 
 3203  startdate=20170327
 3204  enddate=20170410
 3205  while [ "$currentDateTs" -le "$endDateTs" ]\ndo\n  date=$(date -j -f "%s" $currentDateTs "+%Y%m%d")\n  echo $date\n  python feature_create_v3.py --vendor eri --dto $date --o ../Src/ModData/0421_feature/ --cache\n\n  currentDateTs=$(($currentDateTs+$offset))\ndone
 3206  feature_path='../../../../DSS\ Internal-Softbank/40\ Intermediate\ Results/20170414\ Feature/0414_Xy_set/Train_Validation_Test/Nokia/xy_train_nok_0106_0312_lbh14_lah1_ival14_t1_f10.csv'
 3207  echo $feature_path
 3208  cd SB
 3209  cd Report/
 3210  git clone git@github.build.ge.com:212310464/Softbank_report.git
 3211  ln -s ~/Box/DSS\ Internal-Softbank/40\ Intermediate\ Results/20170421\ Feature/0421_Xy_set/ /Users/212339410/Projects/SB/Src/ModData/0421_Xy_set
 3212  feature_path='../Src/ModData/Train_Test/Nokia/xy_train_nok_0106_0327_lbh14_lah1_ival14_t1_f10.csv'
 3213  python model_selection.py --ifeat $feature_path --iconf model_selection.ini --t Prob_Urgent --o $result_path --score precision --olog ./log/model_selection_nok0421.logfeature_path='/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170421 Feature/0421_Xy_set/Train_Test/Nokia/xy_train_nok_0106_0327_lbh14_lah1_ival14_t1_f10.csv'
 3214  python model_selection.py --ifeat $feature_path --iconf model_selection.ini --t Prob_Urgent --o $result_path --score precision --olog ./log/model_selection_nok0421.log
 3215  feature_path='/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170421 Feature/0421_Xy_set/Train_Test/Nokia/xy_train_nok_0106_0327_lbh14_lah1_ival14_t1_f10.csv'
 3216  model_path0328
 3217  echo $model_path0328
 3218  echo $model_path 0328
 3219  echo {$model_path}0328
 3220  echo ${model_path}0328
 3221  python model_selection.py --ifeat $feature_path --iconf model_selection.ini --t Prob_Urgent --o $result_path --score accuracy --olog ./log/model_selection_nok0421.log
 3222  echo ${model_path}
 3223  python prediction_validation.py --ifeat $feature_test_path  --model_dir $model_path --vendor nok --o ${model_path}0328_0411_validation_ --t Prob_Urgent
 3224  feature_valid_path='/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170421 Feature/0421_Xy_set/Train_Validation_Test/Nokia/xy_test_nok_0328_0411_lbh14_lah1_ival14_t1_f0.csv'
 3225  python prediction_validation.py --ifeat feature_valid_path  --model_dir $model_path --vendor nok --o ${model_path}0328_0411_validation_ --t Prob_Urgent
 3226  result_path='../Src/ModData/0421_model/xy_train_nok_0106_0312_lbh14_lah1_ival14_t1_f10.csv'
 3227  result_path='../Src/ModData/0421_model/xy_train_eri_0115_0312_lbh14_lah1_ival14_t1_f10.csv'
 3228  result_path='../Src/ModData/0421_model/xy_train_eri_0115_0327_lbh14_lah1_ival14_t1_f10.csv'
 3229  result_path='../Src/ModData/0421_model/xy_train_nok_0106_0327_lbh14_lah1_ival14_t1_f10.csv'
 3230  model_path='../Src/ModData/0421_model/'
 3231  python eval_model_dt.py -i "../Src/ModData/0421_Xy_set/Train_Test/Nokia/xy_train_nok_0106_0327_lbh14_lah1_ival14_t1_f10.csv" -o "../Src/ModData/model_eval_rf" --target "Prob_Urgent"  --vendor "nok" -exp True  
 3232  echo 'Nokia'
 3233  feature_train_path='/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170421 Feature/0421_Xy_set/Train_Validation_Test/Nokia/xy_train_nok_0106_0312_lbh14_lah1_ival14_t1_f10.csv'
 3234  feature_valid_path='/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170421 Feature/0421_Xy_set/Train_Validation_Test/Nokia/xy_validation_nok_0313_0327_lbh14_lah1_ival14_t1_f0.csv'
 3235  result_path='../Src/ModData/0422_model/xy_train_nok_0106_0312_lbh14_lah1_ival14_t1_f10.csv'
 3236  python model_selection.py --ifeat $feature_train_path --iconf model_selection.ini --t Prob_Urgent --o $result_path --score accuracy --olog ./log/model_selection_nok0422.log
 3237  echo 'Ericsson'
 3238  feature_train_path='/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170421 Feature/0421_Xy_set/Train_Validation_Test/Ericsson/xy_train_eri_0115_0312_lbh14_lah1_ival14_t1_f10.csv'
 3239  feature_valid_path='/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170421 Feature/0421_Xy_set/Train_Validation_Test/Ericsson/xy_validation_eri_0313_0327_lbh14_lah1_ival14_t1_f0.csv'
 3240  result_path='../Src/ModData/0422_model/xy_train_eri_0115_0312_lbh14_lah1_ival14_t1_f10.csv'
 3241  model_path='../Src/ModData/0422_model/'
 3242  python model_selection.py --ifeat $feature_train_path --iconf model_selection.ini --t Prob_Urgent --o $result_path --score accuracy --olog ./log/model_selection_eri0421.log
 3243  python prediction_validation.py --ifeat $feature_valid_path  --model_dir $model_path --vendor eri --o ${model_path}0328_0411_validation_ --t Prob_Urgent
 3244  result_path='../Src/ModData/0421_test/xy_train_nok_0106_0327_lbh14_lah1_ival14_t1_f10.csv'
 3245  result_path='../Src/ModData/0421_test/xy_train_eri_0115_0327_lbh14_lah1_ival14_t1_f10.csv'
 3246  model_path='../Src/ModData/0421_test/'
 3247  # Ericsson
 3248  python model_selection.py --ifeat $feature_train_path --iconf model_selection.ini --t Prob_Urgent --o $result_path --score precision --olog ./log/model_selection_nok0421.log
 3249  python model_selection.py --ifeat $feature_train_path --iconf model_selection.ini --t Prob_Urgent --o $result_path --score accuracy --olog ./log/model_selection_nok0421.log
 3250  feature_train_path='/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170421 Feature/0421_Xy_set/Train_Test/Ericsson/xy_train_eri_0115_0327_lbh14_lah1_ival14_t1_f10.csv'
 3251  feature_test_path='/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170421 Feature/0421_Xy_set/Train_Test/Ericsson/xy_test_eri_0328_0411_lbh14_lah1_ival14_t1_f0.csv'
 3252  result_path='../Src/ModData/0422_test/xy_train_eri_0115_0327_lbh14_lah1_ival14_t1_f10.csv'
 3253  python prediction_validation.py --ifeat $feature_test_path  --model_dir $model_path --vendor eri --o ${model_path}0328_0411_test_ --t Prob_Urgent
 3254  feature_train_path='/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170421 Feature/0421_Xy_set/Train_Test/Nokia/xy_train_nok_0106_0327_lbh14_lah1_ival14_t1_f10.csv'
 3255  feature_test_path='/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170421 Feature/0421_Xy_set/Train_Test/Nokia/xy_test_nok_0328_0411_lbh14_lah1_ival14_t1_f0.csv'
 3256  result_path='../Src/ModData/0422_test/xy_train_nok_0106_0327_lbh14_lah1_ival14_t1_f10.csv'
 3257  model_path='../Src/ModData/0422_test/'
 3258  #python model_selection.py --ifeat $feature_train_path --iconf model_selection.ini --t Prob_Urgent --o $result_path --score accuracy --olog ./log/model_selection_nok0421.log
 3259  python prediction_validation.py --ifeat $feature_test_path  --model_dir $model_path --vendor nok --o ${model_path}0328_0411_test_ --t Prob_Urgent
 3260  cd Projects/SB/py
 3261  mkdir ../Src/ModData/0424_feature
 3262  python feature_create_v3.py --vendor eri --dto 20170410 --o ../Src/ModData/0424_feature/ 
 3263  python feature_create_v3.py --vendor eri --dto 20170410 --o ../Src/ModData/0424_feature/ --cache
 3264  startdate=20170120
 3265  startdate=20170130
 3266  while [ "$currentDateTs" -le "$endDateTs" ]\ndo\n  date=$(date -j -f "%s" $currentDateTs "+%Y%m%d")\n  echo $date\n  python feature_create_v3.py --vendor $vendor --dto $date --o ../Src/ModData/0424_feature/ --cache\n\n  currentDateTs=$(($currentDateTs+$offset))\ndone
 3267  python feature_create_v3.py --vendor eri --dto 20170411 --o ../Src/ModData/0424_feature/ --cache
 3268  python feature_create_v3.py --vendor nok --dto 20170411 --o ../Src/ModData/0424_feature/ --cache
 3269  python model_selection.py --ifeat $feature_train_path --iconf model_selection.ini --t Prob_Urgent --o $result_path --score roc_auc  --olog ./log/model_selection0424.log
 3270  feature_train_path=''
 3271  declare -a vendor=("nok" "eri")
 3272  man declare
 3273  declare
 3274  declare -a
 3275  vendor=("nok" "eri")
 3276  for i in "${vendor[@]}"\ndo\necho $i\n\n\ndone
 3277  if [ vendor=='nok' ]\nthen\necho $i\nelif [ vendor=='eri'  ]\nthen\necho $i\nfi
 3278  echo $vendor
 3279  for i in "${vendor[@]}"\ndo\necho $i\n\nif [ $i  -eq 'nok' ]\nthen\necho $i\nelif [ $i -eq 'eri'  ]\nthen\necho $i\nfi\n\ndone
 3280  for i in "${vendor[@]}"\ndo\necho $i\n\nif [ $i  == 'nok' ]\nthen\necho $i\n\nelif [ $i == 'eri'  ]\nthen\necho $i\nfi\n\n\ndone
 3281  for i in "${vendor[@]}"\ndo\necho $i\n\nif [ "$i"  == "nok" ]\nthen\necho $i\n\nelif [ "$i" == "eri"  ]\nthen\necho $i\nfi\n\n\ndone
 3282  echo $model_path'xy_train_nok_0106_0312_lbh14_lah1_ival14_t1_f10.csv'
 3283  model_path='../Src/ModData/0424_model/'
 3284  feature_train_path='/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170424 Feature/0424_Xy_set/Train_Validation_Test/Nokia/xy_train_nok_0106_0312_lbh14_lah1_ival14_t1_f10.csv'
 3285  feature_valid_path='/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170424 Feature/0424_Xy_set/Train_Validation_Test/Nokia/xy_validation_nok_0313_0327_lbh14_lah1_ival14_t1_f0.csv'
 3286  result_path=$model_path'xy_train_nok_0106_0312_lbh14_lah1_ival14_t1_f10.csv'
 3287  cd ../Src/ModData
 3288  cd 0424_model_selection
 3289  mv ./*0326* ../0424_test
 3290  mv xy_train_nok_0106_0327* ../0424_test/
 3291  python prediction_validation.py --ifeat $feature_valid_path  --model_dir $model_path --vendor nok --o ${model_path}0328_0411_validation_ --t Prob_Urgent
 3292  python prediction_validation.py --ifeat $feature_valid_path  --model_dir $model_path --vendor nok --o ${model_path}0328_0411_validation_ --t Prob_Urgentfeature_train_path='/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170424 Feature/0424_Xy_set/Train_Test/Nokia/xy_train_nok_0106_0327_lbh14_lah1_ival14_t1_f10.csv'
 3293  feature_train_path='/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170424 Feature/0424_Xy_set/Train_Test/Nokia/xy_train_nok_0106_0327_lbh14_lah1_ival14_t1_f10.csv'
 3294  feature_test_path='/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170424 Feature/0424_Xy_set/Train_Test/Nokia/xy_test_nok_0328_0411_lbh14_lah1_ival14_t1_f0.csv'
 3295  result_path=$model_path'xy_train_nok_0106_0327_lbh14_lah1_ival14_t1_f10.csv'
 3296  python prediction_validation.py --ifeat $feature_valid_path  --model_dir $model_path --vendor nok --o ${model_path}0328_0411_test_ --t Prob_Urgent
 3297  # Train_Validation_Test
 3298  feature_train_path='/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170424 Feature/0424_Xy_set/Train_Validation_Test/Ericsson/xy_train_eri_0116_0312_lbh14_lah1_ival14_t1_f10.csv'
 3299  python prediction_validation.py --ifeat $feature_valid_path  --model_dir $model_path --vendor nok --o ${model_path}0313_0327_validation_ --t Prob_Urgent
 3300  csvcut -n $feature_train_path
 3301  csvcut -n $feature_valid_path
 3302  csvcut -n $feature_valid_pathfeature_train_path='/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170424 Feature/0424_Xy_set/Train_Validation_Test/Ericsson/xy_train_eri_0116_0312_lbh14_lah1_ival14_t1_f10.csv'
 3303  feature_valid_path='/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170424 Feature/0424_Xy_set/Train_Validation_Test/Ericsson/xy_validation_eri_0313_0327_lbh14_lah1_ival14_t1_f0.csv'
 3304  model_path='../Src/ModData/0424_model_selection/'
 3305  result_path=$model_path'xy_train_eri_0116_0312_lbh14_lah1_ival14_t1_f10.csv'
 3306  model_path='../Src/ModData/0424_test/'
 3307  result_path=$model_path'xy_train_eri_0116_0327_lbh14_lah1_ival14_t1_f10.csv'
 3308  python prediction_validation.py --ifeat $feature_valid_path  --model_dir $model_path --vendor $vendor --o ${model_path}0328_0411_test_ --t Prob_Urgent
 3309  python prediction_validation.py --ifeat $feature_valid_path  --model_dir $model_path --vendor $vendor --o ${model_path}0313_0327_validation_ --t Prob_Urgent
 3310  # Common config
 3311  log_path='./log/model_selection_0424.log'
 3312  scoring='roc_auc'
 3313  model_path_validation='../Src/ModData/0424_model_selection/'
 3314  model_path_test='../Src/ModData/0424_test/'
 3315  # Train-Test
 3316  feature_train_path='/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170424 Feature/0424_Xy_set/Train_Test/Ericsson/xy_train_eri_0116_0327_lbh14_lah1_ival14_t1_f10.csv'
 3317  feature_test_path='/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170424 Feature/0424_Xy_set/Train_Test/Ericsson/xy_train_eri_0116_0327_lbh14_lah1_ival14_t1_f1.csv'
 3318  result_path=$model_path_test'xy_train_eri_0116_0327_lbh14_lah1_ival14_t1_f10.csv'
 3319  python model_selection.py --ifeat $feature_train_path --iconf model_selection.ini --t Prob_Urgent --o $result_path --score $scoring --olog $log_path
 3320  python prediction_validation.py --ifeat feature_test_path  --model_dir $model_path_test --vendor $vendor --o ${model_path_test}0328_0411_test_ --t Prob_Urgent
 3321  mv ~/Downloads/colfax-access-key-3447 ~/.ssh
 3322  chmod 600 ~/.ssh/colfax-access-key-3447
 3323  man chmod
 3324  chmod 600 ~/.ssh/config
 3325  ssh colfax
 3326  man qsub
 3327  python model_selection.py --ifeat ../Src/ModData/0425_model_selection/xy_train_nok_0106_0312_lbh14_lah1_ival14_t1_f10.csv  --iconf model_selection.ini --t Prob_Urgent --o ../Src/ModData/0425_model_selection --score roc_auc --olog $log_path
 3328  pip install cufflinks 
 3329  python feature_create_v3.py 
 3330  python feature_create_v3.py -h
 3331  while [ "$currentDateTs" -le "$endDateTs" ]\ndo\n  date=$(date -j -f "%s" $currentDateTs "+%Y%m%d")\n  echo $date\n  python feature_create_v3.py --vendor $vendor --dto $date --o ../Src/ModData/0424_feature/ --cache -lb 28 -la 14 \n\n  currentDateTs=$(($currentDateTs+$offset))\ndone
 3332  while [ "$currentDateTs" -le "$endDateTs" ]\ndo\n  date=$(date -j -f "%s" $currentDateTs "+%Y%m%d")\n  echo $date\n  python feature_create_v3.py --vendor $vendor --dto $date --o ../Src/ModData/0424_feature/ --cache --lb 28 --la 14\n\n  currentDateTs=$(($currentDateTs+$offset))\ndone
 3333  while [ "$currentDateTs" -le "$endDateTs" ]\ndo\n  date=$(date -j -f "%s" $currentDateTs "+%Y%m%d")\n  echo $date\n  python feature_create_v3.py --vendor $vendor --dto $date --o ../Src/ModData/0424_feature/ --cache --lb 14 --la 14\n\n  currentDateTs=$(($currentDateTs+$offset))\ndone
 3334  # eri
 3335  startdate=20170116
 3336  vendor='eri'
 3337  # Look back = 14
 3338  # Look ahead = 14
 3339  # Nokia
 3340  startdate=2017028
 3341  enddate=20170411
 3342  vendor='nok'currentDateTs=$(date -j -f "%Y%m%d" $startdate "+%s")
 3343  vendor='nok'
 3344  currentDateTs=$(date -j -f "%Y%m%d" $startdate "+%s")
 3345  endDateTs=$(date -j -f "%Y%m%d" $enddate "+%s")
 3346  offset=86400
 3347  while [ "$currentDateTs" -le "$endDateTs" ]\ndo\n  date=$(date -j -f "%s" $currentDateTs "+%Y%m%d")\n  echo $date\n  python feature_create_v3.py --vendor $vendor --dto $date --o ../Src/ModData/0425_feature/ --cache --lb 14 --la 14\n\n  currentDateTs=$(($currentDateTs+$offset))\ndone
 3348  cd ../Report
 3349  open SB_template.pdf
 3350  cd submit
 3351  python run_feature_file1.0.py
 3352  cd ~/Box/DSS-Internal-Newmont-Challenge
 3353  cd 10\ Data\ Received
 3354  cd PI\ Data
 3355  cd Weather
 3356  csvcut -n AI699001S1E.PV.csv
 3357  head -n 5 AI699001S1E.PV.csv | csvcut -l
 3358  head -n 5 AI699001S1E.PV.csv | csvcut -l -d | 
 3359  head -n 5 AI699001S1E.PV.csv | csvcut -l | csvlook
 3360  head -n 5 AI699001S1E.PV.csv | csvlook
 3361  csvlook --max-rows 5 AI699001S1E.PV.csv
 3362  .tables
 3363  cd ../Notebook
 3364  cd ~/Projects/SB/Notebook
 3365  python preprocessing_bsw.py --i ~/SB_box_ingest_working_dir/20170428_Ingestion/Input/  --o ./ --dfrom 20170411 --dto 20170425
 3366  l | grep bsw
 3367  mv bsw_20170411_20170425.csv.gz ~/SB_box_ingest_working_dir/20170428_Ingestion/Output
 3368  wc -l ~/SB_box_ingest_working_dir/Talen_Working_Directory
 3369  wc -l ~/SB_box_ingest_working_dir/Talen_Working_Directory/bsw_20170411_20170425.csv
 3370  vim  ~/SB_box_ingest_working_dir/Talen_Working_Directory/bsw_20170411_20170425.csv
 3371  gzipcat 
 3372  zcat /Users/212339410/Box/DSS Internal-Softbank/12 Working Directory for Ingestion/20170428_Ingestion/Output/bsw_20170411_20170425.csv.gz
 3373  zcat '/Users/212339410/Box/DSS Internal-Softbank/12 Working Directory for Ingestion/20170428_Ingestion/Output/bsw_20170411_20170425.csv.gz'
 3374  zcat "/Users/212339410/Box/DSS Internal-Softbank/12 Working Directory for Ingestion/20170428_Ingestion/Output/bsw_20170411_20170425.csv.gz"
 3375  zmore /Users/212339410/Box/DSS Internal-Softbank/12 Working Directory for Ingestion/20170428_Ingestion/Output/bsw_20170411_20170425.csv.gz
 3376  qqzless /Users/212339410/Box/DSS Internal-Softbank/12 Working Directory for Ingestion/20170428_Ingestion/Output/bsw_20170411_20170425.csv.gz
 3377  zless /Users/212339410/Box/DSS Internal-Softbank/12 Working Directory for Ingestion/20170428_Ingestion/Output/bsw_20170411_20170425.csv.gz
 3378  zless "/Users/212339410/Box/DSS Internal-Softbank/12 Working Directory for Ingestion/20170428_Ingestion/Output/bsw_20170411_20170425.csv.gz"
 3379  vim /Users/212339410/Box/Analytics/Projects/SB/cache/select_*_from_bsw_count_table_where_vendor_=_'nokia'__and_date_>_'2017-04-12'_and_date_<_'2017-04-13'.csv.gzip
 3380  vim "/Users/212339410/Box/Analytics/Projects/SB/cache/select_*_from_bsw_count_table_where_vendor_=_'nokia'__and_date_>_'2017-04-12'_and_date_<_'2017-04-13'.csv.gzip"
 3381  zless "/Users/212339410/Box/Analytics/Projects/SB/cache/select_*_from_bsw_count_table_where_vendor_=_'nokia'__and_date_>_'2017-04-12'_and_date_<_'2017-04-13'.csv.gzip"
 3382  zmore "/Users/212339410/Box/Analytics/Projects/SB/cache/select_*_from_bsw_count_table_where_vendor_=_'nokia'__and_date_>_'2017-04-12'_and_date_<_'2017-04-13'.csv.gzip"
 3383  vim "/Users/212339410/Box/Analytics/Projects/SB/cache/select_*_from_bsw_count_table_where_vendor_=_'nokia'__and_date_>_'2017-04-12'_and_date_<_'2017-04-13'.csv"
 3384  vim /Users/212339410/Box/DSS Internal-Softbank/12 Working Directory for Ingestion/20170428_Ingestion/Output/bsw_20170411_20170425.csv
 3385  vim "/Users/212339410/Box/DSS Internal-Softbank/12 Working Directory for Ingestion/20170428_Ingestion/Output/bsw_20170411_20170425.csv"
 3386  pip install rpy2 --upgrade
 3387  conda install -c r rpy2 
 3388  echo $LC_ALL
 3389  locale -a
 3390  cd ~/Applications/Utilities
 3391  python script_feature_create_v3.py
 3392  cd '/Users/212339410/Box/GE Digital Data Science 2016/MBS Personal/';clear;
 3393  cd '/Applications/';clear;
 3394  cd '/Users/212339410/Box/';clear;
 3395  PATH=/Users/212339410/Python/OptModel/optmodel/optmodel.py:$PATH
 3396  export PATH=/Users/212339410/Python/OptModel/optmodel/optmodel.py:$PATH
 3397  optmodel
 3398  python optmodel --demo
 3399  python script_feature_create_v3.py -h
 3400  csvcut -n ../../SB/Src/ModData/demo_result.csv
 3401  csvstat ../../SB/Src/ModData/demo_result.csv
 3402  python optmodel.py h
 3403  python optmodel.py -f ../data/boston.csv -t target -c optmodel.ini
 3404  python optmodel.py -f ../data/boston.csv -t target -c optmodel.ini -o ./
 3405  python optmodel.py -f ../data/boston.csv -t target -c optmodel.ini -o './result'
 3406  python optmodel.py -f ../data/boston.csv -t target -c optmodel.ini -o './'
 3407  gless '/Users/212339410/Box/Analytics/Projects/SB/Src/ModData/0428_feature/feature_ingredients/feature_eri_0312_0326_lbh14_lah1_ival14.csv.gz'
 3408  zless '/Users/212339410/Box/Analytics/Projects/SB/Src/ModData/0428_feature/feature_ingredients/feature_eri_0312_0326_lbh14_lah1_ival14.csv.gz'
 3409  zvim '/Users/212339410/Box/Analytics/Projects/SB/Src/ModData/0428_feature/feature_ingredients/feature_eri_0312_0326_lbh14_lah1_ival14.csv.gz'
 3410  vim '/Users/212339410/Box/Analytics/Projects/SB/Src/ModData/0428_feature/feature_ingredients/feature_eri_0312_0326_lbh14_lah1_ival14.csv.gz'
 3411  file optmodel.py
 3412  vim '/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170424 Feature/0424_Xy_set/Train_Test/Nokia/xy_train_nok_0106_0327_lbh14_lah1_ival14_t1_f1.csv'
 3413  cd ~/SB_box_ingest_working_dir
 3414  pyrhon create_xy_feature.py
 3415  man csvcut
 3416  csvlook '/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170502_Feature/0502_Xy_set/xy_train_nok_0106_0327_lbh14_lah30_ival14.csv.gz'
 3417  vim '/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170502_Feature/0502_Xy_set/xy_train_nok_0106_0327_lbh14_lah30_ival14.csv.gz'
 3418  head -n 5 '/Users/212339410/Box/Analytics/Projects/SB/Src/ModData/survival_time_v2.csv' | csvlook
 3419  head -n 100 '/Users/212339410/Box/Analytics/Projects/SB/Src/ModData/survival_time_v2.csv' | csvlook
 3420  head -n 1000 '/Users/212339410/Box/Analytics/Projects/SB/Src/ModData/survival_time_v2.csv' | csvlook
 3421  csvsql
 3422  jupyter notebook --NotebookApp.iopub_data_rate_limit=10000000  Showcase.ipynb 
 3423  jupyter notebook --NotebookApp.iopub_data_rate_limit=10000000  tstk\ Visualization\ Module\ Part\ 2\ Advanced\ Topics.ipynb
 3424  python create_xy_feature_final.py
 3425  sphinx-apidoc -of ./doc ./tstk 
 3426  git aad -A 
 3427  rm -rf f
 3428  git commit -m 'remove unwanted file and dir'
 3429  git reset --hard HEAD~1
 3430  git checkout -b document_update
 3431  man  ipython nbconvert
 3432  ipython nbconvert tstk\ Tutorial\ -\ Text\ and\ Annotations.ipynb  --to rst --output ../doc
 3433  ipython nbconvert tstk\ Tutorial\ -\ Text\ and\ Annotations.ipynb  --to rst --output ../doc/tstk_tutorial_text_annotations.ipynb
 3434  ipython nbconvert examples/tstk\ Tutorial\ -\ Scatter\ Plots.ipynb  --to rst --output ./doc/tstk_tutorial_scatter_plots.rst
 3435  ipython nbconvert tstk\ Tutorial\ -\ Scatter\ Plots.ipynb  --to rst --output ./doc/tstk_tutorial_scatter_plots.rst
 3436  ipython nbconvert tstk\ Tutorial\ -\ Scatter\ Plots.ipynb  --to rst --output doc/tstk_tutorial_scatter_plots.rst
 3437  ipython nbconvert tstk\ Tutorial\ -\ Scatter\ Plots.ipynb  --to rst 
 3438  cd '/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170502_Feature/0502_Xy_set_final'
 3439  rm xy_train_eri_0106_0327_lbh14_lah30_ival14.csv.gz xy_train_nok_0106_0327_lbh14_lah14_ival14.csv.gz xy_train_nok_0106_0327_lbh14_lah1_ival14.csv.gz xy_train_nok_0106_0327_lbh14_lah30_ival14.csv.gz 
 3440  cd ~/Python/Op
 3441  head -n 10 ../data/boston.csv| csvlook 
 3442  head -n 20 ../data/boston.csv| csvlook 
 3443  head -n 30 ../data/boston.csv| csvlook 
 3444  open â/Users/212339410/Box/Analytics/Python/OptModel/optmodel/results/optmodel_report_roc_auc.csvâ
 3445  open â/Users/212339410/Box/Analytics/Python/OptModel/optmodel/results/optmodel_report_roc_auc.csvâ
 3446  open '/Users/212339410/Box/Analytics/Python/OptModel/optmodel/results/optmodel_report_roc_auc.csv'
 3447  cd Python/OptModel/optmodel
 3448  head -n 30 ../data/boston.csv| csvlook
 3449  python optmodel.py -f ../data/boston.csv -t target -c optmodel.ini -o './results' -S
 3450  cd Projects/SB/Python
 3451  cd '/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170502_Feature/0502_Xy_set'
 3452  rm xy_train_nok_0106_0327_lbh14_lah30_ival14_t1_f10.csv
 3453  cd ../../Projects
 3454  cd TSTK_dashboard
 3455  Rscript
 3456  Rscript dashboard_script.R
 3457  defaults write org.rstudio.RStudio force.LANG en_US.UTF-8
 3458  defalts 
 3459  defaults
 3460  defaults help
 3461  vim /Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170502_Feature/0504_Xy_set/xy_train_nok_0106_0327_lbh14_lah30_ival14_t1_f10.csv.gz
 3462  vim '/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170502_Feature/0504_Xy_set_final/xy_train_nok_0106_0327_lbh14_lah3_ival14_t1_f10.csv.gz'
 3463  vim '/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170502_Feature/0504_Xy_set/xy_train_nok_0106_0327_lbh14_lah30_ival14_t1_f10.csv.gz'
 3464  python create_xy_feature.py
 3465  cd '/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170502_Feature/0504_Xy_set'
 3466  rm ./* 
 3467  .
 3468  cd ../0504_Xy_set_final
 3469  rm ./xy_train_eri_0106_0327_lbh14_lah30_ival14_t1_f10.csv.gz ./xy_train_nok_0106_0327_lbh14_lah14_ival14_t1_f10.csv.gz ./xy_train_nok_0106_0327_lbh14_lah1_ival14_t1_f10.csv.gz ./xy_train_nok_0106_0327_lbh14_lah30_ival14_t1_f10.csv.gz ./xy_train_nok_0106_0327_lbh14_lah3_ival14_t1_f10.csv.gz ./xy_train_nok_0106_0327_lbh14_lah5_ival14_t1_f10.csv.gz ./xy_train_nok_0106_0327_lbh14_lah7_ival14_t1_f10.csv.gz 
 3470  cd Documents/WebEx
 3471  npm install -g gifify
 3472  cd /Users/212339410/Box/GE Digital Data Science 2016/Utility/MOVtoGif
 3473  cd '/Users/212339410/Box/GE Digital Data Science 2016/Utility/MOVtoGif'
 3474  brew unlink gifsicle 
 3475  brew install giflossy
 3476  brew upgrade imagemagick
 3477  cd Box/GE\ Digital\ Data\ Science\ 2016/Utility/MOVtoGif
 3478  for file in ./\ndo\necho $file\ndone
 3479  for file in ./*\ndo\necho $file\ndone
 3480  for file in ./*.mov\ndo\necho $file\ndone
 3481  for file in ./*.mov\ndo\necho $file\necho $(basename "$file")\necho "${filename##*.}"\necho "${filename%.*}"\ndone
 3482  for file in ./*.\ndo\necho $file\necho $(basename "$file")\necho "${filename##*.}"\necho "${filename%.*}"\ndone
 3483  for file in ./*\ndo\necho $file\necho $(basename "$file")\necho "${filename##*.}"\necho "${filename%.*}"\ndone
 3484  for file in .*\ndo\necho $file\nfilename=$(basename "$file")\necho "${filename##*.}"\necho "${filename%.*}"\ndone
 3485  for file in ./*\ndo\necho $file\nfilename=$(basename "$file")\necho "${filename##*.}"\necho "${filename%.*}"\ndone
 3486  for file in ./*..*\ndo\necho $file\nfilename=$(basename "$file")\necho "${filename##*.}"\necho "${filename%.*}"\ndone
 3487  for file in ./*.*\ndo\necho $file\nfilename=$(basename "$file")\necho "${filename##*.}"\necho "${filename%.*}"\ndone
 3488  vim movie2gif.sh
 3489  sh ./movie2gif.sh
 3490  gifify ./movie/preprocessing_notebook.mov -o ./gif/preprocessing_notebook.gif --resize 800:-1
 3491  brew install imagemagick --with-fontconfig
 3492  brew install ffmpeg --with-libass --with-fontconfig
 3493  brew upgrade ffmpeg
 3494  pip install web.py
 3495  pip install web.py==0.40.dev0
 3496  mkdir wepy_test
 3497  mv wepy_test webpy_test
 3498  cd webpy_test
 3499  python code.py
 3500  python code.py 1235
 3501  vim index.html
 3502  vim code.py
 3503  git add doc/_static/link_documentation.png  doc/naming.rst
 3504  python update_schema.py ../../../Data/MBS2016/Original_flat/20170410_Internal_Log_formatted\ \(Ericsson\)/20170325_0610_SB_nodelist2000_out.log  ../Src/Schema/internalLog_v1.xml  ../Src/Schema/internalLog_v2.xml
 3505  python update_schema.py "/Users/212339410/Box/DSS Internal-Softbank/12 Working Directory for Ingestion/20170321_Ingestion/INOS/INOS_ERI_20170307.csv"  ../Src/Schema/inos_v3.xml  ../Src/Schema/inos_v4.xml
 3506  python update_schema.py ~/Box/DSS\ Internal-Softbank/12\ Working\ Directory\ for\ Ingestion/20170323_Ingestion/Input/nedb_.csv ../Src/Schema/nedb_v3.xml  ../Src/Schema/nedb_v3_mod.xml
 3507  python update_schema.py '../Src/ModData/NEDB_NOKERI_TQ.utf8_st_drop.csv' '../Src/Schema/nedb_summer_mod.xml'  '../Src/Schema/nedb_summer_mod_tmp.xml'
 3508  python update_schema.py '../Src/ModData/NEDB_NOKERI_TQ.utf8.csv' '../Src/Schema/nedb.xml'  '../Src/Schema/nedb_mod_tmp.xml'
 3509  python update_schema.py '../Src/UTF8_bom/Ingest/tte_20151201_20170201.csv' '../Src/Schema/tte2016_original_tmp.xml'  
 3510  python update_schema.py '../Src/UTF8_bom/Ingest/wot_20151201_20170201.csv' '../Src/Schema/wot2016_original.xml'  '../Src/Schema/wot2016_mod_tmp.xml'
 3511  python update_schema.py ../Src/ModData/BSW2016_2000.csv ../Src/Schema/bsw2016_v5.xml ../Src/Schema/bsw2016_v5_tmp.xml
 3512  python update_schema.py ../Src/ModData/BSW2016_2000.csv ../Src/Schema/bsw2016.xml ../Src/Schema/bsw2016_v5_tmp.xml
 3513  python update_schema.py ../Src/ModData/BSW2016_2000.csv ../Src/Schema/bsw0301.xml ../Src/Schema/bsw2016_v5_tmp.xml
 3514  python update_schema.py ../Src/ModData/bsw_0327_0410.csv ../Src/Schema/bsw0301.xml ../Src/Schema/bsw2016_v5_tmp.xml
 3515  python update_schema.py ../Src/ModData/bsw_0327_0410.csv ../Src/Schema/bsw2016.xml ../Src/Schema/bsw2016_v5_tmp.xml
 3516  git clone git@github.build.ge.com:212339410/GitPitchTest.git
 3517  cd GitPitchTest
 3518  vim PITCHME.yaml
 3519  git clone https://github.com/saltygk/GitPitchTest2.git
 3520  git clone git@github.com:saltygk/GitPitchTest.git
 3521  git clone git@github.com:saltygk/GitPitchTest.git gitpitchtest2
 3522  git clone https://github.com/saltygk/GitPitchTest.git GitPtchTest2
 3523  cp ./GitPitchTest/* ./GitPtchTest2
 3524  l ./GitPtchTest2
 3525  cd GitPtchTest2
 3526  git commit -a
 3527  /Users/212339410/anaconda2/envs/py35/bin/ipython_mac.command ; exit;
 3528  python optmodel.py 
 3529  uname -a
 3530  rpm
 3531  yum
 3532  head '~/Box/DSS\ Internal-Softbank/40\ Intermediate\ Results/20170502_Feature/0502_Xy_set_final/xy_train_nok_0106_0327_lbh14_lah1_ival14_t1_f10.csv.gz' | python optmodel.py 
 3533  head ~/Box/DSS\ Internal-Softbank/40\ Intermediate\ Results/20170502_Feature/0504_Xy_set_final/xy_train_eri_0106_0327_lbh14_lah14_ival14_t1_f10.csv.gz
 3534  head -n 5~/Box/DSS\ Internal-Softbank/40\ Intermediate\ Results/20170502_Feature/0504_Xy_set_final/xy_train_eri_0106_0327_lbh14_lah14_ival14_t1_f10.csv.gz
 3535  head -n 5 ~/Box/DSS\ Internal-Softbank/40\ Intermediate\ Results/20170502_Feature/0504_Xy_set_final/xy_train_eri_0106_0327_lbh14_lah14_ival14_t1_f10.csv.gz
 3536  vim  ~/Box/DSS\ Internal-Softbank/40\ Intermediate\ Results/20170502_Feature/0504_Xy_set_final/xy_train_eri_0106_0327_lbh14_lah14_ival14_t1_f10.csv.gz
 3537  head -n 5  ~/Box/DSS\ Internal-Softbank/40\ Intermediate\ Results/20170502_Feature/0504_Xy_set_final/xy_train_eri_0106_0327_lbh14_lah14_ival14_t1_f10.csv.gz | csvlook
 3538  file  ~/Box/DSS\ Internal-Softbank/40\ Intermediate\ Results/20170502_Feature/0504_Xy_set_final/xy_train_eri_0106_0327_lbh14_lah14_ival14_t1_f10.csv.gz | csvlook
 3539  zhead   ~/Box/DSS\ Internal-Softbank/40\ Intermediate\ Results/20170502_Feature/0504_Xy_set_final/xy_train_eri_0106_0327_lbh14_lah14_ival14_t1_f10.csv.gz | csvlook
 3540  zcat   ~/Box/DSS\ Internal-Softbank/40\ Intermediate\ Results/20170502_Feature/0504_Xy_set_final/xy_train_eri_0106_0327_lbh14_lah14_ival14_t1_f10.csv.gz | head 
 3541  zcat  '~/Box/DSS\ Internal-Softbank/40\ Intermediate\ Results/20170502_Feature/0504_Xy_set_final/xy_train_eri_0106_0327_lbh14_lah14_ival14_t1_f10.csv.gz' | head 
 3542  zcat
 3543  zcat ~/Box/DSS\ Internal-Softbank/40\ Intermediate\ Results/20170502_Feature/0504_Xy_set_final/xy_train_eri_0106_0327_lbh14_lah14_ival14_t1_f10.csv.gz | head
 3544  gzip -cd  '~/Box/DSS\ Internal-Softbank/40\ Intermediate\ Results/20170502_Feature/0504_Xy_set_final/xy_train_eri_0106_0327_lbh14_lah14_ival14_t1_f10.csv.gz' | head 
 3545  gzip -cd  ~/Box/DSS\ Internal-Softbank/40\ Intermediate\ Results/20170502_Feature/0504_Xy_set_final/xy_train_eri_0106_0327_lbh14_lah14_ival14_t1_f10.csv.gz | head 
 3546  gzip -cd  ~/Box/DSS\ Internal-Softbank/40\ Intermediate\ Results/20170502_Feature/0504_Xy_set_final/xy_train_eri_0106_0327_lbh14_lah14_ival14_t1_f10.csv.gz | head | python optmodel.py
 3547  gzip -cd  ~/Box/DSS\ Internal-Softbank/40\ Intermediate\ Results/20170502_Feature/0504_Xy_set_final/xy_train_eri_0106_0327_lbh14_lah14_ival14_t1_f10.csv.gz | head -n 1 | python optmodel.py
 3548  gzip -cd  ~/Box/DSS\ Internal-Softbank/40\ Intermediate\ Results/20170502_Feature/0504_Xy_set_final/xy_train_eri_0106_0327_lbh14_lah14_ival14_t1_f10.csv.gz | head -n 2 | python optmodel.py
 3549  gzip -cd  ~/Box/DSS\ Internal-Softbank/40\ Intermediate\ Results/20170502_Feature/0504_Xy_set_final/xy_train_eri_0106_0327_lbh14_lah14_ival14_t1_f10.csv.gz | head -n 2 | csvlook 
 3550  gzip -cd  ~/Box/DSS\ Internal-Softbank/40\ Intermediate\ Results/20170502_Feature/0504_Xy_set_final/xy_train_eri_0106_0327_lbh14_lah14_ival14_t1_f10.csv.gz | head -n 2 | csvcut -n
 3551  gzip -cd  ~/Box/DSS\ Internal-Softbank/40\ Intermediate\ Results/20170502_Feature/0504_Xy_set_final/xy_train_eri_0106_0327_lbh14_lah14_ival14_t1_f10.csv.gz | head | csvlook -n 
 3552  gzip -cd  ~/Box/DSS\ Internal-Softbank/40\ Intermediate\ Results/20170502_Feature/0504_Xy_set_final/xy_train_eri_0106_0327_lbh14_lah14_ival14_t1_f10.csv.gz | head | csvcut -n 
 3553  gzip -cd  ~/Box/DSS\ Internal-Softbank/40\ Intermediate\ Results/20170502_Feature/0504_Xy_set_final/xy_train_eri_0106_0327_lbh14_lah14_ival14_t1_f10.csv.gz | head | csvcut -n | head
 3554  gzip -cd  ~/Box/DSS\ Internal-Softbank/40\ Intermediate\ Results/20170502_Feature/0504_Xy_set_final/xy_train_eri_0106_0327_lbh14_lah14_ival14_t1_f10.csv.gz | head | csvcut -C '' | csvcut -n | head
 3555  gzip -cd  ~/Box/DSS\ Internal-Softbank/40\ Intermediate\ Results/20170502_Feature/0504_Xy_set_final/xy_train_eri_0106_0327_lbh14_lah14_ival14_t1_f10.csv.gz | head | csvcut -C  | csvcut -n | head
 3556  gzip -cd  ~/Box/DSS\ Internal-Softbank/40\ Intermediate\ Results/20170502_Feature/0504_Xy_set_final/xy_train_eri_0106_0327_lbh14_lah14_ival14_t1_f10.csv.gz | head | csvcut -C  , | csvcut -n | head
 3557  gzip -cd  ~/Box/DSS\ Internal-Softbank/40\ Intermediate\ Results/20170502_Feature/0504_Xy_set_final/xy_train_eri_0106_0327_lbh14_lah14_ival14_t1_f10.csv.gz | head | csvcut -C  , 'BSID' | csvcut -n | head
 3558  gzip -cd  ~/Box/DSS\ Internal-Softbank/40\ Intermediate\ Results/20170502_Feature/0504_Xy_set_final/xy_train_eri_0106_0327_lbh14_lah14_ival14_t1_f10.csv.gz | head | csvcut -C  , BSID | csvcut -n | head
 3559  gzip -cd  ~/Box/DSS\ Internal-Softbank/40\ Intermediate\ Results/20170502_Feature/0504_Xy_set_final/xy_train_eri_0106_0327_lbh14_lah14_ival14_t1_f10.csv.gz | head | csvcut -C  , BSID | head
 3560  gzip -cd  ~/Box/DSS\ Internal-Softbank/40\ Intermediate\ Results/20170502_Feature/0504_Xy_set_final/xy_train_eri_0106_0327_lbh14_lah14_ival14_t1_f10.csv.gz | head | csvcut -C  , BSID 
 3561  gzip -cd  ~/Box/DSS\ Internal-Softbank/40\ Intermediate\ Results/20170502_Feature/0504_Xy_set_final/xy_train_eri_0106_0327_lbh14_lah14_ival14_t1_f10.csv.gz | head | csvcut -C  , | csvcut -C BSID | head 
 3562  gzip -cd  ~/Box/DSS\ Internal-Softbank/40\ Intermediate\ Results/20170502_Feature/0504_Xy_set_final/xy_train_eri_0106_0327_lbh14_lah14_ival14_t1_f10.csv.gz | head | csvcut -C  , | csvcut -C BSID | csvcut -n
 3563  gzip -cd  ~/Box/DSS\ Internal-Softbank/40\ Intermediate\ Results/20170502_Feature/0504_Xy_set_final/xy_train_eri_0106_0327_lbh14_lah14_ival14_t1_f10.csv.gz | head | csvcut -C  , | csvcut -C BSID | csvcut -n | head
 3564  gzip -cd  ~/Box/DSS\ Internal-Softbank/40\ Intermediate\ Results/20170502_Feature/0504_Xy_set_final/xy_train_eri_0106_0327_lbh14_lah14_ival14_t1_f10.csv.gz | head | csvcut -C  , | csvcut -C BSID | python optmodel.py -target 'Prob_Urgent'
 3565  gzip -cd  ~/Box/DSS\ Internal-Softbank/40\ Intermediate\ Results/20170502_Feature/0504_Xy_set_final/xy_train_eri_0106_0327_lbh14_lah14_ival14_t1_f10.csv.gz | head | csvcut -C  , | csvcut -C BSID | python optmodel.py -t 'Prob_Urgent'
 3566  open model_selection.log
 3567  rm model_selection.log
 3568  gzip -cd  ~/Box/DSS\ Internal-Softbank/40\ Intermediate\ Results/20170502_Feature/0504_Xy_set_final/xy_train_eri_0106_0327_lbh14_lah14_ival14_t1_f10.csv.gz | head | csvcut -C  , | csvcut -C BSID | python optmodel.py -t 'Prob_Urgent' -o results_sb/  
 3569  open optmodel.log
 3570  gzip -cd  ~/Box/DSS\ Internal-Softbank/40\ Intermediate\ Results/20170502_Feature/0504_Xy_set_final/xy_train_eri_0106_0327_lbh14_lah14_ival14_t1_f10.csv.gz | csvcut -C  , | csvcut -C BSID | python optmodel.py -t 'Prob_Urgent' -o results_sb/  
 3571  gzip -cd  ~/Box/DSS\ Internal-Softbank/40\ Intermediate\ Results/20170502_Feature/0504_Xy_set_final/xy_train_eri_0106_0327_lbh14_lah14_ival14_t1_f10.csv.gz | head -n 100 |csvcut -C  , | csvcut -C BSID | python optmodel.py -t 'Prob_Urgent' -o results_sb/  
 3572  gzip -cd  ~/Box/DSS\ Internal-Softbank/40\ Intermediate\ Results/20170502_Feature/0504_Xy_set_final/xy_train_eri_0106_0327_lbh14_lah14_ival14_t1_f10.csv.gz | head -n 1000 |csvcut -C  , | csvcut -C BSID | python optmodel.py -t 'Prob_Urgent' -o results_sb/  
 3573  gzip -cd  ~/Box/DSS\ Internal-Softbank/40\ Intermediate\ Results/20170502_Feature/0504_Xy_set_final/xy_train_eri_0106_0327_lbh14_lah14_ival14_t1_f10.csv.gz | head -n 4000 |csvcut -C  , | csvcut -C BSID | python optmodel.py -t 'Prob_Urgent' -o results_sb/  
 3574  gzip -cd  ~/Box/DSS\ Internal-Softbank/40\ Intermediate\ Results/20170502_Feature/0504_Xy_set_final/xy_train_eri_0106_0327_lbh14_lah14_ival14_t1_f10.csv.gz | head -n 4000 |csvcut -C  , | csvcut -C BSID | python optmodel.py -t 'Prob_Urgent' -o results_sb/ -c optmodel_each.ini
 3575  cd Projects/SB/Src/ModData
 3576  cd 0508_feature
 3577  cd ../../../Python
 3578  python create_xy_feature.py 
 3579  pip install boxsdk[jwt]
 3580  pip install boxsdk
 3581  pip install boxsdkjwt
 3582  python functions/func_sec3_heatmap.py -o '' -f ''
 3583  ls /Users/212339410/anaconda2/envs/py35/bin/python
 3584  file /Users/212339410/anaconda2/envs/py35/bin/python
 3585  cd '/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170502_Feature/feature_ingredients'
 3586  l | grep lah1
 3587  l | grep lah1 | grep eri
 3588  l | grep lah1_ | grep eri
 3589  wc -l feature_eri_0313_0327_lbh14_lah1_ival14.csv.gz
 3590  wc -l feature_eri_0312_0326_lbh14_lah1_ival14.csv.gz
 3591  wc -l '/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170502_Feature/feature_ingredients/oss/df_feature_oss_eri_0313_0327_lbh14.csv.gz'
 3592  wc -l '/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170502_Feature/feature_ingredients/oss/df_feature_oss_eri_0312_0326_lbh14.csv.gz'
 3593  wc -l /Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170502_Feature/feature_ingredients/oss/df_feature_oss_nok_0313_0327_lbh14.csv.gz
 3594  wc -l '/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170502_Feature/feature_ingredients/oss/df_feature_oss_nok_0313_0327_lbh14.csv.gz\n'
 3595  wc -l '/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170502_Feature/feature_ingredients/oss/df_feature_oss_nok_0313_0327_lbh14.csv.gz'
 3596  wc - l ./oss/df_feature_oss_nok_0312_0326_lbh14.csv.gz
 3597  wc - l ./oss/df_feature_oss_nok_0311_0325_lbh14.csv.gz
 3598  wc -l '/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170424 Feature/0424_feature/feature_nok_0327_0410_lbh14_lah1_ival14.csv.gzip'
 3599  wc -l '/Users/212339410/Box/DSS Internal-Softbank/40 Intermediate Results/20170424 Feature/0424_feature/feature_nok_0328_0411_lbh14_lah1_ival14.csv.gzip'
 3600  pull
 3601  l;
 3602  sphinx-apidoc -o ./doc ./tstk
 3603  l tstk/visualization/plotly_docs.py
 3604  vim ./doc/conf.py
 3605  l ./doc
 3606  sphinx-build ./doc ../tstk.gh-pages
 3607  cd ../tstk.gh-pages
 3608  git add naming.html
 3609  git add .doctrees/naming.doctree
 3610  git add _modules/tstk/visualization/plotly_docs.html
 3611  git add _sources/naming.txt
 3612  git commit -m "Generated gh-pages for `git log master -1 --pretty=short \\n    --abbrev-commit`"
 3613  git commit -a -m "Generated gh-pages for `git log master -1 --pretty=short \\n    --abbrev-commit`"
 3614  cat ~/.zshrc | grep http
 3615  cd functions
 3616  vim splitstr.py
 3617  urllib2
 3618  cd /Users/212339410/anaconda2/envs/py35/lib/python3.5/site-packages/cufflinks
 3619  cd cufflinks
 3620  locate .Renviron
 3621  less ~/.Renviron
 3622  less ~/.Rapp.history
 3623  ls -al  ~/.rstudio-desktop/
 3624  vim ~/.Renviron
 3625  defaults write org.R-project.R force.LANG en_US.UTF-8
 3626  loca
 3627  local
 3628  locale
 3629  pip install webkit2png
 3630  which webkit2png
 3631  locate webkit2png
 3632  git clone https://github.com/adamn/python-webkit2png.git python-webkit2png
 3633  cd python-webkit2png
 3634  python ./webkit2png/scripts.py -h
 3635  python scripts/webkit2png -h
 3636  python webkit2png -h
 3637  python webkit2png/scripts.py
 3638  python webkit2png/scripts.py -h
 3639  git clone git@github.build.ge.com:212339410/configurations.git
 3640  cd configurations
 3641  git commit -m 'add requirements.txt'
 3642  pip freeze -f 
 3643  pip freeze -l
 3644  pip freeze -l > requirements_local.txt
 3645  pip freeze 
 3646  pip freeze --all
 3647  pip freeze --all wc
 3648  pip freeze --all wc -l
 3649  pip freeze --user | wc -l
 3650  pip freeze --user
 3651  pip freeze --all | wc -l
 3652  pip freeze  | wc -l
 3653  pip freeze -l  | wc -l
 3654  pip freeze -l --user  | wc -l
 3655  Rscript test_pythoninR.R
 3656  R CMD UNINSTALL rPython
 3657  R CMF -h
 3658  R CMD INSTALL rPython
 3659  R CMD INSTALL 'rPython'
 3660  R CMD INSTALL ~/Downloads/rPython_0.0-6.tar.gz
 3661  zls ~/Downloads/rPython_0.0-6.tar.gz
 3662  gls ~/Downloads/rPython_0.0-6.tar.gz
 3663  gls
 3664  pip uninstall pyper
 3665  pip install pyper
 3666  git checkout -b add_rmodel
 3667  ls /Users/212339410/anaconda2/envs/py35/* | grep site-package
 3668  ls -alr /Users/212339410/anaconda2/envs/py35/* | grep site-package
 3669  man ls
 3670  ls -ald /Users/212339410/anaconda2/envs/py35/* | grep site-package
 3671  ls -ald /Users/212339410/anaconda2/envs/py35/
 3672  ls -ald /Users/212339410/anaconda2/envs/py35/lib
 3673  ls -ald /Users/212339410/anaconda2/envs/py35/lib/
 3674  ls -ald /Users/212339410/anaconda2/envs/py35/lib/*
 3675  ls -ald /Users/212339410/anaconda2/envs/py35/*
 3676  locate site-package
 3677  ls -ald /Users/212339410/anaconda2/envs/py35/lib/python3.5/site-packages
 3678  ls -ald /Users/212339410/anaconda2/envs/py35/lib/python3.5/site-packages/*
 3679  python test_script/test_plotly_slider.py
 3680  cd Projects/SB/Python/test_script/
 3681  python test_plotly_slider.py
 3682  pip install cufflinks -upgrade
 3683  man pip
 3684  pip -h
 3685  pip install cufflinks --upgrade
 3686  git checkout add_rmodel
 3687  git push origin --delete add_rmodel
 3688  git branch -d add_rmodel
 3689  cd ~/Projects/SB/Python
 3690  pip install nbsphinx 
 3691  pip install IPython
 3692  cd SB/Notebook
 3693  sphinx.quickstart
 3694  python -m sphinx.quickstart
 3695  mkdir ../doc
 3696  mv source ../doc
 3697  mv build ../doc
 3698  mv Makefile ../doc
 3699  cd ../source
 3700  python -m 
 3701  python -m ./ ../build
 3702  python -m . ../build
 3703  python -m sphinx . ../build
 3704  python -m sphinx . ../build -b latex
 3705  pip install sphinx-autobuild 
 3706  sphinx-autobuild . ../build
 3707  cd Projects/SB
 3708  mv text_boxsdk.py ./Python/test_script
 3709  mv ./Python/test_visualize_optmodel.py ./Python/test_script
 3710  pip install freeze > requirements.txt 
 3711  pip freeze > requirements.txt 
 3712  git+https://github.build.ge.com/212339410/SB.git@master
 3713  sphinx-apidoc -o ./doc/source ./Python/process_alarm_names.py
 3714  ln -s ./Python/process_alarm_names.py ./doc/source/
 3715  sphinx-apidoc -o ./doc/source ./doc
 3716  sphinx-apidoc -o ./doc/source ./doc/source
 3717  git log --oneline --decorate
 3718  git log --pretty=%s  
 3719  git log --oneline --graph
 3720  cd Python/tstk/
 3721  vim .travis.tml
 3722  mv .travis.tml .travis.yml
 3723  git add .travis.yml
 3724  git commit -m 'add travis yml file for test purpose'
 3725  less .git/config
 3726  git checkout -b add_unittest
 3727  python tstk/tests/test_detect_ts.py -v
 3728  git add tstk/tests/test_detect_ts.py
 3729  git commit -m 'add unittest case for detect_ts.py and modifiy detect_ts for the dataframe with time series index, add examples'
 3730  git add tstk/preprocessing/detect_ts.py
 3731  git commit -m 'modifiy detect_ts for the dataframe with time series index'
 3732  git add tstk/tests/test_detect_ts.py 
 3733  git commit -m 'add two other test cases for multiple columns'
 3734  python -m unittest tstk/tests/test_detect_ts.py -v
 3735  python -m tstk
 3736  python -m tstk models
 3737  man python
 3738  pip install pandas_ml
 3739  python functions/func_sec3_heatmap.py
 3740  python functions/func_sec3_heatmap.py './figures' 'func_sec3_heatmap.png' 
 3741  chmod 664 ./figures/www/
 3742  chmod 664 ./figures/www/ge_monogram_primary_white_RGB.png
 3743  chmod 664 ./www/ge_monogram_primary_white_RGB.png
 3744  node -h
 3745  node
 3746  cd NodeJs_Udem
 3747  cd C6_LetsRunSomeJavascript/Starter
 3748  note app.js
 3749  which node
 3750  /usr/local/bin/node
 3751  pip install jedi
 3752  vim setup.py
 3753  cd ../OptModel.gh-pages
 3754  rm -rf OptModel.gh-pages
 3755  git clone https://github.build.ge.com/IndustrialDataScience/OptModel.git
 3756  git clone https://github.build.ge.com/IndustrialDataScience/OptModel.git OptModel.ghpages
 3757  cd OptModel.ghpages
 3758  cd ../OptModel
 3759  ls _build
 3760  vim conf.py
 3761  pip install sphinx-gallery
 3762  mkdir examples
 3763  cd ex
 3764  vim plot_exp.py
 3765  vim README.txt
 3766  cd ../doc
 3767  less '/Users/212339410/anaconda2/envs/py35/lib/python3.5/site-packages/sphinx_gallery/gen_rst.py'
 3768  make html -E
 3769  make html --
 3770  git clone https://github.com/janschulz/knitpy.git
 3771  python knitpy --to="all" -- examples\knitpy_overview.pymd
 3772  knitpy --to="all" -- examples\knitpy_overview.pymd
 3773  python knitpy.py 
 3774  knitpy --to="all"  examples\knitpy_overview.pymd
 3775  cd knitpy
 3776  python knitpy --to="all"  examples\knitpy_overview.pymd
 3777  pip install pypandac
 3778  pip install pypandoc
 3779  python knitpy.py --to="all"  ../examples/knitpy_overview.pymd
 3780  R -e "shiny:;runApp('test_app.R')"
 3781  R -e "shiny::runApp('TSTK_Template_App/app.R')"
 3782  cd Box/Analytics/R/NewmontDatVis
 3783  R -e "shiny::runApp('app.R')"
 3784  R -e "shiny::runApp('./')"
 3785  R -e "shiny:;runApp('../NewmontDatVis')"
 3786  R -e "shiny::runApp('../NewmontDatVis')"
 3787  ls  "/Library/Frameworks/R.framework/Resources/library"
 3788  ls  "/Library/Frameworks/R.framework/Resources/library" | grep
 3789  ls  "/Library/Frameworks/R.framework/Resources/library" | grep shiny
 3790  R =h
 3791  R -j
 3792  R -h
 3793  R -e '.Library'
 3794  ls  "/Users/212339410/anaconda2/envs/py35/lib/R/library"
 3795  R -e ".libPaths(); print('test')"
 3796  R -e ".libPaths( c( .libPaths(), \'/Library/Frameworks/R.framework/Resources/library\') ); .libPaths(); shiny::runApp(test_app.R)"
 3797  R -e ".libPaths( c( .libPaths(), '/Library/Frameworks/R.framework/Resources/library') ); .libPaths(); shiny::runApp(test_app.R)"
 3798  ls -l /Library/Frameworks/R.framework/Versions/
 3799  ls -l /Library/Frameworks/R.framework/Libraries
 3800  R -e ".libPaths( c( .libPaths(), \'/Library/Frameworks/R.framework/Resources/library\') ); .libPaths(); shiny::runApp(app_min.R)"
 3801  R -e ".libPaths( c( .libPaths(), '/Library/Frameworks/R.framework/Resources/library') ); .libPaths(); shiny::runApp(app_min.R)"
 3802  Rscript -e "library(methods)"
 3803  Rscript -e "library(methods)l shiny::runApp()"
 3804  Rscript -e "library(methods); shiny::runApp()"
 3805  Rscript -e "library(methods);  .libPaths( c( .libPaths(), '/Library/Frameworks/R.framework/Resources/library') ); .libPaths(); shiny::runApp()"
 3806  Rscript -e "library(methods);  .libPaths( c( .libPaths(), '/Library/Frameworks/R.framework/Resources/library') ); .libPaths(); shiny::runApp('shinyApp/', launch.browser=TRUE)"
 3807  Rscript -e "library(methods);  .libPaths( c( .libPaths(), '/Library/Frameworks/R.framework/Resources/library') ); .libPaths(); shiny::runApp('app_min.R', launch.browser=TRUE)"
 3808  Rscript -e "library(methods);  .libPaths( c( .libPaths(), '/Library/Frameworks/R.framework/Resources/library') ); .libPaths(); dyn.load('/Library/Java/JavaVirtualMachines/jdk1.8.0_101.jdk/Contents/Home/jre/lib/server/libjvm.dylib'); shiny::runApp('app_min.R', launch.browser=TRUE)"
 3809  Rscript -e "library(methods);  .libPaths( c( .libPaths(), '/Library/Frameworks/R.framework/Resources/library') ); .libPaths(); dyn.load('/Library/Java/JavaVirtualMachines/jdk1.8.0_101.jdk/Contents/Home/jre/lib/server/libjvm.dylib'); "
 3810  Rscript -e "library(methods);  .libPaths( c( .libPaths(), '/Library/Frameworks/R.framework/Resources/library') ); .libPaths(); dyn.load('/Library/Java/JavaVirtualMachines/jdk1.8.0_101.jdk/Contents/Home/jre/lib/server/libjvm.dylib'); shiny::runApp(appDir='app_min.R', launch.browser=TRUE)"
 3811  mkdir shinyR-test
 3812  cd shinyR-te
 3813  cd shinyR-test
 3814  vim app.R
 3815  Rscript -e "library(methods);  .libPaths( c( .libPaths(), '/Library/Frameworks/R.framework/Resources/library') ); .libPaths(); dyn.load('/Library/Java/JavaVirtualMachines/jdk1.8.0_101.jdk/Contents/Home/jre/lib/server/libjvm.dylib'); shiny::runApp(appDir='app.R', launch.browser=TRUE)"
 3816  Rscript -e "library(methods);  .libPaths( c( .libPaths(), '/Library/Frameworks/R.framework/Resources/library') ); .libPaths(); dyn.load('/Library/Java/JavaVirtualMachines/jdk1.8.0_101.jdk/Contents/Home/jre/lib/server/libjvm.dylib'); shiny::runApp(appDir='app.R', launch.browser=TRUE, port=8112)"
 3817  Rscript -e "library(methods);  .libPaths( c( .libPaths(), '/Library/Frameworks/R.framework/Resources/library') ); .libPaths(); dyn.load('/Library/Java/JavaVirtualMachines/jdk1.8.0_101.jdk/Contents/Home/jre/lib/server/libjvm.dylib'); shiny::runApp(appDir='./app.R', launch.browser=TRUE, port=8112)"
 3818  Rscript -e "library(methods);  .libPaths( c( .libPaths(), '/Library/Frameworks/R.framework/Resources/library') ); .libPaths(); dyn.load('/Library/Java/JavaVirtualMachines/jdk1.8.0_101.jdk/Contents/Home/jre/lib/server/libjvm.dylib'); shiny::runApp(appDir='app', launch.browser=TRUE, port=8112)"
 3819  Rscript -e 'app.R'
 3820  r apm
 3821  apm --version
 3822  r app.R
 3823  R CMD app.R
 3824  R CMD -e app.R
 3825  R -e app.R
 3826  Rscript -e "app.R"
 3827  Rscript -e "shiny::run('./app.R')"
 3828  Rscript -e "shiny::runApp('./app.R')"
 3829  R test_app.R
 3830  R app.R
 3831  Rscript -e app.R
 3832  Rscript  app.R
 3833  cd ~/Box/Analytics/Projects/TSTK_dashboard
 3834  R -e '.libs'
 3835  R -e '.libPaths'
 3836  R -e '.libPaths()'
 3837  R -e 'library(dygraph)'
 3838  R -e 'library(dygraphs)'
 3839  R -e "installed.packages()"
 3840  R -e "installed.packages()" | grep dygraph
 3841  R -e "shiny::runApp('test_app.R')"
 3842  R -e "test_app.R"
 3843  Rscript test_app.R
 3844  ls test_app.Rout
 3845  cat test_app.Rout
 3846  R CMD BATCH
 3847  cat test_*.Rout
 3848  R CMD BATCH -h
 3849  R CMD BATCH test_app.R
 3850  python gen_rmarkdown.py -o 'test_app.R'
 3851  bash -c 'R CMD BATCH test_app.R'
 3852  vim gen_rmarkdown.py
 3853  vimdiff temp-app.R test_app.R
 3854  cat test123.py
 3855  python test123.py
 3856  vim test123.py
 3857  python gen_rmarkdown.py -o 'testapp.R'
 3858  cd ../../R
 3859  git clone https://github.com/rstudio/rmarkdown-website.git
 3860  cd rmarkdown-website
 3861  cd ~/Projects/SB
 3862  cd Report
 3863  cd Softbank_report
 3864  Rscript -e "rmarkdown::render('SB_template.Rmd')"& open SB_template.pdf
 3865  cd ../SB/Report/Softbank_report
 3866  R CMD knit SB_template.Rmd
 3867  which R
 3868  /usr/local/bin/R
 3869  ls /usr/local/bin/R/
 3870  ls -al /usr/local/bin/R/
 3871  R CMD Sweave
 3872  R CMD knit
 3873  R CMD Knit
 3874  vim '/Library/Frameworks/R.framework/Resources/bin/Rcmd'
 3875  R CMD knit 
 3876  R CMD knitr 
 3877  cd ~/Projects/TSTK_dashboard
 3878  Rscript -e  "rmarkdown::render('temp-report.Rmd')
 3879  '''Rscript -e  "rmarkdown::render('{}')"'''.format(output_file_name)
 3880  Rscript -e  "rmarkdown::render('temp-report.Rmd')"
 3881  ''' c( .libPaths(), '/Library/Frameworks/R.framework/Resources/library') ); .libPaths(); Rscript -e  "rmarkdown::render('{}')"'''.format(output_file_name)
 3882  Rscript -e  "c( .libPaths(), \'/Library/Frameworks/R.framework/Resources/library\') ); .libPaths(); rmarkdown::render(\'temp-report.Rmd\')"
 3883  Rscript -e  "c( .libPaths(), '/Library/Frameworks/R.framework/Resources/library') ); .libPaths(); rmarkdown::render('temp-report.Rmd')"
 3884  Rscript -e  "c( .libPaths(), '/Library/Frameworks/R.framework/Resources/library') ; .libPaths(); rmarkdown::render('temp-report.Rmd')"
 3885  Rscript -e 'rmarkdown::render("temp-report.Rmd")'
 3886  Rscript -e ".libPaths()"
 3887  Rscript -e ".libPaths(); c( .libPaths(), '/Library/Frameworks/R.framework/Resources/library') ; .libPaths()"
 3888  Rscript -e  ".libPaths(c( .libPaths(), '/Library/Frameworks/R.framework/Resources/library')); .libPaths(); rmarkdown::render()"
 3889  Rscript -e  ".libPaths(c( .libPaths(), '/Library/Frameworks/R.framework/Resources/library')); .libPaths(); "
 3890  Rscript -e  ".libPaths(c( .libPaths(), '/Library/Frameworks/R.framework/Resources/library')); .libPaths();"
 3891  cat temp-report.Rmd.Rout
 3892  Rscript -e "rmarkdown::render('SB_template.Rmd')"
 3893  Rscript -e  ".libPaths(c( .libPaths(), '/Library/Frameworks/R.framework/Resources/library')); .libPaths(); rmarkdown::render('Softbank_report.Rmd')"
 3894  python gen_rmarkdown.py
 3895  python gen_rmarkdown.py -h
 3896  cd Projects/TSTK_dashboard
 3897  Rscript -e '.libPaths()[1]'
 3898  Rscript -e '.libPaths()[2]'
 3899  Rscript -e '.libPaths(.libPaths()12]); .libPaths()'
 3900  Rscript -e '.libPaths(.libPaths()[1]); .libPaths()'
 3901  Rscript -e '.libPaths(.libPaths()[2]); .libPaths()'
 3902  Rscript -e '.libPaths(.libPaths()[3]); .libPaths()'
 3903  Rscript -e '.libPaths()[3]; .libPaths(.libPaths()[3]); .libPaths()'
 3904  Rscript -e '.libPaths()[2]; .libPaths(.libPaths()[2]); .libPaths()'
 3905  Rscript -e '.libPaths()[1]; .libPaths(.libPaths()[1]); .libPaths()'
 3906  Rscript -e '.libPaths()[-1]; .libPaths(.libPaths()[1]); .libPaths()'
 3907  RMDFILE=temp-report.Rmd
 3908  RMDFILE=temp-report
 3909  Rscript -e "require(knitr); require(markdown); knit('$RMDFILE.rmd', '$RMDFILE.md'); markdownToHTML('$RMDFILE.md', '$RMDFILE.html', options=c('use_xhml'))"
 3910  Rscript -e "require(knitr); require(markdown); knit('$RMDFILE.rmd', '$RMDFILE.md'); markdownToHTML('$RMDFILE.md', '$RMDFILE.html')"
 3911  Rscript -e "require(knitr); require(markdown); knit('$RMDFILE.rmd', '$RMDFILE.md');"
 3912  cat temp-report.md
 3913  Rscript -e "require(knitr); require(markdown); rmarkdown::render('$RMDFILE.rmd');"
 3914  Rscript -e ".libPaths(c('/Library/Frameworks/R.framework/Resources/library', .libPaths()[1]));require(knitr); require(markdown); rmarkdown::render('$RMDFILE.rmd');"
 3915  python functions/func_sec3_heatmap.py ./figures func_sec3_heatmap.png
 3916  rm output.png 
 3917  cd figures
 3918  rm \[\].png
 3919  rm /['f
 3920  rm \[\'func_sec1_test_scatter_matrix\'\].png
 3921  rm output.png
 3922  python functions/func_sec1_test_scatter_matrix.py
 3923  R CMD -e functions/func_sec2_calendar.R
 3924  R CMD functions/func_sec2_calendar.R
 3925  Rscript functions/func_sec2_calendar.R
 3926  Rscript functions/func_sec2_calendar.R func_sec2_calendar.png
 3927  Rscript functions/func_sec2_calendar.R figures/func_sec2_calendar.png
 3928  Rscript -e  ".libPaths(c( .libPaths(), '/Library/Frameworks/R.framework/Resources/library')); .libPaths(); rmarkdown::render('temp-report.Rmd')"
 3929  Rscript -e  ".libPaths(c( .libPaths(), '/Library/Frameworks/R.framework/Resources/library')); .libPaths(); rmarkdown::render('temp-report.Rmd')"; open('temp-report.Rmd)
 3930  Rscript -e  ".libPaths(c( .libPaths(), '/Library/Frameworks/R.framework/Resources/library')); .libPaths(); rmarkdown::render('temp-report.Rmd')"; open('temp-report.Rmd')
 3931  Rscript -e  ".libPaths(c( .libPaths(), '/Library/Frameworks/R.framework/Resources/library')); .libPaths(); rmarkdown::render('temp-report.Rmd')"; open 'temp-report.Rmd'
 3932  open 'temp-report.Rmd'
 3933  open temp-report.pdf
 3934  python -m 'import pandas as pd; pd.__version__'
 3935  python -m 'import pandas as pd;'
 3936  git clone https://github.build.ge.com/IndustrialDataScience/SchindlerMSK.git
 3937  Rscript -e  ".libPaths(c( .libPaths(), '/Library/Frameworks/R.framework/Resources/library')); .libPaths(); rmarkdown::render('SB_template.Rmd')"
 3938  echo $LD_LIBRARY_PATH
 3939  set
 3940  which Rscript
 3941  cd Reporting_Toolkit
 3942  cd ../Reporting_Toolkit
 3943  Rscript -e  "installed.packages()"
 3944  vim store_packages.R
 3945  Rscript -e  store_packages.R ./store_packages.rda
 3946  Rscript -e  "store_packages.R" ./store_packages.rda
 3947  git clone https://github.build.ge.com/212339410/ReportingTK.git
 3948  cd ReportingT
 3949  cd ReportingTK
 3950  Rscript -e  ".libPaths(c( .libPaths(), '/Library/Frameworks/R.framework/Resources/library')); .libPaths(); rmarkdown::render('output/temp-report.Rmd')"; open output/temp-report.pdf
 3951  Rscript -e  ".libPaths(c( .libPaths(), '/Library/Frameworks/R.framework/Resources/library')); .libPaths(); rmarkdown::render('output/temp-report.Rmd')"
 3952  cd ../LazyReport
 3953  python gen_rmarkdown.py 
 3954  python lazyreport/gen_rmarkdown.py 
 3955  python lazyreport/gen_rmarkdown.py -h
 3956  python lazyreport/gen_rmarkdown.py -o ./chapters
 3957  python lazyreport/gen_rmarkdown.py -o /chapters/parents_report.rmd
 3958  python lazyreport/gen_rmarkdown.py -o ./chapters/parents_report.rmd
 3959  python lazyreport/gen_rmarkdown.py -o /chapters/parents_report.rmd -h
 3960  git push origin add_unittest
 3961  kl
 3962  cd ../Box/Development/Predix
 3963  git clone https://github.com/PredixDev/predix-webapp-starter.git
 3964  cd predix-
 3965  cd predix-webapp-starter
 3966  conda3
 3967  pip install dagobah
 3968  python -v
 3969  locate dagobahd.yml
 3970  ls ~/.dagobahd.yml
 3971  cat ~/.dagobahd.yml
 3972  vim /Users/212339410/dagobah/daemon/dagobahd.yml
 3973  sudo vim /Users/212339410/dagobah/daemon/dagobahd.yml
 3974  mkdir dagobah
 3975  cd dagobah
 3976  mkdir daemon
 3977  cd daemon
 3978  vim dagobahd.yml
 3979  dagobahd
 3980  man ssh-keygen
 3981  conda config --set ssl_verify false
 3982  conda create -n sch27 python=2.7 anaconda
 3983  cd Box/Analytics/R
 3984  git clone https://github.com/rich-iannone/pointblank.git
 3985  Rscript -e
 3986  R CMD -e 'devtools::install_github("rich-iannone/pointblank")'
 3987  R CMD 'devtools::install_github("rich-iannone/pointblank")'
 3988  Rscript -h
 3989  Rscript 
 3990  Rscript  'devtools::install_github("rich-iannone/pointblank")'
 3991  R CMD INSTALL devtools::install_github("rich-iannone/pointblank")
 3992  which bower
 3993  gulp -help
 3994  gulp compile:index
 3995  gulp --tasks
 3996  gulp compile:sass
 3997  gulp --task
 3998  conda --envs
 3999  conda -env
 4000  conda versions
 4001  enda envs
 4002  open lazyReport
 4003  open lazyreport.log
 4004  pip install stable-req.txt
 4005  cd ../Projects/LazyReport
 4006  python lazyreport/gen_rmarkdown.py -i ./lazyreport/test/data/test_report_config_all_brank_heading_level3.xlsx --child
 4007  git clone https://github.build.ge.com/IndustrialDataScience/SchindlerMSK.git SchindlerMSK.ghpages
 4008  l -al
 4009  echo 'doc/build/html' >> .gitignore
 4010  pip install sphinxcontrib-napoleon
 4011  pip install sphinx_rtd_theme
 4012  git remote add origin git git@github.build.ge.com:IndustrialDataScience/SchindlerMSK.git
 4013  git commit -a -m 'initial commit for gh-pages'
 4014  touch .nojekyll
 4015  git commit -a -m 'add .nojekyll
 4016  git commit -a -m 'add .nojekyll'
 4017  sphinx-apidoc -h
 4018  python lazyreport/gen_rmarkdown.py
 4019  jupyter notebook --NotebookApp.iopub_data_rate_limit=10000000 
 4020  cp FD_MSK006_v2.ipynb
 4021  cp FD_MSK006_v2.ipynb FD_MSK006_v2_hiro.ipynb
 4022  cat stable-req.txt | grep sklearn
 4023  cat stable-req.txt
 4024  pip install -r stable-req.txt
 4025  conda install --yes --file stable-req.txt
 4026  pip install -r stable-req.txt --no-index 
 4027  brew install unixodbc
 4028  brew install freetds
 4029  brew upgrade freetds
 4030  pip install --upgrade -r stable-req.txt
 4031  vim stable-req.txt
 4032  cd dss_msk_dev
 4033  pip install scikit-learn==0.18.1
 4034  python -m 'import sklearn; sklearn.__version__'
 4035  which python 
 4036  nh
 4037  ls &
 4038  bg
 4039  which jupyter
 4040  less .gitignore
 4041  Last login: Fri May 19 08:39:53 on console
 4042  (py35) 212339410@SFO1212339410M:~% pip install matplotlib -U
 4043  Retrying (Retry(total=4, connect=None, read=None, redirect=None)) after connection broken by 'NewConnectionError('<pip._vendor.requests.packages.urllib3.connection.VerifiedHTTPSConnection object at 0x1062d0c18>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known',)': /simple/matplotlib/
 4044  (py35) 212339410@SFO1212339410M:~% proxy
 4045  pip install matplotlib -U
 4046  python optmodel/optmodel.py
 4047  cd Projects/LazyReport
 4048  python optmodel.py -f ~/Box/DSS\ Internal-Softbank/40\ Intermediate\ Results/20170502_Feature/0502_Xy_set_final/xy_train_nok_0106_0327_lbh14_lah1_ival14_t1_f10.csv.gz  -t Prob_Urgent -c optmodel.ini -o './results_sb' -S
 4049  git commit -m 'resolved merge conflict'
 4050  cd Op
 4051  python optmodel.py -f ../data/boston.csv 
 4052  python optmodel.py -f ../data/boston.csv -t target -o ./results/demo_result_0519.csv
 4053  python optmodel.py -f ../data/boston.csv -t target -o ./results/
 4054  cd ~/Python/tstk
 4055  git checkout -b jenkinsfile-test
 4056  git checkout jenkinsfile-test
 4057  git remote --help
 4058  git remote set-url origin git@github.build.ge.com:212339410/tstk.git
 4059  git push origin jenkinsfile-test
 4060  cd Python/tstk
 4061  gs
 4062  python -m unittest tstk/tests/test_detect_ts.py
 4063  python lazyreport/gen_rmarkdown.py -i ./lazyreport/test/data/test_report_config_all_brank_heading_level3.xlsx
 4064  sphinx-apidoc -o ./doc/source ./dss_msk
 4065  sphinx-build ./doc/source ./doc/build
 4066  cd doc/build/
 4067  sphinx-apidoc -f -o source ../dss_msk/
 4068  cd source/ht
 4069  cat ../../../.gitignore
 4070  git commit -m 'doc update'
 4071  csvcut ~/Python/OptModel/data/boston.csv
 4072  csvcut -n ~/Python/OptModel/data/boston.csv
 4073  csvcut -C 'target' ~/Python/OptModel/data/boston.csv | csvlook
 4074  csvcut -c 'target' ~/Python/OptModel/data/boston.csv | csvlook
 4075  python optmodel/optmodel.py -f data/CombinedFeature_DoorTypeBinaryOnlyShort.csv 
 4076  python optmodel/optmodel.py -f data/CombinedFeature_DoorTypeBinaryOnlyShort.csv  -t label -i
 4077  python optmodel/optmodel.py -f data/boston.csv  -t label -c ./optmodel/optmodel.ini -o ./optmodel/results
 4078  csvlook ./data/boston.csv
 4079  csvcut -n ./data/CombinedFeature_DoorTypeBinaryOnlyShort.csv
 4080  csvcut -C 'label' ./data/CombinedFeature_DoorTypeBinaryOnlyShort.csv
 4081  csvcut -c 'label' ./data/CombinedFeature_DoorTypeBinaryOnlyShort.csv
 4082  csvcut -c 'label' ./data/CombinedFeature_DoorTypeBinaryOnlyShort.csv | csvlook
 4083  python optmodel/optmodel.py -f data/boston.csv  -t target -c ./optmodel/optmodel.ini -o ./optmodel/results
 4084  csvlook ./data/CombinedFeature_DoorTypeBinaryOnlyShort.csv
 4085  vim ./data/CombinedFeature_DoorTypeBinaryOnlyShort.csv
 4086  python optmodel/optmodel.py -f data/CombinedFeature_DoorTypeBinaryOnlyShort.csv  -t label -c ./optmodel/optmodel.ini -o ./optmodel/results
 4087  pip install matplotlib==1.4.3
 4088  cd doc/source
 4089  cd ../Generic
 4090  cd test_script
 4091  python command_pandas_ml.py -h
 4092  csvlook ../data/boston.csv
 4093  csvlook -n ../data/boston.csv
 4094  csvcut -n ../data/boston.csv
 4095  csvcut -r ../data/boston.csv
 4096  csvcut -b ../data/boston.csv
 4097  csvcut  ../data/boston.csv
 4098  csvcut  ../data/boston.csv | python command_pandas_ml.py --train 
 4099  csvcut  ../data/boston.csv | python command_pandas_ml.py --train  -t target 
 4100  csvcut  ../data/boston.csv | python command_pandas_ml.py --train  -t target | python command_pandas_ml.py --predict --test_data ../data/boston.csv
 4101  csvcut -C target ../data/boston.csv > ../data/boston_test.csv
 4102  csvcut  ../data/boston.csv | python command_pandas_ml.py --train  -t target | python command_pandas_ml.py --predict --test_data ../data/boston_test.csv
 4103  csvcut  ../data/boston.csv | python command_pandas_ml.py --train  -t target | python command_pandas_ml.py --predict --test_data ../data/boston_test.csv | python command_pandas_ml.py predicted_count
 4104  csvcut  ../data/boston.csv | python command_pandas_ml.py --train  -t target | python command_pandas_ml.py --predict --test_data ../data/boston_test.csv | 
 4105  csvcut  ../data/boston.csv | python command_pandas_ml.py --train  -t target | python command_pandas_ml.py --predict --test_data ../data/boston_test.csv 
 4106  csvcut  ../data/boston.csv | python command_pandas_ml.py --train  -t target | python command_pandas_ml.py --predict --test_data ../data/boston_test.csv | python command_pandas_ml.py --predicted_count
 4107  cf login -a https://api.system.asv-pr.ice.predix.io
 4108  cf s
 4109  cf delete-service uaatest-1
 4110  cf delete-service uaa-personal
 4111  cf -h 
 4112  cf -h  | grep unbind
 4113  cf unbind-service 
 4114  cf unbind-service predixseed
 4115  cf unbind-service predixseedvcf apps
 4116  cf app anomaly-detection-api
 4117  cf app test-0926-predix-nodejs-starter
 4118  cf app 
 4119  cf app dashboard_v1.R
 4120  cf env anomaly-detection-api
 4121  cf env test-0926-predix-nodejs-starter
 4122  cf unbind-service test-0926-predix-nodejs-starter
 4123  cf unbind-service test-0926-predix-nodejs-starter predix-uaa
 4124  cf envs anomaly-detection-api
 4125  cf delete app test-0926-predix-nodejs-starter
 4126  cf delete test-0926-predix-nodejs-starter
 4127  cf delete dashboard_v1.R
 4128  cf delete start_shiny_app.R
 4129  cf delete-service 
 4130  cf delete-service predixseed
 4131  git h
 4132  git add ../../../dss_msk/algorithms/__init__.py
 4133  git add ../../../dss_msk/models/__init__.py
 4134  git add ../../../dss_msk/preprocessing/__init__.py
 4135  git add ../../../dss_msk/tests/__init__.py
 4136  git commit -m 'add __init__.py for documentation'
 4137  git add ../../../dss_msk/algorithms/robust_distance.py
 4138  git add ../../../dss_msk/algorithms/steady_state.py
 4139  git add ../../../dss_msk/algorithms/sub_event_counter.py
 4140  git commit -m 'update copyright string to comment out using #'
 4141  git push master 
 4142  cd source
 4143  cd ../build
 4144  git checkout -b gh-pages
 4145  git -a -m 'update documentation'
 4146  git commit -a -m 'update documentation'
 4147  git remot eadd origin git@github.build.ge.com:IndustrialDataScience/SchindlerMSK.git
 4148  git remote add origin git@github.build.ge.com:IndustrialDataScience/SchindlerMSK.git
 4149  cd ~/Python/OptModel
 4150  python optmodel/optmodel.py -h
 4151  cd ~/Projects/SchindlerMSK/doc/build/html
 4152  git push -u origin gh-pages
 4153  git pull gh-pages origin
 4154  git add searchindex.js
 4155  cd doc/build
 4156  git commit -m 'update DOC'
 4157  git pull gh-pages gh-pages
 4158  git pull origin gh-pages
 4159  git git pull origin gh-pages --allow-unrelated-histories
 4160  git pull origin gh-pages --alow-unrelated-histories
 4161  git pull origin gh-pages --allow-unrelated-histories
 4162  rm .buildinfo
 4163  rm .git
 4164  rm -rf .git 
 4165  git clone git@github.build.ge.com:IndustrialDataScience/SchindlerMSK.git
 4166  cat CombinedFeature_DoorTypeBinaryOnlyShort.csv
 4167  cat CombinedFeature_DoorTypeBinaryOnlyShort1.csv
 4168  head -n 5 CombinedFeature_DoorTypeBinaryOnlyShort.csv | csvlook
 4169  mv CombinedFeature_DoorTypeBinaryOnlyShort1.csv CombinedFeature_DoorTypeMultiClassOnlyShort.csv
 4170  rm -rf SchindlerMSK
 4171  python lazyreport/gen_rmarkdown.py -i config/report_config.xlsx
 4172  python optmodel/optmodel.py --demo
 4173  python optmodel.py --demo
 4174  python optmodel.py --demo -S
 4175  git add ../optmodel/optmodel.ini ../optmodel/optmodel.pyl
 4176  cd optmodel
 4177  mkdir results
 4178  python optmodel.py -f ../data/CombinedFeature_DoorTypeMultiClassOnlyShort.csv -o results -S -t label
 4179  python optmodel.py -f ../data/CombinedFeature_DoorTypeMultiClassOnlyShort.csv -o results -S -t label -c ./optmodel_demo.ini
 4180  python optmodel.py -f ../data/CombinedFeature_DoorTypeMultiClassOnlyShort.csv -o results -S -t label -c ./optmodel_demo.ini -s accuracy
 4181  python optmodel.py -f ../data/boston_cat_input.csv -o results0523 -S -t label -c ./optmodel_demo.ini -s accuracy
 4182  python optmodel.py -f ../data/boston_cat_input.csv -o results0523 -S -t target -c ./optmodel_demo.ini -s accuracy
 4183  python optmodel.py -h
 4184  cd  ../data
 4185  python optmodel/optmodel.py --C target, target1, b, c
 4186  python optmodel/optmodel.py -C 'target, target1, b, c'
 4187  cd ./build/html
 4188  git clone git@github.build.ge.com:IndustrialDataScience/SchindlerMSK.git SchndlerMSK.gh-pages
 4189  cd SchndlerMSK.gh-pages
 4190  git checkout gh-pages
 4191  git commit -m 'update doc'
 4192  cd build/html
 4193  git clone git@github.build.ge.com:212339410/Test-Docs.git
 4194  cd Test-Docs
 4195  mkdir docs
 4196  pip install --upgrade
 4197  ase also report this if it was a user error, so that a better error message can be provided next time.
 4198  A bug report can be filed in the tracker at <https://github.com/sphinx-doc/sphinx/issues\>. Thanks!
 4199  make: *** [html] Error 1
 4200  (sch27) 212339410@SFO1212339410M:~l
 4201  cd ../../SchindlerMSK.gh-pages
 4202  git commit -m 'DOC update copy right'
 4203  cd SchindlerMSK/doc/source/
 4204  cd html
 4205  cp -a ./ ../../../../SchindlerMSK.gh-pages
 4206  cp -a ./. ../../../../SchindlerMSK.gh-pages
 4207  git log master -1 --pretty=short --abbrev-commit
 4208  l | grep Schindler
 4209  git add development/Notes/how_to_update_documents_with_sphinx.md
 4210  git commit -m 'DOC add development/Notes/how_to_update_documents_with_sphinx.md'
 4211  ssh -i /Users/212339410/Box/GE Digital Data Science 2016/10 Schindler Personal/AWS/DataScience.pem ubuntu@ec2-54-214-160-173.us-west-2.compute.amazonaws.com
 4212  ssh -i '/Users/212339410/Box/GE Digital Data Science 2016/10 Schindler Personal/AWS/DataScience.pem' ubuntu@ec2-54-214-160-173.us-west-2.compute.amazonaws.com
 4213  cd doc/
 4214  git commit -m 'DOC update'
 4215  git push origin gh-pages
 4216  git commit -m 'DOC add the link to how to update documentation'
 4217  cd ~/Projects/SchindlerMSK
 4218  cd LazyReport
 4219  cd ../SchindlerMSK.gh-pages
 4220  cd ../tstk
 4221  pip install PIL
 4222  docker ps
 4223  host
 4224  docker-machine ls
 4225  chkconfig
 4226  service
 4227  docker pull centos
 4228  docker pull centos:7
 4229  docker run -it centos:7 /bin/bash
 4230  locate pem
 4231  locate .pem
 4232  locate DataScience.pem
 4233  mv ~/Box/GE\ Digital\ Data\ Science\ 2016/10\ Schindler\ Personal/AWS/DataScience.pem ~/.ssh/
 4234  ssh -i ~/.ssh/DataScience.pem ubuntu@ec2-54-214-160-173.us-west-2.compute.amazonaws.com -L 8400:10.43.51.205:22
 4235  ssh -i ~/.ssh/DataScience.pem ubuntu@ip-10-43-51-90.us-west-2.compute.internal -L 8400:10.43.51.205:22
 4236  ssh -i ~/.ssh/DataScience.pem ubuntu@10.43.51.90 -L 8400:10.43.51.205:22
 4237  nxi_bastion
 4238  chmod 400 DataScience.pem
 4239  ssh -i ./DataScience.pem ubuntu@ec2-54-214-160-173.us-west-2.compute.amazonaws.com
 4240  ssh sch_hiro
 4241  vim config
 4242  nslookup 10.43.51.205
 4243  source activate sch
 4244  pip install --upgrade awscli
 4245  pip install --upgrade --user awscli
 4246  echo sch27
 4247  echo `sch27`
 4248  which aws
 4249  pip install awscli
 4250  brew install awsclie
 4251  brew install awscli
 4252  aws --version
 4253  aws s3
 4254  aws ec2
 4255  aws ec2 run-instances
 4256  defaults write com.apple.
 4257  defaults write com.apple.screencapture location /Users/212339410/Box/Analytics/Notes/Figures
 4258  killall SystemUIServer
 4259  cat ~/.local/bin
 4260  ssh -i 'DataScience.pem' ubuntu@ec2-54-214-160-173.us-west-2.compute.amazonaws.com -L 8400:10.43.51.205:22
 4261  cd ~/.ssh
 4262  brew install sshfs
 4263  osxfufe -v
 4264  osxfuse -v
 4265  fuse
 4266  cat co
 4267  cat config
 4268  sshfs ec2-54-214-160-173.us-west-2.compute.amazonaws.com
 4269  mkdir remote_mount
 4270  cd remote_mount
 4271  ubunt@sshfs ec2-54-214-160-173.us-west-2.compute.amazonaws.com /home/ubuntu ./
 4272  sshfs ubuntu@ec2-54-214-160-173.us-west-2.compute.amazonaws.com /home/ubuntu ./
 4273  sshfs ubuntu@ec2-54-214-160-173.us-west-2.compute.amazonaws.com: /home/ubuntu ./
 4274  sshfs ubuntu@ec2-54-214-160-173.us-west-2.compute.amazonaws.com: / ./
 4275  sshfs ubuntu@ec2-54-214-160-173.us-west-2.compute.amazonaws.com: / remote_mount
 4276  sshfs ubuntu@ec2-54-214-160-173.us-west-2.compute.amazonaws.com: / ~/remote_mount
 4277  sshfs ubuntu@ec2-54-214-160-173.us-west-2.compute.amazonaws.com: / ~/remote_mount/
 4278  sshfs ubuntu@ec2-54-214-160-173.us-west-2.compute.amazonaws.com: / 
 4279  sshfs ubuntu@ec2-54-214-160-173.us-west-2.compute.amazonaws.com: / -o IdentityFile=~/.ssh/DataScience.pem 
 4280  sshfs ubuntu@ec2-54-214-160-173.us-west-2.compute.amazonaws.com: / -o IdentityFile=~/.ssh/DataScience.pem
 4281  sshfs ubuntu@ec2-54-214-160-173.us-west-2.compute.amazonaws.com: /home/ubuntu -o IdentityFile=~/.ssh/DataScience.pem
 4282  sshfs ubuntu@ec2-54-214-160-173.us-west-2.compute.amazonaws.com: /home/ubuntu/ -o IdentityFile=~/.ssh/DataScience.pem
 4283  sshfs ubuntu@ec2-54-214-160-173.us-west-2.compute.amazonaws.com:/home/ubuntu/ -o IdentityFile=~/.ssh/DataScience.pem
 4284  sshfs ubuntu@ec2-54-214-160-173.us-west-2.compute.amazonaws.com:/home/ubuntu/ ~/remote_mount -o IdentityFile=~/.ssh/DataScience.pem
 4285  mkdir remote_centos
 4286  sshfs centos@localhost: /home/centos/ ~/remote_cnetos
 4287  sshfs centos@localhost:/home/centos/ ~/remote_cnetos
 4288  sshfs centos@localhost:/home/centos/ ~/remote_cnetos -o IdentityFile=~/.ssh/DataScience.pem
 4289  sshfs centos@localhost:/home/centos/ ~/remote_centos -o IdentityFile=~/.ssh/DataScience.pem
 4290  nslookup 10.43.51.90
 4291  sudo ssh -f centos@10.43.51.90 -L 22:ec2-54-214-160-173.us-west-2.compute.amazonaws.com:22 -N
 4292  su -
 4293  su - 
 4294  su
 4295  sudo ls
 4296  sudo 
 4297  sudo ssh sch_bastion
 4298  ssh -f centos@10.43.51.90 -L 22:ec2-54-214-160-173.us-west-2.compute.amazonaws.com:22 -N 
 4299  sudo ssh -f centos@10.43.51.90 -L 22:ec2-54-214-160-173.us-west-2.compute.amazonaws.com:22 -N 
 4300  ssh -i 'DataScience.pem' ubuntu@ec2-54-214-160-173.us-west-2.compute.amazonaws.com -L 8400:10.43.51.239:22
 4301  ssh -i '~/.ssh/DataScience.pem' ubuntu@ec2-54-214-160-173.us-west-2.compute.amazonaws.com -L 8400:10.43.51.239:22
 4302  ssh -i ~/.ssh/DataScience.pem ubuntu@ec2-54-214-160-173.us-west-2.compute.amazonaws.com -L 22:10.43.51.239:22
 4303  ssh -i ~/.ssh/DataScience.pem ubuntu@ec2-54-214-160-173.us-west-2.compute.amazonaws.com -L 8400:10.43.51.239:22
 4304  sshfs -p 8400 centos@10.43.51.239:/home/centos 
 4305  sshfs -p 8400 centos@10.43.51.239:/home/centos ~/remote_centos
 4306  cd Box/GE\ Digital\ Data\ Science\ 2016
 4307  cd docker
 4308  docker run 
 4309  docker -d
 4310  service docker start
 4311  osboot2docker up
 4312  boot2docker up
 4313  open -a docker
 4314  docker image
 4315  docker 
 4316  docker images
 4317  cd ~/Projects/LazyReport
 4318  git add doc/*
 4319  vim doc/source/index.rst
 4320  git add doc/source/index.rst
 4321  git commit -m 'add doc/ directory'
 4322  python ]
 4323  ssh sch_bastion -A
 4324  ssh -i 'DataScience.pem' ubuntu@ec2-54-214-160-173.us-west-2.compute.amazonaws.com -L 8400:10.43.51.90:22
 4325  ssh -i '~/.ssh/DataScience.pem' ubuntu@ec2-54-214-160-173.us-west-2.compute.amazonaws.com -L 8400:10.43.51.90:22
 4326  ssh -i ~/.ssh/DataScience.pem ubuntu@ec2-54-214-160-173.us-west-2.compute.amazonaws.com -L 8400:10.43.51.90:22
 4327  vim ~/.zshrc.pre-oh-my-zsh
 4328  ipfoncifg
 4329  cat ~/.ssh/config -a
 4330  ifconfig -all
 4331  scp sch_centos:/home/centos/.vimrc ~/Box/Analytics/Projects/
 4332  vim ~/Box/Analytics/Projects/
 4333  l ~/Box/Analytics/Projects/
 4334  vim ~/Box/Analytics/Projects/.vimrc
 4335  ssh sch_centos -L localhost:8888:localhost:8889 
 4336  ssh sch_centos -N -f -L localhost:8888:localhost:8889 
 4337  kill 41192
 4338  ps aux | grep localhost
 4339  ssh sch_centos -N -L localhost:8888:localhost:8889 
 4340  ssh sch_centos -N -L localhost:8888:localhost:8889 centos:10.43.51.90
 4341  ssh sch_centos -N -L localhost:8888:10.43.51.90:8889 sch_bastion
 4342  ssh  -L localhost:8888:10.43.51.90:8889 ec2-54-214-160-173.us-west-2.compute.amazonaws.com
 4343  ssh  -L localhost:8888:10.43.51.90:8889 ec2-54-214-160-173.us-west-2.compute.amazonaws.com -i ~/.ssh/DataScience.pem
 4344  ssh  -i ~/.ssh/DataScience.pem -L localhost:8888:10.43.51.90:8889 ec2-54-214-160-173.us-west-2.compute.amazonaws.com 
 4345  ssh  -i ~/.ssh/DataScience.pem -L localhost:8888:10.43.51.90:8889 ubuntu@ec2-54-214-160-173.us-west-2.compute.amazonaws.com 
 4346  ssh  -i ~/.ssh/DataScience.pem -L 13389:10.43.51.90:8889 centos@10.43.51.90
 4347  ssh  -i ~/.ssh/DataScience.pem -L localhost:13389:10.43.51.90:8889 centos@10.43.51.90
 4348  ssh sch_centos -N -L localhost:8888:10.43.51.90:88889 
 4349  ssh -i ~/.ssh/DataScience.pem -o 'ProxyCommand ssh sch_bastion -W %h:%p' -N -n -L localhost:8889:10.43.51.9:8889 centos@10.43.51.9
 4350  ssh -i ~/.ssh/DataScience.pem -o 'ProxyCommand ssh sch_bastion -W %h:%p' -N -n -L 127.0.0.1:8889:127.0.0.1:8889 10.43.51.9
 4351  ssh -i ~/.ssh/DataScience.pem -N -n -L 127.0.0.1:8889:127.0.0.1:8889 sch_bastion
 4352  ssh sch_centos -N -L localhost:8888:10.43.51.90:8889 
 4353  crontab -h
 4354  man crontab 
 4355  aws ec2 describe-instance-status 
 4356  aws ec2 describe-instance-status help
 4357  gzip
 4358  gzip -best 
 4359  gzip -best Document_D 
 4360  gzip --best Document_D 
 4361  tar 
 4362  man tar
 4363  tar -czf Document_D.tar.gz Document_D -o gzip:compression-level=9
 4364  cd Box\ Sync\ Backup
 4365  tar -czf Analtytics201612.tar.gz Analytics -o gzip:compression-level=9
 4366  man zip
 4367  rm Analtytics201612.tar.gz
 4368  rm Document_D.tar.gz
 4369  GZIP=-9 tar cvzf Document_D.tar.gz ./Document_D
 4370  rm -rf Document_D
 4371  diff -rq ./Analytics ~/Box/Analytics
 4372  diff  ./Analytics ~/Box/Analytics
 4373  diff -rq ./Analytics/Data ../Box/Analytics/Data
 4374  diff Analytics/Python/ ../Box/Analytics/Python
 4375  diff -qr dirA dirB | grep -v -e 'DS_Store' -e 'Thumbs' | 
 4376  open diffs_analytics.txt
 4377  rm -rf scikit-learn*
 4378  cd Analyticscd ..
 4379  cd Analytics
 4380  diff -qr ./Analytics ../Box/Analytics  | grep -v -e 'DS_Store' -e 'Thumbs' | sort > diffs_analytics.txt
 4381  mkdir Vim
 4382  git clone https://github.com/skilldrick/vim-exercises.git
 4383  cd vim-exercises
 4384  vim vim_ex.txt
 4385  pip install glumpy
 4386  pip install pyopengl
 4387  pip install triangle
 4388  pip install --upgrade cython
 4389  glxinfo
 4391  pip install vispy
 4392  pip install PyQt
 4393  brew install PyQt --with-python3
 4394  pip install flake8
 4395  brew install pyqt
 4396  brew upgrade pyqt
 4397  conda install bokeh
 4398  bokeh sampledata
 4399  pip install utils
 4400  env
 4401  vim crontab
 4402  crontab
 4403  cd anaconda
 4404  cd ../Box/Analytics/Projects
 4405  cd ../Development/Vim
 4406  mkdir crontask
 4407  cd crontask
 4408  cd Vim
 4409  rm crontask
 4410  rm crontask -rf
 4411  rm -rf crontask 
 4412  crontab -u 30 7 * * 1-5 aws ec2 stop-instances --instance-ids i-0b06ff9cce802df7b
 4413  man crontab
 4414  start-instances --instance-ids i-0b06ff9cce802df7b
 4415  ssh -i DataScience.pem  -N -n -L 127.0.0.1:8889:127.0.0.1:8889 centos@10.43.51.90 -Y âjupyter notebook --no-browser --port=8889 â
 4416  ssh -i  ~/.ssh/DataScience.pem  -N -n -L 127.0.0.1:8889:127.0.0.1:8889 sch_centos -Y # @bastion, portforwarding centos to bastion
 4417  sudo service ssh restart
 4418  ssh sch_bastion -v
 4419  grep sshd /etc/hosts.equiv
 4420  iptables -n -L -v
 4421  ifconfig -n
 4422  ifconfig 
 4423  ifconfig -L
 4424  ping 10.43.51.90
 4425  ssh nxi_bastion -vvv
 4426  netstat -anp | grep sshd
 4427  netstat -anp
 4428  netstat 
 4429  netstat  | grep sshd
 4430  sshd
 4431  ls ~/.ssh/
 4432  sudo dscacheutil -flushcache
 4433  ssh sch_bastion -vvv
 4434  ssh sch_bastion 
 4435  ssh -i ~/.ssh/DataScience.pem -N -n -L 127.0.0.1:8889:127.0.0.1:8889 sch_bastion -Y  # @local pc, portforwarding bastion to local 
 4436  :
 4437  * ssh -i ~/.ssh/DataScience.pem -N -n -L 127.0.0.1:8889:127.0.0.1:8889 sch_bastion -Y -o ServerAliveInterval=50  # @local pc, portforwarding bastion to local
 4438  ssh -i ~/.ssh/DataScience.pem -N -n -L 127.0.0.1:8889:127.0.0.1:8889 sch_bastion -Y -o ServerAliveInterval=50  # @local pc, portforwarding bastion to local
 4439  ssh sch_bastion "ssh -i  DataScience.pem  -N -n -L 127.0.0.1:8889:127.0.0.1:8889 centos@10.43.51.90 -Y  -o ServerAliveInterval=50"
 4440  ipython qtconsole
 4441  scp sch_centos:~/.vimrc ~/.vimrc.centos
 4442  vim ~/.vimrc.centos
 4443  cd Box/Development/Vim
 4444  curl https://raw.githubusercontent.com/Shougo/neobundle.vim/master/bin/install.sh > install.sh
 4445  sh ./install.sh
 4446  cp ~/.vimrc.centos ./
 4447  vim ~/.vimrc.swp
 4448  ls ~/
 4449  l ~/.vim*
 4450  mv ~/.vimrc.centos ./
 4451  vim ~/.vim
 4452  vim ~/.vimrc
 4453  aws ec2 stop-instances --instance-ids i-0b06ff9cce802df7b
 4454  calendar
 4455  ssh sch_centos 
 4456  ssh sch_centos -X
 4457  sudo echo "X11Forwarding yes" >> /etc/ssh/sshd_config
 4458  sudo service sshd restart
 4459  sudo launchctl stop com.openssh.sshd
 4460  sudo launchctl start com.openssh.sshd
 4461  ssh sch_centos -X -C
 4462  $PATH
 4463  echo $PATH | grep xauth
 4464  locate auth
 4465  locate xauth
 4466  launchtl unload ssh.plist
 4467  ssh sch_centos -Y -X -C
 4468  ssh sch_centos -Y -V
 4469  mv LazyReport AgileReport
 4470  rm lazyReport agilereport
 4471  mv lazyReport agilereport
 4472  vim /etc/ssh/sshd_config
 4473  sudo vim /etc/ssh/sshd_config
 4474  lanchtl
 4475  cd /System/Library/LaunchDaemons
 4476  launchctl unload ssh.plist
 4477  launchctl load ssh.plist
 4478  sudo launchctl unload  /System/Library/LaunchDaemons/ssh.plist 
 4479  sudo launchctl load -w /System/Library/LaunchDaemons/ssh.plist 
 4480  ssh sch_centos -Y -v
 4481  ssh -X -i ~/.ssh/DataScience.pem centos@10.43.51.90 'ssh sch_bastion -W %h:%p' 
 4482  ssh -X -i ~/.ssh/DataScience.pem centos@10.43.51.90 'ssh sch_bastion -W %h:%p' -v
 4483  ssh -X -i ~/.ssh/DataScience.pem centos@10.43.51.90 -o ProxyCommand='ssh sch_bastion -W %h:%p' -v
 4484  ssh -Y -i ~/.ssh/DataScience.pem centos@10.43.51.90 -o ProxyCommand='ssh sch_bastion -W %h:%p' -v
 4485  ssh sch_bastion -L 2022:10.43.51.90:22 &
 4486  ssh -X -p 2022 locahost
 4487  ssh -X -p 2022 127.0.0.1
 4488  ssh sch_centos -XC
 4489  man ps
 4490  kill 71705
 4491  kill 71704
 4492  ssh -i ~/.ssh/DataScience.pem -N -n -L 127.0.0.1:2022:127.0.0.1:22 sch_bastion -Y -o ServerAliveInterval=50  # @local pc, portforwarding bastion to local 
 4493  ssh -i ~/.ssh/DataScience.pem -N -n -L 127.0.0.1:2023:127.0.0.1:22 sch_bastion -Y -o ServerAliveInterval=50  # @local pc, portforwarding bastion to local 
 4494  ssh sch_bastion -L 2023:10.43.51.90:22 
 4495  ssh -i  ~/.ssh/DataScience.pem  -X -p 2023 localhost
 4496  ssh -i  ~/.ssh/DataScience.pem  -X -p 2023 127.0.0.1
 4497  ssh sch_bastion -L 2024:10.43.51.90:22 
 4498  ssh -i  ~/.ssh/DataScience.pem  -X -p 2024 127.0.0.1
 4499  l ~/.ssh/
 4500  cat ~/.ssh/known_hosts
 4501  ssh -L 127.0.0.1:2022:10.43.51.90 -i ~/.ssh/DataScience.pem centos@10.43.51.90 -o ProxyCommand='ssh sch_bastion -W %h:%p'
 4502  ssh -L 127.0.0.1:2022:10.43.51.90:22 -i ~/.ssh/DataScience.pem centos@10.43.51.90 -o ProxyCommand='ssh sch_bastion -W %h:%p'
 4503  ssh -L 127.0.0.1:2025:10.43.51.90:22 -i ~/.ssh/DataScience.pem centos@10.43.51.90 -o ProxyCommand='ssh sch_bastion -W %h:%p'
 4504  ssh -p 2025 -X 212339410@127.0.0.1
 4505  man ssh
 4506  ssh -p 2025 -X 127.0.0.1
 4507  ssh -Y sch_centos
 4508  ssh -Y sch_centos -v
 4509  xeyes
 4510  ssh -L 8999:10.43.51.90:22 sch_bastion 
 4511  ssh -X -p 9990 localhost
 4512  ssh -X -p 9990 127.0.0.1
 4513  ssh -L 8999:ec2-54-214-160-173.us-west-2.compute.amazonaws.com:22 sch_bastion 
 4514  vim ~/.ssh/known_hosts
 4515  ssh -X -p 8999 127.0.0.1
 4516  ssh -X -p 8999 127.0.0.1 -i ~/.ssh/DataScience.pem
 4517  ssh -i ~/.ssh/DataScience.pem -X -p 8999 127.0.0.1
 4518  ssh -i ~/.ssh/DataScience.pem -L 2022:ec2-54-214-160-173.us-west-2.compute.amazonaws.com:2022 ubuntu@ec2-54-214-160-173.us-west-2.compute.amazonaws.com
 4519  kill 74040
 4520  kill 84039
 4521  kill 74039
 4522  kill 74407
 4523  kill 74574
 4524  ssh -L 8999:ec2-54-214-160-173.us-west-2.compute.amazonaws.com:22 -i ~/.ssh/DataScience.pem ubuntu@ec2-54-214-160-173.us-west-2.compute.amazonaws.com
 4525  aws ec2 describe-instance-status      --instance-ids i-0b06ff9cce802df7b
 4526  aws configure --profile user2
 4527  asw s3 ls
 4528  echo $SHELL
 4529  complete -C '/user/local/bin/aws_completer' aws
 4530  which aws_completer
 4531  complete -C `which aws_completer` aws
 4532  complete
 4533  source /usr/local/bin/aws_zsh_completer.sh
 4534  aws ec2 help
 4535  aws ec2 describe-instance help
 4536  ping 4.2.2.2
 4537  curl http://www.google.com
 4538  sshfs sch_bastion
 4539  ecronta -l
 4540  ssh sch_centos -v
 4541  xclock
 4542  ssh sch_centos -Y
 4543  sshfs sch_centos:/home/centos/ /Users/212339410/remote_centos
 4544  cd Mount/remote_centos
 4545  git
 4546  mkdir test-repo
 4547  cd test-repo
 4548  nslookup 
 4549  nslookup ec2-54-214-160-173.us-west-2.compute.amazonaws.com
 4550  ssh sch_bastion -Y
 4551  ssh sch_centos
 4552  iwlist wlan0 scan
 4553  networksetup
 4554  networksetup -setairportnetwork en0 Internet 
 4555  ssh -i ~/.ssh/DataScience.pem -o ProxyCommand='ssh sch_bation'
 4556  ssh -i ~/.ssh/DataScience.pem -o ProxyCommand='ssh sch_bation' centos@10.43.72.230
 4557  ssh -i ~/.ssh/DataScience.pem -o ProxyCommand='ssh sch_bastion' centos@10.43.72.230
 4558  scp '/Users/212339410/Box/GE Digital Data Science 2016/10 Schindler Personal/docker/pds-msk-analytics-base-image.tar' sch_docker:
 4559  ssh sch_docker -vvv
 4560  scp -i ~/.ssh/DataScience.pem -o ProxyCommand='ssh sch_bastion -W %h:%p' '/Users/212339410/Box/GE Digital Data Science 2016/10 Schindler Personal/docker/pds-msk-analytics-base-image.tar' Centos@10.43.72.230:/home/centos/ 
 4561  scp -i ~/.ssh/DataScience.pem -o ProxyCommand='ssh sch_bastion -W %h:%p' '/Users/212339410/Box/GE Digital Data Science 2016/10 Schindler Personal/docker/pds-msk-analytics-base-image.tar' centos@10.43.72.230:/home/centos/ 
 4562  ssh sch_docker -v
 4563  cat ~/.ssh/known_hosts.old
 4564  locate deny.
 4565  ssh sch_docker
 4566  git clone git@github.build.ge.com:IndustrialDataScience/IndustrialImageAnalyticsToolkit.git
 4567  cd IndustrialImageAnalyticsToolkit
 4568  cd iitk
 4569  brew install tesseract
 4570  python ocrtables.py
 4571  ssh -i ~/.ssh/DataScience.pem -o ProxyCommand='ssh sch_bastion -W %h:%p' centos@10.43.72.230 
 4572  ssh sch_centos_hiro 'cat ~/.vimrc'
 4573  ssh sch_bastion âssh -i  DataScience.pem  -N -n -L 127.0.0.1:8889:127.0.0.1:8889 centos@10.43.51.90 -Y  -o ServerAliveInterval=50â
 4574  ssh sch_centos_hiro -X 'jupyter notebook'
 4575  ssh sch_centos_hiro -Y 'jupyter notebook'
 4576  scp sch_centos_siva:aws_run_fd_msk.py sch_centos_hiro:hiro/
 4577  ssh sch_centos_
 4578  ls ~/Library/Application\ Support/iTerm
 4579  vim ~/Library/Application\ Support/iTerm/version.txt
 4580  scp sch_centos_siva:aws_run_fd_msk.py Projects/SchindlerMSK/development/Pythonh
 4581  scp sch_centos_siva:aws_run_fd_msk.py sch_centos_hiro:Siva/
 4582  aws ec2 start-instances --instance-ids i-0b06ff9cce802df7b
 4583  man df
 4584  autofsd
 4585  locate mnt
 4586  locate mnt | mnt/
 4587  locate mnt | grep mnt/
 4588  locate mnt/
 4589  locate /mnt/
 4590  sudo mkdir /Volumes/sch_centos_hiro
 4591  sudo vim /etc/fstab.hd
 4592  locate fstab
 4593  ls /etc/fstab
 4594  vim /etc/fstab
 4595  sshfs#sch_centos_hiro:/ /mnt/sch_centos_hiro
 4596  sshfs sch_centos_hiro:/ /mnt/sch_centos_hiro
 4597  sshfs sch_centos_hiro:/ /Volumes/sch_centos_hiro
 4598  sudo sshfs sch_centos_hiro:/ /Volumes/sch_centos_hiro
 4599  sudo sshfs sch_centos_hiro /Volumes/sch_centos_hiro
 4600  sudo sshfs sch_centos_hiro: /Volumes/sch_centos_hiro
 4601  sshfs sch_centos:/home/centos/ /Volumes/sch_centos_hiro
 4602  sudo sshfs sch_centos:/home/centos/ /Volumes/sch_centos_hiro
 4603  sshfs sch_centos:/home/centos/ /Users/212339410/Mount/remote_centos
 4604  sudo sshfs sch_centos_hiro:/home/centos/ /Volumes/sch_centos_hiro
 4605  cd SchindlerEdgeAnalytics/
 4606  cp ../SchindlerMSK/dss_msk/ .
 4607  cp ../SchindlerMSK/dss_msk/ . -a
 4608  cp -a ../SchindlerMSK/dss_msk/ .
 4609  mkdir doc
 4610  rm main.pyc
 4611  cat .gitignore
 4612  /pyc
 4613  cat README_v011.md
 4614  mkdir seanalytics
 4615  cp -a algorithms seanalytics
 4616  mv -r features seanalytics/
 4617  man mv
 4618  rm -rf algorithms 
 4619  mv *.py seanalytics/
 4620  mv config_files seanalytics
 4621  mv models features seanalytics
 4622  mv preprocessing tests use_cases seanalytics
 4623  mv README_v011.md doc/realse_notesv011.md
 4624  vim use_case1.py
 4625  vim __init__.py
 4626  git add doc/
 4627  git add seanalytics/algorithms/
 4628  mkdir scripts
 4629  cd models
 4630  rm models
 4631  rm -rf models
 4632  rm -rf use_cases
 4633  rm main.py
 4634  cd config_files
 4635  vim DMDM_parameters_config.ini
 4636  cat config.ini
 4637  cat no_event_config.ini
 4638  vim dss_msk/main.py
 4639  vim __main__.py
 4640  cp ../SchindlerMSK/dss_msk
 4641  cp ../SchindlerMSK/dss_msk/__main__.py ./seanalytics/ 
 4642  cp ../SchindlerMSK/dss_msk/main.py ./seanalytics/ 
 4643  cp ../SchindlerMSK/dss_msk/__init__.py ./seanalytics/ 
 4644  mv tests ../
 4645  git -rf rm .
 4646  git -r  .
 4647  git rm --cached seanalytics/algorithms/DMDM_parameters_config.ini
 4648  git rm --cached seanalytics/algorithms/MSK006_results.csv
 4649  git rm --cached seanalytics/algorithms/align_old.py
 4650  git rm --cached seanalytics/features/features_README.md
 4651  mv seanalytics/features/features_README.md ./doc
 4652  cd doc
 4653  git rm --cached seanalytics/algorithms/config.ini
 4654  git rm --cached seanalytics/algorithms/no_event_config.ini
 4655  git rm --cached seanalytics/algorithms/*.ini
 4656  cd Projects/SchindlerA
 4657  git rm --cached seanalytics/algorithms/event_detection2.py
 4658  git rm --cached seanalytics/algorithms/no_event2.py
 4659  vim tests/test_README.md
 4660  git rm --cached tests/test_README.md
 4661  git commit -m 'initial setup'
 4662  git commit -m 'DOC modified reference in README.md'
 4663  gid add doc/features_README.md
 4664  git add doc/features_README.md
 4665  git commit -m 'add feature documents'
 4666  git clone git@github.build.ge.com:212339410/SchindlerEdgeAnalytics.git seanalytics-forked
 4667  mv test.txt doc
 4668  git commit -m 'test commit'
 4669  git pull --all
 4670  cd ../SchindlerEdgeAnalytics
 4671  git commit -m 'update README.md'
 4672  ssh -i ~/.ssh/DataScience.pem -N -n -L 127.0.0.1:8889:127.0.0.1:8889 sch_bastion -Y -o ServerAliveInterval=50 ssh sch_bastion 'ssh -i  DataScience.pem  -N -n -L 127.0.0.1:8889:127.0.0.1:8889 centos@10.43.51.90 -Y  -o ServerAliveInterval=50'
 4673  kill 37856
 4674  lsof -n -i4TCP:$PORT | grep LISTEN # Verified on macOS Sierra
 4675  lsof -n -iTCP:$PORT | grep LISTEN
 4676  netstat -p tcp | grep $POR
 4677  netstat -p tcp | grep $PORT
 4678  netstat -p tcp | grep 8889
 4679  netstat -p tcp
 4680  sphinx-quickstart -h
 4681  ssh  -N -n -L 127.0.0.1:8889:127.0.0.1:8889 sch_bastion -Y -o ServerAliveInterval=50
 4682  ssh  -N -n -L 127.0.0.1:8889:127.0.0.1:8889 sch_bastion -Y 
 4683  ssh  -Y -N -n -L 127.0.0.1:8889:127.0.0.1:8889 sch_bastion 
 4684  sudo chmod 600 /etc/auto_centos_hiro
 4685  pws
 4686  sshfs sch_centos_hiro:/home/centos/ /Users/212339410/Mount/remote_centos
 4687  man sshfs 
 4688  sshfs sch_centos_siva:/home/centos/ /Users/212339410/Mount/remote_centos_siva
 4689  ssh  -YNn -L 127.0.0.1:8889:127.0.0.1:8889 sch_bastion 
 4690  bokeh serve
 4691  pip install --upgrade bokeh
 4692  pip install bokeh 
 4693  mkdir bokeh_test
 4694  cd bo
 4695  cd bokeh_test
 4696  vim  download_sample_data.py
 4697  python download_sample_data.py
 4698  cd daily
 4699  ''' Create a simple stocks correlation dashboard.\nChoose stocks to compare in the drop down widgets, and make selections\non the plots to update the summary and histograms accordingly.\n.. note::\n    Running this example requires downloading sample data. See\n    the included `README`_ for more information.\nUse the ``bokeh serve`` command to run the example by executing:\n    bokeh serve stocks\nat your command prompt. Then navigate to the URL\n    http://localhost:5006/stocks\n.. _README: https://github.com/bokeh/bokeh/blob/master/examples/app/stocks/README.md\n'''
 4700  try:
 4701  except ImportError:
 4702  from os.path import dirname, join
 4703  from bokeh.io import curdoc
 4704  from bokeh.layouts import row, column
 4705  from bokeh.models import ColumnDataSource
 4706  from bokeh.models.widgets import PreText, Select
 4707  from bokeh.plotting import figure
 4708  Use the ``bo =Use the ``bokeh serve`` command to run the examni    bokeh serve stocks
 4709  at your command prompt. Then navigate che(at your command prompke    http://localhost:5006/stocks\n.. _README: ht t.. _README: https://github.com/ad'''\ntry:\n    from functools import lru_cache\nexcept ImportError:\n    # Python 2 doe,   oo   'o    # Python 2 doe,   oo   'o    # d    # Python 2 doee'    # Python 2 doe,   oo   'o    # Python 2 doe,   oo   'o    #at    # Python 2 doe,   oo   'o    # Python 2 doe:\n    # Python 2 doe,   oo   'o    # Python 2 doe,   oo   'o    # d    # Pythf1    # Python 2 doe, d    # Python 2 doe)\n    # Python 2 doe,   oo   'o  at    # Python 2 doe,   oo   'o  at    ns    # Python 2 doe,rn    # Python 2 doret    # Python 2 doe,   oo   'o  a re    # Python 2 doep w    # Python 2 doe,   oo  xt    # Python 2 doe,   oo   'o  at   ue    # Python 2 doe,   oo   'o  at    # PKE    # Python 2 doe,   oo   'o  at    # Python 2('    # Python 2 doe,   oo   'o  aup    # Python 2 doe,   oo   'o  at    # Python 2e=    # Python 2 doe,   oo   'o  at    # Python 2 doe,   oo   'o  at    ns    # Pythonta=dict(date=[], t1=[], t2=[], t1_returns=[], t2_returns=[]))
 4710  tools = 'pan,wheel_zoom,xbox_select,reset'
 4711  corr = figure(plot_width=350, plot_height=350,\n              tools='pan,wheel_zoom,box_select,reset')
 4712  corr.circle('t1_returns', 't2_returns', size=2, source=source,\n            selection_color="orange", alpha=0.6, nonselection_alpha=0.1, selection_alpha=0.4)
 4713  ts1 = figure(plot_width=900, plot_height=200, tools=tools, x_axis_type='datetime', active_drag="xbox_select")
 4714  ts1.line('date', 't1', source=source_static)
 4715  ts1.circle('date', 't1', size=1, source=source, color=None, selection_color="orange")
 4716  ts2 = figure(plot_width=900, plot_height=200, tools=tools, x_axis_type='datetime', active_drag="xbox_select")
 4717  ts2.x_range = ts1.x_range
 4718  ts2.line('date', 't2', source=source_static)
 4719  ts2.circle('date', 't2', size=1, source=source, color=None, selection_color="orange")
 4720  # set up callbacks
 4721  def ticker1_change(attrname, old, new):
 4722  bokeh serve 
 4723  cd Python/bokeh_test
 4724  netsat -vanp | grep 5006
 4725  netstat -vanp | grep 5006
 4726  netstat -vanp tcp | grep 5006
 4727  netstat -vanp tcp 
 4728  bokeh serve .
 4729  locate export_csv
 4730  which bokeh
 4731  mkdir stocks
 4732  mv daily download_sample_data.py stocks
 4733  mv main.py stocks
 4734  mkdir export_csv
 4735  cd export_csv
 4736  vim export_csv
 4737  vim export_csv.py
 4738  vim main.py
 4739  ssh sch_bastion 'ssh -i  DataScience.pem  -N -n -L 127.0.0.1:5006:127.0.0.1:5007 centos@10.43.51.90 -Y  -o ServerAliveInterval=50'
 4740  ssh -i ~/.ssh/DataScience.pem -N -n -L 127.0.0.1:5007:127.0.0.1:5007 sch_bastion -Y -o ServerAliveInterval=50
 4741  ssh sch_bastion 'ssh -i  DataScience.pem  -N -n -L 127.0.0.1:5007:127.0.0.1:5007 centos@10.43.51.90 -Y  -o ServerAliveInterval=50'
 4742  vim ~/.psswd-s3fs
 4743  echo  $ACCESS_KEY
 4744  ACCESS_KEY=AKIAJEWBXRC44KGNZV5Q
 4745  SECRET_KEY=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
 4746  echo $ACCESS_KEY:$SECRET_KEY > ~/.passwd-s3fs
 4747  mkdir -p ~/Mount/s3-ge-schindler-ds
 4748  brew install fuse
 4749  sudo brew cask install osxfuse
 4750  man s3fs
 4751  s3fs ge-schindler-ds 
 4752  brew install Caskroom/cask/osxfuse
 4753  brew install homebrew/fuse/s3fs
 4754  find Cellar
 4755  find remote
 4756  find remote_bastion
 4757  s3fs cat ~/.passwd-s3fs
 4758  brew link --force gettext
 4759  which fuse
 4760  which osxfuse
 4761  git clone --recursive -b support/osxfuse-3 git://github.com/osxfuse/osxfuse.git osxfuse
 4762  git clone --recursive -b support/osxfuse-3 https://github.com/osxfuse/osxfuse.git  osxfuse
 4763  ./build.sh
 4764  cd SchindlerMSK/development/Pythonh
 4765  mkdir watchdir
 4766  fswatch
 4767  cat watch
 4768  vim pywatch_test.py
 4769  python pywatch_test.py
 4770  cat watch.log
 4771  fswatch -o ./watchdir  | xargs -n1 
 4772  vim watchdir
 4773  vim change.sh
 4774  bash change.sh
 4775  man xargs
 4776  brew install fswatch
 4777  which sshfs
 4778  sshfs sch_centos_hiro:/home/centos/ /Users/212339410/Mount/remote_centos_hiro
 4779  pkgutil
 4780  sudo rm /usr/local/bin/sshfs
 4781  sudo rm /usr/local/share/man/man1/sshfs.1
 4782  sudo pkgutil --forget com.github.osxfuse.pkg.SSHFS
 4783  locate uninstall_osxfuse
 4784  cd osxfuse
 4785  ./build.sh -t distribution
 4786  ls /tmp/osxfuse/distribution
 4787  ./build.sh -v 5 -t distribution
 4788  sudo brew install autoconf automake libtool gettext
 4789  brew install autoconf automake libtool gettext
 4790  locate libosxfuse
 4791  locate sshfs
 4792  locate fuse
 4793  brew uninstall osxfuse
 4794  brew uninstall csk/osxfuse
 4795  brew -l
 4796  brew -h
 4797  brew list
 4798  rm /usr/local/var/homebrew/locks/osxfuse--3.5.8.dmg.incomplete.lock
 4799  rm -rf /usr/local/var/homebrew/locks/osxfuse--3.5.8.dmg.incomplete.lock
 4800  l /usr/local/var/homebrew/locks/osxfuse--3.5.8.dmg.incomplete.lock
 4801  l /usr/local/var/homebrew/locks/osxfuse
 4802  l /usr/local/var/homebrew/locks/
 4803  brew 
 4804  brew help
 4805  brew list osx
 4806  brew list Cask
 4807  brew list Cask.
 4808  brew list Cask/
 4809  brew cask search osxfuse
 4810  ssfs --version
 4811  sshfs --version
 4812  brew install cask/osxfuse
 4813  brew install cask osxfuse
 4814  brew search osxfuse
 4815  brew install caskroom/cask/osxfuse
 4816  brew install caskroom/cask/osxfuse -v
 4817  rm -rf /Library/Filesystems/osxfuse.fs/*
 4818  locate osxfuse
 4819  rm -rf /private/var/db/receipts/com.github.osxfuse.pkg*
 4820  ls /usr/local/Homebrew/Library/Homebrew/*osxfuse*
 4821  locate osxfuse | grep  /usr/local/Homebrew/Library/Homebrew/*osxfuse*
 4822  locate osxfuse | grep  /usr/local/Homebrew/Library/Homebrew/ | rm 
 4823  locate osxfuse | grep  /usr/local/Homebrew/Library/Homebrew/ | xargs -0 rm 
 4824  locate osxfuse | grep  /usr/local/Homebrew/Library/Homebrew/
 4825  locate osxfuse | grep  -l /usr/local/Homebrew/Library/Homebrew/* 
 4826  locate osxfuse | grep  --null /usr/local/Homebrew/Library/Homebrew/* 
 4827  locate osxfuse | grep   /usr/local/Homebrew/Library/Homebrew/ 
 4828  locate osxfuse | grep   /usr/local/Homebrew/Library/Homebrew/  | xargs -0 rm
 4829  brew install osxfuse
 4830  brew tap caskroom/cask
 4831  brew cask uninstall osxfuse --force --debug
 4832  brew cask install osxfuse -v 
 4833  cat /var/log/install.log
 4834  tail -n 1000 brew cask uninstall osxfuse --force --debug
 4835  tail -n 1000 /var/log/install.log
 4836  sshfs
 4837  sshfs sch_centos_hiro:/home/centos/ /Users/212339410/Mount/remote_centos_hiro -v
 4838  sshfs sch_centos_hiro:/home/centos/ /Users/212339410/Mount/remote_centos_hiro 
 4839  sshfs sch_centos_hiro:/home/centos/ /Users/212339410/Mount/remote_centos_hiro volname=remote_centos_hiro
 4840  s3fs
 4841  s3fs ge-schindler-ds ~/Mount/s3-ge-schindler-ds
 4842  s3fs ge-schindler-ds:FromPredixBasic/ ~/Mount/s3-ge-schindler-ds
 4843  s3fs ge-schindler-ds:/FromPredixBasic/ ~/Mount/s3-ge-schindler-ds
 4844  chmod 600 .passwd-s3fs
 4845  s3fs ge-schindler-ds:/FromPredixBasic/ /Users/212339410/Mount/s3-ge-schindler-ds -o passwd_file=.psswd-s3fs
 4846  which s3fs
 4847  s3fs ge-schindler-ds:/FromPredixBasic/ /Users/212339410/Mount/s3-ge-schindler-ds
 4848  fuser 8889/tcp -v
 4849  fuser 8889/tcp 
 4850  fuser 8889
 4851  fuser --all
 4852  fuser --a
 4853  fuser -a
 4854  man fuser
 4855  brew install golang
 4856  -e
 4857  [ ! -e ~/go/ ] && mkdir ~/go
 4858  go install github.com/kahing/goofys
 4859  go install github.com/kahing/goofys -v
 4860  go install -v github.com/kahing/goofys
 4861  go install
 4862  go install -h
 4863  go help build
 4864  go help install
 4865  go help build 
 4866  go clean
 4867  go get github.com/kahing/goofys
 4868  go get -h
 4869  go get 
 4870  brew install goofys
 4871  goofys ge-schindler-ds /Users/212339410/Mount/s3-ge-schindler-ds 
 4872  csvcyt
 4873  csvcut -n ~/Mount/s3-ge-schindler-ds/FromPredixBasic/Aligned/MSK0
 4874  csvcut
 4875  csvcut -h
 4876  csvcut -n MSK001_20170101_0000_20170102_0000_30ms.csv
 4877  head -n 100 MSK001_20170101_0000_20170102_0000_30ms.csv | csvlook
 4878  ssh hiroaki@3.39.73.245
 4879  cd ../MSK001/
 4880  head ./Apr03_04/MSK001_20170403_0600_20170403_0730_caLevMag.csv | csvlook
 4881  head ./Apr03_04/MSK001_20170403_0600_20170403_0730_caLevMag.csv
 4882  head ./Apr04_04/MSK001_20170403_0600_20170403_0730_caLevMag.csv | csvlook
 4883  head ./Apr28_29/MSK001_20170428_0600_20170428_0730_* | csvlook
 4884  for file in ./Apr28_29/MSK001_20170428_0600_20170428_0730_*; do; echo $file; head $file  | csvlook; done
 4885  cd ../Aligned
 4886  man find
 4887  l -t
 4888  ls | grep LevMag
 4889  find . | levmag
 4890  find . | grep levmag
 4891  ssh hio
 4892  cd MSK001/Apr03_04
 4893  head MSK001_20170403_0600_20170403_0730_caAccX.csv| csvlook
 4894  locate .olm
 4895  locate Outlook for Mac
 4896  locate olm
 4897  locate *.olm
 4898  find / *.olm
 4899  find / .olm
 4900  find / | grep .olm
 4901  find / | grep \.olm
 4902  find / | grep /.olm
 4903  find / | grep ..olm
 4904  cd ../../Aligned
 4905  head MSK001_20161219_0000_20161220_0000_30ms.csv| csvlook
 4906  fuser 5006/tcp
 4907  kill 36574
 4908  netstat
 4909  netstat | grep 5006
 4910  netstat -vanp | grep 62126
 4911  netstat -van | grep 62126
 4912  netstat -van | grep 5004
 4913  netstat -van | grep 5006
 4914  ssh sch_bastion 'ssh -i  DataScience.pem  -N -n -L 127.0.0.1:5006:127.0.0.1:5006 centos@10.43.51.90 -Y  -o ServerAliveInterval=50'
 4915  ssh -i ~/.ssh/DataScience.pem -N -n -L 127.0.0.1:5006:127.0.0.1:5006 sch_bastion -Y -o ServerAliveInterval=50
 4916  pip install --upgrade ipython
 4917  ps 
 4918  conda upgrade notebook
 4919  cp -r ~/Mount/remote_centos_hiro ~/Box/GE\ Digital\ Data\ Science\ 2016/10\ Schindler\ Personal/Backup/centos_hiro 
 4920  sshfs 
 4921  sshfs  -h
 4922  sshfs umount -f ~/Mount/remote_centos_hiro
 4923  sshfs sch_centos_hiro:/home/centos/ /Users/212339410/Mount/remote_centos_hiro -o volname=remote_centos_hiro -p sshfs_debug
 4924  pgrep
 4925  pgrep -lf
 4926  pgrep -lf s
 4927  pgrep -lf fs
 4928  diskutl
 4929  diskutil umout ~/Mount/remote_centos_hiro
 4930  diskutil umount ~/Mount/remote_centos_hiro
 4931  ls Mount
 4932  l Mount
 4933  cd Mount/remote_centos_hiro
 4934  cd remote_centos_siva
 4935  cp -r remote_centos_hiro/hiro/result ~/Box/GE\ Digital\ Data\ Science\ 2016/10\ Schindler\ Personal/Backup/centos_hiro/remote_centos_hiro/hiro/ 
 4936  cp -r remote_centos_hiro/hiro/result/0621 ~/Box/GE\ Digital\ Data\ Science\ 2016/10\ Schindler\ Personal/Backup/centos_hiro/remote_centos_hiro/hiro/result/0621
 4937  cp -cr remote_centos_hiro/hiro/result/0621 ~/Box/GE\ Digital\ Data\ Science\ 2016/10\ Schindler\ Personal/Backup/centos_hiro/remote_centos_hiro/hiro/result
 4938  cp -h
 4939  man cp
 4940  scp sch_centos_hiro:hiro/result/0621/ ~/Box/GE\ Digital\ Data\ Science\ 2016/10\ Schindler\ Personal/Backup/centos_hiro/remote_centos_hiro/hiro/result/
 4941  scp -rc sch_centos_hiro:hiro/result/0621/ ~/Box/GE\ Digital\ Data\ Science\ 2016/10\ Schindler\ Personal/Backup/centos_hiro/remote_centos_hiro/hiro/result/
 4942  scp -r sch_centos_hiro:hiro/result/0621/ ~/Box/GE\ Digital\ Data\ Science\ 2016/10\ Schindler\ Personal/Backup/centos_hiro/remote_centos_hiro/hiro/result/
 4943  compgen
 4944  sudo vim /etc/autofs.conf
 4945  sudo vim /etc/fstab
 4946  man fstab
 4947  sudo umount ~/Mount/remote_centos_hiro
 4948  sudo vim /etc/auto_home
 4949  sudo automount -vc
 4950  man mail
 4951  aws
 4952  vim tmp
 4953  rm tmp
 4954  git rm dss_msk/algorithms/floor_detection.py
 4955  git rm --help
 4956  git statsh --help
 4957  git stash show
 4958  git stash save -k
 4959  git checkout -b floor_detection/error_correction
 4960  git add dss_msk/algorithms/floor_detection.py
 4961  git commit -m 'add docstring for encode and cabin_levels_crossed'
 4962  ssh sch_centos_hiro 'jupyter notebook --no-browser --port=8889 # @centos, run jupyter notebook'
 4963  cd dss_msk
 4964  cd ../../development
 4965  ls devt
 4966  ps aux | grep sshfs
 4967  unmount /Users/212339410/Mount/remote_centos_hiro
 4968  unmount -h /Users/212339410/Mount/remote_centos_hiro
 4969  unmount 
 4970  man unmount
 4971  pgrep -lf ss
 4972  pgrep -lf ssh
 4973  pgrep -lf mount
 4974  unmount -f ~/Mount/remote_centos_hiro
 4975  cd ../Projects
 4976  mkdir script
 4977  mkdir utils
 4978  touch utils/__init__.py
 4979  mv config_files ../
 4980  mkdir activity_detection
 4981  mkdir outlier_detection
 4982  mkdir floor_detection
 4983  touch activity_detection/__init__.py
 4984  touch outlier_detection/__init__.py
 4985  touch floor_detection/__init__.py
 4986  cd features
 4987  find . | grep aws
 4988  git commit -m 'change directory structure '
 4989  cd ../SchindlerMSK
 4990  ps | 8889
 4991  ps | grep 8889
 4992  git clone git@github.build.ge.com:IndustrialDataScience/NEC-SCM.git
 4993  jupyetr notebook
 4994  jupyter-theme -r        
 4995  pip install Geohash
 4996  cd NEC-SCM/notebooks
 4997  2to3 util.py
 4998  2to3 -w util.py
 4999  conda upgrade cufflinks
 5000  conda install cufflinks
 5001  pip install --upgrade cufflinks
 5002  pip install talib
 5003  pip install ta-lib
 5004  mkdir talib
 5005  cd talib
 5006  wget http://prdownloads.sourceforge.net/ta-lib/ta-lib-0.4.0-src.tar.gz
 5007  tar -xzf ta-lib-0.4.0-src.tar.gz
 5008  cd ta-lib
 5009  make isntall
 5010  make
 5011  brew install ta-lib
 5012  python -c 'import talib'
 5013  pip install gantt
 5014  brew install postgresql
 5015  psql -h 3.39.83.161 -p 5432 -U dss_usr -W dss2017! dss_usr
 5016  pip install openpyxl
 5017  pip install pywind32
 5018  pip install pywin32
 5019  pip install pyexcel-xls
 5020  pip install pyexcel-xlsx
 5021  unmount ~/Mount/remote_centos_hiro
 5022  diskutil unmount ~/Mount/remote_centos_
 5023  kill 87062
 5024  ps | grep remote_centos_hiro
 5025  ln -s  /Applications/OpenOffice.app/Contents/program/soffice ~/.local/bin
 5026  ln -s  /Applications/OpenOffice.app/Contents/program/soffice /usr/bin
 5027  soffice -help
 5028  ls -al soffice
 5029  /Applications/OpenOffice.app/Contents/program/soffice
 5030  /Applications/OpenOffice.app/Contents/MacOS/soffice
 5031  ln -s /Applications/OpenOffice.app/Contents/program/soffice ~/bin
 5032  ls ~/bin/
 5033  ls ~/bin/*
 5034  ls ~/bin
 5035  ls ~/bin -al
 5036  ls ~/bin/ -al
 5037  cd ~/bin
 5038  ls -al ~/
 5039  rm ~/bin
 5040  ln -s /Applications/OpenOffice.app/Contents/MacOS/soffice ~/.local/bin
 5041  rm ~/.local/bin/soffice
 5042  which office
 5043  ln -s /Applications/OpenOffice.app/Contents/MacOS/soffice /bin/soffice
 5044  sudo ln -s /Applications/OpenOffice.app/Contents/MacOS/soffice /bin/soffice
 5045  sudo ln -s /Applications/OpenOffice.app/Contents/MacOS/soffice /usr/local/bin
 5046  soffice
 5047  which soffice
 5048  /usr/local/bin/soffice
 5049  cd /Users/212339410/Box/GE Digital Data Science 2016/10 NEC/Ingestion/Alarm Report
 5050  cd '/Users/212339410/Box/GE Digital Data Science 2016/10 NEC/Ingestion/Alarm Report'
 5051  rm -rf Alarm\ Report
 5052  rm Alarm\ Report.zip
 5053  rm Daily\ Progress\ Report.zip
 5054  rm Alarms\ -\ XLSX.zip
 5055  cd Daily\ Progress\ Report
 5056  cd Alarms\ -\ XLSX
 5057  cd ../Daily\ Progress\ Report
 5058  cd ../Alarms\ -\ XLSX
 5059  cd ~/Projects/NEC-SCM/code
 5060  python ingest_india_dpr.py
 5061  ssh pi@10.15.22.54 -p
 5062  ssh pi@10.15.22.54 -l
 5063  ssh raspy
 5064  vim cheatsheets_postgres.md
 5065  which chrome
 5066  start chrome
 5067  open -a Google\ Chrome
 5068  ps -a
 5069  ps -au
 5070  ps -ax
 5071  kill 40451
 5072  lskill 40451
 5073  ps -ax | grep chrome
 5074  sudo kill 40451
 5075  less ~/Library/Preferences/com.googlecode.iterm2.plist
 5076  fuser 8889/tcp
 5077  sshfs sch_centos_siva:/home/centos/ /Users/212339410/Mount/remote_centos_siva -o volname=remote_centos_siva
 5078  ssh sch_centos_siva
 5079  cd remote_centos_hiro/hiro/cache
 5080  head -n 10 df_trip_MSK005_20170504.csv | csvlook
 5081  head -n 100 df_trip_MSK005_20170504.csv | csvlook
 5082  tail -n 100 df_trip_MSK005_20170504.csv | csvlook
 5083  head -n 100 df_result_ed_MSK005_20170503.csv | csvlook 
 5084  head -n 1000 df_result_ed_MSK005_20170503.csv | csvlook 
 5085  vim Master\ -\ Progress\ NEC\ Indosat\ -\ Progress\ Master\ Tab.csv
 5086  file -I Master\ -\ Progress\ NEC\ Indosat\ -\ Progress\ Master\ Tab.csv
 5087  iconv -f iso-8859 -t utf-8 Master\ -\ Progress\ NEC\ Indosat\ -\ Progress\ Master\ Tab.csv Master_Indosat_utf8.csv
 5088  iconv -l
 5089  iconv -l | grep iso
 5090  iconv -l | grep ISO
 5091  iconv -l | grep ISO-8859-
 5092  iconv -l | grep ISO-8859-1
 5093  iconv -f ISO-8859-1 -t utf-8 Master\ -\ Progress\ NEC\ Indosat\ -\ Progress\ Master\ Tab.csv Master_Indosat_utf8.csv
 5094  iconv -f ISO-8859-1 -t utf-8 < Master\ -\ Progress\ NEC\ Indosat\ -\ Progress\ Master\ Tabd.csv > Master_Indosat_utf8.csv
 5095  iconv -f ISO-8859-1 -t utf-8 < 'Master\ -\ Progress\ NEC\ Indosat\ -\ Progress\ Master\ Tab.csv' > Master_Indosat_utf8.csv
 5096  iconv -f ISO-8859-1 -t utf-8 < Master\ -\ Progress\ NEC\ Indosat\ -\ Progress\ Master\ Tab.csv > Master_Indosat_utf8.csv
 5097  cp PROYT-235\ DSS\ -\ KPI\ Development\ -\ %\ on-time\ completion\ of\ installation\ for\ Indonesia.ipynb PROYT-233DSS - KPI Development - Time from invoice trigger to issue of invoice for Indonesia.ipynb
 5098  cp PROYT-235\ DSS\ -\ KPI\ Development\ -\ %\ on-time\ completion\ of\ installation\ for\ Indonesia.ipynb 'PROYT-233DSS - KPI Development - Time from invoice trigger to issue of invoice for Indonesia.ipynb'
 5099  git add PROYT*
 5100  git commit -m 'add notebook for PROYT 233 and 235'
 5101  git rm 
 5102  git rm .DS_Store
 5103  brew tap-pin dbcli/tap
 5104  Fl
 5105  cd '/Users/212339410/Box/GE Digital Data Science 2016/10 NEC/Ingestion/Performance Report'
 5106  unzip find ./ -name \*.zip
 5107  find ./ -name \*.zip -exec unzip {} 
 5108  find ./ -name \*.zip -exec unzip {} \;
 5109  cd ~/Projects/NEC-SCM
 5110  cd code
 5111  cd ingestion
 5112  pgcli
 5113  pgcli --help
 5114  pgcli postgres://dss_user:dss20175432/dss_usr
 5115  python ingest_india_performance.py
 5116  psql -h 3.39.83.161 -p 5432 -U dss_usr dss_usr
 5117  pgcli -h 3.39.83.161 -p 5432 -U dss_usr dss_usr
 5118  git rm 'notebooks/PROYT-233DSS - KPI Development - Time from invoice trigger to issue of invoice for Indonesia.ipynb'
 5119  git add ./code/ingestion/ingest_ind*
 5120  git commit -m 'add ingestion code and update notebook for user story 233 & 235'
 5121  git add 'notebooks/PROYT-233 DSS - KPI Development - Time from invoice trigger to issue of invoice for Indonesia.ipynb'
 5122  git commit -m 'add notebook for PROYT 233'
 5123  git rm notebooks/.ipynb_checkpoints/
 5124  git rm -r notebooks/.ipynb_checkpoints/
 5125  git commit -m 'remove checkpoints for ipynb'
 5126  git rm code/.DS_Store
 5127  git rm Dashboards/html/.DS_Store
 5128  git commit -m 'remove files to be ignored'
 5129  cd Projects/seanalytics-forked
 5130  git commit -m 'initial commit'
 5131  cd ../remote_centos_hiro
 5132  cd hiro/cache
 5133  head -n 20 df_trip_MSK005_20170503.csv| csvlook
 5134  head -n 20 df_result_ed_MSK005_20170503.csv| csvlook
 5135  git clone git@github.build.ge.com:IndustrialDataScience/AssetFingerPrinting.git
 5136  head -n 100 df_trip_MSK005_20170503.csv| csvlook
 5137  head -n 100 df_trip_MSK005_20170504.csv| csvlook
 5138  cat df_result_ed_MSK005_20170504.csv
 5139  head -n 100 df_result_ed_MSK005_20170504.csv | csvlook
 5140  gzip -c df_result_ed_MSK005_20170504.csv
 5141  gzip  df_result_ed_MSK005_20170504.csv
 5142  cd seanalytics-forked
 5143  git add  seanalytics/floor_detection/floor_detection.py
 5144  git commit -m 'update forward error correction and bug fix for merge the result'
 5145  sudo umount -f ~/Mount/remote_centos_hiro
 5146  ssh pi@10.15.22.54 
 5147  git add dss_msk/algorithms/floor_detection_v2.py
 5148  git commit -m 'add floor_detection v2'
 5149  git push origin master 
 5150  ;l
 5151  tree -h
 5152  man tree
 5153  tree  -d
 5154  tree  -d SchindlerEdgeAnalytics
 5155  tree  -d ../Projects/SchindlerMSK
 5156  vim ~/Python/OptModel/optmodel/optmodel.py
 5157  cd AlignedFeatures
 5158  l | grep 201612
 5159  git clone  git@github.build.ge.com:212339410/SchindlerEdgeAnalytics.git
 5160  git clone  git@github.build.ge.com:212339410/SchindlerEdgeAnalytics.git Seanalytics
 5161  git commit -m 'copy from dss_msk v0.3.1 as-is'
 5162  cd Notes
 5163  pandoc
 5164  man pandoc
 5165  pandoc naming_conventions.md -o naming_conventions.pdf
 5166  pandoc naming_conventions.md -o naming_conventions.html
 5167  open naming_conventions.html
 5168  head ~/.zshrc
 5169  ls ~/Mount/s3-ge-schindler-ds/FromPredixBasic/Aligned/ | grep 201612
 5170  ls ~/Mount/s3-ge-schindler-ds/FromPredixBasic/Aligned/ | grep 20161219
 5171  ls ~/Mount/s3-ge-schindler-ds/FromPredixBasic/Aligned/ | grep 20161219_0000_2
 5172  head -n 10 Mount/s3-ge-schindler-ds/FromPredixBasic/Aligned/MSK001_20161219_0000_20161220_0000_30ms.csv | csvlook 
 5173  head -n 10 Mount/s3-ge-schindler-ds/FromPredixBasic/Aligned/MSK002_20161219_0000_20161220_0000_30ms.csv | csvlook 
 5174  head -n 10 Mount/s3-ge-schindler-ds/FromPredixBasic/Aligned/MSK003_20161219_0000_20161220_0000_30ms.csv | csvlook 
 5175  git commit -m 'restructure the directories'
 5176  git push development
 5177  cd utils
 5178  vim schutil.py
 5179  cd NEC-SCM/notebooks/
 5180  vim Rebuild\ dashboard\ in\ excel\ by\ python\ 0711.ipynb
 5181  jupyter notebook --NotebookApp.iopub_data_rate_limit=10000000000
 5182  git add notebooks/Rebuild\ dashboard\ in\ excel\ by\ python\ 0711.ipynb
 5183  git add notebooks/PROYT-23*
 5184  git commit -m 'update notebooks for Demo'
 5185  cd ~/Mount
 5186  cd s3-ge-schindler-ds/FromPredixBasic
 5187  l | grep MSK057
 5188  head -n 10 MSK057_20170710_0000_20170711_0000_30ms.csv | csvlook
 5189  head -n 10 MSK057_20170711_0000_20170711_0000_30ms.csv | csvlook
 5190  head -n 10 MSK057_20170711_0000_20170712_0000_30ms.csv | csvlook
 5191  git: diff
 5192  git quick stage
 5193  cat ~/.aws/credentials
 5194  cat .aws/con
 5195  cat ~/.aws/config
 5196  cd Box/Analytics
 5197  mv GitP* Archive
 5198  mv -rf PB ./Archive
 5199  mv -r PB ./Archive
 5200  mv  PB ./Archive
 5201  cd ~/Mount/
 5202  rm remote_centos_siva
 5203  rm -r remote_centos_siva
 5204  ln -s ~/Projects/Seanalytics/ ~/Mount/remote_centos_hiro/hiro/python/Seanalytics
 5205  ln  ~/Projects/Seanalytics/ ~/Mount/remote_centos_hiro/hiro/python/Seanalytics
 5206  echo $ZDOTDIR
 5207  ls ~/.zprofile
 5208  jupyetr notebook --help
 5209  jupyter notebook --NotebookApp.iopub_data_rate_limit=10000000000 --help
 5210  cd tstk
 5211  cd ../tstk-master
 5212  cat setup.py
 5213  cd ../../Projects/Seanalytics
 5214  git commit -m 'modify gitignore: exclude .idea\n'
 5215  autofs
 5216  cat /etc/auto_master
 5217  vim /etc/auto_mnt_test
 5218  mkdir ~/Mount/remote_centos_rohit
 5219  diskutil unmount ~/Mount/mnt_test
 5220  mount_centos
 5221  diskutil
 5222  umount ~/Mount/mnt_test
 5223  diskutil ~/Mount/mnt_test
 5224  mount list
 5225  diskutil umount ~/Mount/mnt_test
 5226  mount_centost_rohit
 5227  vim ~/.zlogin
 5228  l ~/Mount
 5229  rm ~/Mount/remote_centos_hiro
 5230  rm -rf ~/Mount/remote_centos_hiro
 5231  l ~/Mount/
 5232  rm -rf Mount/remote_centos_hiro
 5233  mkdir Mount/remote_centos_hiro
 5234  diskutil umount ./Mount/remote_centos_hiro
 5235  diskutil umount ./Mount/remote_centos_rohit
 5236  umount ~/Mount/remote_centos_rohit
 5237  sudo diskutil unmount ~/Mount/remote_centos_rohit
 5238  sudo umount -f ~/Mount/remote_centos_rohit
 5239  ps -ef | grep -w sshfs
 5240  kill 21267
 5241  head -n 10 MSK004_20170413_0000_20170414_0000_30ms.csv| csvlook
 5242  head -n 100 MSK006_20170413_0000_20170414_0000_30ms.csv| csvlook
 5243  head -n 100 MSK0015_20170413_0000_20170414_0000_30ms.csv| csvlook
 5244  ssh rohit
 5245  head -n 10 00~9
 5246  head -n 
 5247  head -n 100 MSK008_HH_20170505_0814_20170506_0000_30ms.csv|csvlook
 5248  head -n 100 MSK008_HH_20170506_0000_20170507_0000_30ms.csv |csvlook
 5249  head -n 100 MSK008_HH_20170507_0000_20170507_1148_30ms.csv |csvlook
 5250  head -n 100 MSK016_20170501_0600_20170501_0730_30ms.csv| csvlook
 5251  head -n 100 MSK004_20170413_0000_20170414_0000_30ms.csv| csvlook
 5252  head -n 1000 MSK004_20170413_0000_20170414_0000_30ms.csv| csvlook
 5253  git clone git@github.build.ge.com:IndustrialDataScience/teaplot.git
 5254  cd teaplot
 5255  git fetch -a
 5256  git checkout -b tutorial
 5257  pip install teaplot
 5258  pip install xarray
 5259  pip install param
 5260  conda update dask
 5261  pip install geoviews
 5262  conda install xarray
 5263  conda install -c conda-forge iris
 5264  cd notebooks/geoviews-examples
 5265  pip install holoviews
 5266  conda install -c conda-forge -c ioam holoviews geoviews
 5267  cd ~/Box/GE\ Digital\ Data\ Science\ 2016
 5268  cd 10\ Schindler\ Personal
 5269  cd 0719
 5270  gzip MSK0*
 5271  rm MSK022_20170517_0644_20170518_0000_30ms.csv.zip.gz
 5272  gzip May03_04
 5273  tar -czf MSK009_2017May03_04.tar.gz May03_04
 5274  mkdir Bitbucket-SchindlerEdgeAnalytics
 5275  git remote add -f origin git@bitbucket.org:ioeeschindler/modularsensorkit.git
 5276  git -version
 5277  git remote add -f origin https://hiro_@bitbucket.org/ioeeschindler/modularsensorkit.git
 5278  git config core.sparseCheckout true
 5279  echo 'SchindlerEdgeAnalytics'
 5280  echo 'SchindlerEdgeAnalytics' >> .git/info/sparse-checkout
 5281  git commit -m 'reorganization based on Ebikon visit'
 5282  cd ../Seanalytics
 5283  pip install colormaps
 5284  pip install colormap
 5285  conda update matplotlib
 5286  conda update bokeh
 5287  conda upgrade matplotlib
 5288  pip install urllib2
 5289  pip install --upgrade matplotlib
 5290  tail -n 100 df_result_ed_MSK005_2017050 | csvlook
 5291  conda upgrade matplot_toolkit
 5292  conda upgrade mpl_toolkits
 5293  pip install mpl_toolkits --upgrade
 5294  unzip basemap-1.1.0.zip
 5295  cd basemap-1.1.0
 5296  export GEOS_DIR=/usr/local
 5297  ./configure
 5298  conda install -c datashader
 5299  conda install -c bokeh datashader
 5300  make; make install
 5301  echo GEOS_DIR
 5302  ls ~/.local/
 5303  ls ~/.local/lib
 5304  echo $GEOS_DIR
 5305  ./configure --prefix=$GEOS_DIR
 5306  cd geos-3.3.3
 5307  make;
 5308  make install
 5309  python setup.py install 
 5310  brew install geos
 5311  cd '/Users/212339410/Box/GE Digital Data Science 2016/10 Schindler Personal/Intermediate result/0719/May03_04'
 5312  gzip MSK009_20170504_0000_20170505_0000_caAccZ.csv
 5313  gulp 
 5314  npm install gulp
 5315  cd server/
 5316  sudo bower install
 5317  npm config get proxy http
 5318  npm config set proxy http https://proxy-src.research.ge.com:8080
 5319  npm get config https
 5320  npm get config proxy
 5321  npm get config https-proxy
 5322  vim /.bowwerrc
 5323  npm config set proxy 'https://proxy-src.research.ge.com:8080'
 5324  npm install 
 5325  terminal
 5326  iterm
 5327  npm config set https-proxy 'https://proxy-src.research.ge.com:8080'
 5328  npm config set https-proxy "https://proxy-src.research.ge.com:8080"
 5329  npm config get 
 5330  npm config set proxy "https://proxy-src.research.ge.com:8080"
 5331  npm get config 
 5332  locate .bowerrc
 5333  vim /Users/212339410/.atom/packages/Remote-FTP/node_modules/jquery/.bowerrc
 5334  vim /Users/212339410/.bowerrc
 5335  ssh -i ~/.ssh/DataScience.pem -N -n -L 127.0.0.1:8889:127.0.0.1:8889 sch_bastion -Y -o ServerAliveInterval=50 # connect loal to bastion
 5336  npm config get
 5337  npm config set proxy ""
 5338  npm config set https-proxy ""
 5339  npm config set https-proxy "htpp://"
 5340  npm config delete https-proxy
 5341  npm config ls -l
 5342  npm config ls -l | grep proxy
 5343  less /Users/212339410/.npmrc
 5344  npm config list --global
 5345  ssh sch_bastion 'ssh -i  DataScience.pem  -N -n -L 127.0.0.1:8889:127.0.0.1:8889 centos@10.43.51.90 -Y  -o ServerAliveInterval=50' # bastion to edge
 5346  ssh sch_centos_hiro 'jupyter notebook --no-browser --port=8889 --NotebookApp.iopub_data_rate_limit=1.0e10 # @centos, run jupyter notebook
 5347  ssh -i ~/.ssh/DataScience.pem -N -n -L 127.0.0.1:8889:127.0.0.1:8889 sch_bastion -Y -o ServerAliveInterval=50 # bridge loal to bastion
 5348  cd '/Users/212339410/Box/GE Digital Data Science 2016/10 Schindler Personal/Intermediate result/0719/MSK009_20170505_0000_20170506_0000_caAccZ.csv'
 5349  cd '/Users/212339410/Box/GE Digital Data Science 2016/10 Schindler Personal/Intermediate result/0719/'
 5350  gzip MSK009_20170505_0000_20170506_0000_caAccZ.csv
 5351  npm config
 5352  cat ~/.npmrc
 5353  cat /usr/local/etc/npmrc
 5354  locate npmrc
 5355  npm
 5356  locate bowerrc
 5357  locate bowerrc | find proxy
 5358  grep -l 'proxy' /Users/212339410/.bowerrc
 5359  grep -rl 'proxy' /Users/212339410/.bowerrc
 5360  man grep
 5361  vim ~/.bowerrc
 5362  grep -r 'proxy' ~/.bowerrc
 5363  grep 'proxy' ~/.npmrc
 5364  grep 'proxy' ~/.bowerrc 
 5365  npm install bower
 5366  bower --version
 5367  npm --version
 5368  gulp -v
 5369  bower install -v
 5370  bower install 
 5371  bower -h
 5372  bower install -V
 5373  ping https://bower.herokuapp.com/packages/px-simple-bar-chart
 5374  nslookup https://bower.herokuapp.com/packages/px-simple-bar-chart
 5375  git clone https://github.build.ge.com/DigitalServicesCOEDev/scm-sample-app-ui-dashboard-sample
 5376  npm install npm@latest -g
 5377  git clone git@github.build.ge.com:DigitalServicesCOEDev/scm-sample-app-ui-dashboard-sample.git
 5378  node -v]
 5379  node -v
 5380  npm -v
 5381  vim ~/.npmrc
 5382  npm --proxy https://proxy-src.research.ge.com:8080
 5383  npm config set proxy https://proxy-src.research.ge.com:8080
 5384  npm config set https-proxy https://proxy-src.research.ge.com:8080
 5385  npm install gulp-cli -g
 5386  vim /usr/local/etc/npmrc
 5387  cd scm-sample-app-ui-dashboard-sample
 5388  alias geproxy='export http_proxy=https://proxy-src.research.ge.com:8080;\nexport https_proxy=$http_proxy;\necho "Now http(s)_proxy is set to GE Proxy:q
 5389  npm config ls
 5390  npm config delete proxy 
 5391  npm config delete https-proxy 
 5392  npm install bower -g
 5393  vim ~/.bowerrc 
 5394  bower install
 5395  vim server/app.js
 5396  cat localConfig.json
 5397  cp localConfig.json.bak
 5398  cp localConfig.json localConfig.json.bak
 5399  rm localConfig.json
 5400  vim localConfig.json
 5401  cd server
 5402  git add localConfig.json.bak
 5403  git add localConfig.json
 5404  git stash save --keep-index
 5405  vim app.js
 5406  gulp dist
 5407  less ~/.zshrc
 5408  cd Box/Development/NodeJs_Udem
 5409  cd C6_LetsRunSomeJavascript
 5410  cd Starter
 5411  node app.js
 5412  scp sch_centos_hiro:/home/centos/hiro/notebook/* ~/Box/GE\ Digital\ Data\ Science\ 2016/10\ Schindler\ Personal/Backup/centos_hiro/remote_centos_hiro/hiro/notebook 
 5413  scp -r sch_centos_hiro:/home/centos/hiro/notebook/ ~/Box/GE\ Digital\ Data\ Science\ 2016/10\ Schindler\ Personal/Backup/centos_hiro/remote_centos_hiro/hiro/notebook 
 5414  scp -r sch_centos_hiro:/home/centos/hiro/python  ~/Box/GE\ Digital\ Data\ Science\ 2016/10\ Schindler\ Personal/Backup/centos_hiro/remote_centos_hiro/hiro/
 5415  cp -r '/Users/212339410/Box/GE Digital Data Science 2016/10 Schindler Personal/Backup/centos_hiro/remote_centos_hiro/hiro/notebook/notebook/.ipynb_checkpoints' '/Users/212339410/Box/GE Digital Data Science 2016/10 Schindler Personal/Backup/centos_hiro/remote_centos_hiro/hiro/notebook/ \n'
 5416  cp -r '/Users/212339410/Box/GE Digital Data Science 2016/10 Schindler Personal/Backup/centos_hiro/remote_centos_hiro/hiro/notebook/notebook/.ipynb_checkpoints' '/Users/212339410/Box/GE Digital Data Science 2016/10 Schindler Personal/Backup/centos_hiro/remote_centos_hiro/hiro/notebook/' 
 5417  rm -r '/Users/212339410/Box/GE Digital Data Science 2016/10 Schindler Personal/Backup/centos_hiro/remote_centos_hiro/hiro/notebook/notebook'
 5418  scp -r sch_centos_hiro:/home/centos/hiro/data  ~/Box/GE\ Digital\ Data\ Science\ 2016/10\ Schindler\ Personal/Backup/centos_hiro/remote_centos_hiro/hiro/
 5419  cd Box/GE\ Digital\ Data\ Science\ 2016/10\ Schindler\ Personal/Intermediate\ result
 5420  cd 0727
 5421  gzip *.csv
 5422  cd ../NEC-SCM/notebooks
 5423  ls /sbin/ | grep mount
 5424  locate osxfusefs.fs
 5425  ls /Library/Filesystems/osxfuse.fs/
 5426  ls /Library/Filesystems/osxfusefs.fs/
 5427  ls /Library/Filesystems/osxfusefs.fs/Support/
 5428  ssh centos@10.43.41.90 -i ~/.ssh/DataScience.pem -o ProxyCommand='sshÂ sch_bastion -W %h:%p'Â -o ServerAliveInterval=10Â 
 5429  ssh centos@10.43.41.90 -i ~/.ssh/DataScience.pem -o ProxyCommand='sshÂ ubuntu@54.214.160.173 -i ~/.ssh/DataScience.pem -o ServerAliveInterval=120 -W %h:%p'Â -o ServerAliveInterval=120
 5430  ssh  -i ~/.ssh/DataScience.pem -o ServerAliveInterval=120 -o ProxyCommand='sshÂ ubuntu@54.214.160.173 W %h:%p -i ~/.ssh/DataScience.pem -o ServerAliveInterval=120 -'Â  centos@10.43.41.90
 5431  ssh ubuntu@54.214.160.173 W %h:%p -i ~/.ssh/DataScience.pem -o ServerAliveInterval=120
 5432  sshÂ ubuntu@54.214.160.173 -W %h:%p -i ~/.ssh/DataScience.pem -o ServerAliveInterval=120 
 5433  ssh ubuntu@54.214.160.173 -i ~/.ssh/DataScience.pem -o ServerAliveInterval=120ssh ubuntu@54.214.160.173 -i ~/.ssh/DataScience.pem -o ServerAliveInterval=120
 5434  ssh ubuntu@54.214.160.173 -i ~/.ssh/DataScience.pem -o ServerAliveInterval=120 -W %h:%p
 5435  ssh  -i ~/.ssh/DataScience.pem -o ServerAliveInterval=120 -o ProxyCommand='sshÂ -W %h:%p ubuntu@54.214.160.173â  -i ~/.ssh/DataScience.pem -o ServerAliveInterval=120 'Â  centos@10.43.41.90
 5436  sshÂ ubuntu@54.214.160.173 -W %h:%p  -i ~/.ssh/DataScience.pem -o ServerAliveInterval=120
 5437  ssh  -W %h:%p  -i ~/.ssh/DataScience.pem -o ServerAliveInterval=120 ubntu@54.214.160.173
 5438  ssh  -W %h:%p ubuntu@54.214.160.173  -i ~/.ssh/DataScience.pem -o ServerAliveInterval=120
 5439  man ssh 
 5440  ssh  -i ~/.ssh/DataScience.pem -o ServerAliveInterval=120 -o ProxyCommand='sshÂ -W %h:%p 54.214.160.173 -W %h:%p  -i ~/.ssh/DataScience.pem -o ServerAliveInterval=120'Â  centos@10.43.41.90
 5441  ssh  -i ~/.ssh/DataScience.pem -o ServerAliveInterval=120 -o ProxyCommand='sshÂ -W %h:%p 54.214.160.173   -i ~/.ssh/DataScience.pem -o ServerAliveInterval=120'Â  centos@10.43.41.90
 5442  ssh  -i ~/.ssh/DataScience.pem -o ServerAliveInterval=120 -o ProxyCommand='sshÂ -w %h:%p ubuntu@54.214.160.173   -i ~/.ssh/DataScience.pem -o ServerAliveInterval=120'Â  centos@10.43.41.90
 5443  ssh  -i ~/.ssh/DataScience.pem -o ServerAliveInterval=120 -o âProxyCommand sshÂ -w %h:%p ubuntu@54.214.160.173   -i ~/.ssh/DataScience.pem -o ServerAliveInterval=120'Â  centos@10.43.41.90
 5444  ssh centos@10.43.41.90 -i ~/.ssh/DataScience.pem -o ServerAliveInterval=120 -o âProxyCommand sshÂ -w %h:%p ubuntu@54.214.160.173   -i ~/.ssh/DataScience.pem -o ServerAliveInterval=120'Â  
 5445  ssh centos@10.43.41.90 -i ~/.ssh/DataScience.pem -o ServerAliveInterval=120 -o 'ProxyCommand sshÂ -w %h:%p ubuntu@54.214.160.173   -i ~/.ssh/DataScience.pem -o ServerAliveInterval=120'Â  
 5446  ssh ubuntu@54.214.160.173 -i ~/.ssh/DataScience.pem -o ServerAliveInterval=120
 5447  ssh centos@10.43.41.90 -i ~/.ssh/DataScience.pem -o ProxyCommand='sshÂ sch_bastion -W %h:%p'Â -o ServerAliveInterval=120
 5448  ssh centos@10.43.51.90 -i ~/.ssh/DataScience.pem -o ProxyCommand='sshÂ sch_bastion -W %h:%p'Â -o ServerAliveInterval=120
 5449  ssh  -i ~/.ssh/DataScience.pem -o ProxyCommand='sshÂ sch_bastion -W %h:%p'Â -o ServerAliveInterval=120 centos@10.43.51.90
 5450  sshÂ sch_bastion -W %h:%p
 5451  ssh centos@10.43.51.90 -i ~/.ssh/DataScience.pem -o ServerAliveInterval=120 -o 'ProxyCommand sshÂ -w %h:%p ubuntu@54.214.160.173   -i ~/.ssh/DataScience.pem -o ServerAliveInterval=120'Â  
 5452  ssh centos@10.43.51.90 -i ~/.ssh/DataScience.pem -o ServerAliveInterval=120 -o 'ProxyCommand sshÂ -W %h:%p ubuntu@54.214.160.173   -i ~/.ssh/DataScience.pem -o ServerAliveInterval=120'Â  
 5453  ssh centos@10.43.51.90 -i ~/.ssh/DataScience.pem -o ServerAliveInterval=120 -o 'ProxyCommand=sshÂ -W %h:%p ubuntu@54.214.160.173   -i ~/.ssh/DataScience.pem -o ServerAliveInterval=120'Â  
 5454  ssh centos@10.43.51.90 -i ~/.ssh/DataScience.pem -o ServerAliveInterval=120 -o 'ProxyCommand=sshÂ -W %h:%p   -i ~/.ssh/DataScience.pem -o ServerAliveInterval=120'Â  
 5455  ssh -o User=centos -o ProxyCommand="ssh -W %h:%p ubuntu@54.214.160.173 -i ~/.ssh/DataScience.pem"  10.43.51.90 -i ~/.ssh/DataScience.pem
 5456  id 212339410
 5457  sudo /etc/auto_master
 5458  sudo vim /etc/auto_sshfs_centos_hiro
 5459  sudo vim /etc/rc.common
 5460  vim /etc/auto_home
 5461  df -Ph 
 5462  lsvfs
 5463  sudo vim /etc/auto_mnt_test
 5464  id -u
 5465  id -g
 5466  sudo vim /etc/auto_centos_hiro
 5467  sudo automount -cv
 5468  man automount
 5469  less /etc/autofs.conf
 5470  vim /etc/auto_master
 5471  sudo vim /etc/auto_master
 5472  sudo automount -cv -v
 5473  sudo automount -cv -u
 5474  sudo automount -cv 
 5475  scp ~/Projects/Seanalytics sch_centos_hiro:hiro/
 5476  man scp
 5477  scp -r ~/Projects/Seanalytics sch_centos_hiro:hiro/
 5478  rsync -chavz --stats sch_centos_hiro:hiro/python/Syanalytics  ~/Projects/Seanalytics 
 5479  cd ../Celtics
 5480  cd notebook/
 5481  cd '/Users/212339410/Box/GE Digital Data Science 2017/10 Schindler Personal/Backup/centos_hiro/remote_centos_hiro/hiro/notebook/' 
 5482  cd '/Users/212339410/Box/Analytics/Projects/Celtics/data'
 5483  head injuries.csv| csvlook
 5484  head injuries.csv| csvcut -n
 5485  cd Sports\ Vu\ Data
 5486  rm -r NBA_FINAL_SEQUENCE_OPTICAL\$2017032928_Q4\ 2
 5487  vim injuries.csv
 5488  cd Projects/Celtics/notebook
 5489  cat ~/.bashrc
 5490  cd Projects/NEC-
 5491  ping 10.15.22.54
 5492  scp ~/Mount/remote_centos_hiro/hiro/python/Seanalytics raspy01:home/pi/IoEE/
 5493  scp -r ~/Mount/remote_centos_hiro/hiro/python/Seanalytics raspy01:home/pi/IoEE/
 5494  rsync -r sch_centos_hiro:hiro/python/Seanalytics ~/Box/Analytics/Projects/
 5495  rsync -chavz --stats sch_centos_hiro::hiro/python/Syanalytics  ~/Projects/Seanalytics 
 5496  vrsync -chavzP --stats sch_centos_hiro:hiro/python/Seanalytics ~/Projects
 5497  rsync -chavzP --stats  ~/Projects/Seanalytics raspy01:home/pi/IoEE/
 5498  scp ~/Projects/Seanalytics raspy01:home/pi/IoEE/
 5499  scp -r ~/Projects/Seanalytics raspy01:home/pi/IoEE/
 5500  rsync -chavzP --stats sch_centos_hiro:hiro/python/Seanalytics ~/Projects
 5501  cd ../scm-sample-app-ui-dashboard-sample_mod
 5502  cd ../scm-sample-app-ui-dashboard-sample
 5503  npm cache clean
 5504  npm install
 5505  npm install lodash --save
 5506  gulp
 5507  rm -rf node_modules/browser-sync
 5508  npm install browser-sync --save
 5509  sshfs raspy01:/home/pi/ /Users/212339410/Mount/raspi01 -o volname=raspi01
 5510  ssh root
 5511  git add notebooks/PROYT-235 DSS - KPI Development - % on-time completion of installation for Indonesia.ipynb
 5512  git add 'notebooks/PROYT-235 DSS - KPI Development - % on-time completion of installation for Indonesia.ipynb'
 5513  git add 'notebooks/Rebuild dashboard in excel by python 0711.ipynb'
 5514  git commit -m 'add notebook for KPI ontime and Indosat excel dashboard conversion'
 5515  cd ../../Seanalytics
 5516  git commit -m 'update floor detection'
 5517  git history
 5518  git --hlep
 5519  git -h -v
 5520  man git
 5521  head -n 100 MSK070_20170717_0000_20170718_0000_30ms.csv | csvlook
 5522  head -n 1000 MSK070_20170717_0000_20170718_0000_30ms.csv | csvlook
 5523  head -n 100 MSK070_20170718_0000_20170719_0000_30ms.csv | csvlook
 5524  cd Projects/scm-sample-app-ui-dashboard-sample
 5525  cd Projects/nec-dashboard-views
 5526  mv -r nec-dashboard-views ./NEC-SCM/Apps
 5527  mv  nec-dashboard-views ./NEC-SCM/Apps
 5528  git add nec-dashboard-views/
 5529  git commit -m 'add nec-dashboard-views'
 5530  git commit -m 'add Active Scope in the bar chart'
 5531  cf loging -a https://predix-io.run.aws-jp01-pr.ice.predix.io
 5532  cat Manifest.yml
 5533  ping sjc1print01
 5534  ping http://3.39.64.13/
 5535  ping 3.39.64.13
 5536  kill 12453
 5537  vim ~/.zshrcc
 5538  less /var/log/system.log
 5539  tail /var/log/system.log
 5540  goofys -f 
 5541  cd Projects/Seanalytics
 5542  cd Seanalytics
 5543  set_sch
 5544  ssh -i ~/.ssh/DataScience.pem -N -n -L 127.0.0.1:8889:127.0.0.1:8889 sch_bastion -Y -o ServerAliveInterval=50 # connect loal to bastion # local to bastion
 5545  tail -n 100 MSK070_20170718_0000_20170719_0000_30ms.csv | csvlook
 5546  ssh sch_centos_hiro 'jupyter notebook --no-browser --port=8889 --NotebookApp.iopub_data_rate_limit=1.0e10 # @centos, run jupyter notebook'
 5547  echo $mount_s3_sch
 5548  type ls
 5549  cat ~/.ssh/config | grep raspy
 5550  ping 10.15.22.129
 5551  ls ./Mount/s3-ge-schindler-ds/FromPredixBasic/Aligned/MSK070_201707
 5552  ls ./Mount/s3-ge-schindler-ds/FromPredixBasic/Aligned/MSK070_201707*
 5553  for file in ./Mount/s3-ge-schindler-ds/FromPredixBasic/Aligned/MSK070_201707*:\necho $file
 5554  for file in ./Mount/s3-ge-schindler-ds/FromPredixBasic/Aligned/MSK070_201707*:\ndo;\necho $file;\ndone
 5555  for file in ~/Mount/s3-ge-schindler-ds/FromPredixBasic/Aligned/MSK070_201707*:\ndo;\necho $file;\ndone
 5556  for file in ~/Mount/s3-ge-schindler-ds/FromPredixBasic/Aligned/MSK070_201707:\ndo;\necho $file;\ndone
 5557  for file in ~/Mount/s3-ge-schindler-ds/FromPredixBasic/Aligned/: \ndo;\necho $file;\ndone
 5558  for file in ~/Mount/s3-ge-schindler-ds/FromPredixBasic/Aligned/* \ndo;\necho $file;\ndone
 5559  for file in ~/Mount/s3-ge-schindler-ds/FromPredixBasic/Aligned/MSK070 \ndo;\necho $file;\ndone
 5560  for file in ~/Mount/s3-ge-schindler-ds/FromPredixBasic/Aligned/MSK070* \ndo;\necho $file;\ndone
 5561  for file in ~/Mount/s3-ge-schindler-ds/FromPredixBasic/Aligned/MSK070* \ndo;\ngecho $file;\ndone
 5562  for file in ~/Mount/s3-ge-schindler-ds/FromPredixBasic/Aligned/MSK070* \ndo;\ngzip -c $file > ~/Box/GE\ Digital\ Data\ Science\ 2017/10\ Schindler\ Personal/Intermediate\ result $file.gz\necho $file;\ndone
 5563  for file in ~/Mount/s3-ge-schindler-ds/FromPredixBasic/Aligned/MSK070* \ndo;\ngzip -c $file > ~/Box/GE\ Digital\ Data\ Science\ 2017/10\ Schindler\ Personal/Intermediate\ result/0807/$file.gz\necho $file;\ndone
 5564  for file in ~/Mount/s3-ge-schindler-ds/FromPredixBasic/Aligned/MSK070* \ndo;\ngzip -c $file > "~/Box/GE\ Digital\ Data\ Science\ 2017/10\ Schindler\ Personal/Intermediate\ result/0807/"$file.gz\necho $file;\ndone
 5565  for file in ~/Mount/s3-ge-schindler-ds/FromPredixBasic/Aligned/MSK070* \ndo;\ngzip -c $file > "~/Box/GE Digital Data Science 2017/10 Schindler Personal/Intermediate result/0807/"$file.gz\necho $file;\ndone
 5566  for file in ~/Mount/s3-ge-schindler-ds/FromPredixBasic/Aligned/MSK070* \ndo;\ngzip -c $file >> "~/Box/GE Digital Data Science 2017/10 Schindler Personal/Intermediate result/0807/"$file.gz\necho $file;\ndone
 5567  for file in ~/Mount/s3-ge-schindler-ds/FromPredixBasic/Aligned/MSK070* \ndo;\ngzip -c $file > "~/Box/GE Digital Data Science 2017/10 Schindler Personal/Intermediate result/0807/$file.gz"\necho $file;\ndone
 5568  for file in ~/Mount/s3-ge-schindler-ds/FromPredixBasic/Aligned/MSK070* \ndo;\ngzip -c $file > "Users/212339410/Box/GE Digital Data Science 2017/10 Schindler Personal/Intermediate result/0807/$file.gz"\necho $file;\ndone
 5569  for file in ~/Mount/s3-ge-schindler-ds/FromPredixBasic/Aligned/MSK070* \ndo;\ngzip -c $file > "/Users/212339410/or file in ~/Mount/s3-ge-schindler-ds/FromPredixBasic/Aligned/MSK070*Box/GE Digital Data Science 2017/10 Schindler Personal/Intermediate result/0807/$file.gz"\necho $file;\ndone
 5570  for file in ~/Mount/s3-ge-schindler-ds/FromPredixBasic/Aligned/MSK070*\ndo;\ngzip -c $file > "/Users/212339410/Box/GE Digital Data Science 2017/10 Schindler Personal/Intermediate result/0807/$file.gz"\necho $file;\ndone
 5571  for file in ~/Mount/s3-ge-schindler-ds/FromPredixBasic/Aligned/MSK070*\ndo;\ngzip -c $file > "/Users/212339410/Box/GE Digital Data Science 2017/10 Schindler Personal/Intermediate result/0807/$file.gz";\necho $file;\ndone
 5572  for fullfile in ~/Mount/s3-ge-schindler-ds/FromPredixBasic/Aligned/MSK070*\ndo;\nfilename=$(basename "$fullfile")\nextension="${filename##*.}"\nfilename="${filename%.*}"\ngzip -c $file > "/Users/212339410/Box/GE Digital Data Science 2017/10 Schindler Personal/Intermediate result/0807/filename.gz";\necho $file;\ndone
 5573  echo $fullfile
 5574  $(basename "$fullfile")
 5575  echo $(basename "$fullfile")
 5576  echo $fullfilefor fullfile in ~/Mount/s3-ge-schindler-ds/FromPredixBasic/Aligned/MSK070*
 5577  filename=$(basename "$fullfile")
 5578  extension="${filename##*.}"
 5579  filename="${filename%.*}"
 5580  ##gzip -c $file > "/Users/212339410/Box/GE Digital Data Science 2017/10 Schindler Personal/Intermediate result/0807/$filename.gz";
 5581  echo $filename;
 5582  for fullfile in ~/Mount/s3-ge-schindler-ds/FromPredixBasic/Aligned/MSK070*\ndo;\nfilename=$(basename "$fullfile")\nextension="${filename##*.}"\nfilename="${filename%.*}"\ngzip -c $file > "/Users/212339410/Box/GE Digital Data Science 2017/10 Schindler Personal/Intermediate result/0807/$filename.gz";\necho $filename;\ndone
 5583  gzip -c '/Users/212339410/Mount/s3-ge-schindler-ds/FromPredixBasic/Aligned/MSK070_20170717_0000_20170718_0000_30ms' > "/Users/212339410/Box/GE Digital Data Science 2017/10 Schindler Personal/Intermediate result/0807/CMSK070_20170717_0000_20170718_0000_30ms.csv.gz\n"
 5584  gzip -c '/Users/212339410/Mount/s3-ge-schindler-ds/FromPredixBasic/Aligned/MSK070_20170717_0000_20170718_0000_30ms.csv' > "/Users/212339410/Box/GE Digital Data Science 2017/10 Schindler Personal/Intermediate result/0807/CMSK070_20170717_0000_20170718_0000_30ms.csv.gz"
 5585  for fullfile in ~/Mount/s3-ge-schindler-ds/FromPredixBasic/Aligned/MSK070*\ndo;\nfilename=$(basename "$fullfile")\nextension="${filename##*.}"\nfilename="${filename%.*}"\n##gzip -c $file > "/Users/212339410/Box/GE Digital Data Science 2017/10 Schindler Personal/Intermediate result/0807/$filename.gz";\necho $filename;\ndone
 5586  sudo diskutil umount 
 5587  sudo diskutil unmount 
 5588  unmount
 5589  ls ~/Mount/s3-ge-schindler-ds/
 5590  ps ax | grep sshfs
 5591  ps ax | grep goofys
 5592  goofys
 5593  gzip -h
 5594  gzip -c '/Users/212339410/Mount/s3-ge-schindler-ds/FromPredixBasic/Aligned/MSK070_20170717_0000_20170718_0000_30ms.csv' > "/Users/212339410/Box/GE Digital Data Science 2017/10 Schindler Personal/Intermediate result/0807/MSK070_20170717_0000_20170718_0000_30ms.csv.gz"
 5595  gzip -1 -c '/Users/212339410/Mount/s3-ge-schindler-ds/FromPredixBasic/Aligned/MSK070_20170717_0000_20170718_0000_30ms.csv' > "/Users/212339410/Box/GE Digital Data Science 2017/10 Schindler Personal/Intermediate result/0807/MSK070_20170717_0000_20170718_0000_30ms.csv.gz"
 5596  ssh sch_centos_hiro o
 5597  scp -r ~/Projects/Seanalytics raspy01:/home/pi/IoEE/
 5598  cat ~/.ssh/id_rsa.pub| ssh raspy01 'cat >> .ssh/authrized_keys'
 5599  cat ~/.ssh/id_rsa.pub| ssh raspy01 'cat >> .ssh/authorized_keys'
 5600  head ~/Mount/s3-ge-schindler-ds/FromPredixBasic/MSK070/Jul17_18/MSK070_20170717_0000_20170718_0000_caAccX.csv
 5601  l seanalytics/models
 5602  git commit -m 'floor prediction test in production'
 5603  scp -r ~/Projects/seanalytics raspy01:/home/pi/IoEE/
 5604  git commit -m 'modify the sigture for stream_floor_prediction, update the pickle model, delete old floor_detection-v2.py, finish error_correction '
 5605  git commit -m 'modify the sigture for stream_floor_prediction, update the pickle model, delete old floor_detection-v2.py, finish error_correction implementation'
 5606  rm ./*.pkl
 5607  rm test_file_hiro_junk.py
 5608  rm algorithms/floor_detection_v2.py
 5609  cd preprocessing
 5610  rm *.pyc
 5611  cd use_cases
 5612  git rm ../tests/test_floorDetection.py
 5613  git rm ../tests/test_floorDetection.py --cached
 5614  git rm models/floor_detection_gmm_n3_MSK070_20170717_d14.pkl
 5615  git rm models/*.npy
 5616  git add algorithms/floor_detection.py
 5617  git add config_files/MSK070_floor_detection.ini
 5618  git commit -m 'update floor_detection with MSK070 ini'
 5619  cat __init__.py
 5620  ls seanalytics/models/floor_detection_gmm_n3_MSK0
 5621  rm seanalytics/models/floor_detection_gmm_n3_MSK0*.npy
 5622  ls seanalytics/models/
 5623  rm -rf ./seanalytics/algorithms
 5624  rm -rf ./seanalytics/config_files
 5625  rm -rf ./seanalytics/models
 5626  rm __init__.py
 5627  vim seanalytics/algorithms/floor_detection
 5628  git commit -m 'resolved conflicts'
 5629  rm ./seanalytics/models/*.pkl
 5630  git fetch origin feat/floor
 5631  ls seanalytics/models
 5632  cd seanalytics/models
 5633  wget https://github.build.ge.com/212339410/SchindlerEdgeAnalytics/blob/feat/floor/seanalytics/models/floor_detection_gmm_n3_MSK070_20170717_d1.pkl
 5634  git clone git@github.build.ge.com:212339410/SchindlerEdgeAnalytics.git seanalytics-conflict
 5635  cd seanalytics-conflict
 5636  git checkout -b 212339410-feat/floor development
 5637  git pull git@github.build.ge.com:212339410/SchindlerEdgeAnalytics.git feat/floor
 5638  git merge --no-ff 212339410-feat/floor
 5639  git clone git@github.build.ge.com:212339410/SchindlerEdgeAnalytics.git seanalytics-test
 5640  cd seanalytics-test
 5641  git checkout -b feat/floor-test
 5642  touch test-file.txt
 5643  git add test-file.txt
 5644  git commit -m 'test and intro'
 5645  git push origin feat/floor-test
 5646  git checkout -b 212339410-feat/floor-test development
 5647  git pull git@github.build.ge.com:212339410/SchindlerEdgeAnalytics.git feat/floor-test
 5648  git merge --no-ff 212339410-feat/floor-test
 5649  grep -rl 'read_excel'
 5650  grep -rl 'read_excel' ./development/Pythonh/*.ipynb
 5651  grep -rl 'read_excel' ./development/Pythonh/
 5652  grep -rl 'read_excel' ./development/
 5653  git commit -m 'make df_trip to align with output format'
 5654  cd ~/Projects/seanalytics/sea
 5655  cd ~/Projects/seanalytics/seanalytics
 5656  sphinx-quickstart
 5657  git checkout -b fix/update-github-from-bitbuckets
 5658  cd rm -rf seanalytics
 5659  rm -rf doc temp_download_comp_aligned.py tests
 5660  git branch master
 5661  git .
 5662  git commit -m 'update github by bitbuckets master branch on Aug 11'
 5663  git push origin fix/update-github-from-bitbuckets
 5664  rm -rf seanalytics-test
 5665  git clone git@github.build.ge.com:IndustrialDataScience/SchindlerEdgeAnalytics.git seanalytics-fix0811
 5666  cd seanalytics-fix0811
 5667  git checkout -b fix/update-github-from-bitbucket
 5668  rm -rf doc/ seanalytics tests
 5669  git push origin fix/update-github-from-bitbucket
 5670  cd ../seanalytics
 5671  git push -d origin fix/update-github-from-bitbuckets
 5672  python seanalytics
 5673  git add seanalytics/__main__.py
 5674  git commit -m 'Include config loading into Class initialization'
 5675  git commit -m 'Split the test data into input and output sample for PDS'
 5676  git remote add upstream git@github.build.ge.com:212339410/SchindlerEdgeAnalytics.git
 5677  git fetch upstream 
 5678  git merge development 
 5679  git merge upstream/master
 5680  git merge --abort
 5681  git revert 0f7382690ce7da78c2aa4f836e10452089b9e75b
 5682  git revert -m 1 0f7382690ce7da78c2aa4f836e10452089b9e75b
 5683  rm ../MSK005_df_hidden.csv
 5684  git remote -a 
 5685  git remote delete upstream 
 5686  git pull upstream feat/floor
 5687  git commit -m 'Delete non-MSK specifc parameters'
 5688  git checkout -b 212558390-development development
 5689  git pull git@github.build.ge.com:212558390/SchindlerEdgeAnalytics.git development
 5690  git checkout -b 212558390-feat/activity_detection development
 5691  git pull git@github.build.ge.com:212558390/SchindlerEdgeAnalytics.git feat/activity_detection
 5692  git commit -m 'Merge 212558390-feat/activity_detectio'
 5693  git merge --no-ff 212558390-feat/activity_detection
 5694  git push origin development -u
 5695  git remote git fetch upstream
 5696  git push upstream development
 5697  git push git status
 5698  git -v
 5699  git clone git@github.build.ge.com:IndustrialDataScience/SchindlerEdgeAnalytics.git seanalytics-hotfix
 5700  cd seanalytics-hotfix
 5701  git commit -m 'Hotfix to merge feat/activity_detection update #26'
 5702  git push -u origin development
 5703  rm -rf seanalytics-hotfix
 5704  rm -r seanalytics-conflict
 5705  rm -rf seanalytics-conflict
 5706  rm -rf seanalytics-fix0811
 5707  git remote -all
 5708  git remote -b
 5709  git branch -d 212558390-development
 5710  git branch -D 212558390-development
 5711  git branch -D 
 5712  git branch -D 212558390-feat/activity_detection
 5713  git branch -d fix/update-github-from-bitbuckets
 5714  git add commit -m 'Add pickles and config file for MSK040'
 5715  git commit -m 'Add pickles and config file for MSK040'
 5716  vim seanalytics/__main__.py
 5717  vim seanalytics/config_files/MSK040_floor_detection.ini
 5718  git commit -m 'Update pickle for MSK040'
 5719  git rm seanalytics/config_files/floor_detection_dev.ini
 5720  git rm seanalytics/models/floor_detection_gmm_n3_MSK009_20170504_d1.pkl
 5721  git rm seanalytics/models/floor_detection_gmm_n3_MSK070_20170717_d1.pkl
 5722  git commit -m 'Remove unnecessary file to avoid the confusion '
 5723  git push -u origin feat/floor
 5724  rm temp_download_comp_aligned.py
 5725  git clone git@github.build.ge.com:IndustrialDataScience/SchindlerEdgeAnalytics.git seanalytics-0816
 5726  cd seanalytics-0816
 5727  git branch feat/floor-detection
 5728  git revert merge
 5729  git reset merge
 5730  git add seanalytics/preprocessing/*
 5731  git commit -m 'Merge from development'
 5732  git checkout feat/floor
 5733  cd ../seanalytics-0816
 5734  git checkout feat/floor-detection
 5735  git rm tests/data/input_sample/MSK005_20170504_100_20170504_1200_30ms.csv
 5736  git reset git reset HEAD~  
 5737  l tests/data/input_sample
 5738  git commit -c ORIG_HEAD
 5739  git fetch origin
 5740  git checkout -b newalgo/anomaly origin/newalgo/anomaly
 5741  git merge development
 5742  vim seanalytics/algorithms/sub_event_counter.py
 5743  git add seanalytics/algorithms/sub_event_counter.py
 5744  git commit -m 'Add docstring'
 5745  git merge --no-ff newalgo/anomaly
 5746  git upstream development
 5747  git pull development
 5748  git pull origin development
 5749  git commit -m 'Update gitignore'
 5750  rm -rf .idea
 5751  git rm .idea/*
 5752  git rm .idea/
 5753  git rm -r .idea/
 5754  git commit -m 'Remove .idea/'
 5755  git push origin development
 5756  git pull upstream
 5757  git pull upstream development
 5758  git remote add upstream git@github.build.ge.com:IndustrialDataScience/SchindlerEdgeAnalytics.git
 5759  git checkout upstream/master
 5760  cat drAccZ_model_standalone.py
 5761  head drAccZ_model_standalone.py
 5762  git checkout development
 5763  rm seanalytics-0816
 5764  git clone git@github.build.ge.com:IndustrialDataScience/SchindlerEdgeAnalytics.git seanalytics-0817
 5765  cd seanalytics-0817
 5766  git brach -v
 5767  git checkout trip_duration_improvement
 5768  git checkout trip_duration_improvement bugfix/trip_duration_improvement
 5769  git checkout -b trip_duration_improvement bugfix/trip_duration_improvement
 5770  git checkout -b bugfix/trip_duration_improvement origin/trip_duration_improvement
 5771  vim SchindlerEdgeAnalytics/seanalytics/algorithms/trip_distance_calculator.py
 5772  git commit -m 'Change Event_Stop_Time into Event_End_Time'
 5773  git push origin bugfix/trip_duration_improvement
 5774  rm -rf seanalytics-0816
 5775  rm -rf seanalytics-0817
 5776  git master
 5777  git checkout master 
 5778  git fetch upstream
 5779  git filter-branch -h
 5780  man git filter-branch
 5781  tar -czf seanalytics.tar.gz seanalytics
 5782  mv seanalytics.tar.gz ~/Box/GE\ Digital\ Data\ Science\ 2017/10\ Schindler\ Personal/Backup/
 5783  z hello
 5784  ls -la
 5785  subl .zlogin
 5786  vim .zlogin
 5787  code .
 5788  exec zsh
 5789  code
 5790  z 
 5791  code ~/.zshrc
 5792  subl
 5793  time zsh -i -c "print -n"
 5794  zsh -i -c -x exit.
 5795  atom .
 5796  code -h
 5797  code --install-extensions
 5798  zsh -i -c -x exit
 5799  git branch- v
 5800  git pull origin master 
 5801  git remote rename origin bitbucket
 5802  git remote add github https://github.build.ge.com/IndustrialDataScience/SchindlerEdgeAnalytics
 5803  cp ./SchindlerEdgeAnalytics/seanalytics/algorithms/trip_distance_calculator.py ../../../../Mount/remote_centos_hiro/hiro/python/seanalytics/seanalytics/algorithms/trip_distance_calculator.py
 5804  cp ./SchindlerEdgeAnalytics/seanalytics/algorithms/tripcounter.py ../../../../Mount/remote_centos_hiro/hiro/python/seanalytics/seanalytics/algorithms/tripcounter.py
 5805  cp ./SchindlerEdgeAnalytics/seanalytics/config_files/tripcounter.ini ../../../../Mount/remote_centos_hiro/hiro/python/seanalytics/seanalytics/config_files/tripcounter.ini
 5806  git push github development
 5807  cd Projects/sea
 5808  git rev-list
 5809  git rev-list --objects 
 5810  git rev-list --objects --all
 5811  git verify-pack -v .git/objects/pack/*.idx 
 5812  git verify-pack -v .git/objects/pack/*.idx | sort -k 3 -n | tail -10 | awk '{print$1}'
 5813  git fetch github 
 5814  git checkout -b fix/update-github-by-bitbucket
 5815  git branch -v -a
 5816  git checkout -b fix/update-github-by-bitbucket remotes/github/development
 5817  git commit -m 'Resolve the confilix by rebase'
 5818  for d ($fpath); do\n    f="$d/VCS_INFO_get_data_git"\n    if [[ -f "$f" ]]; then\n        command shasum "$f"\n    fi\ndone
 5819  chsh -s /usr/local/bin/zsh
 5820  echo $ZSH_VERSION
 5821  ls /etc/shells| grep zsh
 5822  mail
 5823  cron -l
 5824  crontab -l
 5825  crontab -e
 5826  locate zsh
 5827  locate zsh | bin
 5828  locate zsh | grep bin
 5829  chsh -s  /usr/local/bin/zsh-5.2 --version
 5830  chsh -s  /usr/local/bin/zsh-5.2 
 5831  /usr/local/Cellar/zsh/5.2/bin --version
 5832  sudo /usr/local/Cellar/zsh/5.2/bin --version
 5833  sudo /usr/local/Cellar/zsh/5.2/bin/zsh --version
 5834  chsh -s /usr/local/Cellar/zsh/5.2/bin/zsh
 5835  brew uninstall zsh zsh-completions
 5836  brew uninstall zsh 
 5837  brew install zsh 
 5838  chsh -s $(homebrew --prefix)/bin/zsh
 5839  homebrew
 5840  homebrew --prefix
 5841  brew --prefix
 5842  chsh -s $(brew --prefix)/bin/zsh
 5843  zsh --version
 5844  /usr/local/opt/python/bin/python -m pip install pylint
 5845  git checkout fix/update-github-by-bitbucket 
 5846  git merge GED --allow-unrelated-histories
 5847  vim .git
 5848  git re
 5849  git README.md
 5850  git add README.md
 5851  git commit -m 'Update github by bitbuckets on AUg21'
 5852  git commit -h
 5853  git commit -e
 5854  git commit -c
 5855  git commit -C 'Update github by bitbucket on Aug 21'
 5856  git commit -C 4eb5f9d
 5857  git push github git push github fix/update-github-by-bitbucket
 5858  git push github fix/update-github-by-bitbucket
 5859  git rm msk/tests_msk/small_datasets/20000_lines_MSK001_SN01_01_x_y_z.csv
 5860  git merge -s SchindlerEdgeAnalytics  --no-commit
 5861  rsync -chavzP --stats sch_centos_hiro:hiro/python/seanalytics ~/Projects
 5862  cd Projects/seanalytics
 5863  rm tests/data/output_sample/.*
 5864  git add seanalytics/algorithms/tr*
 5865  git add seanalytics/config_files/tri*
 5866  git commit -m 'Update trip counter'
 5867  git add tests/floor_detection_test.py
 5868  git commit -m 'Move __main__.py into floor_detection_test.py'
 5869  git pull feat/floor
 5870  git pull origin feat/floor
 5871  git push origin feat/floor
 5872  git checkout feat/dev-floor-detection
 5873  rm tests/data/input_sample/MSK005_20170504_0000_20170505_0000_distance_travelled.csv
 5874  git rm tests/data/input_sample/MSK005_20170504_0000_20170505_0000_distance_travelled.csv
 5875  git commit -m 'Remove large file'
 5876  java -jar bfg.jar
 5877  git clone --mirror git://example.com/some-big-repo.git
 5878  git clone https://github.com/rtyley/bfg-repo-cleaner.git
 5879  mv ~/Downloads/bfg-1.12.15.jar ~/.local/bin/bfg-1.12.15.jar
 5880  java -jar ~/.local/bin/bfg-1.12.15.jar --strip-blobs-bigger-than 100M ./seanalytics/.git
 5881  java -jar ~/.local/bin/bfg-1.12.15.jar --delete-folders .git --delete-files .git --no-blob-protection ./seanalytics/.git
 5882  git push origin feat/dev-floor-detection
 5883  git remotes
 5884  git remote -v -a
 5885  git branch -a 
 5886  git branch -d fix/update-github-by-bitbucket
 5887  git branch -D fix/update-github-by-bitbucket
 5888  git  checkout -b feat/floor-detection-caRefPa
 5889  git checkout feat/floor-detection-caRefPa
 5890  rsync -chavznP --stats ./Bitbucket-SchindlerEdgeAnalytics/ sch_centos_hiro:hiro/python/Bitbucket-SchindlerEdgeAnalytics/
 5891  /Users/212339410/anaconda/envs/sch27/bin/python /Users/212339410/Mount/remote_centos_hiro/hiro/python/Bitbucket-SchindlerEdgeAnalytics/SchindlerEdgeAnalytics/seanalytics/algorithms/floor_detection_ref.py
 5892  print 'test'
 5893  cf login -a https://api.system.aws-jp01-pr.ice.predix.io
 5894  cf login -a https://api.system.aws-jp01-pr.ice.predix.io -l hiroaki.shioi@ge.com
 5895  cf log --recent
 5896  cf logs nec-data-manager --recent
 5897  cf logs nec-data-manager --recent > error_log.txt
 5898  ssh solcon@82.165.75.33
 5899  git stash -h
 5900  cd Mount/remote_centos_hiro/hiro/python/Bitbucket-SchindlerEdgeAnalytics
 5901  cd Apps/nec-dashboard-views/
 5902  conda23
 5903  mount_centos_hiro 
 5904  cd ~/Projects/Bitbucket-SchindlerEdgeAnalytics
 5905  git branch 
 5906  git remote get-url origin
 5907  git pull bitbucket
 5908  jupyter notebook --NotebookApp.iopub_data_rate_limit=10000000000 --port 9001
 5909  jupyter notebook --NotebookApp.iopub_data_rate_limit=10000000000 --port 9001  --no-browser
 5910  jupyter notebook --no-browser --port=9001 --Notebook.iopub_data_rate_limit=1.0e10
 5911  cd
 5912  cd /Users/212339410/anaconda2/envs/py35/lib/python3.5/site-packages/tstk/visualization/plot.py
 5913  vim /Users/212339410/anaconda2/envs/py35/lib/python3.5/site-packages/tstk/visualization/plot.py
 5914  pip install bson
 5915  python application.py 
 5916  cd Projects/NEC-SCM/data
 5917  vim sample_output_bymilestone.txt
 5918  git add .gitignore 
 5919  git commit -m 'Add .vscode in .gitignore'
 5920  git commit -m 'Add /keymilestones endpoint'
 5921  l data/
 5922  jupyter notebook --no-browser --port=9001 --Notebook.iopub_data_rate_limit=1.0e12
 5923  git checkout notebooks/Gantt\ Chart\ for\ Daily\ Progress\ Report\ India.ipynb
 5924  git checkout notebooks/NEC_India_CA_Exploratory_Analysis.ipynb
 5925  git commit -m 'Resolve conflicts'
 5926  git update-index --assume-unchanged ./notebooks/geoviews-examples
 5927  git update-index --assume-unchanged ./notebooks/geoviews-examples/
 5928  git update-index --assume-unchanged ./notebooks/geoviews-examples/*
 5929  git add data/
 5930  git rm data/Master\ -\ Progress\ NEC\ Indosat\ -\ Progress\ Master\ Tab.csv
 5931  git rm data/Master\ -\ Progress\ NEC\ Indosat\ -\ Progress\ Master\ Tab_nobomb.csv
 5932  git rm data/sample_output_bymilestone.txt
 5933  git reset HEAD data/sample_output_bymilestone.txt
 5934  git reset HEAD data/Master - Progress NEC Indosat - Progress Master Tab_nobomb.csv
 5935  git reset HEAD data/Master - Progress NEC Indosat - Progress Master Tab.csv
 5936  git reset HEAD 'data/Master - Progress NEC Indosat - Progress Master Tab.csv'
 5937  git reset HEAD 'data/Master - Progress NEC Indosat - Progress Master Tab_nobomb.csv'
 5938  git commit -m 'Add csv used in notebook'
 5939  jupyter notebook --no-browser --port=9001 --Notebook.iopub_data_rate_limit=100000000000000
 5940  git commit -m 'Update .gitignore'
 5941  git commit -m 'Convert tstk function into plotly trace'
 5942  git commit -m 'Remove comments'
 5943  export FLASK_DUBUG=1
 5944  git commit -m 'Create endpoint /keymilestones and keymilestones-circle, keymilestones-hopid to filter by hopid and circle'
 5945  git commit -m 'Remove scatter plots along with coords_points'
 5946  git commit -m 'Refactor the code and remove print etc.'
 5947  cf push -h
 5948  cd Apps/nec-dashboard-view
 5949  cf login -a https://api.system.aws-jp01-pr.ice.predix.io -u hiroaki.shioi@ge.com
 5950  cf apps 
 5951  cd ../nec-dashboard-view3
 5952  cd ../nec-dashboard-view
 5953  cd ../nec-dashboard-views
 5954  git add Apps/nec-dashboard-views/*
 5955  git add -m 'Avoid to use seaborn, which cannot be imported'
 5956  git commit  -m 'Avoid to use seaborn, which cannot be imported'
 5957  rm scope_by_circle.html
 5958  git commit -m 'Delete import datetime to avoid the confusion with from datetime import datetime'
 5959  git rm scope_by_circle.html
 5960  git commit -m 'Delete unnecessary file'
 5961  cd da
 5962  vim sample_output_keymilestones.json
 5963  vim sample_output_keymilestones_circle_delhi.json
 5964  vim sample_output_keymilestones_hopid.json
 5965  mv sample_output_keymilestones_hopid.json sample_output_keymilestones_hopid_I-AP-AVNG-ENB-6000-I-AP-NGYL-ENB-I00wq.json
 5966  /Users/212339410/anaconda2/envs/py35/bin/python /Users/212339410/Box/Analytics/Projects/NEC-SCM/Apps/nec-dashboard-views/application.py
 5967  _fusermount -u ~/Mount/s3-ge-schindler-ds
 5968  fusermount
 5969  umount -f ~/Mount/s3
 5970  umount
 5971  umoun -l
 5972  umount -l
 5973  umount -a
 5974  diskutil unmount ~/Mount/remote_centos_rohit
 5975  diskutil unmount ~/Mount/remote_centos_hiro
 5976  cf target -s
 5977  cf login -a 
 5978  cf log -R
 5979  cf logs -R
 5980  cf logs --recent
 5981  cf logs nec-scm-db-view-0825 
 5982  cf logs nec-scm-db-view-0825
 5983  cf logs -h
 5984  cf logs nec-scm-db-view-0825 --recent 100000
 5985  cd FromPredixBasic/Aligned
 5986  head MSK050_20170815_1007_20170816_0000_30ms.csv.gz| csvlook
 5987  head MSK050_20170815_1007_20170816_0000_30ms.csv.gz| csvcut
 5988  head MSK050_20170815_1007_20170816_0000_30ms.csv.gz
 5989  csvlook
 5990  gzip -cd  MSK050_20170815_1007_20170816_0000_30ms.csv.gz | head | csvlook
 5991  gzip -cd  MSK050_20170815_1007_20170816_0000_30ms.csv.gz | head -1000 | csvlook
 5992  gzip -cd  MSK050_20170815_1007_20170816_0000_30ms.csv.gz | tail -1000 | csvlook
 5993  gzip -cd  MSK057_20170712_0000_20170712_1739_30ms.csv  | head  | csvlook
 5994  head MSK057_20170711_0000_20170712_0000_30ms.csv| csvlook
 5995  ls MSK058
 5996  ls | grep MSK059
 5997  ls | grep MSK050 \| MSK053
 5998  ls  grep MSK050 \| MSK053
 5999  ls . |  grep MSK050 \| MSK053
 6000  ls  |  grep 'MSK050 \| MSK053'
 6001  ls  |  grep 'MSK050.* \| MSK053.*'
 6002  ls | grep MSK057
 6003  cd Projects/NEC-SCM/notebooks
 6004  python export FLASK_APP=application.py
 6005  cf loging
 6006  cf target
 6007  cf target -s dev
 6008  cf logs nec-scm-db-view-0825 --recent
 6009  git commit -m 'Update progressbyweek and progressbymilestone using db view'
 6010  ssh-add
 6011  ssh-add -h
 6012  cd FromPredixBasic/Aligned/
 6013  ls | grep MSK050
 6014  git stash pop
 6015  git diff nec-dashboard-views/application.py
 6016  git commit -n 'Change the gantt chart to drive from DB views '
 6017  git commit -m 'Change the gantt chart to drive from DB views '
 6018  git commit -m "Remove commented code and refactor"
 6019  git commit -m 'Refactor the code'
 6020  python -f Manifest.yml
 6021  git add Apps/nec-dashboard-views/Manifest.yml
 6022  git commit -
 6023  cd Projects/NEC-SCM/Apps/nec-dashboard-view
 6024  cal
 6025  man cal
 6026  rsync -chavzP --stats ./Bitbucket-SchindlerEdgeAnalytics/ sch_centos_hiro:hiro/python/Bitbucket-SchindlerEdgeAnalytics/
 6027  rsyncl -chavzP --stats ./Bitbucket-SchindlerEdgeAnalytics/ sch_centos_hiro:hiro/python/Bitbucket-SchindlerEdgeAnalytics/
 6028  rsync -chavzP --stats sch_centos_hiro:SchindlerEdgeAnalytics  ~/Projects/SchindlerEdgeAnalytics
 6029  rsync -chavzP --stats sch_centos_hiro:./hiro/python/Bitbucket-SchindlerEdgeAnalytics  ~/Projects/SchindlerEdgeAnalytics
 6030  cd Projects/Schindler
 6031  cd Projects/SchindlerEdgeAnalytics
 6032  cp -r SchindlerEdgeAnalytics/Bitbucket-SchindlerEdgeAnalytics Bitbucket-SchindlerEdgeAnalytics 
 6033  git clone git@github.build.ge.com:IndustrialDataScience/Bitbucket-SchindlerEdgeAnalytics.git 
 6034  git remote add bitbucket git clone git@bitbucket.org:ioeeschindler/modularsensorkit.git
 6035  git remote add bitbucket git@bitbucket.org:ioeeschindler/modularsensorkit.git
 6036  git remote set-url github git@github.build.ge.com:IndustrialDataScience/Bitbucket-SchindlerEdgeAnalytics.git
 6037  git remote add  github git@github.build.ge.com:IndustrialDataScience/Bitbucket-SchindlerEdgeAnalytics.git
 6038  git pull bitbucket 
 6039  git push github
 6040  git push github remotes/bitbucket/develop 
 6041  java -jar ~/.local/bin/bfg-1.12.15.jar --strip-blobs-bigger-than 50M `pwd`
 6042  java -jar ~/.local/bin/bfg-1.12.15.jar --strip-blobs-bigger-than 100M ~/Projects/Bitbucket-SchindlerEdgeAnalytics/.git
 6043  git push github 
 6044  git push github bitbucket/GED
 6045  git push github GED
 6046  git commit -m 'Change the endpoint to receive the parameters e.g., ca_id, customer_id, project_id and pass to db'
 6047  git add Apps/nec-dashboard-views/application.py
 6048  git commit -m 'Resolve the conflict'
 6049  gitcheckout bitbucket/GED
 6050  git checkout bitbucket/GED
 6051  git checkout bitbucket/master
 6052  git checkout bitbucket/develop
 6053  git clone git@github.build.ge.com:IndustrialDataScience/SchindlerMSK.git github-Seanalytics
 6054  rm -rf github-Seanalytics
 6055  git clone git@github.build.ge.com:IndustrialDataScience/SchindlerEdgeAnalytics.git
 6056  git clone git@github.build.ge.com:IndustrialDataScience/SchindlerEdgeAnalytics.git github-Seanalytics
 6057  git branch -av
 6058  git checkout -b feat/floor-detection
 6059  cp ../Bitbucket-SchindlerEdgeAnalytics/SchindlerEdgeAnalytics/seanalytics/algorithms/floor_detection.py ./seanalytics/algorithms/floor_detection.py
 6060  vim ./seanalytics/algorithms/floor_detection.py
 6061  cp ../Bitbucket-SchindlerEdgeAnalytics/SchindlerEdgeAnalytics/seanalytics/config_files/MSK053_floor_detection.ini ./seanalytics/config_files/MSK053_floor_detection.ini
 6062  cp ../Bitbucket-SchindlerEdgeAnalytics/SchindlerEdgeAnalytics/seanalytics/models/floor_detection_ref_kmeans_n5_MSK053_20170815_d9.pkl ./seanalytics/models/floor_detection_ref_kmeans_n5_MSK053_20170815_d9.pkl
 6063  vim seanalytics/config_files/MSK053_floor_detection.ini
 6064  git add seanalytics/config_files/MSK053_floor_detection.ini
 6065  git add seanalytics/models/floor_detection_ref_kmeans_n5_MSK053_20170815_d9.pkl
 6066  git commit -m 'Add model and updated config file for MSK053'
 6067  git push origin feat/floor-detection
 6068  pip install hmmlearn
 6069  git checkout -b sync/clean-up
 6070  rm -rf seanalytics 
 6071  rm -rf tests
 6072  git commit -m 'Remove all files under seanalaytics and tests dir'
 6073  git push sync/clean-up
 6074  cd ../Bitbucket-SchindlerEdgeAnalytics
 6075  git remote -va
 6076  git branch -va
 6077  git commit -m 'Sync seanalytics and tests dir from bitbucket develop branch on 09/05'
 6078  ls 
 6079  vim seanalytics/algorithms/floor_detection.py
 6080  git checkout bitbucket/feat/floor-detection-caRefPa
 6081  ls SchindlerEdgeAnalytics/tests/data/input_sample/
 6082  cp SchindlerEdgeAnalytics/tests/data/input_sample/MSK053_20170815_1422_20170815_1427_30ms.csv.gz ../github-Seanalytics/tests/data/input_sample/MSK053_20170815_1422_20170815_1427_30ms.csv.gz
 6083  python seanalytics/algorithms/floor_detection.py
 6084  git add tests/data/input_sample/*
 6085  git add seanalytics/algorithms/floor_detection.py
 6086  git commit -m 'Modify the floor detection __main__ for testing and add input_sample for PDS'
 6087  cd ~/Projects/NEC-SCM/Apps/nec-dashboard-views
 6088  git commit -m 'Add try-except and revert the default value from "" to None for req args'
 6089  git commit -m 'Debug try-except and revert None to ""'
 6090  git commit -m 'reformat the code'
 6091  cf delete app
 6092  cf delete nec-scm-db-view-0825
 6093  git add nec-dashboard-views/application.py
 6094  git commit -m 'Debug for exception handling '
 6095  git push master
 6096  cd ../../Bitbucket-SchindlerEdgeAnalytics
 6097  git checkout -b temp/floor-detection-visual-groundtruth
 6098  mkdir script 
 6099  cp ../SchindlerMSK/development/Pythonh/floor_detection_visual_grount_truth.py ./script
 6100  l script
 6101  git add script/
 6102  git commit -m 'Create the temporary script folder for PDS'
 6103  git push orgin temp/floor-detection-visual-groundtruth
 6104  git push temp/floor-detection-visual-groundtruth
 6105  cd Projects/NEC-SCM/Apps
 6106  FLASK_APP=application.py
 6107  git diff application.py
 6108  git commit -m 'Copy /milestonesbyhop from view1 to views2'
 6109  umount 
 6110  umount ~/Mount/remote_centos_hiro
 6111  cd Projects/Bitbucket-SchindlerEdgeAnalytics
 6112  git branch 0a
 6113  git branch -d 0a
 6114  rsync -chavzP --stats sch_centos_hiro:./hiro/python/Bitbucket-SchindlerEdgeAnalytics  ~/Projects/Bitbucket-SchindlerEdgeAnalytics
 6115  git commit -m 'Remove debug purpose statement'
 6116  git commit -m 'Make today variable is dynamic'
 6117  git commit -m 'Remove debug purpose print'
 6118  git stash apply(stash@{0})
 6119  git stash apply stash@{0}
 6120  git add app
 6121  git diff f6ab68dc01e82368253d3486dec5b9edbf15cc43
 6122  git git log
 6123  git show a13d19bd01aa285e320170503b86352bf9d2c9ca
 6124  git commit --amend a13d19bd01aa285e320170503b86352bf9d2c9ca
 6125  git rebase -i 0abbed7ef492d2b53ee77496fcb4edc1f736a94d
 6126  git commit --amend -m 'Display the hop based on the total length of work orders'
 6127  git reflog
 6128  git logs
 6129  git rebase -continue
 6130  git commit -m 'Optimize module'
 6131  git commit -m 'Merge the changes'
 6132  git show 0abbed7ef492d2b53ee77496fcb4edc1f736a94d
 6133  git show 1d1b5c3e0004687aac2b65ea124b9ec83ff4f065
 6134  git stash apply stash@{2}
 6135  git commit -m 'Re-commit the change for 1d1b5c3e0004687aac2b65ea124b9ec83ff4f065'
 6136  cd NEC-SCM/Apps
 6137  cd Projects/NEC-SCM/Apps/nec-dashboard-views
 6138  cd Apps/nec-dashboard-views
 6139  df 
 6140  git commit -m '/progressbyweek: return the drop down list as list_workorder'
 6141  git commit -m 'Use dbAppUrl for db URL'
 6142  python -m flask run
 6143  git commit -m 'Add decorator for before_request and after_request to measure the time for each function'
 6144  cf logs nec-scm-db-view1
 6145  https://help.plot.ly/date-format-and-time-series/
 6146  git commit -m 'Add argument return_empty() function to dynamically align with the actual visualization'
 6147  git commit -m 'Remove dubug print and cosmetic change'
 6148  git add ../nec-dashboard-view2/application.py
 6149  git commit -m 'Update /milestonesbyhop in view2'
 6150  cd nec-dashboard-view2/
 6151  cf log nec-scm-db-view2 --recent
 6152  python PCA-T2/pca_T2.py
 6153  python application.py --port 9001
 6154  python -m flask run -h'
 6155  python -m flask run -h
 6156  python -m flask run -port 8009
 6157  git commit -m 'add load_data() function in view2'
 6158  z /Users/212339410/Box/Analytics/Projects/NEC-SCM/Apps/nec-dashboard-view2
 6159  python -m flask run -p 8009
 6160  z view2
 6161  git add ../nec-dashboard-views/application.py
 6162  git commit -m 'Remove /milestonesbyhop from view1'
 6163  git commit -m 'Change the color palette'
 6164  z /Users/212339410/Box/Analytics/Projects/NEC-SCM/Apps/nec-dashboard-view
 6165  git commit -m 'Remove background gray area using cufflinks iplot'
 6166  git commit -m 'Reformat'
 6167  cf logs nec-scm-db-view --recent
 6168  mssql -s alpidcdiha001v.cloud.ge.com -u 'dssb' -p 'b@nklak3_123' -d 'SB'
 6169  z /Users/212339410/Box/Analytics/Projects/NEC-SCM/Apps/nec-dashboard-views
 6170  cd nec-dashboard-view
 6171  cd nec-dashboard-views
 6172  rm nec-dashboard-views
 6173  rm -rf nec-dashboard-views
 6174  mssql -s alpidcdiha001v.cloud.ge.com -u 'sa' -p 'datalake123' -d 'ATI_Data_Analysis' -e -t 20000
 6175  echo ZnJvbnRfYXBwX2NpZDp1djA5MDN2YWlwdng | base64 --decode 
 6176  echo ZnJvbnRfYXBwX2NpZDp1djA5MDN2YWlwdng=  | base64 --decode` 
 6177  echo ZnJvbnRfYXBwX2NpZDp1djA5MDN2YWlwdng=  | base64 --decode`
 6178  echo ZnJvbnRfYXBwX2NpZDp1djA5MDN2YWlwdng=  | base64 --decode
 6179  uaac 
 6180  uaac targets
 6181  uaac target d32c6e0e-0932-4f33-b0d9-74167fd0a90f.predix-uaa.run.aws-jp01-pr.ice.predix.io
 6182  uaac clients
 6183  uaac token client get admin -s uv0903vaip
 6184  echo ZnJvbnRfYXBwX2NpZDp1djA5MDN2YWlwdng | base64 --decode
 6185  uaac target predix-uaa.run.aws-jp01-pr.ice.predix.io
 6186  uaac contexts
 6187  uaac token client get admin -s ginjqzwr3z95
 6188  git clone git@github.build.ge.com:IndustrialDataScience/NEC-SCM-Apps.git
 6189  sshi hiro
 6190  pip install httplib2
 6191  cd NEC-SCM-Apps/
 6192  cat
 6193  bind '"^[b": backward-word'
 6194  l /Applications
 6195  open /Applications/Sublime\ Text.app/Contents/SharedSupport/bin/subl
 6196  /Applications/Sublime\ Text.app/Contents/SharedSupport/bin/subl; exit;
 6197  ln -s /Applications/Sublime\ Text.app/Contents/SharedSupport/bin/subl /usr/local/bin/subl
 6198  subl 
 6199  z views
 6200  cd NEC-SCM-Apps
 6201  git commit -m 'Change the request for auth on /progressmiles and /progresbyweek'
 6202  git add nec-dashboard-view1/application.py
 6203  git commit -m 'Merge conflict'
 6204  git show diff
 6205  git commit -m 'Remove debut print'
 6206  git add ../
 6207  git commit -m 'Modified load_data for auth'
 6208  locate syslog
 6209  locate syslog | grep goo
 6210  cat ~/.zshrc | grep mount_s3_sch
 6211  locate syslog | goofys
 6212  locate syslog | grep goofys
 6213  goofys ge-schindler-ds /Users/212339410/Mount/s3-ge-schindler-ds -o volname=s3-ge-schindler-ds --debug_fuse --debug_s3
 6214  goofys --debug_fuse
 6215  goofys --debug_s3
 6216  goofys --debug_s3  ge-schindler-ds /Users/212339410/Mount/s3-ge-schindler-ds -o volname=s3-ge-schindler-ds
 6217  goofys --debug_fuse  ge-schindler-ds /Users/212339410/Mount/s3-ge-schindler-ds -o volname=s3-ge-schindler-ds
 6218  goofys --region 
 6219  aws help
 6220  aws s3 
 6221  aws s3  help
 6222  aws configure help
 6223  aws configure list
 6224  goosys --version
 6225  goofys --version
 6226  brew outdated
 6227  z /Users/212339410/Box/Analytics/Projects/NEC-SCM-Apps 
 6228  cd Apps/nec-dashboard-view1
 6229  cf log nec-scm-db-view1 --recent
 6230  git commit -m 'Add exception handling for fillna and drop()'
 6231  z /view
 6232  man z
 6233  cd Projects/NEC-SCM-Apps/Apps/nec-dashboard-view1
 6234  git add Apps/nec-dashboard-view1/application.py
 6235  git commit -m 'fix the double count for "Installation" work order in /progressbymilestones'
 6236  python application.py
 6237  git commit -m 'Add the script to sort the y-axis based on workordertype_relative_seq on /progressbymilestones'
 6238  cd Apps
 6239  cd nec-dashboard-view1
 6240  cf Apps
 6241  cf login -a https://api.system.aws-jp01-pr.ice.predix.io -u hiroaki.shioi@ge.com -s nec-dev
 6242  cf target 
 6243  cf login -a https://api.system.aws-jp01-pr.ice.predix.io -u hiroaki.shioi@ge.com 
 6244  git commit -m 'Remove double count for "Install-Commission" /progresbymistones and /progressbyweek'
 6245  git commit -m 'Fix double count and add nonnegative limit for yaxis for /progressbyweek '
 6246  git push
 6247  cd ../NEC-SCM-Apps/Apps/nec-dashboard-view1
 6248  cf login -a https://api.system.aws-jp01-pr.ice.predix.io -u hiroaki.shioi@ge.com -s dev
 6249  cd ../NEC-SCM-Apps/Apps/nec-dashboard-view2
 6250  anaconda2
 6251  z /Users/212339410/Box/Analytics/Projects/NEC-SCM-Apps/Apps/nec-dashboard-view1
 6252  git commit -m 'Fix the bug to show the given number of hops in /milestonesbyhop'
 6253  cd Projects/NEC-SCM-Apps/Apps/nec-dashboard-view2
 6254  cd ../NEC-SCM-Apps
 6255  git checkout -b bugfix/hover-milstonesbyhop
 6256  git stash apply 
 6257  git commit -m 'workaround for removing hover x,y cordinates and return duration for each workorder'
 6258  git push origin bugfix/hover-milstonesbyhop
 6259  cd Pro
 6260  cd Bitbucket-SchindlerEdgeAnalytics
 6261  git rm -rf ./ 
 6262  git commit -m 'remove all files for clean-up'
 6263  git remove -v
 6264  git push origi sync/clean-up
 6265  cp modularsensorkit ./github-Seanalytics
 6266  cp -r modularsensorkit ./github-Seanalytics
 6267  cd seanalytics
 6268  cd algorithms
 6269  cd github-Seanalytics
 6270  rm -rf seanalytics
 6271  git add modularsensorkit/SchindlerEdgeAnalytics/*
 6272  git add modularsensorkit/TESTDATA/*
 6273  git commit -m 'Update github based on bitbucket on 0922'
 6274  git push origin sync/clean-up
 6275  cd modularsensorkit/
 6276  scp sch_centos_hiro:SchindlerEdgeAnalytics/script ../../
 6277  scp -r sch_centos_hiro:SchindlerEdgeAnalytics/script ../../
 6278  git checkout temp/floor-detection-visual-groundtruth
 6279  cd script
 6280  git rm floor_detection_visual_grount_truth.py
 6281  git add floor_detection_visual_ground_truth.py
 6282  git commit -m 'update floor detection visual ground truth script'
 6283  git push origin temp/floor-detection-visual-groundtruth
 6284  rsync -chavzP --stats sch_centos_hiro:./hiro/python/Bitbucket-SchindlerEdgeAnalytics  ~/Projects/
 6285  conda envs list
 6286  vim .gitignore_global 
 6287  git config --global core.excludesfile ~/.gitignore
 6288  git remote remove github
 6289  git branch -r
 6290  git ls-remote -h
 6291  rm -rf SchindlerEdgeAnalytics
 6292  rm -rf TESTDATA
 6293  rm -rf modularsensorkit
 6294  vim .gitignore_global
 6295  git checkout GED -- TESTDATA
 6296  git commit -m 'Update based on GED branch on bitbucket on 20170927'
 6297  git checkout GED -- MSKAlgorithmStackDeployment
 6298  git checkout GED -- HW03AutoConfiguration MSKDataLiveCheck MSKAlgorithmStackDeployment MSKSubscriber SchindlerEdgeAnalytics TESTDATA
 6299  git checkout GED -- README.md
 6300  git commit -m 'Add other directories'
 6301  git branch -v
 6302  git push -d github  sync/clean-up
 6303  git push -d github  development
 6304  cd Apps/nec-dashboard-view2
 6305  export FLASK_APP=application.py
 6306  cd ~/Projects/NEC-SCM-Apps/Apps/nec-dashboard-view1
 6307  z /Users/212339410/Box/Analytics/Projects/NEC-SCM-Apps/Apps/nec-dashboard-view2
 6308  cf logs nec-scm-db-view1 --recent
 6309  git push origin 
 6310  git commit -m 'Change the marker size from 3 to 6 for /progressbyweek'
 6311  python -m flask run 
 6312  cd ../nec-dashboard-view2
 6313  cf logs nec-scm-db-view2 --recent
 6314  cd ../nec-dashboard-view1
 6315  git add application.py
 6316  git commit -m 'Fix the number of shown HOP for /milestonesbyhop'
 6317  git add ../nec-dashboard-view1/application.py 
 6318  git commit -m 'Change cosmetic (add W in x-axes and change the title)'
 6319  git commit -m 'Remove dubug print'
 6320  cd Projects/NEC-SCM-Apps
 6321  ssh sch_bastion 'ssh -i  DataScience.pem  -N -n -L 127.0.0.1:8050:127.0.0.1:8050 centos@10.43.51.90 -Y  -o ServerAliveInterval=50'
 6322  git checkout -b github-mastser GED 
 6323  git push github github-master
 6324  git pull github master
 6325  git checkout github/master --track
 6326  git push github github-mastser
 6327  git checkout -b github-master github/master
 6328  git branch -d github-master
 6329  git branch -d github-mastser
 6330  git checkout -b github-master-old github/master
 6331  rm SchindlerEdgeAnalytics/seanalytics/config_files/accordion_model_sampled_scores_num_rows_10001_n_iter_100_datarows.csv
 6332  rm SchindlerEdgeAnalytics/seanalytics/config_files/regular_model_sampled_scores_num_rows_10001_n_iter_100_datarows.csv
 6333  rm SchindlerEdgeAnalytics/tests/data/input_sample/2017-09-14_12-38-04_MSK050_Floor_Detection_*
 6334  subl README.md
 6335  git mergetool
 6336  git checkout -b github-update GED
 6337  git push github github-update
 6338  git rebae --abort 
 6339  git rebase --abort 
 6340  git checkout GED -- SchindlerEdgeAnalytics
 6341  git comit -m 'Update github based on bitbucket on 20171002'
 6342  git commit -m 'Update github based on bitbucket on 20171002'
 6343  git branch -d github-update
 6344  git branch -D github-update
 6345  rsync -chavzP --stats sch_centos_hiro: ~/Box/GE\ Digital\ Data\ Science\ 2017/10\ Schindler\ Personal/Backup
 6346  cd Box/GE\ Digital\ Data\ Science\ 2017/10\ Schindler\ Personal
 6347  cd Backup
 6348  rm .DS_Store .TemporaryItems .Xauthority ._.TemporaryItems .aws .bash_history .bash_logout .bash_profile .bashrc .bashrc-anaconda2.bak .cache .conda .config .continuum .cufflinks .dbus .git-completion.bash .gitconfig .gitignore_global .idea .ipython .jupyter .lesshst .local .pki .plotly .ropeproject .ssh .vim .viminfo .vimrc .vimrc.old 
 6349  rm -r .* 
 6350  rm -rf anaconda2
 6351  ssh -i ~/.ssh/DataScience.pem -N -n -L 127.0.0.1:9181:127.0.0.1:9181 sch_bastion -Y -o ServerAliveInterval=50
 6352  pip install dash
 6353  pip install dash_html_components
 6354  pip install dash_core_components
 6355  pip install loremipsum
 6356  pip install rq_scheduler
 6357  pip install boto3
 6358  pip install botocore
 6359  ssh sch_bastion 'ssh -i  DataScience.pem  -N -n -L 127.0.0.1:9181:127.0.0.1:9181 centos@10.43.51.90 -Y  -o ServerAliveInterval=50'
 6360  ssh -i
 6361  type mount_s3_sch
 6362  goofys -jh
 6363  goofys --debug-s3
 6364  ps
 6365  goofys --debug-s3 
 6366  df
 6367  goofys -version
 6368  brew install upgrade goofys
 6369  cat ~/.viminfo
 6370  cat ~/.zsh-update
 6371  cat ~/.zsh_history
 6372  cd Mount/s3-ge-schindler-ds/
 6373  ls | grep MSK053
 6374  ssh 3.39.83.135 -a hiros
 6375  ssh 3 hiros@3.39.83.135
 6376  source root
 6377  gzip -cd  MSK053_20171011_0000_20171012_0000_30ms.csv.gz | head | csvlook
 6378  gzip -cd  MSK053_20171011_0000_20171012_0000_30ms.csv.gz | head -100 | csvlook 
 6379  l | grep MSK053
 6380  source ~/.ssh/config
 6381  cd git@github.com:h1r03/dotfiles.git
 6382  git git@github.com:h1r03/dotfiles.git
 6383  git clone git@github.com:h1r03/dotfiles.git -v
 6384  git clone https://github.com/h1r03/dotfiles.git
 6385  git config
 6386  git config --global
 6387  git config --global --get user.name
 6388  git config --global --get-all
 6389  git clone git@github.com:h1r03/dotfiles.git
 6390  ipconfig /flushdns
 6391  cat ~/.vimrc
 6392  ;
 6393  ping github.com
 6394  git config -l
 6395  nslookip www.amazon.com
 6396  nslookup www.amazon.com
 6397  nslookup www.google.com
 6398  nslookup www.github.com
 6399  aws config
 6400  aws config s3
 6401  aws configure s3
 6402  aws configure s3 list
 6403  aws list s3 
 6404  aws s3 configure
 6405  aws s3 configure ls
 6406  aws configure
 6407  cat ~/.zshrc | grep conda
 6408  scp ~/.ssh/DataScience.pem dss_server:
 6409  less ~/.aws/cco
 6410  less ~/.aws/credentials
 6411  less ~/.aws/config
 6412  openssl version
 6413  curl http://s3.amazonaws.com -v
 6414  cat ~/.ssh/co
 6415  ssh -i .ssh/DataScience.pem ubuntu@54.214.160.173
 6416  ping 54.214.160.173
 6417  less .ssh/co
 6418  ]
 6419  aws s3 ls
 6420  nc --proxy sjc1intproxy01.crd.ge.com:8080 %h %p
 6421  which nc
 6422  /usr/bin/nc --proxy sjc1intproxy01.crd.ge.com:8080 %h %p
 6423  /usr/bin/nc -x sjc1intproxy01.crd.ge.com:8080 %h %p
 6424  ssh -i .ssh/DataScience.pem ubuntu@54.214.160.173 -v
 6425  cd Projects/NEC-SCM
 6426  cat .zshrc | grep anaconda
 6427  cd Mount
 6428  ssh raspy01_ss
 6429  ssh raspy01_ss -v
 6430  nc -x sjc1intproxy01.crd.ge.com:8080 
 6431  mount_s3_sch 
 6432  rsync -chavzP --stats  ~/Projects/modularsensorkit/SchindlerEdgeAnalytics raspy01:home/pi/SchindlerEdgeAnalytics
 6433  scp -r ~/Mount/remote_centos_hiro/modularsensorkit/SchindlerEdgeAnalytics raspy01:home/pi/SchindlerEdgeAnalytics
 6434  scp -r ~/Mount/remote_centos_hiro/modularsensorkit/SchindlerEdgeAnalytics raspy01:home/pi/SchindlerEdgeAnalytics/
 6435  scp -r ~/Mount/remote_centos_hiro/modularsensorkit/SchindlerEdgeAnalytics raspy01:SchindlerEdgeAnalytics
 6436  cd /Users/212339410/Downloads/example/Datasets
 6437  cd "/Users/212339410/Downloads/example/Datasets"
 6438  conda2
 6439  sch27
 6440  mount_centos_b
 6441  l anaconda/envs
 6442  l anaconda2/envs
 6443  anaconda/bin/anaconda env --info
 6444  anaconda/bin/anaconda info --envs
 6445  anaconda/bin/conda info --envs
 6446  anaconda2/bin/conda info --envs
 6447  anaconda2/bin/conda env export 
 6448  anaconda2/bin/conda env export -n GEDS-conda
 6449  echo $HOME
 6450  l tmp
 6451  NOW=$(date "+%Y-%m-%d")
 6452  mkdir $HOME/tmp/envs-$NOW
 6453  mkdir $HOME/tmp/envs-$NOW -r
 6454  mkdir -r $HOME/tmp/envs-$NOW 
 6455  mkdir --help
 6456  mkdir -p $HOME/tmp/envs-$NOW 
 6457  echo $ENVS
 6458  for env in $ENVS; do\necho $env\ndone
 6459  for env in $ENVS; do\n    source activate $env\n    conda $env export > $HOME/tmp/envs-$NOW/$env.yml\n    echo "Exporting $env"\ndone
 6460  for env in $ENVS; do\n    source activate $env\n    conda env export > $HOME/tmp/envs-$NOW/$env.yml\n    echo "Exporting $env"\ndone
 6461  for env in $ENVS; do\n    source activate -n $env\n    conda env export > $HOME/tmp/envs-$NOW/$env.yml\n    echo "Exporting $env"\ndone
 6462  source activate GEDS-conda
 6463  echo $env
 6464  ENVS=$(conda env list | grep '^\w' | cut -d' ' -f1)
 6465  for env in $ENVS; do echo $env + "environemnt"; done
 6466  for env in $ENVS; do\n   echo $env;  source activate -n $env\n    conda env export > $HOME/tmp/envs-$NOW/$env.yml\n    echo "Exporting $env"\ndone
 6467  vim save-conda-envs.sh
 6468  bash save-conda-envs.sh
 6469  conda info env
 6470  conda info envs
 6471  conda env remove --name GEDS-conda
 6472  conda env remove --name opencv
 6473  conda env remove --name snowflakes
 6474  cp save-conda-envs.sh save-conda2-envs.sh
 6475  anaconda2/bin/conda
 6476  vim save-conda2-envs.sh
 6477  mkdir envs2-2017-10-26
 6478  bash save-conda2-envs.sh
 6479  anaconda2/bin/conda envs --info
 6480  ./anaconda2/bin/conda envs --info
 6481  ~/anaconda2/bin/conda envs --info
 6482  ~/anaconda2/bin/conda env --info
 6483  ~/anaconda2/bin/conda info envs
 6484  conda env remove --name python2
 6485  ~/anaconda2/bin/conda env remove --name python2
 6486  ~/anaconda2/bin/conda info --envs
 6487  ~/anaconda2/bin/conda env remove --name GED-conda
 6488  ~/anaconda2/bin/conda env remove --name GEDS-conda
 6489  conda env create --file ./tmp/envs2-2017-10-26/py35.yml
 6490  conda create --file ./tmp/envs2-2017-10-26/py35.yml
 6491  conda create -n py35 --file ./tmp/envs2-2017-10-26/py35.yml
 6492  conda create --file $HOME/tmp/envs2-2017-10-26/py35.yml
 6493  $HOME/anaconda/bin/conda-env -f $HOME/tmp/envs2-2017-10-26/py35.yml 
 6494  cp tmp/envs2-2017-10-26/py35.yml tmp/envs2-2017-10-26/py35.yml.bak
 6495  vim tmp/envs2-2017-10-26/py35.yml
 6496  $HOME/anaconda/bin/conda-env create -f $HOME/tmp/envs2-2017-10-26/py35.yml 
 6497  cd Projects/SchindlerMSK/development/Pythonh
 6498  sudo yum update git
 6499  /usr/local/git/uninstall.sh
 6500  git --version
 6501  sudo rm -rf /usr/bin/git
 6502  cd /usr/bin
 6503  ls -al | grep git
 6504  brew install git
 6505  locate git
 6506  locate git-2.14
 6507  locate git & 2.14
 6508  python ../annotation_tool_floor_detection.py
 6509  cd Projects/SchindlerMSK
 6510  cd development
 6511  cd Pythonh
 6512  cd /Users/212339410/Downloads/Annotation-Tool
 6513  cd ../Projects/SchindlerMSK/development
 6514  ln -s /Users/212339410/Projects/SchindlerMSK/development/Pythonh /Users/212339410/Python/Schindler-Python
 6515  cd annotation-tool
 6516  mkdir floor-detection 
 6517  mv annotation_tool_floor_detection.py floor-detection
 6518  mv -r input_data floor-detection
 6519  mv -f input_data floor-detection
 6520  mv output_data floor-detection
 6521  mv floor-detection floor
 6522  mkdir trip
 6523  cp ~/Downloads/Annotation-Tool/door_acceleration_visual_inspecter.py ./trip
 6524  cp ~/Downloads/Annotation-Tool/Datasets trip
 6525  cp ~/Downloads/Annotation-Tool/Datasets trip/
 6526  cp -r ~/Downloads/Annotation-Tool/Datasets trip/
 6527  mv annotation-tool annotation-tool-tmp
 6528  git clone git@github.build.ge.com:212339410/Annotation-Tool.git
 6529  cp annotation-tool-tmp/* Annotation-Tool/
 6530  cp -r annotation-tool-tmp/* Annotation-Tool/
 6531  rm -r annotation-tool-tmp
 6532  rm MSK090_20171020_0000_20171021_0000_30ms.csv.gz
 6533  cd trip
 6534  head MSK055_SN01_01_x_y_z.csv
 6535  head MSK055_eventstable.csv
 6536  head MSK055_eventstable2.csv
 6537  rm MSK055_eventstable2.csv
 6538  cat MSK055out.txt
 6539  rm *.txt
 6540  cd ../../../
 6541  mv trip door
 6542  ls ~/Python/Schindler-Python/Annotation-Tool/door/Datasets/MSK055/
 6543  cd -P ..
 6544  python door_acceleration_visual_inspecter.py
 6545  git commit -m 'Add documentation'
 6546  cd floor/input_data
 6547  gzip MSK090_20171019_0000_20171020_0000_30ms.csv.gz
 6548  gzip MSK090_20171019_0000_20171020_0000_30ms.csv.gz -d
 6549  brew csvkit
 6550  brew install csvkit
 6551  brew install csvtool 
 6552  sudo pip install csvkit
 6553  csvkit 
 6554  head csvlook 
 6555  csvlook -h
 6556  csvstat --count MSK090_20171019_0000_20171020_0000_30ms.csv
 6557  wc -l MSK090_20171019_0000_20171020_0000_30ms.csv
 6558  head -n 280002 > MSK090_20171019_0000_20171020_0000_30ms.csv
 6559  echo  mount_s3_sch
 6560  ~/Box/Analytics/Projects/SchindlerMSK/development/Pythonh/Annotation-Tool/floor/input_data
 6561  cat MSK090_20171019_0000_20171020_0000_30ms.csv
 6562  gz -d MSK090_20171019_0000_20171020_0000_30ms.csv
 6563  gzip -d MSK090_20171019_0000_20171020_0000_30ms.csv
 6564  man gzip
 6565  head -n 280000 MSK090_20171019_0000_20171020_0000_30ms.csv > MSK090_20171019_0000_20171020_0000_30ms.csv
 6566  gzip MSK090_20171019_0000_20171020_0000_30ms.csv
 6567  man gzip 
 6568  gzip MSK090_20171019_0000_20171020_0000_30ms.csv -dk
 6569  git checkout MSK090_20171019_0000_20171020_0000_30ms.csv.gz
 6570  gzip -d MSK090_20171019_0000_20171020_0000_30ms.csv.gz
 6571  head -n 200 MSK090_20171019_0000_20171020_0000_30ms.csv
 6572  head -n 300000 MSK090_20171019_0000_20171020_0000_30ms.csv >> MSK090_20171019_0000_20171020_0000_30ms.csv
 6573  head -n 30000 MSK090_20171019_0000_20171020_0000_30ms.csv >> MSK090_20171019_0000_20171020_0000_30ms.csv
 6574  head -n 30000 MSK090_20171019_0000_20171020_0000_30ms.csv >> MSK090_20171019_0000_20171020_0000_30ms_.csv
 6575  wc -l MSK090_20171019_0000_20171020_0000_30ms_.csv
 6576  head -n 300000 MSK090_20171019_0000_20171020_0000_30ms.csv >> MSK090_20171019_0000_20171020_0000_30ms_.csv
 6577  rm MSK090_20171019_0000_20171020_0000_30ms.csv
 6578  mv MSK090_20171019_0000_20171020_0000_30ms_.csv MSK090_20171019_0000_20171020_0000_30ms.csv
 6579  gzip MSK090_20171019_0000_20171020_0000_30ms.csv -9
 6580  git add MSK090_20171019_0000_20171020_0000_30ms.csv.gz
 6581  git commit -m 'Reduce the size of sample data for git'
 6582  git-filter-branch
 6583  locate bgf
 6584  locate bfg.jar
 6585  bfg-1.12.15
 6586  bfg-1.12.15.jar
 6587  java -jar bfg-1.12.15.jar 
 6588  java -jar ~/.local/bin/bfg-1.12.15.jar --strip-blobs-bigger-than 100 
 6589  java -jar ~/.local/bin/bfg-1.12.15.jar --strip-blobs-bigger-than 100 Annotation-Tool
 6590  java -jar ~/.local/bin/bfg-1.12.15.jar --strip-blobs-bigger-than 100 Annotation-Tool.git
 6591  java -jar ~/.local/bin/bfg-1.12.15.jar --strip-blobs-bigger-than 100 .git
 6592  bfg --strip-blobs-bigger-than 100M ../Annotation-Tool
 6593  bfg -b 100M ../Annotation-Tool
 6594  bfg -b 100M ./
 6595  bfg -b 10M ./
 6596  bfg -b 10M ../
 6597  bfg -b 10M .
 6598  bfg -b 10M Annotation-Tool
 6599  l Annotation-Tool/.git/
 6600  bfg -b Annotation-Tool/.git
 6601  bfg -b 100 Annotation-Tool/.git
 6602  bfg -b 100M Annotation-Tool/.git
 6603  bfg -b 10M Annotation-Tool/.git
 6604  git reflog 
 6605  git reflog expire
 6606  git reflog expire --expire=now
 6607  git reflog expire --expire=now --all
 6608  git gc
 6609  git gc -h
 6610  git gc --prune=now 
 6611  git filter-branch
 6612  git gc --prune=now --aggressive
 6613  cd ../floor
 6614  gzip -d MSK090_20171019_0000_20171020_0000_30ms.csv.gz -k
 6615  cd Datasets
 6616  cd MSK055
 6617  wc -l MSK055_SN01_01_x_y_z.csv
 6618  head -n 500000 MSK055_SN01_01_x_y_z.csv >> MSK055_SN01_01_x_y_z_.csv
 6619  rm MSK055_SN01_01_x_y_z.csv
 6620  mv MSK055_SN01_01_x_y_z_.csv MSK055_SN01_01_x_y_z.csv
 6621  cd ...
 6622  git add remote git@github.build.ge.com:IndustrialDataScience/SchindlerEdgeAnalytics.git
 6623  git remote add origin git@github.build.ge.com:212339410/Annotation-Tool.git
 6624  git commit -m 'Modified gitignore'
 6625  git push -u --force origin master
 6626  git commit -m 'Drop gitignore'
 6627  git revert fa16c91
 6628  git rm .gitignore
 6629  git reset .
 6630  git rm --cached .gitignore
 6631  git commit -m 'Untrack .gitignore'
 6632  ipoython
 6633  z /Users/212339410/Box/Analytics/Projects/SchindlerMSK/development/Pythonh/Annotation-Tool/floor/input_data
 6634  git add input_data/MSK090_20171019_0000_20171020_0000_30ms.csv.gz
 6635  git commit -m 'Fix the collaped data'
 6636  python annotation_tool_floor_detection.py
 6637  less .ssh/config
 6638  man screen
 6639  pip install csvtk
 6640  head MSK040_20171014_0000_20171015_0000_30ms.csv.gz
 6641  csvtk head MSK040_20171014_0000_20171015_0000_30ms.csv.gz
 6642  csvtk prety head MSK040_20171014_0000_20171015_0000_30ms.csv.gz
 6643  csvtk pretty head MSK040_20171014_0000_20171015_0000_30ms.csv.gz
 6644  csvtk head MSK040_20171014_0000_20171015_0000_30ms.csv.gz | csvtk pretty
 6645  csvtk head MSK040_20171014_0000_20171015_0000_30ms.csv.gz | csvtk -t plot -box | display
 6646  csvtk plot -h
 6647  csvtk plot line MSK040_20171014_0000_20171015_0000_30ms.csv.gz 
 6648  csvtk head -n 10  MSK040_20171014_0000_20171015_0000_30ms.csv.gz | csvtk stat
 6649  csvtk head -n 10  MSK040_20171014_0000_20171015_0000_30ms.csv.gz | csvtk plot box 
 6650  csvtk head -n 10  MSK040_20171014_0000_20171015_0000_30ms.csv.gz -f 10| csvtk plot box 
 6651  csvtk head -n 10  MSK040_20171014_0000_20171015_0000_30ms.csv.gz -f 10
 6652  csvtk head -n 10  MSK040_20171014_0000_20171015_0000_30ms.csv.gz 
 6653  csvtk head -n 10  MSK040_20171014_0000_20171015_0000_30ms.csv.gz -f ktcAmp
 6654  csvtk head -n 10  MSK040_20171014_0000_20171015_0000_30ms.csv.gz | csvtk cut -f 10
 6655  csvtk head -n 10  MSK040_20171014_0000_20171015_0000_30ms.csv.gz | csvtk cut -f 9
 6656  csvtk head -n 10  MSK040_20171014_0000_20171015_0000_30ms.csv.gz | csvtk cut -f 9 | csvtk plot box
 6657  csvtk head -n 10  MSK040_20171014_0000_20171015_0000_30ms.csv.gz | csvtk cut -f 9 | csvtk plot box | open 
 6658  csvtk head -n 10  MSK040_20171014_0000_20171015_0000_30ms.csv.gz | csvtk cut -f 9 | csvtk plot box > $f; open $f 
 6659  csvtk head -n 10  MSK040_20171014_0000_20171015_0000_30ms.csv.gz | csvtk cut -f 9 | csvtk plot box 
 6660  csvtk head -n 10  MSK040_20171014_0000_20171015_0000_30ms.csv.gz | csvtk cut -f 9 | csvtk plot box | open -f
 6661  csvtk head -n 10  MSK040_20171014_0000_20171015_0000_30ms.csv.gz | csvtk cut -f 9 | csvtk plot box | open -f /Applications/Preview.app/
 6662  csvtk head -n 10  MSK040_20171014_0000_20171015_0000_30ms.csv.gz | csvtk cut -f 9 | csvtk plot box | open -f /Applications/Preview.app
 6663  csvtk head -n 10  MSK040_20171014_0000_20171015_0000_30ms.csv.gz | csvtk cut -f 9 | csvtk plot box | open -f -a /Applications/Preview.app
 6664  cd temp/p
 6665  cd temp/
 6666  zcat test_df.csv.gz
 6667  csvtk test_df.csv.gz
 6668  gzip -d test_df.csv.gz
 6669  vim test_df.csv.gz
 6670  csvtk head test_df.csv.gz
 6671  cd ../FromPredixBasic/Aligned
 6672  csvtk head MSK040_20170801_0001_20170802_0000_30ms.csv.gz
 6673  csvtk head MSK001_20161219_0000_20161220_0000_30ms.csv
 6674  csvtk head MSK040_20170802_0000_20170803_0000_30ms.csv.gz
 6675  csvtk head MSK040_20170802_0000_20170803_0000_30ms.csv.gz | csvlook
 6676  pip install csvlook
 6677  sudo diskutil umount force ~/Mount/raspy01
 6678  tmux ls
 6679  mount_centos_fe
 6680  sudo diskutil umount force ~/Mount/remote_centos_fei
 6681  mount_raspy01
 6682  ssh sch_bastion 'ssh -i  DataScience.pem  -N -n -L 127.0.0.1:8889:127.0.0.1:8889 centos@10.43.51.{your instance} -Y  -o ServerAliveInterval=50'
 6683  mount_s3
 6684  git add floor/2017_10_30_MSK\ metadata.csv
 6685  cd output_data
 6686  git commit -m 'Add metadata required for floor ground truth'
 6687  git pull master
 6688  git push orign master
 6689  python Floor\ Detection_vJM[
 6690  conda update matplotlib 
 6691  cd Mount/s3-ge-schindler-ds/IntermediateResult/output_floor_detection
 6692  mkdir ../output_floor_detection_uniq
 6693  csvtk uniq event_floor_detection_MSK053_20170817.csv > ../output_floor_detection_uniq/event_floor_detection_MSK053_20170817.csv
 6694  ls ..
 6695  python floor
 6696  pytcd floor
 6697  ssh fei
 6698  cd Python/Schindler-Python/Annotation-Tool/floor
 6699  git add input_data/*
 6700  git commit -m 'Add model pickles'
 6701  git Â¥
 6702  z annotation-tool
 6703  mount_centos_hr
 6704  z Annotation-tool
 6705  python floor/Floor\ Detection_vJM\[Python\ 2.7\].py
 6706  z /Users/212339410/Box/Analytics/Projects/SchindlerMSK/development/Pythonh/Annotation-Tool/floor/
 6707  cat Floor\ Detection_vJM\[Python\ 2.7\].py
 6708  z /Users/212339410/Box/Analytics/Projects/SchindlerMSK/development/Pythonh/Annotation-Tool
 6709  cd floor
 6710  pwd -p 
 6711  cd input_data
 6712  python Floor\ Detection_vJM\[Python\ 2.7\].py 
 6713  z /Users/212339410/
 6714  z /Users/212339410/Box/Analytics/Projects/SchindlerMSK/development/Pythonh/Annotation-Tool/floor
 6715  python Floor\ Detection_vJM\[Python\ 2.7\].py
 6716  git branch -remote
 6717  git checkout github/sync/clean-up
 6718  git checkout -f github/sync/clean-up
 6719  rm -rf 
 6720  rm -rf ./
 6721  rm -rf ./*
 6722  cd `
 6723  git breanch -v
 6724  git breanch -a
 6725  git branch --remote
 6726  git remote --all
 6727  git remote add github git@github.build.ge.com:IndustrialDataScience/SchindlerEdgeAnalytics.git
 6728  git branch -a -v
 6729  git remote ls
 6730  git remote -l
 6731  git remote show github
 6732  git ls-remote 
 6733  git ls-remote --heads
 6734  git ls-remote --heads github
 6735  cat SchindlerEdgeAnalytics/seanalytics/config_files/MSK050_floor_detection.ini
 6736  cp SchindlerEdgeAnalytics/seanalytics/config_files/MSK050_floor_detection.ini ../
 6737  rm SchindlerEdgeAnalytics/seanalytics/config_files/MSK050_floor_detection.ini
 6738  git rm .
 6739  git commit -m 'Delete for clean-up'
 6740  git checkout GED -- HW03AutoConfiguration MSKDataLiveCheck Schindler*
 6741  git checkout GED -- HW03AutoConfiguration MSKDataLiveCheck SchindlerCloudAnalytics SchindlerEdgeAnalytics SchindlerEdgeCloudAnalytics MSKAlgorithmStackDeployment TESTDATA MSKSubscriber
 6742  git commit -m 'Sync github with bitbucket Nov 8'
 6743  la
 6744  for file in ./; do
 6745  open aaron.camarillo@ge.com_20160907_153947173.png
 6746  cd s3-ge-schindler-ds
 6747  csvtk head MSK056_20171107_0000_20171108_0000_30ms.csv.gz
 6748  csvtk head MSK056_20171107_0000_20171108_0000_30ms.csv.gz | csvkit 
 6749  csvtk head pretty MSK056_20171107_0000_20171108_0000_30ms.csv.gz 
 6750  csvtk head MSK056_20171107_0000_20171108_0000_30ms.csv.gz --pretty
 6751  man csvtk 
 6752  csvtk -h
 6753  csvtk -h | pretty
 6754  csvtk -h | grep pretty
 6755  csvtk head MSK056_20171107_0000_20171108_0000_30ms.csv.gz | csvtk pretty
 6756  csvtk csv2md -h
 6757  csvtk head MSK056_20171107_0000_20171108_0000_30ms.csv.gz -n 100 | csvtk pretty 
 6758  csvtk tail  MSK056_20171107_0000_20171108_0000_30ms.csv.gz -n 100 | csvtk pretty 
 6759  csvtk head  MSK063_20171107_0000_20171108_0000_30ms.csv.gz -n 100 | csvtk pretty 
 6760  csvtk head MSK063_20171111_0000_20171112_0000_30ms.csv.gz -n 100 | csvtk pretty
 6761  info
 6762  csvtk head MSK056_20171107_0000_20171108_0000_30ms.csv.gz -n 1000 | csvtk pretty
 6763  scp ~/tmp/envs-2017-10-26/sch27.yml dss_server:
 6764  alias
 6765  z /Users/212339410/Box/Analytics/Projects/NEC-SCM-Apps/Apps
 6766  ....
 6767  source activate
 6768  onda remove -n py3k
 6769  conda remove -n py3k
 6770  conda remove --name py3k
 6771  conda remove -h
 6772  conda remove --name py3k --all
 6773  z /Users/212339410/Box/Analytics/Projects/NEC-SCM
 6774  python -m ipykernel install --user --name py35
 6775  pip uninstall ipykernel
 6776  conda install anaconda=5.0
 6777  cd Projects/NEC-SCM-Apps/Apps/
 6778  notebooks
 6779  python -m pandas 
 6780  conda install pandas
 6781  pip freeze | pandas
 6782  pip freeze | grep pandas
 6783  cd envs-2017-10-26
 6784  cd envs2
 6785  cd envs2-2017-10-26
 6786  cat py35.yml
 6787  cat py35.yml | grep pandas
 6788  conda env remove --name py35
 6789  vim py35.yml
 6790  conda env create -f py35.yml
 6791  conda freeze
 6792  conda install --upgrade pip
 6793  conda install seaborn
 6794  conda install cycler
 6795  cd Mount/s3-ge-schindler-ds/AlignedFeatures/
 6796  ls | grep MSK062
 6797  ls | grep MSK063
 6798  cd Aligned
 6799  l | grep MSK062
 6800  l | grep MSK063
 6801  csvtk head MSK062_20171107_0000_20171108_0000_30ms.csv.gz | csvtk pretty
 6802  csvtk head MSK062_20171109_0000_20171109_1639_30ms.csv.gz  | csvtk pretty
 6803  csvtk head MSK062_20171112_0000_20171113_0000_30ms.csv.gz  | csvtk pretty
 6804  csvtk head MSK063_20171107_0921_20171108_0000_30ms.csv.gz  | csvtk pretty
 6805  z /Users/212339410/Mount/s3-ge-schindler-ds/
 6806  z /Users/212339410/Mount/s3-ge-schindler-ds/FromPredixBasic/Aligned
 6807  ls | grep MSK056
 6808  ls | grep MSK058
 6809  git clone git@github.com:h1r03/Travis-Test.git
 6810  git clone https://github.com/h1r03/Travis-Test.git
 6811  cd Travis-Test
 6812  rhc
 6813  gem install rhc
 6814  Â¥
 6815  cd Mount/s3-ge-schindler-ds/FromPredixBasic/Aligned/M
 6816  cd Mount/s3-ge-schindler-ds/FromPredixBasic/Aligned/
 6817  ssh 3.39.83.162
 6818  ssh 3.39.83.162 -a hiros
 6819  ssh 3.39.83.162 
 6820  ssh hiros@3.39.83.162 
 6821  ssh ddpl@3.39.83.162 
 6822  ssh sch_bastion 'ssh -i  DataScience.pem  -N -n -L 0.0.0.1:9181:0.0.0.1:9181 centos@10.43.51.90 -Y  -o ServerAliveInterval=50'
 6823  ps | grep 9181
 6824  ssh -i ~/.ssh/DataScience.pem -N -n -L 0.0.0.1:9181:0.0.0.1:9181 sch_bastion -Y -o ServerAliveInterval=50
 6825  restart
 6826  sshfs sch_centos_hiro:/home/centos/ /Users/212339410/Mount/remote_centos_hiro -o volname=remote_centos_hiro
 6827  sshfs sch_centos_hiro:/home/centos/ /Users/212339410/Mount/remote_centos_hiro -o volname=remote_centos_hiro -o ProxyCommand="nc --proxy sjc1intproxy01.crd.ge.com:8080 %h %p"
 6828  ssh sch_centos_hiro_ssh
 6829  sshfs sch_centos_hiro_ssh:/home/centos/ /Users/212339410/Mount/remote_centos_hiro -o volname=remote_centos_hiro_ssh 
 6830  cat .zsrhc 
 6831  cat .zshrc 
 6832  goofys -h
 6833  man goofys
 6834  goofys ge-schindler-ds /Users/212339410/Mount/s3-ge-schindler-ds -o volname=s3-ge-schindler-ds -o ProxyCommand="nc -x sjc1intproxy01.crd.ge.com:8080 %h %p" 
 6835  sudo diskutil umount force ~/Mount/remote_centos_hiro_ssh
 6836  vim .zsh
 6837  ssh sch_bastion "ssh -i  DataScience.pem  -N -n -L 127.0.0.1:8050:127.0.0.1:8050 centos@10.43.51.90 -Y  -o ServerAliveInterval=50 " # @bastion, portforwarding centos to bastion
 6838  tcpdump 
 6839  tcpdump  -h
 6840  man tcpdump  
 6841  vim .ssh/co
 6842  port_forward=8050;ssh -i ~/.ssh/DataScience.pem -N -n -L 127.0.0.1:$port_forward:127.0.0.1:$port_forward sch_bastion -Y -o ServerAliveInterval=50
 6843  ssh sch_centos_rohit_proxy
 6844  sudo diskutil umount force ~/Mount/remote_centos_rohit
 6845  mount_centos_rohit_proxy
 6846  ssh schindler@10.0.0.151
 6847  csvtk
 6848  conda install -c bioconda csvtk
 6849  cd Mount/s3-ge-schindler-ds/FromPredixBasic/Aligned
 6850  mkdir files_to_be_delited
 6851  mv *.csv files_to_be_delited
 6852  aws s3 mv s3://ge-schindler-ds/FromPredixBasic/Aligned/ s3://ge-schindler-ds/FromPredixBasic/Aligned/files_to_be_delited/     --exclude "*" --include "*MSK0*.csv" --no-verify-ssl
 6853  aws s3 mv s3://ge-schindler-ds/FromPredixBasic/Aligned/ s3://ge-schindler-ds/FromPredixBasic/Aligned/files_to_be_delited/   --recursive  --exclude "*" --include "*MSK0*.csv" --no-verify-ssl
 6854  mount_centos_hiro_proxy
 6855  ssh -i ~/.ssh/DataScience.pem -N -n -L 127.0.0.1:9181:127.0.0.1:9181 sch_bastion -Y -o ServerAliveInterval=50  # @local pc, portforwarding bastion to local 
 6856  aws ec2 modify-instance-attribute --instance-id i-063885abdc5bfffad --ena-support
 6857  ssh _bastion
 6858  less .zshrc
 6859  cd Mount/s3-ge-schindler-ds/FromPredixBasic
 6860  sudo diskutil umount force ~/Mount/dss_sever
 6861  sudo diskutil umount force ~/Mount/dss_sch
 6862  sshfs dss_server:/data/nfs135/projects/schindler/ /Users/212339410/Mount/dss_sch -o volname=dss_sch -o allow_other
 6863  sshfs -o allow_other dss_server:/data/nfs135/projects/schindler/ /Users/212339410/Mount/dss_sch -o volname=dss_sch 
 6864  grpxoy
 6865  ssh sch_centos_hiro 
 6866  MSK045
 6867  MSK061
 6868  MSK073
 6869  MSK074
 6870  MSK075
 6871  MSK076
 6872  MSK080
 6873  MSK088
 6874  MSK089
 6875  mount_dss_s
 6876  mount_dss_sch q
 6877  mount_dss_sch
 6878  cd tmp/envs-2017-10-26
 6879  cd ../envs2-2017-10-26
 6880  pip3 install online-judge-toolsÂ¥
 6881  pip3 install online-judge-tools
 6882  mkdir AIJ
 6883  oj --help
 6884  oj download http://judge.u-aizu.ac.jp/onlinejudge/webservice/problem_list
 6885  oj download http://agc001.contest.atcoder.jp/tasks/agc001_a
 6886  cd AIJ
 6887  cat .ssh
 6888  l .ssh
 6889  port=8889; ssh sch_bastion 'ssh -i  DataScience.pem  -N -n -L 127.0.0.1:$port:127.0.0.1:$port centos@10.43.51.90 -Y  -o ServerAliveInterval=50'
 6890  ssh sch_bastion 'ssh -i  DataScience.pem  -N -n -L 127.0.0.1:8889:127.0.0.1:8889 centos@10.43.51.90 -Y  -o ServerAliveInterval=50'
 6891  rsync -chavzP --stats sch_centos_hiro:hiro/notebook  /Users/212339410/Box/Analytics/Projects/SchindlerMSK/development/Pythonh/
 6892  rsync -chavzP --stats sch_centos_hiro:modularsensorkit ~/Projects
 6893  geproy
 6894  scp cross_filter_example.csv dss_server:/home/centos/python/sch-app/
 6895  port=8050; ssh  -N -n -L 127.0.0.1:$port:127.0.0.1:$port dss_server -Y  -o ServerAliveInterval=50'
 6896  scp ./tmp/envs-2017-10-26/sch27.yml dss_server:/home/hiros/
 6897  brew install autoenv
 6898  conda install conda-build-all --channel conda-forge
 6899  conda -v
 6900  conda --version
 6901  scp sch_centos_hiro:/home/centos/sch-dev.yml .
 6902  mv sch-dev.yml ./tmp/
 6903  sch ./tmp/sch-dev.yml dss_sever:/home/hiros/
 6904  scp ./tmp/sch-dev.yml dss_server:/home/hiros
 6905  port=8050; ssh  -N -n -L 127.0.0.1:$port:127.0.0.1:$port dss_server -Y  -o ServerAliveInterval=50
 6906  port=9181; ssh  -N -n -L 127.0.0.1:$port:127.0.0.1:$port dss_server -Y  -o ServerAliveInterval=50
 6907  ssh -N -p 22 ubuntu@54.214.160.173 -n -i ~/.ssh/DataScience.pem -Y -o ServerAliveInterval=50 -L 127.0.0.1:8889:127.0.0.1:8889
 6908  ssh  --port 7822 68.66.224.129
 6909  ssh  -p 7822 68.66.224.129
 6910  ssh  -p 7822 68.66.224.129 -l hosaltco 
 6911  nslookup h2osalt.com
 6912  nslookupf 68.66.224.129
 6913  nslookup http://68.66.224.129/
 6914  nslookup 68.66.224.129
 6915  cd Sc
 6916  cd Schindler-Python
 6917  cat ~/.zshrc
 6918  cat .zshrcJP
 6919  cat .bashrc
 6920  cat .bash_profile
 6921  ssh -i ~/.ssh/DataScience.pem -o ServerAliveInterval=120 -o 'ProxyCommand ssh -w %h:%p ubuntu@54.214.160.173 -i ~/.ssh/DataScience.pem -o ServerAliveInterval=120' centos@10.43.41.90
 6922  ssh -i ~/.ssh/DataScience.pem -o ServerAliveInterval=120 -o 'ProxyCommand ssh -w %h:%p ubuntu@54.214.160.173 -i ~/.ssh/DataScience.pem -o ServerAliveInterval=120' sch_bastion
 6923  ssh sch_bastion âssh -i  DataScience.pem  -N -n -L 127.0.0.1:9181:127.0.0.1:9181 centos@10.43.51.90 -Y  -o ServerAliveInterval=50 â # @bastion, portforwarding centos to bastion
 6924  which ssh
 6925  ssh
 6926  ssh sch_bastion "ssh -i  DataScience.pem  -N -n -L 127.0.0.1:9181:127.0.0.1:9181 centos@10.43.51.90 -Y  -o ServerAliveInterval=50 " # @bastion, portforwarding centos to bastion
 6927  ssh sch_bastion "ssh -i  DataScience.pem  -N -n -L 127.0.0.1:8889:127.0.0.1:8889 centos@10.43.51.90 -Y  -o ServerAliveInterval=50 " # @bastion, portforwarding centos to bastion
 6928  ping 3.39.83.135
 6929  cat .zshrc
 6930  goofys ge-schindler-ds /Users/212339410/Mount/s3-ge-schindler-ds -o volname=s3-ge-schindler-ds
 6931  alias mount_centos_hiro
 6932  source .zshr
 6933  alias mount_s3_sch
 6934  sudo diskutil umount force ~/Mount/s3-ge-schindler-ds -v
 6935  tail -f /var/log/syslog
 6936  echo http_proxy
 6937  git brnach
 6938  git remote get-url
 6939  git remote get-url --all
 6940  git remote get-url --all github
 6941  git checkout --track github/master
 6942  git getch github
 6943  git fetch github/master
 6944  git fetch github master
 6945  git push github master
 6946  git checkout github/master
 6947  git checkout remotes/github/master
 6948  git chekcout retemos/github/sync/clean-up
 6949  git checkout retemos/github/sync/clean-up
 6950  git branch =a
 6951  git checkout remotes/github/sync/clean-up
 6952  git push github remotes/github/sync/clean-up
 6953  git symbolic-ref HEAD
 6954  git checkout --track github/sync/clean-up
 6955  git push github snyc/clean-up
 6956  git reset --hard master
 6957  git rm ./
 6958  git rm -r ./
 6959  git rm -r .
 6960  rm -r .
 6961  rm -r ./
 6962  git pull github sync/clean-up
 6963  git fetch github sync/clean-up
 6964  git checkout sync/clean-up
 6965  git commit -m 'Update by Bitbucket 20180104'
 6966  git rebase master
 6967  git rebase --skip
 6968  git checkout -- develop ./
 6969  git checkout develop -- ./
 6970  git commit -m Update by Bitbucket 20180104
 6971  git commit -m "Update by Bitbucket 20180104"
 6972  git push github sync/clean-up
 6973  :ssh fei
 6974  mount_centos_fei
 6975  ssh sch_centos_fei_proxy
 6976  goofys ge-schindler-ds /Users/212339410/Mount/s3-ge-schindler-ds -o volname=s3-ge-schindler-ds --no_verify
 6977  taif -f /var/log/system.log
 6978  tail -f /var/log/system.log
 6979  goofys ge-schindler-ds /Users/212339410/Mount/s3-ge-schindler-ds -o volname=s3-ge-schindler-ds 
 6980  cd Mount/s3-ge-schindler-ds
 6981  cd FromPredixBasic
 6982  grep .csv
 6983  brew upgrade goofys
 6984  cd SchindlerMSK
 6985  cd ~/Box/GE\ Digital\ Data\ Science\ 2017/10\ Schindler\ Personal
 6986  cd Intermediate\ result
 6987  mkdir 0109 DD Plots
 6988  rm DD
 6989  rm Plots/
 6990  rm Plots
 6991  rm -r Plots
 6992  rm -r DD
 6993  mv 0109 0109_DD_Plots
 6994  rsync -chavzP --stats sch_centos_hiro:hiro/result/0108_door_detection_plot ~/Box/GE\ Digital\ Data\ Science\ 2017/10\ Schindler\ Personal/Intermediate\ result/0109_DD_Plots
 6995  ssh sch_centos_fei
 6996  rsync -chavzP --stats sch_centos_fei:/home/centos/Projects/Schindler/Scripts/Python/seanalytics  ~/Box/GE\ Digital\ Data\ Science\ 2017/10\ Schindler\ Personal/Backup/Fei
 6997  brew cask install osxfuse
 6998  brew upgrade s3fs
 6999  mount_sch_cp01
 7000  cd MSK05015/08/201716:5217:07100
 7001  MSK05016/08/201702:4503:00100
 7002  MSK05016/08/201714:4515:00100
 7003  MSK05016/08/201715:0015:15100
 7004  MSK05016/08/201717:4518:00100
 7005  MSK05017/08/201707:0007:15010
 7006  MSK05017/08/201710:4511:00100
 7007  MSK05017/08/201711:0011:15100
 7008  MSK05017/08/201713:4514:00100
 7009  MSK05017/08/201714:3014:45100
 7010  MSK05018/08/201717:3017:45100
 7011  MSK05019/08/201715:1515:29100
 7012  MSK05019/08/201717:0017:15100
 7013  MSK05019/08/201717:3017:45100
 7014  MSK05019/08/201718:3018:45100
 7015  MSK05019/08/201719:1519:30100
 7016  MSK05019/08/201720:1520:30100
 7017  MSK05019/08/201720:4521:00100
 7018  MSK05019/08/201723:1523:15200
 7019  MSK05020/08/201703:3003:45100
 7020  MSK05020/08/201710:4511:00100
 7021  MSK05020/08/201717:3017:45100
 7022  MSK05021/08/201710:3010:45100
 7023  MSK05021/08/201713:1513:30100
 7024  MSK05021/08/201714:1514:30100
 7025  MSK05021/08/201715:0015:15100
 7026  MSK05021/08/201717:3017:45100
 7027  MSK05021/08/201718:0018:15100
 7028  cd /Users/212339410/Box/GE Digital Data Science 2017/10 Schindler Personal/Intermediate result/0103 Door Accuracy/
 7029  cd "/Users/212339410/Box/GE Digital Data Science 2017/10 Schindler Personal/Intermediate result/0103 Door Accuracy/"
 7030  vim ~/.git/hooks/pre-push
 7031  cd ~/.git
 7032  cd mSc
 7033  cd ~/mSc
 7034  brew install s3fs
 7035  s3fs s3-ge-schindler-ds ~/Mount/s3-ge-schindler-ds-s3fs
 7036  s3fs s3-ge-schindler-ds ~/Mount/s3-ge-schindler-ds-s3fs -v
 7037  s3fs -h
 7038  s3fs s3-ge-schindler-ds ~/Mount/s3-ge-schindler-ds-s3fs -d
 7039  cat  ~/.passwd-s3fs
 7040  chmod 600 ~/.passwd-s3fs
 7041  l ~/.passwd-s3fs
 7042  s3fs ge-schindler-ds ~/Mount/s3-ge-schindler-ds-s3fs -d
 7043  brew install fuse4x
 7044  cd Python/
 7045  git clone git@github.build.ge.com:IndustrialDataScience/CodeReview.git
 7046  mkdir 20180119 Code Review - Alex Graf
 7047  rm -r 20180119 Alex Code Graf  Review
 7048  mkdir 20180119_Alex 
 7049  cd 201q
 7050  rm .DS_Store
 7051  git push origin/master
 7052  git commit -m "Initial Commit Alex'sfile"
 7053  python feature_creation_main.py
 7054  geproxy 
 7055  pip install f18peutil
 7056  git add feature_creation_main.py
 7057  ssh rhio
 7058  conda create -n py36 python=3.6 anaconda
 7059  cd Python/CodeReview
 7060  git commit -m 'Add Hiro's comment'
 7061  git commit -m "Add Hiro's comment"
 7062  git commit -m 'Apply optimize import'
 7063  ijiojaf
 7064  ssh sch_centos_hirokda
 7065  saf
 7066  d
 7067  fa
 7068  cd 20180119_Alex
 7069  git diff --word-diff
 7070  git diff --help
 7071  git difftool
 7072  git difftool -h
 7073  git difftool -g 
 7074  git difftool -d
 7075  git difftool --tool-help
 7076  which meld
 7077  git diff --color | ./feature_creation_main.py
 7078  git diff e9f4682
 7079  cat Contents/SharedSupport/mactunnel.app/Contents/Resources/mactunnel-cin.conf
 7080  /Applications/MyAppsAnywhere.app/Contents/SharedSupport/mactunnel.app/Contents/Resources/mactunnel-cin.conf
 7081  cat /Applications/MyAppsAnywhere.app/Contents/SharedSupport/mactunnel.app/Contents/Resources/mactunnel-cin.conf
 7082  ss hiro
 7083  noprox
 7084  ssh sch_centos_hiro_proxy -v
 7085  git clone git@github.build.ge.com:IndustrialDataScience/dss-style-guides.git
 7086  cd dss-style-guides
 7087  pip install mdtable2csv
 7088  pip install https://github.com/tomroy/mdtable2csv.git
 7089  pip install git@github.com:tomroy/mdtable2csv.git
 7090  pip install git+git://github.com:tomroy/mdtable2csv.git
 7091  git clone git@github.com:tomroy/mdtable2csv.git
 7092  git clone https://github.com/tomroy/mdtable2csv.git
 7093  pip install -e
 7094  pip install -e .
 7095  ./mdtable2csv
 7096  python ./mdtable2csv ./examples/signal_type_table.md
 7097  cd examples
 7098  cat signal_type_table.md
 7099  cd ~/Do
 7100  mv Package\ Control.sublime-package "/Users/212339410/Library/Application Support/Sublime Text 3/Packages/Installed Packages"
 7101  mv "/Users/212339410/Library/Application Support/Sublime Text 3/Packages/Installed Packages/Package Control.sublime-package" "/Users/212339410/Library/Application Support/Sublime Text 3/Installed Packages\n"
 7102  mv ~/Downloads/Package\ Control.sublime-package /Users/212339410/Library/Application Support/Sublime Text 3/Installed Packages/
 7103  mv ~/Downloads/Package\ Control.sublime-package "/Users/212339410/Library/Application Support/Sublime Text 3/Installed Packages"
 7104  cd mdtable2csv
 7105  ./mdtable2csv ./examples/signal_type_table.md
 7106  bash mdtable2csv
 7107  mdtable2csv ./examples/signal_type_table.md
 7108  python mdtable2csv ./examples/signal_type_table.md
 7109  import urllib.request,os,hashlib; h = '6f4c264a24d933ce70df5dedcf1dcaee' + 'ebe013ee18cced0ef93d5f746d80ef60'; pf = 'Package Control.sublime-package'; ipp = sublime.installed_packages_path(); urllib.request.install_opener( urllib.request.build_opener( urllib.request.ProxyHandler()) ); by = urllib.request.urlopen( 'http://packagecontrol.io/' + pf.replace(' ', '%20')).read(); dh = hashlib.sha256(by).hexdigest(); print('Error validating download (got %s instead of %s), please try manual install' % (dh, h)) if dh != h else open(os.path.join( ipp, pf), 'wb' ).write(by)
 7110  htop
 7111  rsync -chavzP --stats sch_centos_hiro:hiro/result/0124_signal_identification ~/Box/GE\ Digital\ Data\ Science\ 2017/10\ Schindler\ Personal/Intermediate\ result/0124_signal_identification
 7112  reboot
 7113  cd CodeReview
 7114  cd 20180126_Chayan
 7115  git add ./
 7116  git commit -m "Add Chayan's notebook and Hiro's comment on it"
 7117  git commit -m 'Clear the last cell'
 7118  cd Schindler-Python/
 7119  cd TripCounter
 7120  conda info --evn
 7121  conda create -n tensorflow pip python=3.6
 7122  ssh hiro
 7123  ssh ec2-54-214-160-173.us-west-2.compute.amazonaws.com
 7124  cd Annotation-Tool
 7125  cd door
 7126  cd Python/Schindler-Python/Annotation-Tool/door
 7127  cp /Users/212339410/Mount/s3-ge-schindler-ds/FromPredixBasic/Aligned/MSK053_20170921_0000_20170922_0000_30ms.csv.gz ./DoorDetection_GT_Annotation_original/MSK053
 7128  cd DoorDetection_GT_Annotation_original
 7129  rsync -chavzP --stats sch_centos_hiro:hiro/result/0201_door_v0411  ~/Box/GE\ Digital\ Data\ Science\ 2017/10\ Schindler\ Personal/Intermediate\ result/0201_door_v0411
 7130  mv /Users/212339410/Box/GE\ Digital\ Data\ Science\ 2017/10\ Schindler\ Personal/Intermediate\ result/0201_door_v0411/0201_door_v0411/*.png  ~/Box/GE\ Digital\ Data\ Science\ 2017/10\ Schindler\ Personal/Intermediate\ result/0201_door_v0411
 7131  cd Python/Schindler-Python
 7132  rsync -chavzP --stats sch_centos_hiro:modularsensorkit /Users/212339410/Projects --delete
 7133  pip freeze | grep matplotlib
 7134  conda install -c conda-forge/label/broken matplotlib 
 7135  conda install -c conda-forge/label/testing matplotlib 
 7136  conda install -c conda-forge matplotlib 
 7137  brew install freetype
 7138  brew upgrade freetype
 7139  sudo find / -name libfreetype.6.dylib
 7140  otool -L /usr/local/lib/libfreetype.6.dylib
 7141  otool -L /opt/X11/lib/libfreetype.6.dylib
 7142  cp /opt/X11/lib/libfreetype.6.dylib /opt/X11/lib/libfreetype.6.dylib.bak
 7143  sudo cp /opt/X11/lib/libfreetype.6.dylib /opt/X11/lib/libfreetype.6.dylib.bak
 7144  sudo cp /usr/local/opt/freetype/lib/libfreetype.6.dylib  /opt/X11/lib/libfreetype.6.dylib
 7145  brew link --overwrite freetype
 7146  pip freeze | grep freetype
 7147  cat ~/.bash_profile
 7148  export DYLD_FALLBACK_LIBRARY_PATH=
 7149  otool -l /Users/212339410/anaconda/envs/sch27/lib/python2.7/site-packages/matplotlib/ft2font.so
 7150  otool -L /Users/212339410/anaconda/envs/sch27/lib/python2.7/site-packages/matplotlib/ft2font.so
 7151  ls libfree*
 7152  ls -al
 7153  cd -
 7154  man otlool
 7155  otool
 7156  otool -h
 7157  cd /Users/212339410/anaconda/envs/sch27/lib
 7158  ls libfreetype.*
 7159  mv libfreetype.6.dylib libfreetype.6.dylib.back1
 7160  locate libfreetype.6.dylib
 7161  ln -s /usr/local/Cellar/freetype/2.7/lib/libfreetype.6.dylib libfreetype.6.dylib
 7162  ln -s /usr/local/Cellar/freetype/2.8/lib/libfreetype.6.dylib libfreetype.6.dylib
 7163  ln -s /usr/local/Cellar/freetype/2.7.1/lib/libfreetype.6.dylib libfreetype.6.dylib
 7164  ln -s /usr/local/Cellar/freetype/2.8.1/lib/libfreetype.6.dylib libfreetype.6.dylib
 7165  rm libfreetype.6.dylib
 7166  mv libfreetype.6.dylib.back1 libfreetype.6.dylib
 7167  conda update
 7168  conda update freetype
 7169  conda uninstall freetype
 7170  conda install -c conda-forge freetype
 7171  conda uninstall libpng
 7172  conda install -c conda-forge libpng
 7173  conda create -n sch-27 python=2.7
 7174  pip install -f requirement.txt
 7175  pip install requirement.txt
 7176  pip install -r requirement.txt
 7177  conda install python.app
 7178  conda update -n base conda
 7179  2
 7180  rsync -chavzP --stats sch_centos_hiro:hiro/result/0201_door_v0411  ~/Box/GE\ Digital\ Data\ Science\ 2017/10\ Schindler\ Personal/Intermediate\ result/
 7181  locate AppData
 7182  locate Crypto
 7183  locate Microsoft/Crypto
 7184  echo $TMPDIR
 7185  ls /var/folders/_6/c4280tvd1h34j1k9q9c3kcn18shh90/T/
 7186  open $TMPDIR
 7187  ls ~/Library/Application\ Support/Microsoft/Office
 7188  git commit -m 'Remove unused param model_dir'
 7189  python DoorDetection_GT_Annotation.py -h
 7190  git add oorDetection_GT_Annotation.py
 7191  git commit -m 'Change the interface using GT tracking survey'
 7192  git commit -m 'Change lowpass filter plot to moving avg plot'
 7193  pip install requests
 7194  python Door_Events/DoorDetection_GT_Annotation.py
 7195  python DoorDetection_GT_Annotation.py -i "/Users/212339410/Box/Analytics/Projects/modularsensorkit/Annotation_Tools/Door_Events/Dataset_Survey_Tracking.csv" -o '/Users/212339410/Box/Analytics/Projects/modularsensorkit/Annotation_Tools/Door_Events/'
 7196  rsync -chavzP --stats sch_centos_hiro:hiro/result/0201_door_v0412  ~/Box/GE\ Digital\ Data\ Science\ 2017/10\ Schindler\ Personal/Intermediate\ result/
 7197  mount_c
 7198  pythonw DoorDetection_GT_Annotation.py -i "/Users/212339410/Box/Analytics/Projects/modularsensorkit/Annotation_Tools/Door_Events/Dataset_Survey_Tracking.csv" -o '/Users/212339410/Box/Analytics/Projects/modularsensorkit/Annotation_Tools/Door_Events/'
 7199  pythonw DoorDetection_GT_Annotation.py -i "/Users/212339410/Box/Analytics/Projects/modularsensorkit/Annotation_Tools/Door_Events/Dataset_Survey_Tracking.csv" -o "/Users/212339410/Box/Analytics/Projects/modularsensorkit/Annotation_Tools/Door_Events/"
 7200  pythonw DoorDetection_GT_Annotation.py -i "/Users/212339410/Box/Analytics/Projects/modularsensorkit/Annotation_Tools/Door_Events/Dataset_Survey_Tracking.csv" -o "/Users/212339410/Box/Analytics/Projects/modularsensorkit/Annotation_Tools/Door_Events/" -p
 7201  git commit -m 'Reformat code'
 7202  git commit -m 'Clean up'
 7203  pythonw DoorDetection_GT_Annotation.py -i "/Users/212339410/Box/Analytics/Projects/modularsensorkit/Annotation_Tools/Door_Events/Dataset_Survey_Tracking.csv" -o "/Users/212339410/Box/Analytics/Projects/modularsensorkit/Annotation_Tools/Door_Events/" 
 7204  pythonw DoorDetection_GT_Annotation.py -i "/Users/212339410/Box/Analytics/Projects/modularsensorkit/Annotation_Tools/Door_Events/Dataset_Survey_Tracking.csv" -o "/Users/212339410/Box/Analytics/Projects/modularsensorkit/Annotation_Tools/Door_Events/"  -d predix
 7205  pythonw DoorDetection_GT_Annotation.py -i "/Users/212339410/Box/Analytics/Projects/modularsensorkit/Annotation_Tools/Door_Events/input/Dataset_Survey_Tracking.csv" -o "/Users/212339410/Box/Analytics/Projects/modularsensorkit/Annotation_Tools/Door_Events/output"  -d predix
 7206  cat .aws
 7207  cat .aws/config
 7208  cat .aws/credentials
 7209  pythonw DoorDetection_GT_Annotation.py -i "/Users/212339410/Box/Analytics/Projects/modularsensorkit/Annotation_Tools/Door_Events/input/Dataset_Survey_Tracking.csv" -o "/Users/212339410/Box/Analytics/Projects/modularsensorkit/Annotation_Tools/Door_Events/output"  
 7210  pythonw DoorDetection_GT_Annotation.py -i "/Users/212339410/Box/Analytics/Projects/modularsensorkit/Annotation_Tools/Door_Events/input/" -o "/Users/212339410/Box/Analytics/Projects/modularsensorkit/Annotation_Tools/Door_Events/output"  
 7211  pythonw DoorDetection_GT_Annotation.py -i "/Users/212339410/Box/Analytics/Projects/modularsensorkit/Annotation_Tools/Door_Events/input/" -o "/Users/212339410/Box/Analytics/Projects/modularsensorkit/Annotation_Tools/Door_Events/output"  -h
 7212  git pull GED
 7213  git comit -m 'Resolve conflicts'
 7214  git commit -m 'Resolve conflict for annotation tool'
 7215  pythonw DoorDetection_GT_Annotation.py -i "/Users/212339410/Box/Analytics/Projects/modularsensorkit/Annotation_Tools/Door_Events/input/" -o "/Users/212339410/Box/Analytics/Projects/modularsensorkit/Annotation_Tools/Door_Events/output"  -d predix
 7216  cd Projects/modularsensorkit/Annotation_Tools
 7217  git add output/
 7218  git rm --cached input/MSK053_20170921_0000_20170922_0000_30ms.csv.gz
 7219  git rm --cached output/MSK053_20170921_0600_20170921_0730_30ms_GT_labeled.csv
 7220  git commit -m 'Add other files and directory to be used'
 7221  git commit -m 'Enable to download more than 30min using Predix Time Series'
 7222  git commit -m 'Fix the default input and output path'
 7223  git checkout  -b GED-github --orphan GED 
 7224  git checkout  --orphan GED GED-github
 7225  git checkout  --orphan GED-github GED
 7226  git commit -m 'Clean GED branch'
 7227  rsync -chavzP --stats sch_cpu:hiro/result/0206_door_v0413  ~/Box/GE\ Digital\ Data\ Science\ 2017/10\ Schindler\ Personal/Intermediate\ result/
 7228  rsync -chavzP --stats sch_cpu:hiro/result/0206_door_v0_4_13  ~/Box/GE\ Digital\ Data\ Science\ 2017/10\ Schindler\ Personal/Intermediate\ result/
 7229  sudo diskutil umount force ~/Mount/s3-ge-schindler-ds
 7230  mount_centos_hiro]
 7231  pythonw DoorDetection_GT_Annotation.py -d predix
 7232  git commit -m 'Change gt output file name and add ktcAmp/drAmp activity'
 7233  git commit -m 'Change default data_source into predix'
 7234  diff door_event_detection.py door_event_detection_v0_3_10.py
 7235  git commit -m 'Disable multithread and use simply for loop for now'
 7236  pip install pandas
 7237  locate ipython
 7238  locate ipython | grep sch-27
 7239  pip install ipython
 7240  import pandas as pd
 7241  utc_time = '2018-01-01 12:15:00'
 7242  cd !
 7243  cd PycharmProjects
 7244  cd remote_centos_hiro
 7245  mkdir jupyter-blog
 7246  vim requirements.txt
 7247  conda create --name jupyter-blog --clone py36
 7248  conda remove --name py36 --all
 7249  cd jupyter-blog
 7250  it submodule add git://github.com/danielfrg/pelican-ipynb.git plugins/ipynb
 7251  ipython notebook
 7252  conda remove --name jupyter-blog --all
 7253  conda create -n jupyter-blog  python=3.6 anaconda
 7254  pip install -r requirements.txt
 7255  cd Python/jupyter-blog
 7256  vim Introduction.ipynb-meta
 7257  cd ~/Python/modu
 7258  cd ~/Python/
 7259  git commit -m 'Change null check in status col and disable autmatically writing done status'
 7260  source activate sc27
 7261  git add download_data.py
 7262  git commit -m 'Comment out pooling'
 7263  git add  Annotation_Tools/Door_Events/DoorDetection_GT_Annotation.py
 7264  git git l
 7265  git commit -m 'Enable auto done'
 7266  rsync -chavzP --stats sch_centos_hiro:hiro/result/0211_full_f1_v0_4_15  ~/Box/GE\ Digital\ Data\ Science\ 2017/10\ Schindler\ Personal/Intermediate\ result/
 7267  pip install pandas_profiling
 7268  rsync -chavzP --stats sch_centos_hiro:hiro/result/0206_door_v0_4_15  ~/Box/GE\ Digital\ Data\ Science\ 2017/10\ Schindler\ Personal/Intermediate\ result/
 7269  rsync -chavzP --stats sch_centos_hiro:hiro/result/0214_door_easy_v0_4_15  ~/Box/GE\ Digital\ Data\ Science\ 2017/10\ Schindler\ Personal/Intermediate\ result/
 7270  git checkout GED-github -- DoorDetection_GT_Annotation.py
 7271  git diff DoorDetection_GT_Annotation.py
 7272  cat DoorDetection_GT_Annotation.py
 7273  tail DoorDetection_GT_Annotation.py
 7274  vim DoorDetection_GT_Annotation.py
 7275  git commit 
 7276  git commit -m 'Update Annotation Tool version 0.3.0 to visualize the existing GT'
 7277  git commit --ammend
 7278  cd Python/notebook
 7279  cd Projects/modularsensorkit/Annotation_Tools/Door_Events
 7280  rsync -chavzP --stats sch_centos_hiro:hiro/result/0215_door_full_f1_v0.5.2  ~/Box/GE\ Digital\ Data\ Science\ 2017/10\ Schindler\ Personal/Intermediate\ result/
 7281  cd Downloads/Schindler_Download
 7282  cd Ground_Truth_Generation_2018
 7283  cd ~/Projects/modularsensorkit/Annotation_Tools/Door_Events/
 7284  python DoorDetection_GT_Annotation.py
 7285  rsync -chavzP --stats sch_centos_hiro:hiro/result/0216_door_full_f1_v0.4.15  ~/Box/GE\ Digital\ Data\ Science\ 2017/10\ Schindler\ Personal/Intermediate\ result/
 7286  rsync -chavzP --stats sch_centos_hiro:hiro/result/0216_door_full_f1_v0.5.3  ~/Box/GE\ Digital\ Data\ Science\ 2017/10\ Schindler\ Personal/Intermediate\ result/
 7287  rsync -chavzP --stats sch_centos_hiro:hiro/result/0216_door_full_f1_v0.5.2  ~/Box/GE\ Digital\ Data\ Science\ 2017/10\ Schindler\ Personal/Intermediate\ result/
 7288  proxy
 7289  rsync -chavzP --stats sch_centos_hiro:hiro/result/0217_door_full_f1_v0.4.15  ~/Box/GE\ Digital\ Data\ Science\ 2017/10\ Schindler\ Personal/Intermediate\ result/
 7290  cd /Users/212339410/Downloads/Schindler_Download/nrew_MSKs/
 7291  diff MSK092_20180216_0600_20180216_1800_GT_labeled.csv MSK092_20180216_0600_20180216_1800_GT_labeled\ copy.csv
 7292  scp sch_centos_hiro:.screenrc ./
 7293  ssh az1-ss2.a2hosting.com
 7294  scp .screenrc h2o-server:
 7295  ssh -p 7822 hosaltco@az1-ss2.a2hosting.com
 7296  scp sch_centos_hiro:.bashrc .bashrc_centos
 7297  scp .bashrc_centos h2o-server:
 7298  vim ~/.bashrc_centos
 7299  rsync -chavzP --stats sch_centos_hiro:hiro/result/0220_reference_sensor_check  ~/Box/GE\ Digital\ Data\ Science\ 2017/10\ Schindler\ Personal/Intermediate\ result/
 7300  rsync -chavzP --stats sch_centos_hiro:hiro/result/0219_door_full_f1_v0.5.3  ~/Box/GE\ Digital\ Data\ Science\ 2017/10\ Schindler\ Personal/Intermediate\ result/
 7301  mount_centos_g
 7302  rsync -chavzP --stats sch_centos_hiro:hiro/result/0221_door_full_f1_v0.4.15_plot  ~/Box/GE\ Digital\ Data\ Science\ 2017/10\ Schindler\ Personal/Intermediate\ result/
 7303  rsync -chavzP --stats sch_centos_hiro:hiro/result/0221_door_full_f1_v0.4.15/full_f1_v0.4.15_*  ~/Box/GE\ Digital\ Data\ Science\ 2017/10\ Schindler\ Personal/Intermediate\ result/
 7304  rsync -chavzP --stats sch_centos_hiro:hiro/result/0221_door_full_f1_v0.4.15/full_f1_v0.4.15_all.csv  ~/Box/GE\ Digital\ Data\ Science\ 2017/10\ Schindler\ Personal/Intermediate\ result/
 7305  rsync -chavzP --stats sch_centos_hiro:hiro/result/0221_door_full_f1_v0.5.3_both-plot/  ~/Box/GE\ Digital\ Data\ Science\ 2017/10\ Schindler\ Personal/Intermediate\ result/
 7306  rsync -chavzP --stats sch_centos_hiro:hiro/result/0221_door_full_f1_v0.5.3_both-plot  ~/Box/GE\ Digital\ Data\ Science\ 2017/10\ Schindler\ Personal/Intermediate\ result/
 7307  rsync -chavzP --stats sch_centos_hiro:hiro/result/0221_door_full_f1_v0.5.3_both  ~/Box/GE\ Digital\ Data\ Science\ 2017/10\ Schindler\ Personal/Intermediate\ result/
 7308  rsync -chavzP --stats sch_centos_hiro:hiro/result/0221_door_full_f1_v0.5.3  ~/Box/GE\ Digital\ Data\ Science\ 2017/10\ Schindler\ Personal/Intermediate\ result/
 7309  rsync -chavzP --stats sch_centos_hiro:hiro/result/0221_door_full_f1_v0.4.15  ~/Box/GE\ Digital\ Data\ Science\ 2017/10\ Schindler\ Personal/Intermediate\ result/
 7310  rsync -chavzP --stats sch_centos_hiro:hiro/result/0222_door_full_f1_v0.4.15  ~/Box/GE\ Digital\ Data\ Science\ 2017/10\ Schindler\ Personal/Intermediate\ result/
 7311  rsync -chavzP --stats sch_centos_hiro:hiro/result/0222_door_full_f1_v0.5.3  ~/Box/GE\ Digital\ Data\ Science\ 2017/10\ Schindler\ Personal/Intermediate\ result/
 7312  rsync -chavzP --stats sch_centos_hiro:hiro/result/0222_door_full_f1_v0.4.15_V2_MSK068_update  ~/Box/GE\ Digital\ Data\ Science\ 2017/10\ Schindler\ Personal/Intermediate\ result/
 7313  cd ~/Projects/modularsensorkit/Annotation_Tools
 7314  rsync -chavzP --stats sch_centos_hiro:hiro/result/0222_door_full_f1_v0.5.3_drAmp_der1.5  ~/Box/GE\ Digital\ Data\ Science\ 2017/10\ Schindler\ Personal/Intermediate\ result/
 7315  rsync -chavzP --stats sch_centos_hiro:hiro/result/0222_door_full_f1_v0.5.3_der1.5_drAmp  ~/Box/GE\ Digital\ Data\ Science\ 2017/10\ Schindler\ Personal/Intermediate\ result/
 7316  rsync -chavzP --stats sch_centos_hiro:hiro/result/0222_door_full_f1_v0.5.3_both_der1.5  ~/Box/GE\ Digital\ Data\ Science\ 2017/10\ Schindler\ Personal/Intermediate\ result/
 7317  rsync -chavzP --stats sch_centos_hiro:hiro/result/0222_door_full_f1_v0.4.15_V2_der1.5  ~/Box/GE\ Digital\ Data\ Science\ 2017/10\ Schindler\ Personal/Intermediate\ result/
 7318  rsync -chavzP --stats sch_centos_hiro:hiro/result/0222_door_full_f1_v0.4.15_der1.5  ~/Box/GE\ Digital\ Data\ Science\ 2017/10\ Schindler\ Personal/Intermediate\ result/
 7319  rsync -chavzP --stats sch_centos_hiro:hiro/result/0226*  ~/Box/GE\ Digital\ Data\ Science\ 2017/10\ Schindler\ Personal/Intermediate\ result/
 7320  rsync -chavzP --stats sch_centos_hiro:hiro/result/ --include='0226' --exclude='*'  ~/Box/GE\ Digital\ Data\ Science\ 2017/10\ Schindler\ Personal/Intermediate\ result/
 7321  rsync -chavzP --stats sch_centos_hiro:hiro/result/ --include='0226*' --exclude='*'  ~/Box/GE\ Digital\ Data\ Science\ 2017/10\ Schindler\ Personal/Intermediate\ result/
 7322  rsync -rchavzP --stats sch_centos_hiro:hiro/result/ --include='0226*' --exclude='*'  ~/Box/GE\ Digital\ Data\ Science\ 2017/10\ Schindler\ Personal/Intermediate\ result/
 7323  man rsync
 7324  cd ~/Library/]
 7325  cd ~/Library/
 7326  cd Caches
 7327  mv com.alfredapp.Alfred/* ~/Box/GE\ Digital\ Data\ Science\ 2017/Cache_backup_20180227
 7328  mv *  ~/Box/GE\ Digital\ Data\ Science\ 2017/Cache_backup_20180227
 7329  cd Box/GE\ Digital\ Data\ Science\ 2017/
 7330  sudo gzip Cache_backup_20180227
 7331  ssh -i ~/.ssh/DataScience.pem -N -n -L 127.0.0.1:8890:127.0.0.1:8890 sch_bastion -Y -o ServerAliveInterval=50
 7332  ps -el
 7333  ps -el | grep ssh
 7334  ssh -i ~/.ssh/DataScience.pem -N -n -L 127.0.0.1:8891:127.0.0.1:8891 sch_bastion -Y -o ServerAliveInterval=50
 7335  ssh -i  ~/.ssh/DataScience.pem  -N -n -L 127.0.0.1:8890:127.0.0.1:8890 sch_centos -Y # @bastion, portforwarding centos to bastion
 7336  mkdir 
 7337  mkdir Floor_Detection
 7338  vim floor_detection_annotation.py
 7339  git add Floor_Detection/floor_detection_annotation.py
 7340  git commit -m 'Add original annotation tool for Floor Detection created by Jojo'
 7341  python Floor_Detection
 7342  cd Floor_Detection
 7343  python floor_detection_annotation.py
 7344  git diff download_data.py
 7345  cd ../Floor_Detection
 7346  git commit -m 'Door_Events/config_door_events.ini'
 7347  git diff ../Door_Events/download_data.py
 7348  git add ../Door_Events/download_data.py
 7349  git commit -m 'Update MSKID for MSK086 and implement get_aligned_data()'
 7350  git commit -m 'Update Floor Detection Annotation Tool (v0.2.0) to have similar user interface with the one for door '
 7351  git add ../Door_Events/DoorDetection_GT_Annotation.py
 7352  cd ../Door_Events
 7353  git add DoorDetection_GT_Annotation.py
 7354  git commit -m 'Plot the last annotated Data (WIP)'
 7355  git add ../Floor_Detection/input/
 7356  git rm ../Floor_Detection/input/MSK053_20170921_0000_20170922_0000_30ms.csv.gz
 7357  git rm --cached ../Floor_Detection/input/MSK053_20170921_0000_20170922_0000_30ms.csv.gz
 7358  git add input/Dataset_Survey_Tracking.csv
 7359  git add ../Floor_Detection/input/Dataset_Survey_Tracking.csv
 7360  git rm --cached ./input/Dataset_Survey_Tracking.csv
 7361  cd input
 7362  git commit -m 'Add survey tracker and metadata'
 7363  cd ../../Floor_Detection
 7364  pythonw floor_detection_annotation.py
 7365  git add floor_detection_annotation.py
 7366  git commit -m 'Update output format and filename'
 7367  git push git@github.build.ge.com:IndustrialDataScience/SchindlerEdgeAnalytics.git GED-github
 7368  git checkout 
 7369  git diff input/Dataset_Survey_Tracking.csv
 7370  git checkout input/Dataset_Survey_Tracking.csv
 7371  rm ../Door_Events/input/Dataset_Survey_Tracking.csv
 7372  git rebase GED
 7373  git rebase --abort
 7374  git push git@github.build.ge.com:IndustrialDataScience/SchindlerEdgeAnalytics.git GED
 7375  git checkout GED-github
 7376  rsync -rchavzP --stats sch_centos_hiro:hiro/result/ --include='0226*' --exclude='/*'  ~/Box/GE\ Digital\ Data\ Science\ 2017/10\ Schindler\ Personal/Intermediate\ result/
 7377  git merge -h
 7378  git merge GED
 7379  git branch -b GED-github-20170227 --orphan
 7380  git branch --orphan  GED-github-20170227
 7381  git checkout --orphan  GED-github-20170227
 7382  git push git@github.build.ge.com:IndustrialDataScience/SchindlerEdgeAnalytics.git GED-github-20170227
 7383  conda install fire -c conda-forge
 7384  rsync -rchavzP --stats sch_centos_hiro:hiro/result/ --include='0227*' --exclude='/*'  ~/Box/GE\ Digital\ Data\ Science\ 2017/10\ Schindler\ Personal/Intermediate\ result/
 7385  sudo reboot
 7386  rsync -rchavzP --stats sch_centos_hiro:hiro/result/ --include='0228*' --exclude='/*'  ~/Box/GE\ Digital\ Data\ Science\ 2017/10\ Schindler\ Personal/Intermediate\ result/
 7387  rsync -rchavzP --stats sch_centos_hiro_proxy:hiro/result/ --include='0228*' --exclude='/*'  ~/Box/GE\ Digital\ Data\ Science\ 2017/10\ Schindler\ Personal/Intermediate\ result/
 7388  rsync -rchavzP --stats sch_centos_hiro_proxy:hiro/result/ --include='0301*' --exclude='/*'  ~/Box/GE\ Digital\ Data\ Science\ 2017/10\ Schindler\ Personal/Intermediate\ result/
 7389  cp ~/Mount/s3-ge-schindler-ds/FromPredixBasic/MSK058/Dec06_07/MSK058_20171206_0012_20171207_0012_drAccZ.csv.gz ~/Box/GE\ Digital\ Data\ Science\ 2017/10\ Schindler\ Personal/Intermediate\ result/0301_data_misaligned
 7390  cp ~/Mount/s3-ge-schindler-ds/FromPredixBasic/MSK058/Dec06_07/MSK058_20171206_0012_20171207_0012_drAccX.csv.gz ~/Box/GE\ Digital\ Data\ Science\ 2017/10\ Schindler\ Personal/Intermediate\ result/0301_data_misaligned
 7391  cp ~/Mount/s3-ge-schindler-ds/FromPredixBasic/MSK058/Dec06_07/MSK058_20171206_0012_20171207_0012_drAccY.csv.gz ~/Box/GE\ Digital\ Data\ Science\ 2017/10\ Schindler\ Personal/Intermediate\ result/0301_data_misaligned
 7392  cp ~/Mount/s3-ge-schindler-ds/FromPredixBasic/MSK066/Dec05_06/MSK066_20171205_0012_20171206_0012_drAcc* ~/Box/GE\ Digital\ Data\ Science\ 2017/10\ Schindler\ Personal/Intermediate\ result/0301_data_misaligned
 7393  git checkout feat/floor-detection-clean-up
 7394  rsync -chavzP --stats sch_centos_hiro:modularsensorkit ~/Projects mount_ceh
 7395  ssh hrio
 7396  vim .bashrc
 7397  vim .zshrc
 7398  source .zshrc
 7399  geproxy_paris
 7400  vim .ssh/config
 7401  locate myqpps
 7402  locate myapps
 7403  ps -ef | grep myqpps
 7404  ps -el | grep myapps
 7405  cd ~/Library
 7406  ll Logs
 7407  cat GEDR.log
 7408  cat Logs/GEDR.log
 7409  locate corkscrew
 7410  git add SchindlerEdgeAnalytics/seanalytics/use_cases/door_event_detection.py
 7411  ssh -i ~/.ssh/DataScience.pem -N -n -L 127.0.0.1:9999:127.0.0.1:9999 sch_bastion -Y -o ServerAliveInterval=50
 7412  ssh -i  ~/.ssh/DataScience.pem  -N -n -L 127.0.0.1:9999:127.0.0.1:9999 sch_centos_rohit -Y # @bastion, portforwarding centos to bastion
 7413  ssh -i  ~/.ssh/DataScience.pem  -N -n -L 127.0.0.1:19999:127.0.0.1:19999 sch_centos_rohit -Y # @bastion, portforwarding centos to bastion
 7414  rsync -rchavzP --stats sch_centos_hiro_proxy:hiro/result/ --include='0305*' --exclude='/*'  ~/Box/GE\ Digital\ Data\ Science\ 2017/10\ Schindler\ Personal/Intermediate\ result/
 7415  git commit -m 'Add exception handling for GT loading and generate Event Time as int '
 7416  cd Annotation_Tools/Door_Events
 7417  pythonw DoorDetection_GT_Annotation.py
 7418  git commit -m 'Fix Exception handling'
 7419  rsync -rchavzP --stats sch_centos_hiro_proxy:hiro/result/ --include='0307*' --exclude='/*'  ~/Box/GE\ Digital\ Data\ Science\ 2017/10\ Schindler\ Personal/Intermediate\ result/
 7420  cd Annotation_Tools
 7421  pythonw DoorDetection_GT_Annotation.py --sep ;
 7422  pythonw DoorDetection_GT_Annotation.py --sep ";"
 7423  git commit -m 'Add seperator option from command line for opening csv with ";" seperator'
 7424  rsync -rchavzP --stats sch_centos_hiro_proxy:hiro/result/ --include='0307*' --exclude='/*'  ~/Box/GE\ Digital\ Data\ Science\ 2017/10\ Schindler\ Personal/Intermediate\ result/ 
 7425  git checkout -b feat/comput_f1
 7426  git rm --cache Scripts/compute_full_f1.py
 7427  git commit -m 'Add compute_full_f1 updated'
 7428  git diff Scripts/compute_full_f1.py
 7429  git checkout --ours Scripts/compute_full_f1.py
 7430  git commit -m 'Resolve conflict'
 7431  rsync -rchavzP --stats sch_centos_hiro_proxy:hiro/result/ --include='0307*' --exclude='/*'  ~/Box/GE\ Digital\ Data\ Science\ 2017/10\ Schindler\ Personal/Intermediate\ result/ --delete
 7432  git commit -m 'Add check_misalign function'
 7433  git commit -m 'Make check_misalign enabled once, not all the time'
 7434  mount_centos_
 7435  rsync -rchavzP --stats sch_centos_hiro_proxy:hiro/result/ --include='0308*' --exclude='/*'  ~/Box/GE\ Digital\ Data\ Science\ 2017/10\ Schindler\ Personal/Intermediate\ result/ --delete
 7436  rsync -rchavzP --stats sch_centos_hiro_proxy:hiro/result/ --include='0308*' --exclude='/*'  ~/Box/GE\ Digital\ Data\ Science\ 2017/10\ Schindler\ Personal/Intermediate\ result/ Â¥
 7437  rsync -rchavzP --stats sch_centos_hiro_proxy:hiro/result/ --include='0308*' --exclude='/*'  ~/Box/GE\ Digital\ Data\ Science\ 2017/10\ Schindler\ Personal/Intermediate\ result/ 
 7438  rsync -rchavzP --stats sch_centos_hiro_proxy:hiro/result/ --include='0309*' --exclude='/*.png'  ~/Box/GE\ Digital\ Data\ Science\ 2017/10\ Schindler\ Personal/Intermediate\ result/ 
 7439  rsync -rchavzP --stats sch_centos_hiro_proxy:hiro/result/0309_door_full_f1_v0.4.15_fix_misaligned_no_std_filter/ --include='*.csv' --exclude='/*'  ~/Box/GE\ Digital\ Data\ Science\ 2017/10\ Schindler\ Personal/Intermediate\ result/ 
 7440  rsync -rchavzP --stats sch_centos_hiro_proxy:hiro/result/0309* --include='*.csv' --exclude='/*'  ~/Box/GE\ Digital\ Data\ Science\ 2017/10\ Schindler\ Personal/Intermediate\ result/ 
 7441  rsync -rchavzP --stats sch_centos_hiro_proxy:hiro/result/ --include='0309**.csv' --exclude='/*'  ~/Box/GE\ Digital\ Data\ Science\ 2017/10\ Schindler\ Personal/Intermediate\ result/ 
 7442  rsync -rchavzP --stats sch_centos_hiro_proxy:hiro/result/ --include='0309*.csv' --exclude='/*'  ~/Box/GE\ Digital\ Data\ Science\ 2017/10\ Schindler\ Personal/Intermediate\ result/ 
 7443  rsync -rchavzP --stats sch_centos_hiro_proxy:hiro/result/ --include='0309*/*.csv' --exclude='/*'  ~/Box/GE\ Digital\ Data\ Science\ 2017/10\ Schindler\ Personal/Intermediate\ result/ 
 7444  rsync -rchavzP --stats sch_centos_hiro_proxy:hiro/result/ --include='0309*' --exclude='/*'  ~/Box/GE\ Digital\ Data\ Science\ 2017/10\ Schindler\ Personal/Intermediate\ result/ 
 7445  rsync -rchavzP --stats sch_centos_hiro_proxy:hiro/result/ --include='0311*' --exclude='/*'  ~/Box/GE\ Digital\ Data\ Science\ 2017/10\ Schindler\ Personal/Intermediate\ result/ 
 7446  rsync -rchavzP --stats sch_centos_hiro:hiro/result/ --include='0312*' --exclude='/*'  ~/Box/GE\ Digital\ Data\ Science\ 2017/10\ Schindler\ Personal/Intermediate\ result/
 7447  scp ~/Downloads/Schindler_Download/Corrected_GT_by_Tomasz_0313/MSK040_20170724_0600_20170724_0730_GT_labeled.csv sch_centos_hiro:/home/centos/data/ground_truth/gt_20180313/
 7448  scp ~/Downloads/Schindler_Download/Corrected_GT_by_Tomasz_0313/MSK0* sch_centos_hiro:/home/centos/data/ground_truth/gt_20180313/
 7449  git diff MSKSubscriber/configs/subscriber.config
 7450  git diff SchindlerEdgeAnalytics/seanalytics/use_cases/door_event_detection.py
 7451  git diff MSKDataDownloader/raw_data_downloader.py
 7452  git diff MSKDataDownloader/raw_data_downloader.py --cached
 7453  git diff --cached MSKDataDownloader/raw_data_downloader.py
 7454  git rm --cached MSKDataDownloader/raw_data_downloader.py
 7455  git reset HEAD MSKDataDownloader/raw_data_downloader.py
 7456  git push bitbucket GED
 7457  git checkout -b feat/enhance-downloader
 7458  git commit -m 
 7459  git commit -m
 7460  git commit
 7461  git push bitbucket feat/enhance-downloader
 7462  git checkout -b feat/misalign-fix
 7463  git commit -m 'Fix misaligned by using auto corr'
 7464  scp ~/Downloads/Schindler_Download/Corrected_GT_by_Tomasz_2018031318000/MSK0* sch_centos_hiro:/home/centos/data/ground_truth/gt_20180313/
 7465  rsync -rchavzP --stats sch_centos_hiro:hiro/result/ --include='0313*' --exclude='/*'  ~/Box/GE\ Digital\ Data\ Science\ 2017/10\ Schindler\ Personal/Intermediate\ result/
 7466  ssh -i ~/.ssh/DataScience.pem -o ServerAliveInterval=120 -o ProxyCommand='ssh ubuntu@54.214.160.173 W %h:%p -i ~/.ssh/DataScience.pem -o ServerAliveInterval=120 ' centos@10.43.41.90
 7467  pip freeze > requirement_sch-27.txt
 7468  cd Annotation_Tools/Door_Events/
 7469  pythonw DoorDetection_GT_Annotation.py 
 7470  pythonw DoorDetection_GT_Annotation.py -h
 7471  vim ./input/Dataset_Survey_Tracking.csv
 7472  pythonw DoorDetection_GT_Annotation.py --sep ','
 7473  rsync -rchavzP --stats sch_centos_hiro:hiro/result/ --include='0314*' --exclude='/*'  ~/Box/GE\ Digital\ Data\ Science\ 2017/10\ Schindler\ Personal/Intermediate\ result/
 7474  l Annotation_Tools/Door_Events/input
 7475  scp ~/Box/GE\ Digital\ Data\ Science\ 2017/10\ Schindler\ Personal/Intermediate\ result/0315_door_staging_check/condition_staging_check_door_20180315.csv sch_centos_hiro:~/data/
 7476  scp ~/Downloads/Schindler_Download/From\ Max/staging_comparison.ipynb sch_centos_hiro:/home/centos/hiro/notebook
 7477  rsync -rchavzP --stats sch_centos_hiro:hiro/result/ --include='0315*' --exclude='/*'  ~/Box/GE\ Digital\ Data\ Science\ 2017/10\ Schindler\ Personal/Intermediate\ result/ --delete
 7478  git checkout MSKSubscriber/configs/subscriber.config
 7479  git rm SchindlerCloudAnalytics/scanalytics/use_cases/anomaly_detection_closing_too_long_by_floor.py
 7480  git rm SchindlerCloudAnalytics/scanalytics/use_cases/anomaly_detection_opening_too_long_by_floor.py
 7481  ls SchindlerEdgeCloudAnalytics/secanalytics/use_cases/anomaly_detection_*
 7482  rm SchindlerEdgeCloudAnalytics/secanalytics/use_cases/anomaly_detection_opened_too_long.py
 7483  rm SchindlerEdgeCloudAnalytics/secanalytics/use_cases/anomaly_detection_opening_too_long.py
 7484  rm SchindlerCloudAnalytics/scanalytics/use_cases/anomaly_detection_closing_too_long_by_floor.py
 7485  rm SchindlerCloudAnalytics/scanalytics/use_cases/anomaly_detection_opening_too_long_by_floor.py
 7486  rm SchindlerEdgeCloudAnalytics/secanalytics/use_cases/anomaly_detection_closing_too_long.py
 7487  git pull bitbucket develop
 7488  cd Schindler
 7489  cd ../Schindler-Python
 7490  git rebase develop
 7491  git vim ../modularsensorkit/Scripts
 7492  git vim ../modularsensorkit/Scripts/compute_full_f1.py
 7493  vim ../modularsensorkit/Scripts/compute_full_f1.py
 7494  git add Scripts/compute_full_f1.py
 7495  vim ../modularsensorkit/SchindlerEdgeCloudAnalytics/secanalytics/algorithms/utils.py
 7496  vim ../modularsensorkit/SchindlerEdgeCloudAnalytics/secanalytics/algorithms/anomaly_factory.py
 7497  vim ../modularsensorkit/SchindlerEdgeCloudAnalytics/secanalytics/use_cases/anomaly_detection_repetitive_door_event.py
 7498  vim ../modularsensorkit/SchindlerEdgeCloudAnalytics/secanalytics/use_cases/anomaly_detection_opening_too_long.py
 7499  vim ../SchindlerEdgeCloudAnalytics/secanalytics/use_cases/anomaly_detection_door_event_too_long.py
 7500  vim ../modularsensorkit/SchindlerEdgeCloudAnalytics/secanalytics/use_cases/anomaly_detection_door_event_too_long.py
 7501  git commit -m 'Resolved conflicts'
 7502  l ../modularsensorkit/MSKDataDownloader
 7503  python -m ipykernel install --name sch27
 7504  diff -rq ~/Mount/remote_centos_hiro/data/ground_truth/gt_20180313 ~/Downloads/Schindler_Download/ground_truth_dump
 7505  diff -rq ~/Mount/remote_centos_hiro/data/ground_truth/gt_20180313 ~/Box/
 7506  diff -rq ~/Mount/remote_centos_hiro/data/ground_truth/gt_20180313 ~/Box/2018\ DS\&A\ -\ Schindler\ IoEE\ -\ GED\ -\ Foghorn\ External/99_archive/Ground\ Truth\ and\ Accuracy\ Summary/Visual\ Ground\ Truth\ for\ Door\ Detection/Visual_Ground_Truth_Generation_2018/Annotated\ Ground\ Truth\ Data/Visual_Ground_Truth_Dump
 7507  diff -q ~/Mount/remote_centos_hiro/data/ground_truth/gt_20180313 ~/Box/2018\ DS\&A\ -\ Schindler\ IoEE\ -\ GED\ -\ Foghorn\ External/99_archive/Ground\ Truth\ and\ Accuracy\ Summary/Visual\ Ground\ Truth\ for\ Door\ Detection/Visual_Ground_Truth_Generation_2018/Annotated\ Ground\ Truth\ Data/Visual_Ground_Truth_Dump
 7508  rsync -rchavzP --stats sch_centos_hiro:hiro/result/ --include='0315*' --exclude='/*'  ~/Box/GE\ Digital\ Data\ Science\ 2017/10\ Schindler\ Personal/Intermediate\ result/
 7509  ls ~/.git/
 7510  l .git/hooks
 7511  rsync -rchavzP --stats sch_centos_hiro:hiro/result/ --include='0321*' --exclude='/*'  ~/Box/GE\ Digital\ Data\ Science\ 2017/10\ Schindler\ Personal/Intermediate\ result/
 7512  rsync -rchavzP --stats sch_centos_hiro:hiro/result/ --include='0320*' --exclude='/*'  ~/Box/GE\ Digital\ Data\ Science\ 2017/10\ Schindler\ Personal/Intermediate\ result/
 7513  rsync -rchavzP --stats sch_centos_hiro:hiro/result/ --include='0321*' --exclude='/*'  ~/Box/GE\ Digital\ Data\ Science\ 2017/10\ Schindler\ Personal/Intermediate\ result/ --delete
 7514  git clone git@github.com:h1r03/h1r03.github.io.git
 7515  conda list --env
 7516  conda env 
 7517  conda env  -h
 7518  conda activate jupyter-blog
 7519  deactivate
 7520  deactivate jupyter-blog
 7521  pip install pelican Markdown
 7522  pelican-quickstart
 7523  mkdir pluging
 7524  rm pluging
 7525  rm pluging -r
 7526  rm pluging -rf
 7527  mkdir plugins
 7528  git submodule add git://github.com/danielfrg/pelican-ipynb.git plugins/ipynb
 7529  vim 20180322_first_post.ipynb-meta
 7530  rm .ipynb_checkpoints
 7531  rm -rf .ipynb_checkpoints
 7532  python -m pe
 7533  vim pelicanconf.py
 7534  vim .gitignore 
 7535  git push 
 7536  ghp-import -h
 7537  git config --global --get user
 7538  git config --global 
 7539  git config --global  -l
 7540  git config --local 
 7541  git config --local --list
 7542  git config user.name 'h1r03'
 7543  ssh-add -l
 7544  git config user.email --global
 7545  git config user.email 'info.hshioi@gmail.com'
 7546  git remote add origin git@github.com:h1r03/h1r03-h1r03.github.io.git
 7547  git reset 
 7548  git reset a2e602a162e9ac19bac3c1ae09928474f7ea6b5c
 7549  git revert a2e602a162e9ac19bac3c1ae09928474f7ea6b5c
 7550  git revert d5fb281bc0919f3b925eaa7e97c87bb17aa26bb4
 7551  rm .git -rf
 7552  git init 
 7553  git config user.email ''
 7554  git config user.email
 7555  git commit -m 'Initial commit with email'
 7556  git filter-branch --env-filter 'WRONG_EMAIL=""  \nNEW_EMAIL='8764683+h1r03@users.noreply.github.com'\nif [ "$GIT_AUTHOR_EMAIL" = "$WRONG_EMAIL" ]\nthen\n    export GIT_AUTHOR_EMAIL="$NEW_EMAIL"\nfi\n' --tag-name-filter cat -- --branches --tags
 7557  git -rf .git
 7558  git remote add origin git@github.com:h1r03/h1r03.github.io.git
 7559  git config user.email 8764683+h1r03@users.noreply.github.com
 7560  git config user.name h1r03
 7561  cat .ssh/co
 7562  cat .ssh/config
 7563  rsync -chavzP --stats sch_centos_hiro:modularsensorkit ~/Projects --delete
 7564  cd MSKDataDownloader
 7565  python run_data_downloader.py -h
 7566  pythonw run_data_downloader.py -h
 7567  scp ~/Downloads/TripCounter_Model.ipynb sch_centos_hiro:~/hiro/notebook/
 7568  cd ~/Python/jupyter-blog
 7569  mkdir themes
 7570  git rm --cache
 7571  git rm --cached
 7572  git rm . --cached
 7573  git rm . --cached -r
 7574  l content
 7575  mkdir theme
 7576  mv theme themes
 7577  git clone https://github.com/gilsondev/pelican-clean-blog
 7578  git commit -m 'Add pelican-clean-blog theme'
 7579  cd pelican-clean-blog
 7580  cd ../..
 7581  rm . 
 7582  rm ./*
 7583  rm ./* -r 
 7584  rm ./* -rf 
 7585  rm -rf ./* 
 7586  cd ,,
 7587  git clone https://github.com/getpelican/pelican-themes/tree/master/built-texts
 7588  git clone https://github.com/jody-frankowski/blue-penguin.git
 7589  pelican content -d
 7590  pelican content -d -h
 7591  pip install --upgrade pelican
 7592  rm -rf themes/blue-penguin
 7593  git rm --cached themes/blue-penguin 
 7594  git submodule add https://github.com/jody-frankowski/blue-penguin.git  themes/blue-penguin
 7595  git commit -m 'Add blue-penguin theme'
 7596  pelican-themes --install ./themes/blue-penguin
 7597  pelican-themes --install ./themes/pelican-clean-blog
 7598  pelican-themes -v -l
 7599  cd theme
 7600  git checkout pelicanconf.py
 7601  git checkout ./ --hard
 7602  rm pelicanconf.py
 7603  mkdir notebooks
 7604  cp 20180322_first_post.ipynb* notebooks
 7605  rm 20180322_first_post.ipynb*
 7606  brew install pipenv
 7607  cd Python/h1r03.github.io/content/notebooks
 7608  pipenv -h nd against PEP 508 markers
 7609  nd against PEP 508 markers
 7610  pipenv check
 7611  echo
 7612  echo $PS1
 7613  alias nopath="export PS1='> '"
 7614  alias default_display="export PS1='%n@%m:%~%#'"
 7615  nopath
 7616  default_display
 7617  cd .
 7618  which Pipfile
 7619  l | grep pipfile
 7620  l | grep Pipfile
 7621  cat Pipfile.lock
 7622  rm -rf jupyter-blog
 7623  cp 20180322_first_post.ipynb-meta No. 0001 What is Pipenv?.ipynb-meta
 7624  cp 20180322_first_post.ipynb-meta "No. 0001 What is Pipenv?.ipynb-meta"
 7625  vim No.\ 0001\ What\ is\ Pipenv\?.ipynb
 7626  vim No.\ 0001\ What\ is\ Pipenv\?.ipynb-meta
 7627  pipenv --python=/Users/212339410/anaconda/envs/jupyter-blog/bin/python --site-packages
 7628  rm ~/Projects/modularsensorkit/.git/index.lock
 7629  git add *
 7630  git checkout -b feat/annotation-tool-lbAmp
 7631  mv Annotation_Tools AnnotationTools
 7632  git add AnnotationTools/
 7633  git commit -m 'Change directory name from Annotation_Tools to AnnotationTools'
 7634  git rm --cached AnnotationTools/Floor_Detection/input/2017_11_15_MSK Installed on field_v04.csv
 7635  mkdir StepEvents
 7636  cp Door_Events/DoorDetection_GT_Annotation.py StepEvents/step_events_annotation.py
 7637  cd StepEvents
 7638  mkdir output
 7639  mkdir input
 7640  pythonw step_events_annotation.py
 7641  pythonw step_events_annotation.py --sep ','
 7642  git commit -m 'Rename directory'
 7643  git Add StepEvents
 7644  git Add StepEvents/*
 7645  git add StepEvents/
 7646  git rm --cached StepEvents/output/MSK091_20171025_0600_20171025_0630_GT_labeled.csv
 7647  git rm --cached StepEvents/output/MSK091_20171025_0600_20171025_0830_step_GT_labeled.csv
 7648  git rm --cached StepEvents/download_data.pyc
 7649  touch StepEvents/output/dummy.txt
 7650  git commit -m 'Add Annotation Tool for Step Events e.g., Light Barrier and Motion Detection Sensor'
 7651  cd Door_Events
 7652  mkdir input 
 7653  git rm --cached input/Dataset_Survey_Tracking.csv
 7654  git add input/
 7655  git commit -m 'Add input directory and survey tracking for Door'
 7656  mv DoorDetection_GT_Annotation.py door_events_annotation.py
 7657  git commit -m 'Rename Annotation tool for Door Events'
 7658  rm Door_Events DoorEvents
 7659  rm -r Door_Events DoorEvents
 7660  mv -r Door_Events DoorEvents
 7661  git checkout Door_Events
 7662  mv Door_Events DoorEvents
 7663  git git reset HEAD~   
 7664  git add DoorEvents/
 7665  git commit -m 'Rename Door_Events Directory'
 7666  mv Floor_Detection FloorEvents
 7667  git add FloorEvents/
 7668  git commit -m 'Rename Floor_Detection Directory'
 7669  cd FloorEvents
 7670  git add StepEvents/step_events_annotation.py
 7671  git commit -m 'Disable plot_annotated_events()'
 7672  git add StepEvents/output/
 7673  git rm --cached StepEvents/output/MSK*
 7674  git commit -m 'Add dummy file in output'
 7675  brew install asciinema
 7676  pythonw door_events_annotation.py --sep ','
 7677  git commit -m 'Fix the Event_Type_Enumeration string'
 7678  git push https://hiro_@bitbucket.org/ioeeschindler/modularsensorkit.git feat/annotation-tool-lbAmp
 7679  cd Box/Administrative/Trip\ to\ Berlin\ 2018\ Mar
 7680  cd Receipt
 7681  cd recepts_paris_berlin_2018
 7682  for f in *.jpg:\ndo echo "${f/IMG_*_/)}"; done
 7683  for f in *.jpg;\necho $f
 7684  for f in *.jpg;\necho ${f/IMG_/}
 7685  for f in *.jpg;\necho mv "$f" "${f/IMG_/}"
 7686  for f in *.jpg;\ndo echo mv "$f" "${f/IMG_/}"; done
 7687  for f in *.jpg;\ndo mv "$f" "${f/IMG_/}"; done
 7688  scp ~/Downloads/Download_data_science/international-airline-passengers.csv sch_centos_hiro ~/data/
 7689  scp ~/Downloads/Download_data_science/international-airline-passengers.csv sch_centos_hiro:~/data/
 7690  git clone https://github.com/ChadFulton/pymar.git
 7691  pipenv 0h
 7692  cd pymar
 7693  rm -rf pymar
 7694  git clone git@github.com:h1r03/pymar.git
 7695  cd py
 7696  cd pymar/
 7697  cd Python/pymar
 7698  git add -a
 7699  git commit -m 'Converted from an older notebook format (v3) to the current notebook format (v4)'
 7700  pip install better_exceptions
 7701  export BETTER_EXCEPTIONS=1  # Linux / OSX
 7702  python -m better_exceptions
 7703  git clone https://github.com/Liuyi-Hu/regime_switch_model.git
 7704  souce deactivate 
 7705  pipenv deactivate
 7706  pipenv exit
 7707  pip install -U --user regime_switch_model
 7708  mount_centos_hiroi
 7709  mount_ceh
 7710  ssh -i ~/.ssh/DataScience.pem -N -n -L 127.0.0.1:8889:127.0.0.1:8889 sch_bastion -Y -o ServerAliveInterval=50
 7711  git log -1 HEAD
 7712  more post-update.sample
 7713  less commit-msg.sample
 7714  more update.sample
 7715  vim post-commit
 7716  git commit -m 'Test'
 7717  egrep GIT
 7718  egrep -h
 7719  man egrep 
 7720  echo Running $BASH_SOURCE
 7721  echo Running $ZSH
 7722  echo $PWD
 7723  chmod +x .git/hooks/post-commit
 7724  scp sch_centos_hiro:/home/centos/.gitignore_global ~/
 7725  l ~/.gitignore_global
 7726  l  /Users/212339410/.gitignore_global
 7727  cd  /Users/212339410/.gitignore_global
 7728  cat  /Users/212339410/.gitignore_global
 7729  q
 7730  mv   /Users/212339410/.gitignore_global /Users/212339410/.gitignore_global_gitignore.bak
 7731  rm /Users/212339410/.gitignore_global
 7732  mkdir /Users/212339410/.gitignore_global
 7733  cd ~/.gitignore_global
 7734  mv ~/.gitignore_global_gitignore.bak ~/.gitignore_global/
 7735  cp .gitignore_global_gitignore.bak .gitignore
 7736  git config --global core.excludesfile ~/.gitignore_global
 7737  git config --global core.excludesfile
 7738  git config --global core.excludesfile ~/.gitignore_global/.gitignore
 7739  rsync -chavzP --stats sch_centos_hiro:modularsensorkit ~/Projects  -d 
 7740  git rev-list 
 7741  git rev-list --parents
 7742  git rev-list --parents .
 7743  rm SchindlerCloudAnalytics/scanalytics/use_cases/anomaly_detection_repetitive_closing_too_long.py
 7744  rm SchindlerCloudAnalytics/scanalytics/use_cases/anomaly_detection_repetitive_closing_too_long*
 7745  rm SchindlerCloudAnalytics/scanalytics/use_cases/anomaly_detection_repetitive_opening_too_long*
 7746  git add test.txt
 7747  git commit -m 'Add test hook'
 7748  git reset --hard HEAD^
 7749  cd TESTDATA
 7750  cd Dashboards
 7751  cd DB_Door_Events_Schindler_Ebikon
 7752  python run_dashboard.py
 7753  pip source dash_renderer
 7754  pip install dash_renderer
 7755  ssh -i ~/.ssh/DataScience.pem -N -n -L 127.0.0.1:8050:127.0.0.1:8050 sch_bastion -Y -o ServerAliveInterval=50
 7756  cd Box/2018\ DS\&A\ -\ Schindler\ IoEE\ -\ GED\ -\ Foghorn\ External/04_Algorithm_Test_Results/ground_truth/door_events
 7757  cd 2018
 7758  cd Visual_Ground_Truth_Dump
 7759  git git add SchindlerEdgeAnalytics/test/test_door_event_detection.py
 7760  git add SchindlerEdgeAnalytics/test/test_door_event_detection.py
 7761  git add TESTDATA/door_event_detection/
 7762  git rm --cached SchindlerEdgeAnalytics/seanalytics/use_cases/door_event_detection_v5.py
 7763  git commit -m 'Add unittest for Door Event Detection V5 (0.6.0) development'
 7764  git reset HEAD~1
 7765  rsync -chavzP --stats ~/Projects/modularsensorkit sch_centos_hiro:/home/centos
 7766  rsync -chavzP --stats sch_centos_hiro:modularsensorkit ~/Projects 
 7767  rsync -rchavzP --stats sch_centos_hiro:hiro/result/ --include='0406*' --exclude='/*'  ~/Box/GE\ Digital\ Data\ Science\ 2017/10\ Schindler\ Personal/Intermediate\ result/
 7768  ln -s /Users/212339410/Box/Analytics/Python/h1r03.github.io/content/md ~/Box/GE_digital_data_science_2018/md
 7769  pelican content -s publishconf.py
 7770  git commit -m 'Add note for No.0002'
 7771  rsync -rchavzP --stats sch_centos_hiro:hiro/result/ --include='0408*' --exclude='/*'  ~/Box/GE\ Digital\ Data\ Science\ 2017/10\ Schindler\ Personal/Intermediate\ result/
 7772  login wsroot
 7773  /dgagent/dgctl --version
 7774  cd .git
 7775  cd hooks
 7776  cat post-commit
 7777  scp -r sch_centos_hiro:/home/centos/.gitignore_global ~/
 7778  screen 
 7779  screen -ls
 7780  ssh hior
 7781  rsync -rchavzP --stats sch_centos_hiro:hiro/result/ --include='0409*' --exclude='/*'  ~/Box/GE\ Digital\ Data\ Science\ 2017/10\ Schindler\ Personal/Intermediate\ result/
 7782  ~~=~-]]]]]aq 
 7783  ~~=~-]]]]]aq r
 7784  rsync -rchavzP --stats sch_centos_hiro:hiro/result/ --include='041*' --exclude='/*'  ~/Box/GE\ Digital\ Data\ Science\ 2017/10\ Schindler\ Personal/Intermediate\ result/
 7785  rsync -rchavzP --stats sch_centos_hiro:hiro/result/ --include='041*' --exclude='/*'  ~/Box/GE\ Digital\ Data\ Science\ 2017/10\ Schindler\ Personal/Intermediate\ result/ -d
 7786  cd Deep-Learning-LSTM-Survival
 7787  conda env --info
 7788  conda list env
 7789  cd agilereport
 7790  pip install xlrd
 7791  python agilereport/gen_rmarkdown.py -o chapters/parents_report.rmd -o config/report_config.xlsx
 7792  cat config/report_config.xlsx
 7793  python agilereport/gen_rmarkdown.py -o chapters/parents_report.rmd -o config/report_config_rm_sample.xlsx
 7794  git checkout config/report_config_rm_sample.xlsx
 7795  git checkout config/report_config.xlsx
 7796  pip install cufflinks
 7797  pip install --upgrade pipi
 7798  python agilereport/gen_rmarkdown.py -o chapters/parents_report.rmd -i config/report_config.xlsx
 7799  Retrying (Retry(total=3, connect=None, read=None, redirect=None)) after connection broken by 'NewConnectionError('<pip._vendor.requests.packages.urllib3.connection.VerifiedHTTPSConnection object at 0x1062d0908>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known',)': /simple/matplotlib/
 7800  Rscript -e  ".libPaths(c( .libPaths(), '/Library/Frameworks/R.framework/Resources/library')); .libPaths(); rmarkdown::render('temp-report.Rmd')"; open temp-report.pdf
 7801  cd config
 7802  cd ../chapters
 7803  Rscript -e  ".libPaths(c( .libPaths(), '/Library/Frameworks/R.framework/Resources/library')); .libPaths(); rmarkdown::render('parents_report.rmd')"; open parents_report.pdf
 7804  Rscript -e  ".libPaths(c( .libPaths(), '/Library/Frameworks/R.framework/Resources/library')); .libPaths(); rmarkdown::render('chapters/parents_report.rmd')"; open parents_report.pdf
 7805  l chapters
 7806  python agilereport/gen_rmarkdown.py -o parents_report_sample.rmd -i config/report_config.xlsx
 7807  Rscript -e  ".libPaths(c( .libPaths(), '/Library/Frameworks/R.framework/Resources/library')); .libPaths(); rmarkdown::render('parents_report_sample.rmd')"
 7808  open parents_report_sample.pdf
 7809  Rscript -e  ".libPaths(c( .libPaths(), '/Library/Frameworks/R.framework/Resources/library')); .libPaths(); rmarkdown::render('parents_report_sample.rmd')"; open parents_report_sample.pdf
 7810  subl ~/.config/karabiner/karabiner.json
 7811  brew cask install keycastr
 7812  brew update\nbrew cask install kawa\n
 7813  gzip -h 
 7814  gzip /Users/212339410/Downloads/Download_Slack/RAW_DATA_MSK056_ALIGNED_1521187200000_1521208800000.csv 
 7815  ssh -i  ~/.ssh/DataScience.pem  -N -n -L 127.0.0.1:8890:127.0.0.1:8890 sch_centos_hiro -Y # @bastion, portforwarding centos to bastion
 7816  ï½
 7817  rsync -rchavzP --stats sch_centos_hiro:hiro/result/ --include='041*' --exclude='/*'  ~/Box/GE\ Digital\ Data\ Science\ 2017/10\ Schindler\ Personal/Intermediate\ result/ --delete
 7818  head /Users/212339410/Box/GE Digital Data Science 2017/10 Schindler Personal/Intermediate result/0416_door_full_f1_v0.6.3_0416/full_f1_v0.6.3_all.csv
 7819  head "/Users/212339410/Box/GE Digital Data Science 2017/10 Schindler Personal/Intermediate result/0416_door_full_f1_v0.6.3_0416/full_f1_v0.6.3_all.csv"
 7820  rsync -rchavzP --stats sch_centos_hiro:hiro/result/ --include='041*' --exclude='/*'  ~/Box/GE\ Digital\ Data\ Science\ 2017/10\ Schindler\ Personal/Intermediate\ result/ 
 7821  rsync -rchavzP --stats sch_centos_hiro:hiro/result/ --include='0418_door_full_f1_v0.6.5_motion_acc*' --exclude='/*'  ~/Box/GE\ Digital\ Data\ Science\ 2017/10\ Schindler\ Personal/Intermediate\ result/ 
 7822  git config user.name
 7823  l ~/.ssh 
 7824  \nssh-keygen -t rsa -b 4096
 7825  cat ~/.ssh/id_rsa_20180418_github
 7826  git -C -h
 7827  ls ~/.ssh
 7828  chmod 400 ~/.ssh/id_rsa_20180418_github.pub
 7829  vim .git/config
 7830  git pull origin dev -v
 7831  git commit -m 'Generate post'
 7832  cd blue-penguin
 7833  pelican 0h
 7834  pelican content
 7835  pelican content -h
 7836  git commit -m 'Change theme'
 7837  git submodule add https://github.com/nairobilug/pelican-alchemy themes/pelican-alchemy
 7838  git commit -m 'Change the theme to pelican-alchemy'
 7839  git commit -m 'Enable https'
 7840  rsync -rchavzP --stats sch_centos_hiro:hiro/result/ --include='0418*' --exclude='/*'  ~/Box/GE\ Digital\ Data\ Science\ 2017/10\ Schindler\ Personal/Intermediate\ result/ 
 7841  git clone https://github.com/haseebr/competitive-programming/blob/master/Materials/Algorithhms%204th%20Edition%20by%20Robert%20Sedgewick%2C%20Kevin%20Wayne.pdf
 7842  souce activate jupyter-blog
 7843  python -m pelican.server --port 8889; open localhost:8889
 7844  python -m pelican.server --port 8889
 7845  python -m pelican.server 
 7846  python -m pelican.server ; open localhost:8000
 7847  open http://localhost:8000
 7848  kill 54262
 7849  python -m pelican.server -h
 7850  python -m pelican.server & open localhost:8000
 7851  kill 54367
 7852  kill 54422
 7853  python -m pelican.server & open http://localhost:8000
 7854  cd output; python -m pelican.server
 7855  git submodule add https://github.com/getpelican/pelican-themes/tree/master/notmyidea-cms themes/notmyidea-cms
 7856  git submodule add https://github.com/gilsondev/pelican-clean-blog.git  themes/pelican-clean-blog
 7857  pelican-themes -c 
 7858  pelican-themes -r pelican-clean-blog
 7859  pelican-themes -r notmyidea
 7860  pip install pelican --upgrade
 7861  git commit -m 'Add python peop8 post'
 7862  cp output/theme/images/icons/youtube.png ./output/favicon.ico
 7863  cd output
 7864  python -m pelican.server
 7865  rm favicon.ico
 7866  l themes
 7867  pelican-themes -h
 7868  pelican-themes -i themes/pelican-clean-blog
 7869  pelican-themes -i themes/pelican-alchemy
 7870  cd themes
 7871  cd pelican-
 7872  cd pelican-alchemy
 7873  cd .. 
 7874  git clone git@github.com:Unity-Technologies/ml-agents.git
 7875  git commit -m 'Update blog'
 7876  mkdir test_tessorflow
 7877  cd test_tessorflow
 7878  tensorboard --logdir ./
 7879  tensorboard 
 7880  pip3
 7881  python -V
 7882  source deacvtivate
 7883  conda list envs
 7884  conda info list
 7885  cd Box/
 7886  cd Development
 7887  mkdir Unity
 7888  cd Unity
 7889  ln -s ~/Python/ml-agents /Users/212339410/Box/Development/Unity/ml-agents
 7890  ln -s ~/Python/ml-agents /Users/212339410/Box/Development/Unity/RollerBall/ml-agents
 7891  cd RollerBall
 7892  unlink ml-agents
 7893  ln -s ~/Python/ml-agents /Users/212339410/Box/Development/Unity/RollerBall/Assets/ml-agents
 7894  unlink ./Assets/ml-agents
 7895  which excel
 7896  locate excel
 7897  locate excel.app
 7898  locate Microsoft Excel.app
 7899  rsync -rchavzP --stats sch_centos_hiro:hiro/result/ --include='0419*' --exclude='/*'  ~/Box/GE\ Digital\ Data\ Science\ 2017/10\ Schindler\ Personal/Intermediate\ result/ 
 7900  rsync -rchavzP --stats sch_centos_hiro:hiro/result/ --include='0420*' --exclude='/*'  ~/Box/GE\ Digital\ Data\ Science\ 2017/10\ Schindler\ Personal/Intermediate\ result/ 
 7901  mount_centos_rohit
 7902  conda remove --name tensorflow --all
 7903  conda list -n sch-27
 7904  conda list -n sch27
 7905  conda create  -n tensorflow --python 3.6 
 7906  conda create  -n tensorflow python=3.6 
 7907  source activate tensorflow
 7908  pip install --upgrade virtualenv
 7909  easy_install -U pip
 7910  pip3 install --upgrade tensorflow  
 7911  pip install numpy
 7912  pip install Scipy
 7913  pip install scikit-learn
 7914  cd ~/Python/ml-agents
 7915  cd python
 7916  vim ~/.ssh/co
 7917  rsync -rchavzP --stats sch_centos_hiro:hiro/result/ --include='0423*' --exclude='/*'  ~/Box/GE\ Digital\ Data\ Science\ 2017/10\ Schindler\ Personal/Intermediate\ result/ 
 7918  r
 7919  which r
 7920  R
 7921  ll
 7922  pip install jupyter --upgrade
 7923  pip install ipython --upgrade
 7924  pip install matplotlib --upgrade
 7925  cp Keras.ipynb-meta US Census in 2014.ipynb-meta
 7926  cp Keras.ipynb-meta "US Census in 2014.ipynb-meta"
 7927  subl US\ Census\ in\ 2014.ipynb-meta
 7928  git commit -m 'Add post for US Census EDA'
 7929  subl pelicanconf.py 
 7930  rsync -rchavzP --stats sch_centos_hiro:hiro/result/ --include='0424*' --exclude='/*'  ~/Box/GE\ Digital\ Data\ Science\ 2017/10\ Schindler\ Personal/Intermediate\ result/ 
 7931  scp sch_centos_rohit:/home/centos/bitbucket/modularsensorkit/AlgorithmPerformance/performance_assessment/rohit_notebook/jupyter_notebook/MSK082_MSK088_prediction.csv ~/Box/GE\ Digital\ Data\ Science\ 2017/10\ Schindler\ Personal/Intermediate\ result/0425_door_full_f1_v0.6.11_all/MSK082_MSK088_prediction.csv
 7932  scp ~/Box/GE\ Digital\ Data\ Science\ 2017/10\ Schindler\ Personal/Intermediate\ result/0425_door_full_f1_v0.6.11_all/MSK082_MSK088_prediction.csv sch_centos_hiro:~/result/0425_door_full_f1_v0.6.10_1013_plot/intermediate_result/MSK082_MSK088_prediction.csv
 7933  rsync -rchavzP --stats sch_centos_hiro:hiro/result/ --include='0425*' --exclude='/*'  ~/Box/GE\ Digital\ Data\ Science\ 2017/10\ Schindler\ Personal/Intermediate\ result/ 
 7934  screen -R
 7935  ssh sch_centos_hiroo
 7936  travis
 7937  norproxy
 7938  gem -h
 7939  gem install travis
 7940  sudo gem install travis
 7941  travis pubkey
 7942  travis encrpty d137598846951b7ce8ad7107eb6a756b737085db --add
 7943  travis encrypt d137598846951b7ce8ad7107eb6a756b737085db --add 
 7944  travis accounts
 7945  travis login -h
 7946  travis login 
 7947  ruby 0v
 7948  rvm update ruby
 7949  \curl -sSL https://get.rvm.io | bash -s stable
 7950  source /Users/212339410/.rvm/scripts/rvm
 7951  source ~/.profile
 7952  rvm list known
 7953  rvm install ruby-2.4.2
 7954  xcode-select --install
 7955  brew install ruby
 7956  rvm use ruby-2.5.1 --default
 7957  em install bundler
 7958  gem install bundler
 7959  sudo em install bundler
 7960  sudo gem install bundler
 7961  export PATH=/usr/local/bin:$PATH
 7962  brew link --overwrite ruby
 7963  rvn -v
 7964  travis login -v
 7965  travis report --pro
 7966  ruby -v
 7967  travis report
 7968  sudo gem uninstall travis
 7969  gem uninstall travis-cl
 7970  sudo gem uninstall travis-cl
 7971  gem update --system
 7972  gem uninstall travis
 7973  gem install travis -v 1.8.8 --no-rdoc --no-ri
 7974  sudo gem install travis -v 1.8.8 --no-rdoc --no-ri
 7975  travis -v
 7976  travis login
 7977  travis encrypt
 7978  travis encrypt a
 7979  travis restart
 7980  travis encrypt a --org
 7981  travis encrypt a --org --add
 7982  travis login --auto
 7983  travis login --pro
 7984  travis encrypt 'GITHUB_TOKEN=d137598846951b7ce8ad7107eb6a756b737085db' --add
 7985  git commit -m 'Add .travis.yml and setup'
 7986  git commit -m 'Update travis key'
 7987  pelican-themes -i ./theme/pelican-clean-blog
 7988  rm requirement.txt
 7989  open content
 7990  pelican content --ignore-cache
 7991  pelican-themes -l
 7992  git commit -m 'Update blog' 
 7993  git commit -m 'Update .submodule'
 7994  git commit -m 'Fix typo'
 7995  git commit -m 'Minimize requirement.txt'
 7996  git commit -m 'Change the theme'
 7997  travis encrypt 'GITHUB_TOKEN=fc957960db03fba19ab4a4c06db682c40300b810' --add
 7998  git commit -m 'Add another github token'
 7999  travis encrypt 'GITHUB_TOKEN=268a22c61dfad5d13364be218ff57c3bf3d93e36' --add
 8000  git commit -m 'Change token with admin-hook'
 8001  ssh-keygen -h
 8002  man ssh-keygen 
 8003  ssh-keygen -f deploy_rw -N 'deploy-read'
 8004  cat ~/.ssh/20161121 Ul
 8005  ssh-keygen -f deploy_rw -N ''
 8006  mv deploy_rw* ~/.ssh
 8007  cat ~/.ssh/deploy_rw.pub
 8008  cat ~/.ssh/deploy_rw
 8009  git commit -m 'Push again for testing'
 8010  ghp-import output -b master
 8011  rsync -rchavzP --stats sch_centos_hiro:hiro/result/ --include='0426*' --exclude='/*'  ~/Box/GE\ Digital\ Data\ Science\ 2017/10\ Schindler\ Personal/Intermediate\ result/ 
 8012  ssh -i  ~/.ssh/DataScience.pem  -N -n -L 127.0.0.1:8890:127.0.0.1:8890 sch_centos_hiro -Y # @bastion, portforwarding centos to bastionr
 8013  rsync -rchavzP --stats sch_centos_hiro:hiro/result/ --include='0427*' --exclude='/*'  ~/Box/GE\ Digital\ Data\ Science\ 2017/10\ Schindler\ Personal/Intermediate\ result/ 
 8014  pip install jupyter 
 8015  pip install nbdime
 8016  nbdime config-git --enable --global
 8017  cd Python/h1r03.github.io
 8018  git checkout content/notebooks/data/Reported_Voting_and_Registration_by_Sex_and_Single_Years_of_Age_2014.xls
 8019  nbgitdiff
 8020  nbdiff 
 8021  nbdiff-web
 8022  git add Makefile
 8023  git commit -m 'Change github publish command with force option'
 8024  git commit -m 'Update US Census notebook'
 8025  git commit -m 'Update Makefile'
 8026  git commit -m 'Update http to https'
 8027  git commit -m 'Add notes for Deep learning'
 8028  git submodule add https://github.com/getpelican/pelican-plugins.git/render_math pluging/render_math
 8029  git submodule add https://github.com/getpelican/pelican-plugins/tree/master/render_math  pluging/render_math
 8030  git clone https://github.com/getpelican/pelican-plugins.git
 8031  cd plugins
 8032  l ipynb
 8033  l ../pelican-plugins/pelican-ipynb
 8034  cd ../pelican-plugins/pelican-ipynb
 8035  pelican-plugins
 8036  git add pelican-plugins/render_math
 8037  git commit -m 'Add render_math plugin'
 8038  git add -A 
 8039  git commit -m 'Update .gitignore and pelicanconf for render_math'
 8040  git commit -m 'Update pelicanconf'
 8041  git commit -m 'Move pelican-plugin ipynb under pelican-plugins'
 8042  git reset ~HEAD
 8043  git add pelican-plugings/ipynb
 8044  git add -r pelican-plugings/ipynb
 8045  git add pelican-plugings/ipynb/
 8046  git add pelican-plugins/ipynb/
 8047  git reset .gitignore
 8048  git checkout .gitignore
 8049  git commit -m 'Move ipynb'
 8050  TRAVIS_REPO_SLUG=h1r03
 8051  make publish github
 8052  make github
 8053  make html
 8054  make publish
 8055  make debug=1 github
 8056  git stauts
 8057  git reset HEAD~  
 8058  git commit -m 'Update'
 8059  git push dev
 8060  git commit -m 'Make clean'
 8061  rm pelican-plugins/ipynb -r
 8062  rm -r pelican-plugins/ipynb
 8063  git commit -m 'Update ipynb'
 8064  git commit -m 'Resolved conflict'
 8065  git commit -m 'Fix siteurl with s'
 8066  git clone https://github.com/getpelican/pelican-plugins.git 
 8067  rm -rf pelican-plugins
 8068  git clone --recursive https://github.com/getpelican/pelican-plugins
 8069  git commit -m 'Update plugin dir'
 8070  cd pluging
 8071  rm -r pluging
 8072  cd pelican-plugins
 8073  git add pelican-plugins/pelican-ipynb/
 8074  git commit -m 'Add pelican-ipynb plugin'
 8075  git statsu
 8076  git commit -m 'Fix the path for ipynb plugin'
 8077  sgit status
 8078  git commit -m 'Add math_render'
 8079  git clone https://github.com/adam-p/markdown-here.wiki.git
 8080  git rm -r markdown-here.wiki/
 8081  git rm -r markdown-here.wiki
 8082  rm -r markdown-here.wiki
 8083  cd md
 8084  source .zsh
 8085  source ~/.zsh
 8086  rm -rf markdown-here.wiki
 8087  vim .gitignore
 8088  git add content/md/*
 8089  git commit -m 'Add posts'
 8090  git add .gitignore
 8091  git commit -m 'Update gitignore to exclude notes'
 8092  git commit -m 'Add ace-editor and test math in deep learning note'
 8093  git add pelicanconf.py
 8094  git add asset/
 8095  git commit -m 'Add asset directory with images'
 8096  pelican --version
 8097  Typogrify
 8098  git commit -m 'Add debug code for math'
 8099  git commit -m 'Update MARKDOWN param'
 8100  git commit -m 'Add logging to debug'
 8101  git commit -m 'Remove render_math'
 8102  git add content/images/
 8103  git commit -m 'Move images under contents'
 8104  git commit -m 'Add images in static path'
 8105  git commit -m 'Add google analytics code'
 8106  pip install smartypants
 8107  git commit -m 'Make Typografy True'
 8108  git commit -m 'Update requirements.txt'
 8109  git psuh origin feat/floor
 8110  gut push 
 8111  pip install pip-chill 
 8112  pip-chill --no-version
 8113  pip-chill 
 8114  conda list --export > requirements.txt
 8115  cat requirements.txt
 8116  git commt -m 'Update requirements.txt using conda list and update tag, add pages'
 8117  git commit -m 'Update requirements.txt using conda list and update tag, add pages'
 8118  git commit -m 'Update conda install script'
 8119  git commit -m 'Rever requirements.txt by pip freeze'
 8120  git commit -m 'Update anaconda navigator 0.1.0'
 8121  pip install anaconda-navigator==0.1.0
 8122  pip install pipenv --upgrade
 8123  pipenv install
 8124  pipenv install three
 8125  rm Pipfile
 8126  pipenv install --three
 8127  pipenv install -r requirements.txt
 8128  . /Users/212339410/.local/share/virtualenvs/Python-CYJ_W0AE/bin/activate
 8129  cd ~/Python/h1r03.github.io
 8130  pip install typogrify
 8131  pelican content 
 8132  make clean
 8133  pelican clean
 8134  pipenv sync
 8135  pipenv update 
 8136  cat Pipfile
 8137  mv Pipfile h1r03.github.io
 8138  mv Pipfile.lock h1r03.github.io
 8139  cd h1r03.github.io
 8140  git add Pipfile.lock
 8141  git add content/pages/
 8142  git commit -m 'Update requirement.txt by pipenv and update tag by ,'
 8143  git commit -m 'Update makefile and .travis based onpipenv'
 8144  git commit -m 'Delete pip install strategy'
 8145  git commit -m 'Add pip install pipenv'
 8146  ipython --version
 8147  pipenv lock
 8148  git commit -m 'Update travis.yml'
 8149  pip install pelican-themes 
 8150  git commit -m 'Add pipenv shell'
 8151  git commit -m u
 8152  git commit -m 'Change pipenv run and url for my blog'
 8153  pipenv install -h
 8154  pipenv install --dev
 8155  .screen
 8156  cat ~/.screenrc
 8157  git commit -m 'Update travis'
 8158  git commit -m 'Update .travis.yml'
 8159  git commit -m 'Updaete travis yml'
 8160  git commit -m 'Add google analytics tracking id'
 8161  pipenv install  typogrify
 8162  git commit -m 'Fix indentation and update pipfile'
 8163  ssh 10.64.17.178
 8164  ssh 68.66.224.129
 8165  cat ~/.ssh/config
 8166  git commit -m 'Update pelican command'
 8167  git commit -m 'Remove double quotes'
 8168  git push origin dev 
 8169  git pull bitbucket GED
 8170  git pull bitbucket GED -v
 8171  which git
 8172  l /usr/local/bin/git
 8173  l  ../git/bin/git
 8174  l  ../git/bin/
 8175  git -h
 8176  rm SchindlerEdgeAnalytics/seanalytics/use_cases/door_event_detection_v5.py
 8177  rm SchindlerEdgeAnalytics/test/test_door_event_detection.py
 8178  rm TESTDATA/door_event_detection/MSK054_2017_0823_0721_0823_0726.csv
 8179  rm TESTDATA/door_event_detection/MSK054_2017_0823_0721_0823_0726_door_GT_labeled.csv
 8180  git fetch https://hiro_@bitbucket.org/ioeeschindler/modularsensorkit.git  -a
 8181  git fetch -h
 8182  git fetch https://hiro_@bitbucket.org/ioeeschindler/modularsensorkit.git  --all
 8183  git fetch  --all
 8184  rm -rf Annotation_Tools
 8185  rm SchindlerEdgeAnalytics/tests/*
 8186  rm SchindlerCloudAnalytics/tests/README
 8187  rm SchindlerEdgeCloudAnalytics/tests/README
 8188  git pull https://hiro_@bitbucket.org/ioeeschindler/modularsensorkit.git master
 8189  git fetch  https://hiro_@bitbucket.org/ioeeschindler/modularsensorkit.git 
 8190  git pull bitbucket GED 
 8191  ssh-keygen 
 8192  ls ~/.ssh 
 8193  echo $GIT_SSH
 8194  which git-upload-pack
 8195  git-upload-pack -h
 8196  ssh bitbucket.org
 8197  ssh bitbucket
 8198  ssh -T git@bitbucket.org
 8199  ssh -T hg@bitbucket.org 
 8200  ssh -v git@bitbucket.org
 8201  ssh -T git@github.com -i ~/.ssh/id_rsa
 8202  ssh -T git@bitbucket.org  -i ~/.ssh/id_rsa
 8203  ssh -T git@bitbucket.org  -i ~/.ssh/id_rsa -h
 8204  ssh -T git@bitbucket.org  -i ~/.ssh/id_rsa -v
 8205  ssh -T git@github.com -i ~/.ssh/id_rsa -v
 8206  ssh -T bitbucket -i ~/.ssh/id_rsa -v
 8207  ssh -T github.com
 8208  git -T bitbucket 
 8209  ssh -T bitbucket
 8210  ssh -T bitbucket -v
 8211  git pull bitbucket master
 8212  git checkout -b feat/annotation-tool-update
 8213  pythonw AnnotationTools/DoorEvents
 8214  pythonw AnnotationTools/DoorEvents/door_events_annotation.py
 8215  tail -f AnnotationTools/DoorEvents/door_events_annotation.log
 8216  git stash 
 8217  git checkout -b fix/raw_data_downloader
 8218  cd AnnotationTools/DoorEvents
 8219  git add MSKDataDownloader/raw_data_downloader.py
 8220  git commit -m 'Fix aligning data using Align class'
 8221  git staut
 8222  vim .git/hooks/post-commit
 8223  git commit -m 'Use data_raw_donwloader under MSKSubscriber and fix next 1 hours button starting from the same time'
 8224  python optmodel/optmodel.py -C target, target1, b, c
 8225  git diff SchindlerEdgeAnalytics/seanalytics/use_cases/door_event_detection_v5.py
 8226  git add SchindlerEdgeAnalytics/seanalytics/use_cases/door_event_detection_v5.py
 8227  git commit -m 'Add minimum window length for rolling std'
 8228  git commit -m 'Update Annotation tool for door to generalize and visualize the preprocessed data'
 8229  git checkout SchindlerEdgeAnalytics/seanalytics/use_cases/floor_detection.py
 8230  git commit -m 'Delete  AnnotationTools/DoorEvents/download_data.py'
 8231  cd ~/Projects/modularsensorkit/AlgorithmPerformance/data_access
 8232  python align_dataset.py get_sensor_header 'MSK400001'
 8233  git checkout AlgorithmPerformance/data_access/align_dataset.py
 8234  git commit -m 'Get the sensor list based on MSK considering HW version using AlignData class'
 8235  ssh -i  ~/.ssh/DataScience.pem  -N -n -L 127.0.0.1:8890:127.0.0.1:8890 sch_centos_hiro -Y # @bastion, portforwarding centos to bastionr\n: 1525204789:0;mount_centos_hiro
 8236  ssh -i  ~/.ssh/DataScience.pem  -N -n -L 127.0.0.1:8889:127.0.0.1:8889 sch_centos_hiro -Y # @bastion, portforwarding centos to bastionr\n: 1525204789:0;mount_centos_hiro
 8237  cd AgileReport
 8238  git clone git@github.build.ge.com:212339410/AgileReport.git AgileReport2
 8239  head /Users/212339410/Box/2018 DS&A - Schindler IoEE - GED - Foghorn External/04_Algorithm_Test_Results/ground_truth/door_events/2018/Visual_Ground_Truth_Dump/MSK082_20180117_0200_20180117_0500_door_GT_labeled.csv
 8240  head "/Users/212339410/Box/2018 DS&A - Schindler IoEE - GED - Foghorn External/04_Algorithm_Test_Results/ground_truth/door_events/2018/Visual_Ground_Truth_Dump/MSK082_20180117_0200_20180117_0500_door_GT_labeled.csv"
 8241  head "/Users/212339410/Box/2018 DS&A - Schindler IoEE - GED - Foghorn External/04_Algorithm_Test_Results/ground_truth/door_events/2018/Visual_Ground_Truth_Dump/MSK068_20170907_0600_20170907_0730_door_GT_labeled.csv"
 8242  ping 172.83.12.106
 8243  ssh ps5yojnts@172.83.12.106
 8244  ssh ps5yojnts@172.83.12.106 -i ~/.ssh/id_rsa_20180418_github
 8245  clear   -N -n -L 127.0.0.1:8889:127.0.0.1:8889 sch_centos_hiro -Y # @bastion, portforwarding centos to bastionr
 8246  ssh 172.83.12.106
 8247  ssh -h
 8248  ssh paperspace@172.83.12.106 -i ~/.ssh/id_rsa_20180418_github 
 8249  cd Projects/AgileReport
 8250  git checkout -b dev
 8251  rm lazyreport.log
 8252  rm Rplots.pdf
 8253  git add lazyreport/*
 8254  rm config/\~\$report_config.xlsx
 8255  rm parents_report_sample.*
 8256  rm temp-report.*
 8257  mkdir test
 8258  mv chapters agilereport
 8259  mv config
 8260  mv config agilereport
 8261  mv figures agilereport
 8262  mv functions agilereport
 8263  mv style agilereport
 8264  cat .Rhistory
 8265  rm .Rhistory
 8266  cat LazyReport.Rproj
 8267  rm LazyReport.Rproj
 8268  git add agilereport/
 8269  git commit -m 'Move directories under agile report and delete obsolte files'
 8270  git add agilereport/*
 8271  git commit -m 'Refactor the structure and rename lazyreport to agilereport'
 8272  git pull git dev
 8274  git checkout dev
 8275  git add test/*
 8276  python python - m unittest test_agile_report.TestSampleClass
 8277  python - m unittest test_agile_report.TestSampleClass
 8278  python -m unittest test_agile_report.TestSampleClass
 8279  pip install fire
 8280  git commit -m 'Add unittest file'
 8281  python agilereport/agile_report.py -h
 8282  python agilereport/agile_report.py --interactive
 8283  python agilereport/agile_report.py gen_gestyle 'Softbank'
 8284  python agilereport/agile_report.py gen_header 'Data Science Report'
 8285  python agilereport/agile_report.py insert_title 
 8286  python agilereport/agile_report.py insert_image_from_func
 8287  python agilereport/agile_report.py insert_image 'image.jpg' 'Caption'
 8288  python agilereport/agile_report.py insert_rmd 'path'
 8289  python agilereport/agile_report.py gen_contribution'
 8290  python agilereport/agile_report.py gen_contribution
 8291  python agilereport/agile_report.py gen_back_to_contents
 8292  python agilereport/agile_report.py is_line_legal
 8293  python agilereport/agile_report.py generate --output_file='./outputfile' --input_config='./'
 8294  python agilereport/agile_report.py generate --output_file_name='./outputfile'
 8295  l agilereport/config/report_config
 8296  python agilereport/agile_report.py generate --output_file='./outputfile' --input_config='agilereport/config/report_config.xlsx'
 8297  python agilereport/agile_report.py generate --output_file='./outputfile' --input_config='./agilereport/config/report_config.xlsx'
 8298  python agilereport/agile_report.py generate --output_file='./outputfile' --input_config='~/Projects/AgileReport/agilereport/config/report_config.xlsx'
 8299  python agilereport/agile_report.py generate --output_file='./outputfile' --input_config='~/Projects/AgileReport/agilereport/config/report_config.xlsx' --help
 8300  python agilereport/agile_report.py generate  --help
 8301  python agilereport/agile_report.py  --help
 8302  python agilereport/agile_report.py  b-child-report
 8303  git commit -m  'Adapt AgileReport class and Fire command line tool'
 8304  source activate jupyter-blog
 8305  git clone git@github.com:h1ros/h1ros.github.io.git
 8306  cd h1r
 8307  subl pelicanconf.py
 8308  subl publishconf.py
 8309  git config --user user.name
 8310  subl .git/config
 8311  git config --local  user.name h1ros
 8312  git commit -m 'Update user.name'
 8313  git push origin dev
 8314  ssh paperspace -i ~/.ssh/id_rsa_20180418_github
 8315  ssh paperspace -i ~/.ssh/id_rsa_20180418_github vim ~/.ssh/config
 8316  ssh paperspace -vvv
 8317  ssh-copy-id -i ~/.ssh/id_rsa_20180418_github.pub paperspace
 8318  vim ~/.bash_profile
 8319  vim ~/.zsh
 8320  mount_hfs
 8321  cd AnnotationTools/DoorEvents/
 8322  git diff ../../AlgorithmPerformance/data_access/add_dataset_to_GT.py
 8323  git checkout ../../AlgorithmPerformance/data_access/add_dataset_to_GT.py
 8324  git diff ../annotation_utilities/io.py
 8325  git commit -m 'Fix the null check function not to add columns based on sensor node'
 8326  git diff door_events_annotation.py
 8327  git commit -m 'Refacotr annotation tool to make more generic and produce the formatted ground truth csv'
 8328  cat ./output/MSK108_20180503_0630_20180503_0700_door_GT_labeled.csv
 8329  tail ~/Projects/modularsensorkit/AnnotationTools/DoorEvents/door_events_annotation.log
 8330  5
 8331  1
 8332  git add 
 8333  git add door_events_annotation.py
 8334  git add ../annotation_utilities/io.py
 8335  git add ../annotation_utilities/visualization.py
 8336  git commit -m 'Enable deleting existing ground truth and plot the current gt as progressed'
 8337  pythonw door_events_annotation.py --sep ,
 8338  git add door_events_annotation.py 
 8339  git commit -m 'Refactor the part to map buttons with callback function '
 8340  port_paperspace
 8341  ssh paperspace -t
 8342  scp ~/Downloads/cudnn-9.0-linux-x64-v7.1.tgz paperspace:
 8343  rm ~/Downloads/cudnn-9.0-linux-x64-v7.1.tgz
 8344  scp paperspace:/home/paperspace/Development/Kaggle/notebook/submission_20180507.csv ~/Downloads
 8345  head /Users/212339410/Downloads/submission_20180507.csv
 8346  sudo subl .ssh/config
 8347  ssh -T git@github.com
 8348  rsync -rchavzP --stats sch_centos_hiro:hiro/result/ --include='0507*' --exclude='/*'  ~/Box/GE\ Digital\ Data\ Science\ 2017/10\ Schindler\ Personal/Intermediate\ result/ 
 8349  cd ~/Python/Schindler-Python
 8350  pythonw AnnotationTools/DoorEvents/door_events_annotation.py -h 
 8351  pythonw AnnotationTools/DoorEvents/door_events_annotation.py --sep , -i 
 8352  cd AnnotationTools/DoorEvents/input
 8353  subl Dataset_Survey_Tracking.csv
 8354  git 
 8355  git add AnnotationTools/annotation_utilities/io.py
 8356  git commit -m 'Load existing GT and if not exists, create new GT from data trakcer survey'
 8357  ssh paperspace  -N -n -L 127.0.0.1:8888:127.0.0.1:8888 paperspace -Y # 
 8358  git clone git@github.com:h1ros/kaggle.git
 8359  pipenv install 
 8360  pipenv --three install tensorflow-gpu keras ipython
 8361  pipenv --three install tensorflow-cpu keras ipython
 8362  pipenv --three install tensorflow keras ipython
 8363  subl Pipfile
 8364  python -m ipykernel install --name kaggle-S-5q8hkO --display-name Python 3 (Kaggle)
 8365  python -m ipykernel install --name kaggle-S-5q8hkO --display-name "Python 3 (Kaggle)"
 8366  cd ~/Python/kaggle
 8367  pip install --user kaggle
 8368  pipenv install kaggle
 8369  cat ~//Users/212339410/Downloads/kaggle.json
 8370  cat /Users/212339410/Downloads/kaggle.json
 8371  mv /Users/212339410/Downloads/kaggle.json  /Users/212339410/.kaggl/kaggle.json
 8372  mkdir ~/.kaggle
 8373  mv /Users/212339410/Downloads/kaggle.json  /Users/212339410/.kaggl/
 8374  kaggle competitions
 8375  kaggle competitions -h
 8376  kaggle competitions submit -c avito-demand-prediction -f ~/Downloads/submission_20180507.csv -m "Test submission"
 8377  kaggle competitions list -s avi
 8378  kaggle competitions list 
 8379  kaggle competitions list  -s avito
 8380  kaggle competitions files -c avito-demand-prediction 
 8381  kaggle competitions download -c avito-demand-prediction -f train.csv.zip
 8382  kaggle competitions download -c avito-demand-prediction -f test.csv.zip
 8383  kaggle competitions download -c avito-demand-prediction -f sample_submission.csv
 8384  pipenv install pandas_profiling
 8385  pip install glob fnmatch joblib copy cufflinks plotly matplotlib ipython
 8386  pip install fnmatch joblib copy cufflinks plotly matplotlib ipython
 8387  pipenv install joblib 
 8388  pipenv install raven
 8389  pipenv uninstall raven --skip-lock
 8390  pipenv uninstall raven
 8391  mv /Users/212339410/Downloads/kaggle.json  /Users/212339410/.kaggle
 8392  kaggle
 8393  kaggle list
 8394  kaggle competitions list
 8395  kaggle competitions submit -c avito-demand-prediction -f ~/.kaggle/competitions/avito-demand-prediction/baseline_submission_20180509.csv -m "Test submission with baseline"
 8396  chmod 600 ~/.kaggle/kaggle.json
 8397  cd ../../
 8398  git add AnnotationTools/DoorEvents/door_events_annotation.py
 8399  git commit -m 'Update the function to save the ground truth without deleted records'
 8400  git push https://hiro_@bitbucket.org/ioeeschindler/modularsensorkit.git GED
 8401  pip install pandas==0.19
 8402  cd kga
 8403  cd kaggle
 8404  . /Users/212339410/.local/share/virtualenvs/kaggle-S-5q8hkO/bin/activate
 8405  pip install autosklearn
 8406  pip install Cython
 8407  pip install auto-sklearn
 8408  Python/kaggle% kaggle competitions submit -c avito-demand-prediction -f ~/.kaggle/competitions/avito-demand-prediction/baseline_submission_20180509.csv -m "submission with encoding to ordinal variables"
 8409  kaggle competitions submit -c avito-demand-prediction -f ~/.kaggle/competitions/avito-demand-prediction/baseline_submission_20180509.csv -m "submission with encoding to ordinal variables"
 8410  l ~/.kaggle/competitions/avito-demand-prediction/
 8411  kaggle competitions submit -c avito-demand-prediction -f ~/.kaggle/competitions/avito-demand-prediction/baseline_submission_20180509_2.csv -m "submission with encoding to ordinal variables with numeric col"
 8412  pip install ddt
 8413  pip install --upgrade pip'
 8414  cookiecutter -h
 8415  mkdir example-cookie-cutter
 8416  cookiecutter https://github.com/patanijo/jupyterdayATL_2018_template.git
 8417  conda env create --file environment.yaml --name cookie-cutter
 8418  vim README.md
 8419  cd deliver
 8420  cd example-cookie-cutter
 8421  cd example_cookie_cutter
 8422  git remote remove upstream
 8423  mv example-cookie-cutter cookie-cutter
 8424  cd Projects/cookie-cutter
 8425  git clone https://github.com/jupyterdayATL/cookiecutter_template.git
 8426  cd cookiecutter_template
 8427  git remote -h
 8428  git remote remote origin
 8429  git remote remove origin
 8430  git remote add origin git@github.build.ge.com:212339410/dss-project-cookie-cutter.git
 8431  git config --global user.name
 8432  git config --global user.email
 8433  git clone git@github.build.ge.com:212339410/dss-project-cookie-cutter.git
 8434  pip install ray
 8435  mount_s3_sch
 8436  cd Projects/modularsensorkit/
 8437  subl input/Dataset_Survey_Tracking.csv
 8438  cd ~/Library/Application\ Support/Firefox/Profiles/67i4qq31.default/
 8439  cd Projects/mo
 8440  cat AnnotationTools/DoorEvents/input/Dataset_Survey_Tracking.csv
 8441  ssh -i  ~/.ssh/DataScience.pem  -N -n -L 127.0.0.1:8889:127.0.0.1:8889 sch_centos_hiro -Y # @bastion, portforwarding centos to bastionr
 8442  ssh -i  ~/.ssh/DataScience.pem  -N -n -L 127.0.0.1:8889:127.0.0.1:8889 sch_cpu -Y # @bastion, portforwarding centos to bastionr
 8443  subl ~/.bashrc
 8444  subl ~/.zrc
 8445  git diff AnnotationTools/annotation_utilities/io.py
 8446  git checkout --orphan GED-github-20170516
 8447  git push github GED-github-20170516Â¥
 8448  git push github GED-github-20170516
 8449  git checkout --orphan GED-github-20180516
 8450  git commit -m 'Snapshot of Bitbucket as of 2017 05/16'
 8451  git push github GED-github-20180516
 8453  git -T github.build.ge.com
 8454  git -t github.build.ge.com
 8455  git fetch github
 8456  git ls-remote github
 8457  git ls-remote github -v
 8458  ssh -T github.build.ge.com
 8459  ssh -T github.build.ge.com -i ~/.ssh/id_rsa_20180418_github
 8460  ssh -T github.build.ge.com -i ~/.ssh/id_rsa_20180418_github.pub
 8461  cat ~/.ssh/github.build.ge.com_rsa.pub
 8462  ssh -T github.build.ge.com -i ~/.ssh/github.build.ge.com_rsa
 8463  ssh -T github.build.ge.com -i ~/.ssh/github.build.ge.com_rsa -l 212339410
 8464  ssh -T 212339410@github.build.ge.com
 8465  cat ~/.ssh
 8466  ssh -T 212339410@github.build.ge.com -i ~/.ssh/github.build.ge.com_rsa
 8467  ssh -T 212339410@github.build.ge.com 
 8468  ssh -T 212339410@github.build.ge.com -v
 8469  ssh -T 212339410@github.build.ge.com -i ~/.ssh/github.build.ge.com_rsa -v
 8470  ssh -T github.build.ge.com -v
 8471  pip install html-testRunner
 8472  scp sch_cpu:/home/centos/modularsensorkit/SchindlerEdgeAnalytics/test/reports/unittest_report/Test_TestDoorDoesNotClose_2018-05-16_18-42-30.html  ~/Downloads
 8473  scp sch_cpu:/home/centos/modularsensorkit/SchindlerEdgeAnalytics/test/reports/unittest_report/Test_TestDoorDoesNotClose_2018-05-16_18-50-50.html  ~/Downloads
 8474  scp sch_cpu:/home/centos/modularsensorkit/SchindlerEdgeAnalytics/test/unittest_report/Test_TestDoorDoesNotClose_2018-05-16_18-35-39.html   ~/Downloads
 8475  scp sch_cpu:/home/centos/modularsensorkit/SchindlerEdgeAnalytics/test/nosetests.xml  ~/Downloads
 8476  scp sch_cpu:/home/centos/modularsensorkit/SchindlerEdgeAnalytics/test/nosetests.html  ~/Downloads
 8477  umount sch_cpu
 8478  mount
 8479  diskutil list
 8480  pgrep 
 8481  pgrep  -lf 
 8482  pgrep  -lf  hdfs
 8483  pgrep -h
 8484  epel
 8485  pgrep -lf sshfs
 8486  ps -el 
 8487  ps -el  | grep sshfs
 8488  ps -el | sch_cpu
 8489  ps -el | grep sch_cpu
 8490  cd Mo
 8491  cd ../Python/Schindler-Python
 8492  pip install pyprof2calltree
 8493  scp sch_cpu:/home/centos/modularsensorkit/SchindlerEdgeAnalytics/door_event_detection.cprof ./
 8494  cat door_event_detection.cprof
 8495  pyprof2calltree -k -i door_event_detection.cprof
 8496  pyprof2calltree -h
 8497  brew install qcachegrind --with-graphviz
 8498  brew install qt
 8499  git clone https://github.com/fastai/fastai.git
 8500  conda env update
 8501  conda env update 
 8502  conda env update -f environment-cpu.yml
 8503  cd checkout GED
 8504  npm search hyper
 8505  mv .hyper
 8506  mv .hyper.js .hyper_bak.js
 8507  vim .hyper.js
 8508  vim ~/.hyper.js
 8509  h1ros.github.io
 8510  cp -r h1r03.github.io h1ros.github.io
 8512  git commit -m 'Update deep learning note'
 8513  cd Python/regime_switch_model
 8514  ssh -i  ~/.ssh/DataScience.pem  -N -n -L 127.0.0.1:8989:127.0.0.1:8989 sch_cpu -Y # @bastion, portforwarding centos to bastionr
 8515  git clone git@github.build.ge.com:212339410/gitpitch-test.git
 8516  cd gitpitch-test
 8517  vim PITCHME.md
 8518  git add PITCHME.md
 8519  python -m SimpleHTTPServer
 8520  python3 -m http.server
 8521  cd PITCHME
 8522  docker run -it -v DESKTOPREPO:/repo -p 9000:9000 store/gitpitch/desktop:pro
 8523  cd ../Python/gitpitch-test
 8524  docker run -it -v ~/Python/gitpitch-test:/repo -p 9000:9000 store/gitpitch/desktop:pro
 8525  cd ~/Downloads/PITCHME
 8526  cd assets/md
 8527  subl PITCHME.md
 8528  scp sch_centos_hiro:history_git.txt ~/Downloads
 8529  ipython 
 8530  hisotry | grep git > history_git_osx.txt
 8531  history | grep git > history_git_osx.txt
 8532  excel history_git_osx.txt
 8533  git help
 8534  scp ~/.ssh/DataScience.pem sch_cpu:~/.ssh/
 8535  scp -r ~/Box/2018\ DS\&A\ -\ Schindler\ IoEE\ -\ GED\ -\ Foghorn\ External/04_Algorithm_Test_Results/ground_truth/door_events/2018/Visual_Ground_Truth_Door_Events_Dump/ sch_cpu:~/hiro/data/gt_dr_dump_20180524/
 8536  git clone git@github.build.ge.com:212339410/git-tutorial.git
 8537  git clone git@github.com:h1ros/git-tutorial-com.git
 8538  cd git-tutorial-com
 8539  git clone git@github.com:h1ros/pipenv.git
 8540  git fetch
 8541  git add -h
 8542  git add upstream git@github.com:pypa/pipenv.git
 8543  git add upstream https://github.com/pypa/pipenv.git
 8544  git remote add upstream https://github.com/pypa/pipenv.git
 8545  git remote =v
 8546  git fetch 
 8547  git remote show origin
 8548  git push origin feat/add-document
 8549  git rebase feat/add-document
 8550  git checkout feat/add-document
 8551  vim ~/.bashrc
 8552  git staus
 8553  touch test.txt
 8554  git commit -m 'Add some changes'
 8555  cd pipenv
 8556  git checkout -b feat/add-document
 8557  git checkout -b feat/add-documents
 8558  git branch -a
 8559  git commit -u
 8560  git add README.rst
 8561  git push origin feat/add-documents
 8562  vim README.rst
 8563  git stash list
 8564  git stash apply
 8565  git clone git@github.build.ge.com:IndustrialDataScience/DSS-Workshops.git
 8566  cd DSS-
 8567  cd sample_code/git_sample
 8568  cp /Users/212339410/Box/GE_digital_data_science_2018/documents/20180524_git_tutorial_beyond_basic/20180525_git_tutorial_beyond_push.pdf ./
 8569  cp ~/Python/h1r03.github.io
 8570  rm del
 8571  git commit -m 'Add pdf and code for git-tutorial-beyond-push-pull'
 8572  cp ~/Python/h1r03.github.io/content/notes/20180525_git_demo.assets/image-20180525125214997.png ./
 8573  open ~/Python/h1r03.github.io/content/notes/20180525_git_demo.md
 8574  cp ~/Python/h1r03.github.io/content/notes/20180525_git_demo.md ./
 8575  git commit -m 'Change flowchart from md to png'
 8576  source activate fastai-cpu
 8577  cd tutorials
 8578  cd fastai
 8579  cd courses
 8580  cd dl1
 8581  ssh -i  ~/.ssh/DataScience.pem  -N -n -L 127.0.0.1:8900:127.0.0.1:8900 sch_centos_rohit -Y # @bastion, portforwarding centos to bastion
 8582  rsync -rchavzP --stats sch_cpu:hiro/result/ --include='0525*' --exclude='/*'  ~/Box/GE\ Digital\ Data\ Science\ 2017/10\ Schindler\ Personal/Intermediate\ result/ 
 8583  rsync -rchavzP --stats sch_cpu:hiro/result/ --include='0529*' --exclude='/*'  ~/Box/GE\ Digital\ Data\ Science\ 2017/10\ Schindler\ Personal/Intermediate\ result/ 
 8584  cd OptModel
 8585  subl .gitignore
 8586  brew update\n
 8587  brew cask install hyper
 8588  hyper i hyper-pane
 8589  hyper
 8590  git remote 
 8591  git remote set-url origin https://github.build.ge.com/IndustrialDataScience/OptModel.git
 8592  subl .git/
 8593  subl ~/.git
 8594  source ~/.gitconfig
 8595  ehoc $https_proxy
 8596  $no_proxy
 8597  echo $no_proxy
 8598  cd Python/OptModel
 8599  git pull
 8600  clear
 8601  rsync -rchavzP --stats sch_cpu:hiro/result/ --include='0601*' --exclude='/*'  ~/Box/GE\ Digital\ Data\ Science\ 2017/10\ Schindler\ Personal/Intermediate\ result/ 
 8602  rsync -rchavzP --stats sch_cpu:hiro/result/ --include='0530*' --exclude='/*'  ~/Box/GE\ Digital\ Data\ Science\ 2017/10\ Schindler\ Personal/Intermediate\ result/ 
 8603  sudo diskutil umount force ~/Mount/remote_sch_cp
 8604  rsync -rchavzP --stats sch_cpu:hiro/data/all_daily_event_data/20180523/ --include='*' --exclude='/*'  ~/Box/GE\ Digital\ Data\ Science\ 2017/10\ Schindler\ Personal/Intermediate\ result/0606_event_table/ 
 8605  ssh -i  ~/.ssh/DataScience.pem  -N -n -L 127.0.0.1:8050:127.0.0.1:8050 sch_centos -Y # @bastion, portforwarding centos to bastion
 8606  ssh -N -f -L localhost:8888:localhost:8888 paperspace@10.64.22.117
 8607  cat ~/.kaggle/kaggle.json
 8608  cat ~/.ssh/id_rsa_20180418_github.pub
 8609  ssh -N -n -L 127.0.0.1:8888:127.0.0.1:8888 paperspace -Y 
 8610  ssh -N -n -L 127.0.0.1:8888:127.0.0.1:8888  -Y paperspace 
 8611  scp ~/Downloads/Download_kaggle/notebook_avito_20180607_2100.ipynb paperspace:~/notebook/
 8612  scp ~/Downloads/Download_kaggle/avito_region_city_features.csv paperspace:~/.kaggle/competitions/avito-demand-prediction/
 8613  ssh paperspace@172.83.10.149
 8614  scp paperspace:~/.kaggle/competitions/avito-demand-prediction/submission_avito_fastai_baseline_20180609.csv ~/Downloads/Download_kaggle
 8615  scp ~/Downloads/Download_kaggle/avito-demand-prediction/aggregated_features.csv paperspace:~/.kaggle/competitions/avito-demand-prediction/
 8616  ssh -N -n -L 127.0.0.1:8890:127.0.0.1:8890 paperspace -Y
 8617  ssh -i  ~/.ssh/DataScience.pem  -N -n -L 127.0.0.1:8050:127.0.0.1:8050 sch_cpu -Y
 8618  ssh -N -n -L 127.0.0.1:8890:127.0.0.1:8890  -Y paperspace 
 8619  ping 172.83.10.149
 8620  ssh -i  ~/.ssh/DataScience.pem  -N -n -L 127.0.0.1:4000:127.0.0.1:4000sch_cpu -Y # @bastion, portforwarding centos to bastion
 8621  ssh -i  ~/.ssh/DataScience.pem  -N -n -L 127.0.0.1:4000:127.0.0.1:4000 sch_cpu -Y # @bastion, portforwarding centos to bastion
 8622  mkdir pipenv
 8623  man source
 8624  conda --info env
 8625  l /Users/212339410/bin
 8626  l /Users/212339410/bin/
 8627  l ~/anaconda/bin
 8628  l ~/.ssh
 8629  cat ~/.ssh/id_rsa.pub | ssh hiros@3.39.83.92 "mkdir -p ~/.ssh && cat >> ~/.ssh/authorized_keys"
 8630  ssh  -N -n -L 127.0.0.1:8989:127.0.0.1:8989 dssgpu  -Y 
 8631  source -h
 8632  ssh -N -n -L 127.0.0.1:8989:127.0.0.1:8989 paperspace -Y 
 8633  ssh paperspace@172.83.10.149 -i ~/.ssh/id_rsa_20180418_github  -v
 8634  ssh paperspace@172.83.10.149 -i ~/.ssh/id_rsa_20180418_github 
 8635  curl -L https://iterm2.com/shell_integration/install_shell_integration_and_utilities.sh | bash
 8636  subl .ssh/config
 8637  ssh raspy01
 8638  ssh pie@10.15.18.211
 8639  ssh pi@10.15.18.211
 8640  geproxy2
 8641  raspy01_proxy
 8642  ssh raspy01_proxy
 8643  ssh raspy01_proxy -vvv
 8644  rsync -chavzP --stats sch_cpu:modularsensorkit ~/Projects 
 8645  cd AnnotationTools
 8646  cd DoorEvents
 8647  subl AnnotationTools/DoorEvents/input/Dataset_Survey_Tracking
 8648  tail -f ~/Projects/modularsensorkit/AnnotationTools/DoorEvents/door_events_annotation.log
 8649  ssh -N -n -L 127.0.0.1:8892:127.0.0.1:8892 dssgpu -Y -o ServerAliveInterval=50
 8650  vim ~/.kaggle/kaggle.json
 8651  echo  $CUDA_HOME
 8652  ssh -N -n -L 127.0.0.1:8890:127.0.0.1:8890 paperspace -Y 
 8653  subl ~/.kaggle/kaggle.json
 8654  kaggle compeitions list
 8655  ssh sch_cpu_proxy
 8656  ssh paperspace
 8657  ssh sch_cpu_
 8658  scp ~/Downloads/Download_kaggle/aggregated_features\ \&\ LightGBM.ipynb paperspace:~/notebook
 8659  ssh sch_centos_hiro_proxy
 8660  vim .gitconfig 
 8661  vim ~/.gitconfig 
 8662  git clone https://github.build.ge.com/IndustrialDataScience/Celtics_Tickets_Phase2.wiki.git
 8663  typora -h
 8664  typora
 8665  open Celtics_Tickets_Phase2.wiki
 8666  cd Celtics_Tickets_Phase2.wiki
 8667  cd ~/Mo
 8668  git config -h
 8669  subl ~/.gitignore_global
 8670  git git revert --quit
 8671  git revert --quitÂ¥
 8672  git revert --quit
 8673  rm test.txt
 8674  subl /Users/212339410/.gitignore_global/.gitignore
 8675  git config --global core.excludesfile 
 8676  git config --global core.excludesfile  /Users/212339410/.gitignore_global/.gitignore
 8677  rm SchindlerEdgeAnalytics/seanalytics/preprocessing/.align_data.py.swp
 8678  rm SchindlerEdgeAnalytics/cproc_output_seanalytics.pstats
 8679  rm -r Dashboards/DB_Events_Logger/
 8680  rm -r HW03AutoConfiguration/
 8681  rm -r HW04AutoConfiguration
 8682  rm SchindlerCloudAnalytics/scanalytics/use_cases/anomaly_detection_repetitive_closing_interruption_without_light.py
 8683  rm SchindlerCloudAnalytics/scanalytics/use_cases/anomaly_detection_repetitive_closing_interruption.py
 8684  rm SchindlerCloudAnalytics/scanalytics/use_cases/anomaly_detection_closing_interruption_without_light.py
 8685  rm MSKSubscriber/algorithms/subscriber_mqtt_event_handler.py
 8686  rm -r Dashboards/DB_Sensor_Data/
 8687  rm SchindlerEdgeAnalytics/test/pycallgraph.png
 8688  rm SchindlerEdgeAnalytics/seanalytics/config_files/floor_detection_test.ini
 8689  rm SchindlerEdgeAnalytics/seanalytics/__main__.py
 8690  rm Scripts/EVENT_DATA_FLOOR_20180605.csv
 8691  rm anomaly_detection_closing_interruption_filtered_drAcc.py
 8692  rm EdgeManager_Packages/MSK_AlgorithmStack/package/content/utils/msk_get_sn.py
 8693  rm EdgeManager_Packages/MSK_AlgorithmStack_Staging/package/content/utils/msk_get_sn.py
 8694  git checkout AnnotationTools/DoorEvents/output/test.txt
 8695  rm MSKSubscriber/algorithms/databuffer.py
 8696  rm MSKSubscriber/test/test_databuffer.py
 8697  rm SchindlerCloudAnalytics/scanalytics/use_cases/anomaly_detection_runtime_too_long.py
 8698  rm SchindlerEdgeAnalytics/seanalytics/models/floor_detection_ref_gmm_n1_MSK053_20180301_f400.pkl
 8699  rm SchindlerEdgeAnalytics/seanalytics/models/floor_detection_ref_gmm_n1_MSK071_20171130_f400.pkl
 8700  rm SchindlerEdgeAnalytics/seanalytics/models/floor_detection_ref_gmm_n1_MSK050_20180301_f400.pkl
 8701  git pull https://hiro_@bitbucket.org/ioeeschindler/modularsensorkit.git GED 
 8702  git checkout develop
 8704  git checkout GED
 8705  rsync -rchavzP --stats sch_cpu:hiro/result/ --include='062*' --exclude='/*'  ~/Box/GE\ Digital\ Data\ Science\ 2017/10\ Schindler\ Personal/Intermediate\ result/ 
 8706  git clone https://github.build.ge.com/IndustrialDataScience/CookieChomper.git
 8707  cd Cookie
 8708  cd CookieChomper
 8709  pip install cookiecutter
 8710  cookiecutter https://github.build.ge.com/IndustrialDataScience/CookieChomper
 8711  cookiecutter git@github.build.ge.com:IndustrialDataScience/CookieChomper.git
 8712  pip install https://github.build.ge.com/IndustrialDataScience/CookieChomper
 8713  pip install https://github.build.ge.com/IndustrialDataScience/CookieChomper.git
 8714  l /Users/212339410/.cookiecutters
 8715  l /Users/212339410/.cookiecutters/CookieChomper
 8716  cookiechomper
 8717  CookieChomper
 8718  make help
 8719  make all
 8720  cd src
 8721  git clone https://github.build.ge.com/IndustrialDataScience/Bridgestone_New_Tyres.git
 8722  git checkout -b hiro/setup
 8723  git commit -m 'Initial commit for directory setup based on CookieChomper'
 8724  git push hiro/setup
 8725  git push origin hiro/setup
 8726  git checkout -b hiro/eda
 8727  cd ../
 8728  mkdir hiro
 8729  pip install pandas-profiling
 8730  mount_centos_hiro
 8731  sudo diskutil umount force ~/Mount/remote_centos_hiro
 8732  norpoxy
 8733  brew install peco
 8734  peco 
 8735  ~
 8736  rsync -chavzP --stats sch_cpu:hiro/notebook ~/Box/GE\ Digital\ Data\ Science\ 2017/10\ Schindler\ Personal/Backup/hiro/ 
 8737  cd /Users/212339410/Downloads/danielsample
 8738  xz --format=lzma cdn1_500m_sigmetrics18.tr.lzma
 8739  xz --format=lzma cdn1_500m_sigmetrics18.tr.lzma --decompress
 8740  rsync -rchavzP --stats sch_cpu:hiro/result/ --include='07*' --exclude='/*'  ~/Box/GE\ Digital\ Data\ Science\ 2017/10\ Schindler\ Personal/Intermediate\ result/ 
 8741  pip install ghq
 8742  cd MyJabberFiles
 8743  vim hiroaki.shioi@ge.com
 8744  cd hiroaki.shioi@ge.com
 8745  cd //
 8746  rm -r MyJabberFiles
 8747  vim .hgignore_global
 8748  vim .v8flags.5.1.281.84.9d3f3d02680cdf940423ae6c89695a4d.json
 8749  git clone https://github.com/motemen/ghq.git
 8750  go get github.com/motemen/ghq
 8751  l ~/.ghq
 8752  ghq get git@github.com:keras-team/keras.git
 8753  ghq get -u git@github.com:keras-team/keras.git
 8754  git@github.com:keras-team/keras.git
 8755  tree /Users/212339410/anaconda/envs/sch-27/ | peco
 8756  source activate sch-27
 8757  source activate sch27Â¥
 8758  pip install matplotlib
 8759  conda list | peco
 8760  subl AnnotationTools/DoorEvents/input/Dataset_Survey_Tracking.csv
 8762  cd content
 8763  rm 20180505_feedback_ååãªãµã¼ãæ¹æ³.docx
 8764  rm Mobile\ Base\ Station.zip
 8765  cd tmp
 8766  cd Box
 8767  mkdir Typora
 8768  cd Typora
 8769  git clone https://github.build.ge.com/IndustrialDataScience/Bridgestone_New_Tyres.wiki.git
 8770  ln -s ~/Python/h1ros.github.io/content content
 8771  unlink content
 8772  ln  ~/Python/h1ros.github.io/content content
 8773  pip install SpeechRecognition
 8774  cd Spee
 8775  pip install PyAudio
 8776  brew install portaudio
 8777  python -m speech_recognition
 8778  spark
 8779  l ~/
 8780  l ~/ | peco
 8781  l ~/Downloads | peco
 8782  tar -xzf spark-2.3.1-bin-hadoop2.7.tgz
 8783  mv spark-2.3.1-bin-hadoop2.7 /opt/spark-2.3.1
 8784  sudo mv spark-2.3.1-bin-hadoop2.7 /opt/spark-2.3.1
 8785  ln -s /opt/spark-2.3.1 /opt/spark
 8786  cd ghq
 8787  rm -r ghq
 8788  mkdir pyspark-test
 8789  cd pyspark-test
 8790  cd ~/Python/pyspark-test
 8791  pyspark
 8792  git clone git@github.com:h1ros/Audio-Spectrum-Analyzer-in-Python.git
 8793  cd Audio-Spectrum-Analyzer-in-Python
 8794  pip install pyaudio
 8795  pip install lightgbm
 8796  gcc-8 --version
 8797  gcc
 8798  gcc -v
 8799  pip uninstall lightgbm
 8800  git clone --recursive https://github.com/Microsoft/LightGBM ; cd LightGBM
 8801  brew unlink giflossy
 8802  echo $CXX
 8803  mkdir build ; cd build
 8804  cc
 8805  cd ~/Python/LightGBM/build
 8806  export CXX=g++-8 CC=gcc-8
 8807  gcc-8
 8808  gcc-8 -v
 8809  brew install cmake
 8810  cmake 
 8811  cmake ..
 8812  make -j4
 8813  pip install --no-binary :all: lightgbm
 8814  cd Bridgestone_New_Tyres.wiki/0.1-Terminology.assets
 8815  git add 0.1-Terminology.md
 8816  git commit -m 'Add table for terminology'
 8817  jupyter notebook  --port=9001 --Notebook.iopub_data_rate_limit=1.0e12
 8818  git diff --cached
 8819  git commit -m 'Add description for Shared Data'
 8820  ! pip install cufflinks --upgrade
 8821  ping www.google.com
 8822  mount_sch_cp
 8823  cd 
 8824  Bridge
 8825  cd Bri
 8826  ssh -i ~/.ssh/DataScience.pem -N -n -L 127.0.0.1:8892:127.0.0.1:8892 dssgpu -Y -o ServerAliveInterval=50
 8827  rsync -rchavzP --stats sch_cpu:hiro/result/ --include='0717' --exclude='/*'  ~/Box/GE\ Digital\ Data\ Science\ 2017/10\ Schindler\ Personal/Intermediate\ result/ 
 8828  rsync -rchavzP --stats sch_cpu:hiro/result/ --include='0717*' --exclude='/*'  ~/Box/GE\ Digital\ Data\ Science\ 2017/10\ Schindler\ Personal/Intermediate\ result/ 
 8829  rsync -rchavzP --stats sch_cpu:hiro/result/ --include='0718*' --exclude='/*'  ~/Box/GE\ Digital\ Data\ Science\ 2017/10\ Schindler\ Personal/Intermediate\ result/ 
 8830  cd ~/Box/Typora
 8831  cd Bridgestone_New_Tyres.wiki
 8832  git checkout ./
 8833  git stautus
 8834  git diff 
 8835  git add -u 
 8836  git commit -m 'Add description for cerrejon data'
 8837  cd hiro
 8838  git add 20180716 EDA and Processing Cerejjan Dataset.ipynb
 8839  git add 20180716\ EDA\ and\ Processing\ Cerejjan\ Dataset.ipynb
 8840  git checkout ../Visualization_Examples.ipynb
 8841  git commit -m 'Add notebook for EDA and Aggregation notebook for Cerejjon data'
 8842  pip install pandas==0.20
 8843  ! pip install kapteyn
 8844  conda upgrade statsmodels
 8845  pip install fbprophet
 8846  pip install pandas --upgrade
 8847  subl .exploratory/userconf.json
 8848  git pull https://hiro_@bitbucket.org/ioeeschindler/modularsensorkit.git develop
 8849  git clone https://github.build.ge.com/212339410/modularsensorkit-temp.git
 8850  cd modularsensorkit-temp
 8851  mkdir -h
 8852  man mkdir
 8853  mkdir -p SchindlerEdgeAnalytics/seanalytics/use_cases/
 8854  cp -r ~/Projects/modularsensorkit/MSKDataDownloader  ~/Projects/modularsensorkit-temp
 8855  l ~/Projects/modularsensorkit
 8856  cp -r ~/Projects/modularsensorkit/AlgorithmPerformance ~/Projects/modularsensorkit-temp
 8857  cp -r ~/Projects/modularsensorkit/SchindlerEdgeAnalytics ~/Projects/modularsensorkit-temp
 8858  cp -r ~/Projects/modularsensorkit/TESTDATA ~/Projects/modularsensorkit-temp/
 8859  cp -r ~/Projects/modularsensorkit/Scripts ~/Projects/modularsensorkit-temp
 8860  git statu
 8861  git commit -m 'Sync the requresd code for door event detection, performance detection, and download tool'
 8862  rsync -rchavzP --stats sch_cpu:hiro/result/ --include='0724*' --exclude='/*'  ~/Box/GE\ Digital\ Data\ Science\ 2017/10\ Schindler\ Personal/Intermediate\ result/ 
 8863  sudo ln -s /opt/spark-2.3.1 /opt/spark
 8864  cd modularsensorkit
 8865  source sch27
 8866  cd SchindlerEdgeAnalytics
 8867  cd test
 8868  ssh sch_centos_rohit
 8869  scp sch_centos_rohit:/home/centos/bitbucket/modularsensorkit/AlgorithmPerformance/performance_assessment/rohit_notebook/jupyter_notebook/Door_Event_Accuracy_pipeline.ipynb ~/Downloads
 8870  scp ~/Downloads/Door_Event_Accuracy_pipeline.ipynb sch_cpu:~/hiro/notebook/ 
 8871  geproxxy
 8872  head ~/Box/GE\ Digital\ Data\ Science\ 2017/10\ BridgeStone/data/part-00000-5c119f09-e122-4295-9e7d-071d5e9ec009.csv
 8873  rsync -rchavzP --stats sch_cpu:hiro/result/ --include='0725*' --exclude='/*'  ~/Box/GE\ Digital\ Data\ Science\ 2017/10\ Schindler\ Personal/Intermediate\ result/ 
 8874  cd dsa-pipeline
 8875  matplotlib-venn
 8876  pip install matplotlib-venn
 8877  rsync -rchavzP --stats sch_cpu:hiro/result/ --include='0730*' --exclude='/*'  ~/Box/GE\ Digital\ Data\ Science\ 2017/10\ Schindler\ Personal/Intermediate\ result/ 
 8878  excel door_events_result_0.6.21_20180730_hw4_merge_closing_int_v2_with_online_learning.csv
 8879  open /Users/212339410/Box/GE Digital Data Science 2017/10 Schindler Personal/Intermediate result/0730_door_full_f1_DoorEventDetection_0.6.21_hw4_merge_closing_int_v2_with_online_learning/door_events_result_0.6.21_20180730_hw4_merge_closing_int_v2_with_online_learning.csv
 8880  open "/Users/212339410/Box/GE Digital Data Science 2017/10 Schindler Personal/Intermediate result/0730_door_full_f1_DoorEventDetection_0.6.21_hw4_merge_closing_int_v2_with_online_learning/door_events_result_0.6.21_20180730_hw4_merge_closing_int_v2_with_online_learning.csv"
 8881  pythonw AnnotationTools/DoorEvents/door_events_annotation.py --sep ,
 8882  cp ~/Box/Internal\ -\ Bridgestone/Codes/Anurag/Bridgestone.ipynb ~/Projects/Bridgestone_New_Tyres/notebooks
 8883  3.39.83.163:7077
 8884  scp ./notebooks/Bridgestone.ipynb dss_server:~/notebook/
 8885  scp ./notebooks/Bridgestone.ipynb dss_server:~/notebook/20180731_Anurag_BrdigeStone.ipynb
 8886  hadoop -h
 8887  norpxoy
 8888  brew search hadoop
 8889  brew install hadoop
 8890  tmux
 8891  rm Bridgestone.ipynb
 8892  hadoop
 8893  hadoop fs -put ~/Box/Internal\ -\ Bridgestone/data/Cerrejon_Data/Processed/ins_df.csv 3.39.83.163:~/hdfs/test
 8894  ping 3.39.83.163:7077
 8895  ping 3.39.83.163
 8896  ssh 3.39.83.163
 8897  ssh 3.39.83.163 -l hiros
 8898  ssh  -N -n -L 127.0.0.1:8888:127.0.0.1:8888 dss_server -Y # @bastion, portforwarding centos to bastion
 8899  ssh  -N -n -L 127.0.0.1:9000:127.0.0.1:9000 dss_server -Y # @bastion, portforwarding centos to bastion
 8900  ssh hiros@3.39.83.163
 8901  ssh hiros@3.39.83.135
 8902  ssh -i  ~/.ssh/DataScience.pem  -N -n -L 127.0.0.1:8080:127.0.0.1:8080 sch_cpu -Y # @bastion, portforwarding centos to bastion
 8903  brew install gdbm
 8904  brew upgrade gdbm
 8905  rsync -rchavzP --stats sch_cpu:hiro/result/ --include='0801*' --exclude='/*'  ~/Box/GE\ Digital\ Data\ Science\ 2017/10\ Schindler\ Personal/Intermediate\ result/ 
 8906  sudo diskutil umount force ~/Mount/remote_cp
 8907  scp ./notebooks/Bridgestone.ipynb dss_server:~/notebooks/20180731_Anurag_BrdigeStone.ipynb
 8908  ssh sch_bastion "ssh -i  DataScience.pem  -N -n -L 127.0.0.1:8050:127.0.0.1:8050 centos@10.43.51.90 -Y  -o ServerAliveInterval=50" # @bastion, portforwarding centos to bastion
 8909  var='abc'
 8910  echo var
 8911  echo $var
 8912  echo "" + $var
 8913  echo "dsfds" + $var
 8914  echo "dsfds" + $var + "dafjdij"
 8915  echo "Now http(s)_proxy is set to GE Proxy (" + $https_proxy + ")"\n
 8916  echo "Now http(s)_proxy is set to GE Proxy (" $https_proxy ")"\n
 8917  brew unlink pcre
 8918  brew link pcre
 8919  brew reinstall gdbm && brew unlink gdbm && brew link gdbm
 8920  brew update
 8921  brew doctor
 8922  brew upgrade
 8923  ssh -i  ~/.ssh/DataScience.pem  -N -n -L 127.0.0.1:8989:127.0.0.1:8989 sch_cpu -Y # @bastion, portforwarding centos to bastion
 8924  mount_sch_cp 
 8925  rsync -rchavzP --stats sch_cpu:hiro/result/ --include='0802*' --exclude='/*'  ~/Box/GE\ Digital\ Data\ Science\ 2017/10\ Schindler\ Personal/Intermediate\ result/ 
 8926  prompt
 8927  jupyter nbextension enable  .jupyter/jupyter_nbconvert_config.json
 8928  cd Projects/modularsensorkit-temp
 8929  git add AutoConfiguration/
 8930  git commit -m 'Add requirements.txt'
 8932  python -m ipykernel install -user
 8933  python -m pip install --user jupyter_contrib_nbextensions
 8934  python -m pip uninstall --user jupyter_contrib_nbextensions
 8935  python -m pip uninstall 
 8936  python -m pip uninstall jupyter_contrib_nbextensions
 8937  ]jupyter contrib nbextension install --user
 8938  conda install -c anaconda-nb-extensions nbbrowserpdf
 8939  conda upgrade python
 8940  cond aupdate python
 8941  conda update python
 8942  jupyter lab --port 8889
 8943  scp -r "/Users/212339410/Box/Internal - Bridgestone/data" dss_server:/data/nfs135/projects/bridgestone/
 8944  python -m jupyterlab --version
 8945  conda update jupyter_contrib_nbextensions 
 8946  pip install fastai
 8947  subl /Users/212339410/.jupyter/jupyter_nbconvert_config.json
 8948  conda env remove -n py37
 8949  conda create -n py37 python=3.7
 8950  conda info
 8951  conda update 
 8952  conda install dask
 8953  aws ec2 list -h
 8954  peco -h
 8955  aws ec2 list | peco
 8956  aws ec2 list -h | peco
 8957  aws ec2 list -h 
 8958  percol -h
 8959  ghq
 8960  ghq list
 8961  cd /Users/212339410/Box/Internal - Bridgestone/data/Cerrejon_Data/Processed
 8962  cd "/Users/212339410/Box/Internal - Bridgestone/data/Cerrejon_Data/Processed"
 8963  head d_10_payload_removal_aggregate.csv
 8964  conda install -c anaconda pyzmq
 8965  conda update --prefix /anaconda/envs/py35 anaconda
 8966  conda update --prefix ~/anaconda/envs/py35 anaconda
 8967  conda update jupyter -n py35
 8968  cd ~/Box/Internal\ -\ Bridgestone
 8969  source activate py37
 8970  pip3 install tornado==4.5.3
 8971  conda -l
 8972  l /Users/212339410/
 8973  source ~/.zshrc
 8974  conda list -e
 8975  conda env export | grep -v "^prefix: " 
 8976  conda env export | grep -v "^prefix: " > environment_py35.yml
 8977  /Users/212339410/anaconda3/bin/conda env create -f environment_py35.yml
 8978  /Users/212339410/anaconda3/bin/conda env --info
 8979  /Users/212339410/anaconda3/bin/conda env --list
 8980  /Users/212339410/anaconda3/bin/conda info --env
 8981  subl environment_py35.yml
 8982  /Users/212339410/anaconda3/bin/conda env create -f environment_py35.yml -p ~/anaconda3/envs/py35
 8983  conda list -e > req.txt
 8984  conda create -n py35 --file req.txt
 8985  /Users/212339410/anaconda3/bin/conda env export | grep -v "^prefix: " > environment_py35.yml
 8986  /Users/212339410/anaconda3/bin/conda create -n py35 --file req.txt
 8987  conda install tornado=4.5.3 
 8988  conda install tornado=4.5.3
 8989  conda env --list
 8990  source activate  /Users/212339410/anaconda/envs/py35
 8991  conda create --name py35 anaconda
 8992  conda create --name py35 anaconda python=3.5
 8993  conda activate py35
 8994  source export PATH="/Users/212339410/anaconda3/bin:$PATH"
 8995  export PATH="/Users/212339410/anaconda3/bin:$PATH"
 8996  conda install -c conda-forge pandas-profiling
 8997  jupyter lab'
 8998  cd Projects/
 8999  python -m ipykernel install --user --name=py35 --display-name "Python 3.5 (py35)"
 9000  jupyter lab
 9001  pip install pylantern
 9002  pip install upgrade pip
 9003  pip install matplotlib_venn
 9004  npm config get proxy
 9005  npm config get https-proxy
 9006  npm config edit
 9007  npm config list
 9008  jupyter labextension install pylantern
 9009  jupyter labextension install pylantern --debug
 9010  jupyter --version
 9011  jupter lab --version
 9012  jupyter lab --version
 9013  jupyter labextension install @jupyter-widgets/jupyterlab-manager
 9014  conda upgrade -c conda-forge jupyterlab
 9015  jupyter labextension install @jupyterlab/toc
 9016  yarn upgrade
 9017  yarn self-update
 9018  yarn -h
 9019  yarn -v
 9020  yarn --version
 9021  yarn --help
 9022  yarn --help | peco
 9023  yarn version
 9024  npm install -g yarn
 9025  brew upgrade yarn
 9026  jupyter labextension install @jupyterlab/plotly-extension
 9027  pip install future
 9028  pip install --upgrade jupyter
 9029  pip sudo pip install --upgrade "ipython[all]"
 9030  sudo pip install --upgrade "ipython[all]"
 9031  sudo pip uninstall Jinja2
 9032  sudo pip uninstall Jinja2 -H
 9033  sudo -H pip uninstall Jinja2 
 9034  pip install flask
 9035  cd ~/Projects/Bridgestone_New_Tyres/dist/demandforecast
 9036  git clone https://github.com/linqs/psl-examples.git
 9037  cd psl-examples
 9038  cd simple-acquaintances/cli
 9039  ./run.sh
 9040  subl simple-acquaintances.data
 9041  subl ../data/knows_obs.txt
 9042  subl ../data/knows_targets.txt
 9043  subl ../data/likes_obs.txt
 9044  subl simple-acquaintances.psl
 9045  cd ~/Projects/modularsensorkit
 9046  git pull https://hiro_@bitbucket.org/ioeeschindler/modularsensorkit.git GED
 9047  pip install yapf
 9048  pip install graphviz
 9049  tabpy -h
 9050  python /Users/212339410/anaconda3/envs/py35/lib/python3.5/site-packages/tabpy_server/tabpy.py
 9051  python /Users/212339410/anaconda3/envs/py35/lib/python3.5/site-packages/tabpy_server/
 9052  subl /Users/212339410/anaconda3/envs/py35/lib/python3.5/site-packages/tabpy_server/common/config.py
 9053  subl /Users/212339410/anaconda3/envs/py35/lib/python3.5/site-packages/tabpy_server/startup.sh
 9054  /Users/212339410/anaconda3/envs/py35/lib/python3.5/site-packages/tabpy_server/startup.sh
 9055  git branch
 9056  git fetch bitbucket
 9057  git fetch https://hiro_@bitbucket.org/ioeeschindler/modularsensorkit.git
 9058  cf
 9059  cf login
 9060  pip install joblib
 9061  l ../
 9062  pip install pydotplus
 9063  cd  ../NEC-SCM
 9064  git pull origin
 9065  cd NEC-SCM
 9066  CF_COLOR=true
 9067  echo $CF_HOME
 9068  cf service
 9069  cf services list
 9070  cf org
 9071  cf orgorgs
 9072  cf orgs
 9073  cf -h
 9074  cf -h | peco
 9075  cf spaces
 9076  cf org-users dev
 9077  cf env
 9078  brew tap cloudfoundry/tap
 9079  brew upgrade cf-cli
 9080  brew install `brew outdated`
 9081  brew upgrade pandoc
 9082  export FLASK_ENV=development
 9083  flask run
 9084  pip install pixiedust
 9085  rsync -avzhe --progress ~/Box/Internal\ -\ Bridgestone/data/Cerrejon_Data/Processed dss_server:/home/nfs135/projects/bridgestone/data/Cerrejon_Data/Processed
 9086  rsync -avzh  ~/Box/Internal\ -\ Bridgestone/data/Cerrejon_Data/Processed dss_server:/home/nfs135/projects/bridgestone/data/Cerrejon_Data/Processed
 9087  cd Box/Internal\ -\ Bridgestone
 9088  subl data_change_log.txt
 9089  subl DATACHANGELOG.md
 9090  rsync -avzh  ~/Box/Internal\ -\ Bridgestone/data/Cerrejon_Data/Processed/ dss_server:/home/nfs135/projects/bridgestone/data/Cerrejon_Data/Processed
 9091  rsync -avzh  ~/Box/Internal\ -\ Bridgestone/data/Cerrejon_Data/Processed/ dss_server:/home/nfs135/projects/bridgestone/data/Cerrejon_Data/Processed --delete
 9092  rsync -avzh  ~/Box/Internal\ -\ Bridgestone/data/Cerrejon_Data/Processed/ dss_server:/home/nfs135/projects/bridgestone/data/Cerrejon_Data/Processed --delete --exclude .DS_Store
 9093  rsync -avzh  ~/Box/Internal\ -\ Bridgestone/data/Cerrejon_Data/Processed/ dss_server:/home/nfs135/projects/bridgestone/data/Cerrejon_Data/Processed --delete --exclude .DS_Store --progress 
 9094  ssh -i -N -n -L 127.0.0.1:8888:127.0.0.1:8888 dss_server -Y
 9095  ssh  -N -n -L 127.0.0.1:8888:127.0.0.1:8888 dss_server -Y
 9096  git add bsanalytics/
 9097  git commit -m 'Change src directory as bsanalytics'
 9098  ! pip install fire
 9099  python tyre_life_pipeline.py -h 
 9100  python tyre_life_pipeline.py --help
 9101  python tyre_life_pipeline.py
 9102  git diff bsanalytics/dashboards/teaboard.py
 9103  git checkout bsanalytics/dashboards/teaboard.py
 9104  git add bsanalytics/pipeline/output_pipeline/
 9105  git add bsanalytics/pipeline/tyre_life_pipeline.py
 9106  git commit -m 'Add pipeline for tyre life'
 9107  ssh Product launche
 9108  ssh h2o-server
 9109  echo "# pricetracker" >> README.md\ngit init\ngit add README.md\ngit commit -m "first commit"\ngit remote add origin git@github.com:h1ros/pricetracker.git\ngit push -u origin master\n
 9110  git clone git@github.com:h1ros/pricetracker.git
 9111  cd pricetracker
 9112  cd Projects/pricetracker
 9113  pip install fix_yahoo_finance
 9114  pip3 install iexfinance
 9115  pip3 install cufflinks
 9116  pip3 install cufflinks --upgrade
 9117  pip3 install plotly --upgrade
 9118  pip uninstall plotly
 9119  pip install plotly==2.7.0
 9120  git commit -m 'Add new notebook'
 9121  git config --user
 9122  git config --local
 9123  git config --local user.name
 9124  git config --local user.email
 9125  git config --local user.name 'h1ros'
 9126  git config --local user.email '8764683+h1ros@users.noreply.github.com'
 9127  git commit -m 'Initial Commit'
 9128  git remote add origin git@github.com:h1ros/pricetracker.git
 9129  pip freeze > requirements.txt
 9130  git commit -m 'Add requirements'
 9131  pip install \ncryptography --upgrade
 9132  pip install pycrypto  --upgrade
 9133  pip install rope --upgrade
 9134  pip uninstall pycrypto
 9135  pip uninstall dlitz/pycrypto
 9136  conda uninstall pycrypto
 9137  conda env export > environment.yml
 9138  rm requirements.txt
 9139  git commit -m 'Replace requirement by environment.yml'
 9140  ssh dss
 9141  git commit -m 'Update functions on bsanalytics/pipeline/tyre_life_pipeline.py'
 9142  git add notebooks/hiro/
 9143  git diff --cached notebooks/hiro/20180803 Merge Data Set for Cerrejon-Copy1.ipynb
 9144  git diff --cached "notebooks/hiro/20180803 Merge Data Set for Cerrejon-Copy1.ipynb"
 9145  git rm notebooks/hiro/mydask.png
 9146  git rm --cached notebooks/hiro/mydask.png
 9147  git pull hiro/eda
 9148  git add notebooks/hiro/20180810 Estimate RTD Used Ratio for Cerrejon Data.ipynb
 9149  git add "notebooks/hiro/20180810 Estimate RTD Used Ratio for Cerrejon Data.ipynb" 
 9150  git commit -m 'Add notebooks/hiro/20180810 Estimate RTD Used Ratio for Cerrejon Data.ipynb as draft'
 9151  git push -u origin master
 9152  ssh 
 9153  man rsync 
 9154  rsync -avzh  ~/Box/Internal\ -\ Bridgestone/data/Cerrejon_Data/Processed dss_server:/home/nfs135/projects/bridgestone/data/Cerrejon_Data/Processed 
 9155  rsync -chavzP --stats sch_cpu:hiro/notebook  ~/Box/GE\ Digital\ Data\ Science\ 2017/10\ Schindler\ Personal/Backup/hiro/
 9156  ls -l testfile
 9157  ssh dss_server "/bin/true" > testfile 
 9158  l 
 9159  vim testfile
 9160  rsync -avzh  ~/Box/Internal\ -\ Bridgestone/data/Cerrejon_Data/Processed/ dss_server:/home/nfs135/projects/bridgestone/data/Cerrejon_Data/Processed 
 9161  rsync -avzh  ~/Box/Internal\ -\ Bridgestone/data/Cerrejon_Data/ dss_server:/home/nfs135/projects/bridgestone/data/Cerrejon_Data/
 9162  rsync -avzh  ~/Box/Internal\ -\ Bridgestone/data/Cerrejon_Data/ dss_server:/home/nfs135/projects/bridgestone/data/Cerrejon_Data/ --delete --exclude .DS_Store --progress 
 9163  ssh hi
 9164  scp sch_cp:requirement.txt ~/Downloads
 9165  scp sch_cpu:requirement.txt ~/Downloads
 9166  subl .zshrc
 9167  git clone git@github.com:h2oai/h2o-2.git
 9168  git clone https://github.com/h2oai/h2o-2.git
 9169  open h2o-2
 9170  which terminal
 9171  which iterm
 9172  iterm2
 9173  which zsh
 9174  rsync -avzh   dss_server:/home/nfs135/projects/bridgestone/data/Cerrejon_Data/Processed   ~/Box/Internal\ -\ Bridgestone/data/Cerrejon_Data/Processed --exclude .DS_Store --progress 
 9175  rsync -h
 9176  rsync -h | peco
 9177  rsync -avzh   dss_server:/home/nfs135/projects/bridgestone/data/Cerrejon_Data/Processed   ~/Box/Internal\ -\ Bridgestone/data/Cerrejon_Data/Processed --exclude .DS_Store --progress --dry-run
 9178  rsync -avzh   dss_server:/home/nfs135/projects/bridgestone/data/Cerrejon_Data/Processed   ~/Box/Internal\ -\ Bridgestone/data/Cerrejon_Data/ --exclude .DS_Store --progress --dry-run
 9179  rsync -avzh   dss_server:/home/nfs135/projects/bridgestone/data/Cerrejon_Data/Processed   ~/Box/Internal\ -\ Bridgestone/data/Cerrejon_Data/ --exclude .DS_Store --progress --include tmp --dry-run
 9180  rsync -avzh   dss_server:/home/nfs135/projects/bridgestone/data/Cerrejon_Data/Processed   ~/Box/Internal\ -\ Bridgestone/data/Cerrejon_Data/ --exclude .DS_Store --progress --include "*/tmp/*" --dry-run
 9181  rsync -avzh   dss_server:/home/nfs135/projects/bridgestone/data/Cerrejon_Data/Processed   ~/Box/Internal\ -\ Bridgestone/data/Cerrejon_Data/ --exclude *  --progress --include "*/tmp/*" --dry-run
 9182  rsync -avzh   dss_server:/home/nfs135/projects/bridgestone/data/Cerrejon_Data/Processed   ~/Box/Internal\ -\ Bridgestone/data/Cerrejon_Data/ --exclude "*"  --progress --include "*/tmp/*" --dry-run
 9183  rsync -avzh   dss_server:/home/nfs135/projects/bridgestone/data/Cerrejon_Data/Processed   ~/Box/Internal\ -\ Bridgestone/data/Cerrejon_Data/ --exclude "*"  --progress --include "*tmp/*" --dry-run
 9184  rsync -avzh   dss_server:/home/nfs135/projects/bridgestone/data/Cerrejon_Data/Processed   ~/Box/Internal\ -\ Bridgestone/data/Cerrejon_Data/ --exclude "*"  --progress --include "*tmp*" --dry-run
 9185  rsync -avzh   dss_server:/home/nfs135/projects/bridgestone/data/Cerrejon_Data/Processed   ~/Box/Internal\ -\ Bridgestone/data/Cerrejon_Data/ --exclude "*"  --progress --include "tmp" --dry-run
 9186  rsync -avzh   dss_server:/home/nfs135/projects/bridgestone/data/Cerrejon_Data/Processed   ~/Box/Internal\ -\ Bridgestone/data/Cerrejon_Data/ --exclude "*" --include "tmp" --dry-run
 9187  rsync -avzh   dss_server:/home/nfs135/projects/bridgestone/data/Cerrejon_Data/Processed   ~/Box/Internal\ -\ Bridgestone/data/Cerrejon_Data/  --include "tmp" --dry-run
 9188  rsync -avzh   dss_server:/home/nfs135/projects/bridgestone/data/Cerrejon_Data/Processed   ~/Box/Internal\ -\ Bridgestone/data/Cerrejon_Data/  --include "Processed/tmp" --dry-run
 9189  rsync -avzh   dss_server:/home/nfs135/projects/bridgestone/data/Cerrejon_Data/Processed   ~/Box/Internal\ -\ Bridgestone/data/Cerrejon_Data/  --include "Processed/tmp*" --dry-run
 9190  rsync -avzh   dss_server:/home/nfs135/projects/bridgestone/data/Cerrejon_Data/Processed   ~/Box/Internal\ -\ Bridgestone/data/Cerrejon_Data/  --include "Processed/tmp*" --exclude '*' --dry-run
 9191  rsync -avzh   dss_server:/home/nfs135/projects/bridgestone/data/Cerrejon_Data/Processed   ~/Box/Internal\ -\ Bridgestone/data/Cerrejon_Data/  --include "Processed/tmp*" --exclude '*.DS_*' --dry-run
 9192  rsync -avzh   dss_server:/home/nfs135/projects/bridgestone/data/Cerrejon_Data/Processed   ~/Box/Internal\ -\ Bridgestone/data/Cerrejon_Data/  --include "Processed/tmp*" --exclude '*.DS_*' "d_*" --dry-run
 9193  rsync -avzh   dss_server:/home/nfs135/projects/bridgestone/data/Cerrejon_Data/Processed   ~/Box/Internal\ -\ Bridgestone/data/Cerrejon_Data/  --include "Processed/tmp*" --exclude "d_*|.*" --dry-run
 9194  rsync -avzh   dss_server:/home/nfs135/projects/bridgestone/data/Cerrejon_Data/Processed   ~/Box/Internal\ -\ Bridgestone/data/Cerrejon_Data/  --include "Processed/tmp*" --exclude "d_*" 
 9195  exit
 9196  nmap -sP 192.168.1.*
 9197  nmap -sP '192.168.1.*'
 9198  nmap
 9199  nmap -h
 9200  man nmap
 9201  nmap 
 9202  brew link nmap
 9203  sudo brew install nmap
 9204  brew install nmap
 9205  `brew link nmap`
 9206  bash
 9207  ifconfig | grep ip
 9208  ifconfig | grep 192
 9209  ping 192.168.86.132
 9210  arp -a
 9211  man arp
 9212  cd bsanalytics
 9213  cd pipeline
 9214  python tyre_life_pipeline.py get_intermediate d_02_inservice
 9215  python tyre_life_pipeline.py get_intermediate d_05_02_cleaned_inservice
 9216  python tyre_life_pipeline.py get_intermediate d_03_payload
 9217  kill 28739
 9218  kill 60124
 9219  cd Projects/modularsensorkit
 9220  cd ~/Projects/Bridgestone_New_Tyres
 9221  cd Projects/Bridgestone_New_Tyres
 9222  pip install pyspark
 9223  pwd
 9224  python bsanalytics/pipeline/tyre_life_pipeline.py tyre_life_pipeline
 9225  sudo diskutil umount force ~/Mount/dss_server
 9226  rsync -avzh   dss_server:/home/nfs135/projects/bridgestone/data/Cerrejon_Data/Processed   ~/Box/Internal\ -\ Bridgestone/data/Cerrejon_Data/  --include "Processed/tmp*" --exclude "d_*" --dry-run
 9227  ssh  -N -n -L 127.0.0.1:9889:127.0.0.1:9889 dss_server -Y
 9228  head /Users/212339410/Box/Internal - Bridgestone/data/Cerrejon_Data/Processed/btag_sensor_df.csv | csvlook
 9229  csvkit
 9230  pip install csvkit
 9231  head "/Users/212339410/Box/Internal - Bridgestone/data/Cerrejon_Data/Processed/btag_sensor_df.csv" | csvlook
 9232  head "/Users/212339410/Box/Internal - Bridgestone/data/Cerrejon_Data/Processed/part-00000-5b022cf4-1674-4762-b4e3-7a3e41d29a46.csv" | csvlook
 9233  tail "/Users/212339410/Box/Internal - Bridgestone/data/Cerrejon_Data/Processed/part-00000-5b022cf4-1674-4762-b4e3-7a3e41d29a46.csv" | csvlook
 9234  git add -u
 9235  git commit -m 'Update dask flow chart'
 9236  vim ~/.gitignore_global
 9237  git add dist
 9238  git add -f dist
 9239  git reset dist/.DS_Store
 9240  git reset dist/demandforecast/__pycache__/app.cpython-35.pyc
 9241  git commit -m 'Add sample flask app for demand forecsat'
 9242  git commit --amend
 9243  git reset HEAD  dist/demandforecast/.DS_Store
 9244  git push origin hiro/eda
 9245  git pull origin hiro/eda
 9246  vim ~/Box/Internal\ -\ Bridgestone/data/Cerrejon_Data/Processed/part-00000-5c119f09-e122-4295-9e7d-071d5e9ec009.csv
 9247  head "/Users/212339410/Mount/dss_server/projects/Bridgestone_New_Tyres/data/Cerrejon_Data/Processed/part-00000-5c119f09-e122-4295-9e7d-071d5e9ec009.csv" | csvlook
 9248  head ~/Box/Internal\ -\ Bridgestone/data/Cerrejon_Data/Processed/part-00000-5c119f09-e122-4295-9e7d-071d5e9ec009.csv
 9249  head ~/Box/Internal\ -\ Bridgestone/data/Cerrejon_Data/Processed/part-00000-5c119f09-e122-4295-9e7d-071d5e9ec009.csv | csvlook
 9250  head -n 1000 ~/Box/Internal\ -\ Bridgestone/data/Cerrejon_Data/Processed/part-00000-5c119f09-e122-4295-9e7d-071d5e9ec009.csv | csvlook
 9251  tail  -n 5 ~/Box/Internal\ -\ Bridgestone/data/Cerrejon_Data/Processed/part-00000-5c119f09-e122-4295-9e7d-071d5e9ec009.csv | csvlook
 9252  git clone git@github.build.ge.com:IndustrialDataScience/bulk-pg-upload-python.git
 9253  git cd bulk-pg-upload-python
 9254  cd bulk-pg-upload-python
 9255  pip install psycopg2
 9256  python bulk-pg-upload.py
 9257  cat manifest.yml
 9258  cat Procfile
 9259  vim manifest.yml
 9260  cf logs
 9261  head /Users/212339410/Mount/dss_server/projects/Bridgestone_New_Tyres/data/Cerrejon_Data/Processed/part-00000-5c119f09-e122-4295-9e7d-071d5e9ec009.csv
 9262  head /Users/212339410/Box/Internal - Bridgestone/data/Cerrejon_Data/Processed/part-00000-5c119f09-e122-4295-9e7d-071d5e9ec009.csv 
 9263  head "/Users/212339410/Box/Internal - Bridgestone/data/Cerrejon_Data/Processed/part-00000-5c119f09-e122-4295-9e7d-071d5e9ec009.csv"
 9264  rsync -avzh   dss_server:/home/nfs135/projects/bridgestone/data/Cerrejon_Data/Processed   ~/Box/Internal\ -\ Bridgestone/data/Cerrejon_Data/  --include "Processed/*" --exclude "d_*" --dry-run
 9265  rsync -avzh   dss_server:/home/nfs135/projects/bridgestone/data/Cerrejon_Data/Processed   ~/Box/Internal\ -\ Bridgestone/data/Cerrejon_Data/  --include "Processed/*" --exclude "d_13*" --dry-run
 9266  rsync -avzh   dss_server:/home/nfs135/projects/bridgestone/data/Cerrejon_Data/Processed   ~/Box/Internal\ -\ Bridgestone/data/Cerrejon_Data/  --include "Processed/*" --exclude "*Processed/d_13*" --dry-run
 9267  rsync -avzh   dss_server:/home/nfs135/projects/bridgestone/data/Cerrejon_Data/Processed   ~/Box/Internal\ -\ Bridgestone/data/Cerrejon_Data/  --include "Processed/*" --exclude "Processed/d_13*" --dry-run
 9268  rsync -avzh   dss_server:/home/nfs135/projects/bridgestone/data/Cerrejon_Data/Processed   ~/Box/Internal\ -\ Bridgestone/data/Cerrejon_Data/Processed --exclude "Processed/d_13*" --dry-run
 9269  rsync -avzh   dss_server:/home/nfs135/projects/bridgestone/data/Cerrejon_Data/Processed   ~/Box/Internal\ -\ Bridgestone/data/Cerrejon_Data/Processed --exclude "Processed/d_13*" "archive" --dry-run
 9270  rsync -avzh   dss_server:/home/nfs135/projects/bridgestone/data/Cerrejon_Data/Processed   ~/Box/Internal\ -\ Bridgestone/data/Cerrejon_Data/Processed --exclude "Processed/d_13*" --exclude "archive" --dry-run
 9271  rsync -avzh   dss_server:/home/nfs135/projects/bridgestone/data/Cerrejon_Data/Processed   ~/Box/Internal\ -\ Bridgestone/data/Cerrejon_Data/Processed --exclude "Processed/d_13*" --exclude "archive" --exclude .DS_Store --dry-run
 9272  rsync -avzh   dss_server:/home/nfs135/projects/bridgestone/data/Cerrejon_Data/Processed   ~/Box/Internal\ -\ Bridgestone/data/Cerrejon_Data/ --exclude "Processed/d_13*" --exclude "archive" --exclude .DS_Store --dry-run
 9273  rsync -avzh   dss_server:/home/nfs135/projects/bridgestone/data/Cerrejon_Data/Processed   ~/Box/Internal\ -\ Bridgestone/data/Cerrejon_Data/ --exclude "Processed/d_13*" --exclude "archive" --exclude .DS_Store --include Processed/d_204_df_sensor_joined.csv --dry-run
 9274  rsync -avzh   dss_server:/home/nfs135/projects/bridgestone/data/Cerrejon_Data/Processed   ~/Box/Internal\ -\ Bridgestone/data/Cerrejon_Data/ --exclude "Processed/d_13*" --exclude "archive" --exclude .DS_Store --include "Processed/d_204_df_sensor_joined.csv" --dry-run
 9275  rsync -avzh   dss_server:/home/nfs135/projects/bridgestone/data/Cerrejon_Data/Processed   ~/Box/Internal\ -\ Bridgestone/data/Cerrejon_Data/ --exclude "Processed/d_13*" --exclude "archive" --exclude .DS_Store --include "d_204_df_sensor_joined.csv" --dry-run
 9276  rsync -avzh   dss_server:/home/nfs135/projects/bridgestone/data/Cerrejon_Data/Processed/d_204   ~/Box/Internal\ -\ Bridgestone/data/Cerrejon_Data/ --exclude "Processed/d_13*" --exclude "archive" --exclude .DS_Store  --dry-run
 9277  rsync -avzh   dss_server:/home/nfs135/projects/bridgestone/data/Cerrejon_Data/Processed/d_204_sensor_joined.csv   ~/Box/Internal\ -\ Bridgestone/data/Cerrejon_Data/ --exclude "Processed/d_13*" --exclude "archive" --exclude .DS_Store  --dry-run
 9278  rsync -avzh   dss_server:/home/nfs135/projects/bridgestone/data/Cerrejon_Data/Processed/d_204_sensor_joined.csv   ~/Box/Internal\ -\ Bridgestone/data/Cerrejon_Data/ --exclude "Processed/d_13*" --exclude "archive" --exclude .DS_Store 
 9279  rsync -avzh   dss_server:/home/nfs135/projects/bridgestone/data/Cerrejon_Data/Processed/d_204_sensor_joined.csv   ~/Box/Internal\ -\ Bridgestone/data/Cerrejon_Data/Processed --exclude "Processed/d_13*" --exclude "archive" --exclude .DS_Store 
 9280  vim ~/Box/Internal\ -\ Bridgestone/data/Cerrejon_Data/Processed/d_204_sensor_joined.csv
 9281  cf push -f Manifest.yml
 9282  git checkout .
 9283  cd Bridgestone_New_Tyres/dist
 9284  cd ~/Python/bulk-pg-upload-python
 9285  cf target -o NEC-SCM
 9286  cf target -o 'NEC-SCM'
 9287  cf login --sso
 9288  cf apps
 9289  cf serivices
 9290  cf services
 9291  cf org-users
 9292  cf org-users hiroaki.shioi@ge.com
 9293  cf push
 9294  cf --version
 9295  predix --version
 9296  cf login -a https://api.system.aws-usw02-pr.ice.predix.io
 9297  cf login -a https://api.system.asv-pr.ice.predix.io --sso
 9298  cf login -a https://predix-io.run.aws-jp01-pr.ice.predix.io
 9299  cd bulk-pg-upload-python/
 9300  make -h
 9301  kill 12164
 9302  kill 10953
 9303  kill 16152
 9304  kill 15272
 9305  kill 18348
 9306  kill 18454
 9307  kill 24002
 9308  kill 99196
 9309  lsof -nP -i4TCP:6311 | grep LISTEN
 9310  fuser
 9311  fuser 6311
 9312  lsof -i:8080
 9313  kill 2605
 9314  kill 2832
 9315  kill 12436
 9316  git clone https://github.com/amywang718/h2o_3_tableau_examples.git
 9317  cd h2o_3_tableau_examples
 9318  kill 4605
 9319  kill 5773
 9320  kill 5846
 9321  kill 11787
 9322  lsof -i:6312
 9323  mount_dss_server
 9324  cd "/Users/212339410/Box/GE Digital Data Science 2017/10 SMFG Perfonal/Data/Data_20180831"
 9325  cd Bridgestone_New_Tyres
 9326  cd dist
 9327  cd demandforecast/
 9328  cf push 
 9329  cf login -a https://api.system.aws-usw02-pr.ice.predix.io --sso
 9330  cd /Users/212339410/Downloads/bsanalytics
 9331  cd ../dist
 9332  cd demandforecast
 9333  source ~/.bashrc
 9334  python app.py
 9335  l ~/Projects
 9336  l ~/Projects/
 9337  less /private/var/log/system.log
 9338  ps aux | peco
 9339  kill 212339410
 9340  lsof -nP -i4TCP:54321 | grep LISTEN
 9341  kill 21607
 9342  lsof -nP -i4TCP:54322 | grep LISTEN
 9343  lsof -nP -i4TCP:54323 | grep LISTEN
 9344  kill 34332
 9345  echo $https_proxy
 9346  pip install googletrans
 9347  python translate.py translate_csv_columns '/Users/212339410/Box/DSA Internal-SMFG/01_data/20180831_NIDEC_processed_data/01_raw_translated/07_04_201807_accounts_receivable_abroad.csv' '/Users/212339410/Box/DSA Internal-SMFG/01_data/20180831_NIDEC_processed_data/01_raw_translated'
 9348  python translate.py --hlep
 9349  python translate.py  -- --help
 9350  python translate.py translate_csv_columns --path-file "/Users/212339410/Box/DSA Internal-SMFG/01_data/20180831_NIDEC_processed_data/01_raw_translated/07_04_201807_accounts_receivable_abroad.csv" --output-dir "/Users/212339410/Box/DSA Internal-SMFG/01_data/20180831_NIDEC_processed_data/01_raw_translated"
 9351  python translate.py translate_csv_columns --path-file "/Users/212339410/Box/DSA Internal-SMFG/01_data/20180831_NIDEC_processed_data/01_raw_translated/07_04_201807_accounts_receivable_abroad.csv" --output-dir "/Users/212339410/Box/DSA Internal-SMFG/01_data/20180831_NIDEC_processed_data/01_raw_translated" 
 9352  python translate.py translate_csv_columns --path-file "/Users/212339410/Box/DSA Internal-SMFG/01_data/20180831_NIDEC_processed_data/01_raw_translated/07_04_201807_accounts_receivable_abroad.csv" --output-dir "/Users/212339410/Box/DSA Internal-SMFG/01_data/20180831_NIDEC_processed_data/01_raw_translated" --path_lookup ''
 9353  python translate.py translate_csv_columns --path-file "/Users/212339410/Box/DSA Internal-SMFG/01_data/20180831_NIDEC_processed_data/01_raw_translated/07_04_201807_accounts_receivable_abroad.csv" --output-dir "/Users/212339410/Box/DSA Internal-SMFG/01_data/20180831_NIDEC_processed_data/01_raw_translated" --path_lookup '' --encoding 'shift-jis'
 9354  python translate.py translate_csv_columns --path-file "/Users/212339410/Box/DSA Internal-SMFG/01_data/20180831_NIDEC_processed_data/01_raw_translated/07_04_201807_accounts_receivable_abroad.csv" --output-dir "/Users/212339410/Box/DSA Internal-SMFG/01_data/20180831_NIDEC_processed_data/01_raw_translated" --path_lookup '' --encoding 'euc-jp'
 9355  python translate.py translate_csv_columns --path-file "/Users/212339410/Box/DSA Internal-SMFG/01_data/20180831_NIDEC_processed_data/01_raw_translated/07_04_201807_accounts_receivable_abroad.csv" --output-dir "/Users/212339410/Box/DSA Internal-SMFG/01_data/20180831_NIDEC_processed_data/01_raw_translated" --path_lookup '' --encoding 'utf-16'
 9356  file /Users/212339410/Box/DSA Internal-SMFG/01_data/20180831_NIDEC_processed_data/01_raw_translated/07_04_201807_accounts_receivable_abroad.csv
 9357  file "/Users/212339410/Box/DSA Internal-SMFG/01_data/20180831_NIDEC_processed_data/01_raw_translated/07_04_201807_accounts_receivable_abroad.cs"
 9358  file "/Users/212339410/Box/DSA Internal-SMFG/01_data/20180831_NIDEC_processed_data/01_raw_translated/07_04_201807_accounts_receivable_abroad.csv"
 9359  vim "/Users/212339410/Box/DSA Internal-SMFG/01_data/20180831_NIDEC_processed_data/01_raw_translated/07_04_201807_accounts_receivable_abroad.csv"
 9360  head -n 1 "/Users/212339410/Box/DSA Internal-SMFG/01_data/20180831_NIDEC_processed_data/01_raw_translated/07_04_201807_accounts_receivable_abroad.csv"
 9361  head -n 1 "/Users/212339410/Box/DSA Internal-SMFG/01_data/20180831_NIDEC_processed_data/01_raw_translated/07_04_201807_accounts_receivable_abroad.csv" > pp.txt
 9362  iconv -f utf-8 -t utf-16 "/Users/212339410/Box/DSA Internal-SMFG/01_data/20180831_NIDEC_processed_data/01_raw_translated/07_04_201807_accounts_receivable_abroad_translated.csv" > pp.csv
 9363  iconv -f utf-8 -t utf-8 "/Users/212339410/Box/DSA Internal-SMFG/01_data/20180831_NIDEC_processed_data/01_raw_translated/07_04_201807_accounts_receivable_abroad_translated.csv" > pp.csv
 9364  vim pp.csv
 9365  open pp.csv
 9366  vim /Users/212339410/Box/DSA Internal-SMFG/01_data/20180831_NIDEC_processed_data/01_raw_translated/07_04_201807_accounts_receivable_abroad_translated.csv
 9367  vim "/Users/212339410/Box/DSA Internal-SMFG/01_data/20180831_NIDEC_processed_data/01_raw_translated/07_04_201807_accounts_receivable_abroad_translated.csv"
 9368  cd 20180831_NIDEC_processed_data/0
 9369  cd 20180831_NIDEC_processed_data/
 9370  cd 01_raw_translated
 9371  cp 07_04_201807_accounts_receivable_abroad_translated.csv pp.csv
 9372  open pp.bom.csv
 9373  file pp.csv
 9374  file 07_04_201807_accounts_receivable_abroad_translated.csv
 9375  file pp.bom.csv
 9376  file 07_04_201807_accounts_receivable_abroad.csv
 9377  python2
 9378  python add_bom.py pp.csv
 9379  python2 add_bom.py pp.csv
 9380  vim add_bom.py
 9381  python translate.py translate_csv_columns --path-file "/Users/212339410/Box/DSA Internal-SMFG/01_data/20180831_NIDEC_processed_data/01_raw_translated/07_04_201807_accounts_receivable_abroad.csv" --output-dir "/Users/212339410/Box/DSA Internal-SMFG/01_data/20180831_NIDEC_processed_data/01_raw_translated" --path_lookup '' --encoding 'utf-8'
 9382  python translate.py translate_csv_columns --path-file "/Users/212339410/Box/DSA Internal-SMFG/01_data/20180831_NIDEC_processed_data/01_raw_translated/07_04_201807_accounts_receivable_abroad.csv" --output-dir "/Users/212339410/Box/DSA Internal-SMFG/01_data/20180831_NIDEC_processed_data/01_raw_translated" --path_lookup 'lookup.csv' --encoding 'utf-8'
 9383  python translate.py translate_csv_columns --path-file "/Users/212339410/Box/DSA Internal-SMFG/01_data/20180831_NIDEC_processed_data/01_raw_translated/07_04_201807_accounts_receivable_abroad.csv" --output-dir "/Users/212339410/Box/DSA Internal-SMFG/01_data/20180831_NIDEC_processed_data/01_raw_translated" --path_lookup '/Users/212339410/Box/DSA Internal-SMFG/01_data/20180831_NIDEC_processed_data/01_raw_translated/lookup.csv' --encoding 'utf-8'
 9384  python translate.py translate_csv_columns --path-file "/Users/212339410/Box/DSA Internal-SMFG/01_data/20180831_NIDEC_processed_data/00_raw_to_unprotected_seperate/01_02_20180831_sales.csv" --output-dir "/Users/212339410/Box/DSA Internal-SMFG/01_data/20180831_NIDEC_processed_data/01_raw_translated" --path_lookup '/Users/212339410/Box/DSA Internal-SMFG/01_data/20180831_NIDEC_processed_data/01_raw_translated/lookup.csv' --encoding 'utf-8'
 9385  python translate.py translate_csv_columns --path-file "/Users/212339410/Box/DSA Internal-SMFG/01_data/20180831_NIDEC_processed_data/00_raw_to_unprotected_seperate/01_03_20180831_remaining.csv" --output-dir "/Users/212339410/Box/DSA Internal-SMFG/01_data/20180831_NIDEC_processed_data/01_raw_translated" --path_lookup '/Users/212339410/Box/DSA Internal-SMFG/01_data/20180831_NIDEC_processed_data/01_raw_translated/lookup.csv' --encoding 'utf-8'
 9386  python translate.py translate_csv_columns --path-file "/Users/212339410/Box/DSA Internal-SMFG/01_data/20180831_NIDEC_processed_data/00_raw_to_unprotected_seperate/Maintenance/10_01_2017_maintenance_f_cost_threshold6.csv" --output-dir "/Users/212339410/Box/DSA Internal-SMFG/01_data/20180831_NIDEC_processed_data/01_raw_translated" --path_lookup '/Users/212339410/Box/DSA Internal-SMFG/01_data/20180831_NIDEC_processed_data/01_raw_translated/lookup.csv' --encoding 'utf-8'
 9387  subl  ~/.Renviron
 9388  pip install import click
 9389  pip install click
 9390  python translate.py 
 9391  python translate.py translate_csv_columns
 9392  python translate.py translate_csv_columns --help
 9393  k
 9394  cd smanalytics
 9395  pip3 install imutils
 9396  conda upgrade jupyter 
 9397  conda conda update jupyter
 9398  subl ~/.npmrc
 9399  jupyter labextension install @jpmorganchase/perspective-jupyterlab
 9400  echo $PREFIX
 9401  which npm
 9402  npm config edit --global
 9403  subl /usr/local/etc/npmrc
 9404  pip install --upgrade "ipython[all]"
 9405  pip uninstall Jinja2
 9406  pip install jupyter
 9407  source activate sch27
 9408  source activate pt35
 9409  pip install perspective
 9410  ssh  -N -n -L 127.0.0.1:9888:127.0.0.1:9888 dss_server -Y
 9411  python translate.py csv_columns 
 9412  python translate.py csv_columns --help
 9413  python translate.py csv_columns -p ../../SMFG/data/20180831_NIDEC_processed_data/00_raw_to_unprotected_seperate/01_01_20180831_po.csv -o ../data/20180831_NIDEC_processed_data/01_raw_translated -l ../data/20180831_NIDEC_processed_data/lookup.csv
 9414  python translate.py csv_contents -p ../../SMFG/data/20180831_NIDEC_processed_data/01_col_translated/01_01_20180831_po_translated.csv -o ../data/20180831_NIDEC_processed_data/02_row_translated -l ../data/20180831_NIDEC_processed_data/lookup.csv
 9415  python translate.py csv_contents -p ../../SMFG/data/20180831_NIDEC_processed_data/01_col_translated/01_01_20180831_po_translated.csv -o ../data/20180831_NIDEC_processed_data/02_row_translated -l ../data/20180831_NIDEC_processed_data/lookup.csv -c "Region name" 
 9416  python translate.py csv_contents -p ../../SMFG/data/20180831_NIDEC_processed_data/01_col_translated/01_01_20180831_po_translated.csv -o ../data/20180831_NIDEC_processed_data/02_row_translated -l ../data/20180831_NIDEC_processed_data/lookup.csv -c "Region name" "Product category"
 9417  python translate.py csv_contents -p ../../SMFG/data/20180831_NIDEC_processed_data/01_col_translated/01_01_20180831_po_translated.csv -o ../data/20180831_NIDEC_processed_data/02_row_translated -l ../data/20180831_NIDEC_processed_data/lookup.csv -c "Region name" "Product category, Region name"
 9418  python translate.py csv_contents -p ../../SMFG/data/20180831_NIDEC_processed_data/01_col_translated/01_01_20180831_po_translated.csv -o ../data/20180831_NIDEC_processed_data/02_row_translated -l ../data/20180831_NIDEC_processed_data/lookup.csv -c "Product category, Region name"
 9419  python trasnlate --help
 9420  python translate.py check_cardinality -p /Users/212339410/Box/DSA Internal-SMFG/01_data/20180831_NIDEC_processed_data/01_col_translated/01_02_20180831_sales_translated.csv
 9421  python translate.py check_cardinality -p "/Users/212339410/Box/DSA Internal-SMFG/01_data/20180831_NIDEC_processed_data/01_col_translated/01_02_20180831_sales_translated.csv"
 9422  python translate.py csv_content  -p "/Users/212339410/Box/DSA Internal-SMFG/01_data/20180831_NIDEC_processed_data/01_col_translated/01_02_20180831_sales_translated.csv" -o ../data/20180831_NIDEC_processed_data/02_row_translated -p ../data/20180831_NIDEC_processed_data/lookup.csv
 9423  python translate.py csv_contents -p "/Users/212339410/Box/DSA Internal-SMFG/01_data/20180831_NIDEC_processed_data/01_col_translated/01_02_20180831_sales_translated.csv" -o ../data/20180831_NIDEC_processed_data/02_row_translated -p ../data/20180831_NIDEC_processed_data/lookup.csv -c "Product category, Region name"
 9424  python translate.py csv_contents -p "/Users/212339410/Box/DSA Internal-SMFG/01_data/20180831_NIDEC_processed_data/01_col_translated/01_03_20180831_remaining_translated.csv" -o ../data/20180831_NIDEC_processed_data/02_row_translated -l ../data/20180831_NIDEC_processed_data/lookup.csv -c "Product category, Region name"
 9425  python translate.py csv_contents -p "/Users/212339410/Box/DSA Internal-SMFG/01_data/20180831_NIDEC_processed_data/01_col_translated/01_02_20180831_sales_translated.csv" -o ../data/20180831_NIDEC_processed_data/02_row_translated -l ../data/20180831_NIDEC_processed_data/lookup.csv -c "Product category, Region name"
 9426  python translate.py csv_contents -p "/Users/212339410/Box/DSA Internal-SMFG/01_data/20180831_NIDEC_processed_data/01_col_translated/01_01_20180831_po_translated.csv" -o ../data/20180831_NIDEC_processed_data/02_row_translated -l ../data/20180831_NIDEC_processed_data/lookup.csv -c "Product category, Region name"
 9427  python translate.py csv_contents -p "/Users/212339410/Box/DSA Internal-SMFG/01_data/20180831_NIDEC_processed_data/01_col_translated/01_01_20180831_po_translated.csv" -o ../data/20180831_NIDEC_processed_data/02_row_translated -l ../data/20180831_NIDEC_processed_data/lookup.csv -c "Product category,Region name"
 9428  python translate.py csv_contents -p "/Users/212339410/Box/DSA Internal-SMFG/01_data/20180831_NIDEC_processed_data/01_col_translated/01_02_20180831_sales_translated.csv" -o ../data/20180831_NIDEC_processed_data/02_row_translated -l ../data/20180831_NIDEC_processed_data/lookup.csv -c "Product category,Region name"
 9429  python translate.py csv_contents -p "/Users/212339410/Box/DSA Internal-SMFG/01_data/20180831_NIDEC_processed_data/01_col_translated/01_03_20180831_remaining_translated.csv" -o ../data/20180831_NIDEC_processed_data/02_row_translated -l ../data/20180831_NIDEC_processed_data/lookup.csv -c "Product category,Region name"
 9430  python check_cardinality -p ../data/20180831_NIDEC_processed_data/01_col_translated/07_01_201807_overdue_receivable_translated.csv
 9431  python translate.py check_cardinality -p ../data/20180831_NIDEC_processed_data/01_col_translated/07_01_201807_overdue_receivable_translated.csv
 9432  python translate.py check_cardinality -p ../data/20180831_NIDEC_processed_data/01_col_translated/07_04_201807_accounts_receivable_abroad_translated.csv
 9433  python translate.py csv_contents -p "../data/20180831_NIDEC_processed_data/01_col_translated/07_04_201807_accounts_receivable_abroad_translated.csv" -o ../data/20180831_NIDEC_processed_data/02_row_translated -l ../data/20180831_NIDEC_processed_data/lookup.csv 
 9434  python translate.py check_cardinality -p ../data/20180831_NIDEC_processed_data/01_col_translated/10_01_2017_maintenance_f_cost_threshold6_translated.csv
 9435  python translate.py csv_contents -p "../data/20180831_NIDEC_processed_data/01_col_translated/10_01_2017_maintenance_f_cost_threshold6_translated.csv" -o ../data/20180831_NIDEC_processed_data/02_row_translated -l ../data/20180831_NIDEC_processed_data/lookup.csv -c "Occurrence month,Distribution department"
 9436  python translate.py --help
 9437  python translate.py csv_columns -p ../data/20180831_NIDEC_processed_data/00_raw_to_unprotected_seperate/02_01_20180831_inventory.csv
 9438  python translate.py csv_columns -p ../data/20180831_NIDEC_processed_data/00_raw_to_unprotected_seperate/02_01_20180831_inventory.csv -o ../data/20180831_NIDEC_processed_data/01_col_translated -p ../data/20180831_NIDEC_processed_data/lookup.csv
 9439  python translate.py csv_columns -p ../data/20180831_NIDEC_processed_data/00_raw_to_unprotected_seperate/02_01_20180831_inventory.csv -o ../data/20180831_NIDEC_processed_data/01_col_translated -l ../data/20180831_NIDEC_processed_data/lookup.csv
 9440  python translate.py csv_columns \\n-p ../data/20180831_NIDEC_processed_data/00_raw_to_unprotected_seperate/02_01_20180831_inventory.csv  \\n-o ../data/20180831_NIDEC_processed_data/01_col_translated  \\n-l ../data/20180831_NIDEC_processed_data/lookup.csv
 9441  python translate.py check_cardinality -p ../data/20180831_NIDEC_processed_data/01_col_translated/02_01_20180831_inventory_translated.csv
 9442  python translate.py csv_contents \\n\t-p "../data/20180831_NIDEC_processed_data/01_col_translated/02_01_20180831_inventory_translated.csv" \\n\t-o ../data/20180831_NIDEC_processed_data/02_row_translated \\n\t-l ../data/20180831_NIDEC_processed_data/lookup.csv \\n\t-c "Major classification name"
 9443  python translate.py csv_columns \\n-p ../data/20180831_NIDEC_processed_data/00_raw_to_unprotected_seperate/07_03_201807_deposit.csv  \\n-o ../data/20180831_NIDEC_processed_data/01_col_translated  \\n-l ../data/20180831_NIDEC_processed_data/lookup.csv
 9444  python translate.py check_cardinality \\n\t-p ../data/20180831_NIDEC_processed_data/01_col_translated/07_03_201807_deposit_translated.csv
 9445  python translate.py csv_columns \\n-p ../data/20180831_NIDEC_processed_data/00_raw_to_unprotected_seperate/07_05_201807_reievevable_summary_by_account.csv  \\n-o ../data/20180831_NIDEC_processed_data/01_col_translated  \\n-l ../data/20180831_NIDEC_processed_data/lookup.csv
 9446  conda 
 9447  conda info --envs
 9448  conda create -n line python3
 9449  conda create -n line python==3
 9450  conda create -n line 
 9451  source activate line
 9452  pip install --upgrade pipenv
 9453  pip install --upgrade pipenv --user
 9454  cd Archive
 9455  mkdir line-bot
 9456  pipenv shell 
 9457  pipenv -h
 9458  pipenv install flask line-bot-sdk ngrok gunicorn
 9459  pipenv
 9460  pipenv graph
 9461  subl ./
 9462  export FLASK_APP=app.py
 9463  export FLASK_DEBUG=1
 9464  brew install heroku/brew/heroku\n
 9465  cd Python/Archive
 9466  cd line-bot
 9467  mkdir .env
 9468  rm -r .env
 9469  vim .env
 9470  vim Procfile
 9471  vim requirement.txt
 9472  vim runtime.txt
 9473  heroku login
 9474  pipenv lock --requirements
 9475  pipenv lock --requirements > requirement.txt
 9476  grok http 5000
 9477  ngrok
 9478  brew install ngrok
 9479  brew cask install ngrok
 9480  ngrok http 5000
 9481  pipenv shell
 9482  flask run --host=0.0.0.0
 9483  git commit -m 'First commit' 
 9484  heroku create vision-app-v1
 9486  git commit -m 'Update to use environment variable' 
 9487  git push heroku master
 9488  heroku ps:scale web=1
 9489  heroku config:set LINE_CHANNEL_SECRET=8b6f14da720a75453657591b950a626f
 9490  heroku config:set LINE_CHANNEL_ACCESS_TOKEN=EeW40mNNp8XQ52CkXKQ2OUT/kcMooJtpOjiuqlvqPqzUnlB3b+EbC+4TGfNgZanGgqTsdudFOQPG1vMLArA7ZA6Jhj7NR7XlB1ZPKv/J5SetS29dK7gaJ8Vj8cnVIRuGzr4SNIM55GQA7c+lyJtWSQdB04t89/1O/w1cDnyilFU=
 9491  heroku open
 9492  heroku logs
 9493  heroku run ls
 9494  ngrok -h
 9495  heroku run python
 9496  cd Projects/SMFG/data/20180
 9497  cd Projects/SMFG/data
 9498  cd 20180831_NIDEC_processed_data/02_row_translated
 9499  vim 07_)07_05_201807_reievevable_summary_by_account_translated.csv
 9500  vim 07_05_201807_reievevable_summary_by_account_translated.csv
 9501  brew install unar
 9502  cd Downloads
 9503  unar 
 9504  unar 0921_ãã¹ã­ã³ã°ä½æ¥­.zip
 9505  source activate py35
 9506  cd ~/Projects/SMFG/tools
 9507  mkdir modules
 9508  cd modules
 9509  cd tools
 9510  python translate.py filenames \\n-i ../data/20180927_NIDEC_raw_data/20180920_ä¾é ¼ERPè³æ  \\n-o ../data/20180927_NIDEC_processed_data/00_20180920_ERP_translated
 9511  ln -s /Users/212339410/Box/Development/My\ Tableau\ Repository /Users/212339410/Documents/My\ Tableau\ Repository
 9512  pip install pyexcel pyexcel-xls pyexcel-xlsx
 9513  pip install pyexcel-cli
 9514  pyexcel transcode "/Users/212339410/Box/SMFG_DTB/Phase 3/DataFromNRCJ/20180927_processed/01_ERP_filename_translated/entity_definition_sales_t.xls" "/Users/212339410/Box/SMFG_DTB/Phase 3/DataFromNRCJ/20180927_processed/01_ERP_filename_translated/entity_definition_sales_t.xlsx"
 9515  pip install xlutils
 9516  cd ~/Downloads/Download_SMFG
 9517  unar 20180928130509.zip
 9518  jupyter labextension install @jpmorganchase/perspective-jupyterlab --debug
 9519  mv .anaconda  .anaconda_bak201809
 9520  conda
 9521  bash Anaconda3-5.2.0-MacOSX-x86_64.sh
 9522  conda update anaconda-navigator
 9523  conda config --set ssl_verify false 
 9524  conda update --all
 9525  conda update jupyter
 9526  source activate 
 9527  python versions.py
 9528  conda create --name py36  python=3.6
 9529  conda activate py36
 9530  conda env ls
 9531  conda info --env
 9532  pip 
 9533  pip list
 9534  conda install list
 9535  conda -h
 9536  conda list 
 9537  conda install pandas 
 9538  conda install matplotlib -c source-forge
 9539  conda config --add channels conda-forge
 9540  conda install matplotlib
 9541  l ~/Library/Jupyter/kernels
 9542  l ~/Library/Jupyter/
 9543  mv  ~/Library/Jupyter/ ~/Library/Jupyter_bak201809
 9544  l  ~/.ipython/extensions
 9545  l  ~/.ipython/nbextensions
 9546  l  ~/.ipython/profile_default
 9547  jupyter --paths
 9548  l /usr/local/Cellar/python@2/2.7.15_1/Frameworks/Python.framework/Versions/2.7/share/jupyter
 9549  l "/usr/local/Cellar/python@2/2.7.15_1/Frameworks/Python.framework/Versions/2.7/share/jupyter"
 9550  l "/usr/local/Cellar/python@2/2.7.15_1/Frameworks/Python.framework/Versions/2.7/share/"
 9551  mv /usr/local/share/jupyter/kernels /usr/local/share/jupyter/kernels_bak201809
 9552  l ~/.local/share/jupyter/kernels/
 9553  l ~/.local/share/jupyter
 9554  l /usr/share/jupyter
 9555  l /usr/local/share/jupyter
 9556  source activate base
 9557  python -m ipykernel install --user --display-name py36
 9558  python -m ipykernel install --user --display-name Python 3 (py36)
 9559  python -m ipykernel install --user --display-name "Python 3 (py36)"
 9560  source deactivate 
 9561  python -m ipykernel install --user --display-name "Python 3 (base)"
 9562  pip uninstall jupyter_contrib_nbextensions
 9564  source py36
 9565  l ~/.zshrc
 9566  vim ~/.zshrc
 9567  rm Untitled.ipynb
 9568  nb_conda_kernels
 9569  SSL_NO_VERIFY=1 conda skeleton pypi a_package
 9570  conda install -c anaconda nb_conda_kernels
 9571  conda install -n notebook_env nb_conda_kernels
 9572  source activate root
 9573  which anaconda
 9574  which conda
 9575  percol
 9576  which pip 
 9577  pip freeze
 9578  which pip  | grep auto
 9579  pip install autopep8
 9580  pip install autopep -h
 9581  pip install autopep8 pip install --upgrade pip
 9582  pip install --upgrade pip
 9583  pip install autopep8 
 9584  which jupyter 
 9585  conda install pip
 9586  conda install ipykernel
 9587  python -m ipykernel install --user --name py36 --display-name "Python (py36)"
 9588  ln -s "/Users/212339410/Box/DSA Internal-SMFG/10_data" data
 9589  conda install pandas-profiling
 9590  subl /Users/212339410/Box/DSA Internal-SMFG/10_data/20180927_NIDEC_processed_data/01_ERP_filename_translated/STSEZO.csv
 9591  subl "/Users/212339410/Box/DSA Internal-SMFG/10_data/20180927_NIDEC_processed_data/01_ERP_filename_translated/STSEZO.csv"
 9592  subl "/Users/212339410/Box/DSA Internal-SMFG/10_data/20180927_NIDEC_processed_data/01_ERP_filename_translated/entity_definition_billing_statement_t.xls"
 9593  docker
 9594  docker start 
 9595  docker start  -h
 9596  docker versionÂ¥
 9597  docker version
 9598  docker run hello-world
 9599  docker pull ufoym/deepo
 9600  nvidia-docker run -it ufoym/deepo bash
 9601  docker run -h
 9602  docker run --help
 9603  docker run ufoym/deepo bash
 9604  docker run -it ufoym/deepo bash
 9605  docker list
 9606  docker -H
 9607  l ~/anaconda3/envs/py35/bin
 9608  open ~/anaconda3/envs/
 9609  echo $http_proxy
 9610  cd Py
 9612  git clone https://github.com/deadbok/py-puml-tools.git
 9613  cd py-puml-tools
 9614  python dbsql2puml/dbsql2puml.py -h
 9615  python dbsql2puml/dbsql2puml.py ../py-puml-tools/dbsql2puml/example/customer.sql
 9616  pip3 install plantweb\n
 9617  plantweb -h 
 9618  plantweb dbsql2puml/example/customer.puml
 9619  open customer.svg
 9620  subl nidec_erp_v1.sql
 9621  subl ../py-puml-tools/dbsql2puml/example/customer.sql
 9622  python dbsql2puml/dbsql2puml.py nidec_erp_v1.sql
 9623  python dbsql2puml/dbsql2puml.py nidec_erp_v1.sql > nidec_erp_v1.uml
 9624  plantweb nidec_erp_v1.uml -h
 9625  plantweb nidec_erp_v1.uml --format png 
 9626  plantweb nidec_erp_v1.uml 
 9627  open . 
 9628  plantweb nidec_erp_v2.uml
 9629  plantweb nidec_erp_foreign_v2.svg
 9630  python dbsql2puml/dbsql2puml.py nidec_erp_v1.sql > nidec_erp_foreign_v2.uml
 9631  plantweb nidec_erp_foreign_v2.uml
 9632  brew install caskformula/caskformula/inkscape
 9633  pip install gTTS --upgrade
 9634  pip uninstall googletrans
 9635  git clone https://github.com/BoseCorp/py-googletrans.git
 9636  cd ./py-googletrans
 9638  git ls
 9639  git diff
 9640  rm -r googletrans* 
 9641  rm -r py-googletrans
 9642  cd ~/Projects/SMFG/data/20181007_NIDEC_raw_data
 9643  unar STURIAGEYOTEI.zip
 9644  git clone 
 9645  git clone https://github.build.ge.com/IndustrialDataScience/MS-HaulTrucksÂ 
 9646  git clone git@github.build.ge.com:IndustrialDataScience/MS-HaulTrucks.git
 9647  git clone git@github.build.ge.com:IndustrialDataScience/MS-CycleWatch.git
 9648  cd MS-CycleWatch
 9649  cd MS-HaulTrucks
 9650  make --help
 9651  less Makefile
 9652  vim Makefile
 9653  subl Makefile
 9654  make data
 9655  unlnink data
 9656  unlink data
 9657  make data 
 9658  brew cask install projectlibre
 9659  open ~/Box/GE\ Digital\ Data\ Science\ 2017/10\ Schindler\ Personal
 9660  cd "/Users/212339410/Box/Analytics/Projects/SMFG/smanalytics"
 9661  open ./Makefile
 9662  open ./README.md
 9663  pwd -P
 9664  ping google.com
 9665  cd h1ros.github.io/
 9666  cd content/notebooks
 9667  conda env list
 9668  pip install pymc3
 9669  pip install seaborn
 9670  fg
 9671  sudo -s
 9672  spctl --master-disable
 9673  cd /Users/212339410/Box/DSA Internal-SMFG/10_data/20181007_NIDEC_raw_data
 9674  cd "/Users/212339410/Box/DSA Internal-SMFG/10_data/20181007_NIDEC_raw_data"
 9675  unar 181016è£½è²©ä¼è­°å¨è³æ_removed.zip
 9676  cd SMFG_DTB
 9677  cd /Users/212339410/Box/DSA Internal-SMFG/01_project_documents/20181024_doc_SMFG
 9678  cd "/Users/212339410/Box/DSA Internal-SMFG/01_project_documents/20181024_doc_SMFG"
 9679  unar 20181025110809.zip
 9680  unar 20181025180627.zip
 9681  pip install lime
 9682  cd 20181031_update_to_NIDEC
 9683  unar 20181101110551.zip
 9684  open ./
 9685  ping 3.39.83.92
 9686  ssh 3.39.83.92
 9687  ssh 3.39.83.93
 9688  ssh 3.39.83.93 -v
 9689  ssh hiros@3.39.83.92
 9690  ssh  3.39.83.92 -l hiros
 9691  ssh  3.39.83.92 -l hiros -v
 9692  jupyter notebooks
 9693  unar 20181106085926.zip
 9694  unar 20181106103524.zip
 9696  cd Dss-
 9697  cd DSS-Workshops
 9698  cd Python/DSS-Workshops
 9699  cd Notebooks
 9700  pip install jupyter_contrib_nbextensions
 9701  jupyter contrib nbextension install --user
 9703  jupyter nbconvert *.ipynb --to slides --post serve
 9704  vim /etc/hosts
 9705  ping localhost
 9706  jupyter nbconvert *.ipynb --to slides --post serve --port 9999
 9707  conda info nbconvert
 9708  \n  
 9709  ps -fA | grep python
 9710  jupyter nbconvert *.ipynb --to slides --post serve 
 9711  jupyter nbconvert dino_viz.ipynb --to slides --post serve \n--SlidesExporter.reveal_theme=serif \n--SlidesExporter.reveal_scroll=True \n--SlidesExporter.reveal_transition=none
 9712  jupyter nbconvert dino_viz.ipynb --to slides --post serve \\n--SlidesExporter.reveal_theme=serif  \\n--SlidesExporter.reveal_scroll=True  \\n--SlidesExporter.reveal_transition=none \\n
 9713  jupyter nbconvert dino_viz.ipynb --to slides --post serve \n--SlidesExporter.reveal_theme=serif 
 9714  jupyter nbconvert dino_viz.ipynb --to slides --post serve --SlidesExporter.reveal_scroll=True
 9715  jupyter nbconvert dino_viz.ipynb --to slides --post serve --SlidesExporter.reveal_theme=black
 9716  jupyter nbconvert dino_viz.ipynb --to slides --help-all
 9717  jupyter nbconvert --help-all
 9718  jupyter nbconvert *.ipynb --to slides --post serve --SlidesExporter.reveal_theme=black
 9719  jupyter nbconvert *.ipynb --to slides --post serve --SlidesExporter.reveal_theme=serif \n--SlidesExporter.reveal_scroll=True \n--SlidesExporter.reveal_transition=none
 9722  cd "/Users/212339410/Box/Analytics/Projects/Tellus/data"
 9723  unar train.zip
 9724  cd ~
 9725  subl ~/.alias
 9726  rm intermediate_result/meta_data_filepath.csv
 9727  brew install cmake pkg-config wget
 9728  mkdir ~/tmp
 9729  wget -O opencv.zip https://github.com/opencv/opencv/archive/4.0.0-beta.zip
 9730  wget -O opencv_contrib.zip https://github.com/opencv/opencv_contrib/archive/4.0.0-beta.zip
 9731  unzip opencv.zip
 9732  brew upgrade cmak
 9733  brew upgrade cmake
 9734  unzip opencv_contrib.zip
 9735  cd ~/tmp
 9736  cd opencv-4.0.0-beta
 9737  mkdir build
 9738  cd build
 9739  echo $VIRTUAL_ENV
 9740  brew install opencv
 9741  brew install jpeg libpng libtiff openexr
 9742  brew install eigen tbb
 9743  ^[[200~conda install -c menpo opencv3 or pip3 install opencv-python~
 9744  conda install -c menpo opencv3 or pip3 install opencv-python
 9745  conda install -c menpo opencv3
 9746  pip install pillow
 9747  pip install imutils h5py requests progressbar2
 9748  pip install  scikit-image
 9749  l ~/.matplotlib
 9750  python --version
 9751  ipythono
 9752  which pip
 9753  which ipython
 9754  subl ~/.keras/keras.json
 9755  subl ~/.zshrc
 9756  source activate py36
 9757  echo $PATH
 9759  open /Users/212339410/anaconda3/envs/py36/
 9760  conda uninstall subprocess32
 9761  python 
 9762  ipython
 9763  conda remove enum34
 9764  rm -rf tmp
 9765  conda remove functools32
 9766  conda remove futures conda-manager
 9767  conda info ipaddress
 9768  conda conda list
 9769  conda list
 9770  conda remove conda list
 9771  conda remove ipaddress
 9772  conda install python=3.6
 9773  conda update conda
 9774  conda update anaconda
 9775  conda create -n py367 python=3.6.7  anaconda
 9776  python -m ipykernel install --user
 9777  pip install ipykernel
 9778  python -m ipykernel install --user --name py367 --display-name "Python (py367)"
 9779  source  deactivate
 9780  jupyter notebook --generate-config
 9781  conda install jupyter
 9782  conda install nb_conda_kernels
 9783  pip install tensorflow
 9784  pip install keras
 9785  python
 9786  \njupyter kernelspec remove py36
 9787  ..
 9788  cd "/Users/212339410/Box/DSA Internal-SMFG/01_project_documents/20181112 Sameer session"
 9789  unar 20181112131238.zip
 9790  brew install opencv3  --with-contrib --with-python3 --HEAD
 9791  brew edit opencv3
 9792  cd "/Users/212339410/Box/Analytics/Projects/Tellus/data/train/PALSAR/diff/negative"
 9793  cd positive
 9794  rm *.tif
 9795  ln -s positive ../../keras-data/train/positive
 9796  cd ../../keras-data
 9797  unlink positive
 9798  cd train
 9799  ln -s ../PALSAR/diff/positive positive
 9800  ../
 9801  ln -s PALSAR/diff/positive keras-data/train/positive
 9802  ln -s PALSAR/diff/positive/ keras-data/train/positive
 9803  ln -s PALSAR/diff/positive/ keras-data/train/
 9804  ln -s PALSAR/diff/positive/ keras-data/train
 9805  cd keras-data/train
 9806  ln -s ../../PALSAR/diff/positive positive
 9807  ln -s ../../PALSAR/diff/negative negative
 9808  cd validation
 9809  cd ~/Projects/
 9810  conda install -c conda-forge jupyter_nbextensions_configurator
 9811  cd Projects/Tellus
 9812  pip install tabpy-server
 9813  pip install tabpy-client
 9814  python tabpy.py
 9815  which tabpy_server
 9816  which python
 9817  /Users/212339410/anaconda3/envs/py367/lib/python3.6/site-packages/tabpy_server/startup.sh
 9818  git commit -m 'Init commit'
 9819  fuser 6311/tcp
 9820  fuser 
 9821  lsof -i 6311
 9822  lsof -i:6311
 9823  kill 84848
 9824  cd /Users/212339410/Downloads/?\n/Users/212339410/Downloads/?
 9825  cd ~/Downloads
 9826  cd "/Users/212339410/Box/DSA Internal-SMFG/10_data/20181012_NIDEC_working_directory/intermediate_result/R_h2o_intermediate_data"
 9827  rm  explain_rf.csv
 9828  rm rf.rds
 9829  rm varimp_rf.csv
 9830  rm dataTest.rds
 9831  cd R_h2o_intermediate_data
 9832  cd "/Users/212339410/Box/DSA Internal-SMFG/10_data/20181007_NIDEC_raw_data/20181116"
 9833  unar 20181116.zip
 9834  cd ~/
 9835  git reset
 9836  git stash
 9837  git remote -a
 9838  git remote set-url origin git@github.build.ge.com:IndustrialDataScience/SMFG.git
 9839  subl ~/.gitconfig
 9840  git remote set-url origin https://github.build.ge.com/IndustrialDataScience/CookieChomper.git
 9841  git remote
 9842  git push --force
 9843  git revert 3464f95f1dcafefdf68935f409174ca749d164f0
 9844  git push origin 3464f95f1dcafefdf68935f409174ca749d164f0:master
 9845  git push origin +3464f95f1dcafefdf68935f409174ca749d164f0^:master
 9846  git push origin +3464f95f1dcafefdf68935f409174ca749d164f0^
 9847  git push origin +3464f95^:master
 9848  git reset HEAD^ --hard
 9849  git push origin +3464f95f1dcafefdf68935f409174ca749d164f0\^:master
 9850  git rebase -i dd61ab23^
 9851  git rebase -i 3464f95f1dcafefdf68935f409174ca749d164f0\^
 9852  git rebase --continue
 9853  git remote set-url 
 9854  git set-url origin https://github.build.ge.com/IndustrialDataScience/SMFG.git
 9855  git add origin https://github.build.ge.com/IndustrialDataScience/SMFG.git
 9856  git remote add origin git@github.build.ge.com:IndustrialDataScience/SMFG.git
 9857  git remote set-url origin https://github.build.ge.com/IndustrialDataScience/SMFG.git
 9858  git log
 9859  rm -rf .git
 9860  git init
 9861  git remote add origin https://github.build.ge.com/IndustrialDataScience/SMFG.git
 9862  git pull 
 9863  git add -A
 9864  git commit -m 'Initial commit'
 9865  git remote -v
 9869  git push -f origin master
 9870  cd ../Tellus
 9871  ssh dss_server 
 9872  cat ~/.ssh/id_rsa.pub
 9873  ssh hiros@138.91.127.19
 9874  cd ~/Projects/Tellus
 9875  rsync -a ./ dss_server:/home/hiros/projects/Tellus
 9876  rsync -avn ./ dss_server:/home/hiros/projects/Tellus
 9877  rsync -avn ./ dss_server:/home/hiros/projects/Tellus --progress
 9878  rsync -av ./ dss_server:/home/hiros/projects/Tellus --progress
 9879  cd Projects/Tellus/data
 9880  rsync -av ./ dss_server:/home/hiros/projects/Tellus/data --progress --include="*.zip"
 9881  cd "/Users/212339410/Box/GE Digital Data Science 2017/10 Schindler Personal/Backup/hiro/notebook"
 9882  l ~/Box
 9883  cd ~/Box/Development
 9884  ln -s ./My\ Tableau\ Prep\ Repository /Users/212339410/Documents/My\ Tableau\ Repository
 9885  ln -h
 9886  ln -s /Users/212339410/Box/Development/My Tableau Repository /Users/212339410/Documents
 9887  ln -s "/Users/212339410/Box/Development/My Tableau Repository" /Users/212339410/Documents
 9888  ln -s /Users/212339410/Box/Development/My\ Tableau\ Repository /Users/212339410/Documents
 9889  ln -s "/Users/212339410/Box/Development/My Tableau Repository" "/Users/212339410/Documents/My Tableau Repository"
 9890  ln -s  "/Users/212339410/Documents/My Tableau Repository"ln -ds ~/Box/Development/My\ Tableau\ Prep\ Repository ~/Documents/My\ Tableau\ Prep\ Repository
 9891  ln -s '/Users/212339410/Box/Development/My Tableau Repository' '/Users/212339410/Documents/'
 9892  ln -s 'My Tableau Repository' '/Users/212339410/Documents/'
 9893  ln -sh
 9894  man ln
 9895  ln -s 'My Tableau Repositorys' /Users/212339410/Documents/My\ Tableau\ Prep\ Repository
 9896  cd ~/Documents
 9897  unlink My\ Tableau\ Prep\ Repository
 9898  ln -s ~/Box/Development/My\ Tableau\ Repository ./My\ Tableau\ Prep\ Repository
 9899  cd Documents
 9900  ln -s ~/Box/Development/My\ Tableau\ Repository My\ Tableau\ Repository
 9901  cd ~/Projects/SMFG
 9902  cd "/Users/212339410/Box/DSA Internal-SMFG/01_project_documents/20181105_NIDEC_update"
 9903  zip -h
 9904  zip 20181120_NIDEC_update_v2 -e
 9905  openssl rand -hex 4
 9906  zip 20181120_NIDEC_update_v2 -er
 9907  zip 20181120_NIDEC_update_v2 -erv
 9908  zip -e 20181120_update_v2.zip 20181120_NIDEC_update_v2
 9909  zip -re 20181120_update_v2.zip 20181120_NIDEC_update_v2
 9910  pip install pyod\npip install --upgrade py
 9911  cd ../../Python
 9912  git clone https://github.com/yzhao062/pyod
 9913  cd pyod
 9914  pip install coupled_biased_random_walks
 9916  git clone https://github.com/dkaslovsky/Coupled-Biased-Random-Walks.git
 9918  cd SMFG/
 9919  jupyter kernelspec list
 9920  jupyter notebook  jupyter notebook --generate-config
 9921  subl /Users/212339410/.jupyter/jupyter_notebook_config.py
 9922  cd "/Users/212339410/Box/SMFG_DTB/NEC Shared/Phase 3/DataFromNRCJ/20181119_NIDEC_dashboard_working_directory"
 9923  subl convert_xlsx_to_csv.py
 9924  ifconfig -a
 9925  ping Yukiko-san\n\n\n10.184.133.235
 9926  ping 10.184.133.235
 9927  cd ~/Projects/SMFG/notebooks
 9928  subl ~/.ssh/known_hosts
 9929  ssh-copy-id -i ~/.ssh/id_rsa.pub hiros@dsspgu
 9930  ssh-copy-id -i ~/.ssh/id_rsa.pub hiros@dssgpu
 9931  ssh hiros@dssgpu
 9932  aws ec2 describe-instances 
 9933  aws -h
 9934  aws ec2 -h
 9935  aws ec2 describe-instances --query 'Reservations[*].Instances[*].InstanceId'
 9936  aws ec2 describe-instances | grep InstanceId 
 9937  aws ec2 list-instances
 9938  aws ec2 list-instances | list
 9939  aws ec2 list-instances | grep list
 9940  curl http://10.43.51.90/latest/meta-data/instance-id
 9941  aws ec2 describe-instances --filters 'Name=tag:hiro,Values=hiro*'
 9942  ssh sch_centos_hiro
 9943  aws ec2 describe-instances --filters 'Name=tag:Name,Values=dev-server-*'
 9944  aws ec2 describe-instances
 9945  ssh sch_cpu
 9946  noproxy
 9947  aws ec2 describe-instances --query 'Reservations[*].Instances[*].[InstanceId,Tags[?Key==`Name`].Value|[0],State.Name,PrivateIpAddress,PublicIpAddress]' --output text | column -t
 9948  ssh sch_bastion
 9949  ssh 10.42.22.159
 9950  ssh ubuntu@10.42.22.159
 9951  cd data
 9952  open .
 9953  mkdir 20181221_JAE_raw_data
 9954  ln -s "/Users/212339410/Box/SMFG_DTB/NEC Shared/Phase 3/DataFromJAE/20181220_raw" 20181221_JAE_raw_data
 9956  cd /Users/212339410/Box/SMFG_DTB/NEC\ Shared/Phase\ 3/DataFromJAE/20181220_raw/
 9957  file -I MAKT_20181220.txt
 9958  file -I ZCMM0016.txt
 9959  chardet
 9960  pip install chardet
 9961  chardet ZCMM0016.txt
 9962  chardetect ZCMM0016.txt
 9963  ssh dss_server
 9964  ssl
 9965  rsync -anv ~/Downloads/ntt_corevo_speaker/ hiros@dssgpu:/home/hiros/project/ntt_corevo_speaker
 9966  rsync -anv ~/Downloads/ntt_corevo_speaker dssgpu:~/project/ntt_corevo_speaker
 9967  rsync -av ~/Downloads/ntt_corevo_speaker dssgpu:~/project/ntt_corevo_speaker
 9968  ssh dssgpu 
 9970  ping 10.7.213.52
 9971  nslookup 10.7.213.52
 9972  ipconfig
 9973  ifconfg 
 9974  ifconfig
 9976  ls "/Users/212339410/Box/DSA Internal-SMFG/10_data/20181221_JAE_raw_data/20190109_raw"
 9977  ls "/Users/212339410/Box/DSA Internal-SMFG/10_data/20181221_JAE_raw_data/20190109_raw/"
 9978  cd '/Users/212339410/Box/DSA Internal-SMFG/10_data/20181221_JAE_raw_data/20190109_raw'
 9979  cd '/Users/212339410/Box/DSA Internal-SMFG/10_data/20181221_JAE_raw_data'
 9980  ln -s '/Users/212339410/Box/SMFG_DTB/NEC Shared/Phase 3/DataFromJAE/20190109_raw' 20190109_raw
 9982  sudo mdutil -E /
 9983  ls
 9984  cd MS-
 9985  subl ~/.ssh
 9987  cd /Users/212339410/Box/SMFG_DTB/Meetings/
 9988  vim JAE_DataQA_20190117.txt
 9989  file -I JAE_DataQA_20190117.txt
 9990  jupyter notebook
 9991  ssh -N -n -L 127.0.0.1:9001:127.0.0.1:9001 dssgpu -Y 
 9996  geproxy
10000  /Volumes/ClickShare/README; exit
10009  cd ../Python
10011  git clone https://github.com/TimeSynth/TimeSynth.git
10012  cd TimeSynth
10014  python setup.py install
10023  git clone https://github.com/udacity/deep-learning-v2-pytorch.git
10024  cd deep-learning-v2-pytorch
10027  cd intro-
10028  cd intro-to-pytorch
10030  conda install pytorch torchvision -c pytorch
10036  mkdir notebook
10037  mv deep-learning-v2-pytorch/intro-to-pytorch/20190202\ Leet\ Tree\ .ipynb notebook
10038  mv SMFG/notebooks/20190201\ Scrape\ Words.ipynb notebook
10039  cd notebook
10043  pip install "Nikola[extras]"
10044  cd ~/Python
10049  git pull origin dev
10050  nikola init .
10053  nikola new_page --title='About' --format=markdown
10054  nikola new_post --title='test' --format=ipynb
10057  nikola new_post --title='test' --format=ipynb --import ./content/notebooks/US\ Census\ in\ 2014.ipynb
10058  nikola new_post --title='US Census in 2014' --format=ipynb --import ./content/notebooks/US\ Census\ in\ 2014.ipynb
10059  rm posts/test.ipynb
10061  conda install -c conda-forge jupyter_contrib_nbextensions
10062  nikola new_post --title="test" --format=ipynb\n
10066  nikola theme -i yesplease
10067  subl conf.py
10069  nikola serve
10070  nikola theme -i hyde\n
10077  cd Python/h1ros.github.io
10082  nikora build
10083  nokila build
10084  nokola build
10088  nikola check -f --clean-files
10093  cd ou
10094  cd ../output
10098  cd listings
10102  cd pages
10105  cd posts
10107  cd 998
10108  subl 998\ Smallest\ String\ Starting\ From\ Leaf.ipynb
10113  nikola build
10114  nikola serve --browser
10115  y
10116  nikola github_deploy
10118  ssh -i -N -n -L 127.0.0.1:9001:127.0.0.1:9001 dssgpu -Y
10120  vim ~/.ssh/config
10121  subl ~/.ssh/config
10124  ssh -i -N -n -L 127.0.0.1:9001:127.0.0.1:9001 dssgpu -Y -o ForwardX11=no
10125  ssh -N -n -L 127.0.0.1:9001:127.0.0.1:9001 dssgpu -Y -o ForwardX11=no
10127  ssh -L 127.0.0.1:9001:127.0.0.1:9001 dssgpu -Y 
10130  cd ~/Projects
10132  cd SMFG
10136  cd Projects/SMFG
10138  cd notebooks
10140  git add 20190207\ NIDEC\ Confidence\ Building.ipynb
10142  git commit -m 'Add notebook for NIDEC confidence building'
10144  vim ~/.gitconfig
10146  git push origin master
10151  cd Projects
10153  cd ..
10157  cd Generic
10159  mkdir ntt_corevo_201902
10160  scp dssgpu:/home/hiros/project/ntt_corevo_speaker/result/ ./
10162  scp dssgpu:/home/hiros/project/ntt_corevo_speaker/results/ubmission_initial_dl_190208.tsv ./
10163  scp dssgpu:/home/hiros/project/ntt_corevo_speaker/result/submission_initial_dl_190208.tsv ./
10164  sudo port install bash-completion
10165  brew install git bash-completion
10168* mv submission_initial_dl_20190208.tsv ntt_corevo_201902
10169* scp hiros@dssgpu:/home/hiros/project/ntt_corevo_speaker/result/submission_initial_dl_20190208.tsv ./
10171* source activate py367
10173* scp hiros@dssgpu:/home/hiros/project/ntt_corevo_speaker/result/submission_initial_dl_20190208.tsv ./ntt_corevo_201902
10176* scp hiros@dssgpu:/home/hiros/project/ntt_corevo_speaker/result/submission_initial_dl_epoch15_20190208_1326.tsv ./ntt_corevo_201902
10177* scp hiros@dssgpu:/home/hiros/project/ntt_corevo_speaker/notebook/loss_trend20190208_1353.jpg
10178* scp hiros@dssgpu:/home/hiros/project/ntt_corevo_speaker/notebook/loss_trend20190208_1353.jpg ./ntt_corevo_201902
10179* jupyter notebook 
10181* ssh -N -n -L 127.0.0.1:9001:127.0.0.1:9001 dssgpu -Y
10182* ssh dssgpu
10183* scp hiros@dssgpu:/home/hiros/project/ntt_corevo_speaker/result/submission_initial_dl_epoch49_20190209_2122.tsv  ./ntt_corevo_201902
10184* scp hiros@dssgpu:/home/hiros/project/ntt_corevo_speaker/result/loss*   ./ntt_corevo_201902
10185* scp hiros@dssgpu:/home/hiros/project/ntt_corevo_speaker/result/loss_trend20190209_2118.jpg   ./ntt_corevo_201902
10186  cd Python
10188  cd h1ros.github.io
10191  git add .
10192  git commit -m 'Update post 998'
10193  git push origin src
10194  git checkout master
10195  git pull origin master
10196  git status
10197  l
10198  history
10199  history -> history20190210.txt 
10200  open history20190210.txt
